National Institute of Technology Karnataka (NITK) , Surathkal: To the graduating batch: What lessons would you want to give to your juniors before you leave?	      In recent years, there has been increasing interest in adapting this technique to prenatal diagnostic imaging.  Previous studies revealed that assessing the placenta perfusion from IVIM imaging may provide valuable diagnostic information about fetal growth~\cite{alison2013use}.  In~\cite{jakab2017intra,jakab2018microvascular} IVIM imaging of other developing organs such as fetal liver, lungs, kidneys and brain has been investigated.   These studies indicate a potential link between IVIM parameters and gestational changes in microstructural development.   Successful clinical translation of prenatal IVIM imaging is severely limited by poor parameter estimation accuracy due to low SNR of clinical fetal MR images and unpredictable fetal movements in utero. Estimating IVIM parameters from observed signal is a challenging \emph{inverse mapping} problem.	      In recent years, there has been increasing interest in adapting this technique to prenatal diagnostic imaging.  Previous studies revealed that assessing the placenta perfusion from IVIM imaging may provide valuable diagnostic information about fetal growth~\cite{alison2013use}.  In~\cite{jakab2017intra,jakab2018microvascular} IVIM imaging of other developing organs such as fetal liver, lungs, kidneys and brain has been investigated.   These studies indicate a potential link between IVIM parameters and gestational changes in microstructural development.   Successful clinical translation of prenatal IVIM imaging is severely limited by poor parameter estimation accuracy due to low SNR of clinical fetal MR images and unpredictable fetal movements in utero. Estimating IVIM parameters from observed signal is a challenging \emph{inverse mapping} problem.	score:416
National Institute of Technology Karnataka (NITK) , Surathkal: To the graduating batch: What lessons would you want to give to your juniors before you leave?	 This scheme is implemented in state-of-the-art distributed deep learning frameworks \citep{abadi2016tensorflow,paszke2017automatic,seide2016cntk}. Recent work in~\citep{You2017:scaling,Goyal2017:large} explores various limitations of this approach, as in general it is reported that performance degrades for too large mini-batch sizes~\citep{keskar2016large,Ma2018interpolation,Yin18a:diversity}.   In this work we follow an orthogonal approach, still with the goal to increase the compute to communication ratio: Instead of increasing the mini-batch size, we reduce the communication frequency. Rather than keeping the sequences on different machines in sync, we allow them to evolve \emph{locally} on each machine, independent from each other, and only average the sequences once in a while (\emph{local SGD}).	 This scheme is implemented in state-of-the-art distributed deep learning frameworks \citep{abadi2016tensorflow,paszke2017automatic,seide2016cntk}. Recent work in~\citep{You2017:scaling,Goyal2017:large} explores various limitations of this approach, as in general it is reported that performance degrades for too large mini-batch sizes~\citep{keskar2016large,Ma2018interpolation,Yin18a:diversity}.   In this work we follow an orthogonal approach, still with the goal to increase the compute to communication ratio: Instead of increasing the mini-batch size, we reduce the communication frequency. Rather than keeping the sequences on different machines in sync, we allow them to evolve \emph{locally} on each machine, independent from each other, and only average the sequences once in a while (\emph{local SGD}).	score:433
National Institute of Technology Karnataka (NITK) , Surathkal: To the graduating batch: What lessons would you want to give to your juniors before you leave?	 	When using a stochastic optimization approach, is it always beneficial to use all available training data, if we have enough time to do so? Is it always best to use a fresh example at each iteration, thus maximizing the number of samples used?  Or is it sometimes better to revisit an old example, even if fresh examples are available?  In this paper, we revisit the notion of ``more data less work'' for stochastic optimization \citep{mdlw}, in light of recently proposed variance-reducing stochastic optimization techniques such as SDCA \citep{original-sdca-paper,shai-tong-analyzis}, SAG \citep{sag} and SVRG \citep{svrg}.   We consider smooth SVM-type training, i.e.,~regularized loss minimization for a smooth convex loss, in the data laden regime. That is, we consider a setting where we have infinite data and are limited only by time budget, and the goal is to get the best generalization (test) performance possible within the time budget (using as many examples as we would like).	 	When using a stochastic optimization approach, is it always beneficial to use all available training data, if we have enough time to do so? Is it always best to use a fresh example at each iteration, thus maximizing the number of samples used?  Or is it sometimes better to revisit an old example, even if fresh examples are available?  In this paper, we revisit the notion of ``more data less work'' for stochastic optimization \citep{mdlw}, in light of recently proposed variance-reducing stochastic optimization techniques such as SDCA \citep{original-sdca-paper,shai-tong-analyzis}, SAG \citep{sag} and SVRG \citep{svrg}.   We consider smooth SVM-type training, i.e.,~regularized loss minimization for a smooth convex loss, in the data laden regime. That is, we consider a setting where we have infinite data and are limited only by time budget, and the goal is to get the best generalization (test) performance possible within the time budget (using as many examples as we would like).	score:433
National Institute of Technology Karnataka (NITK) , Surathkal: To the graduating batch: What lessons would you want to give to your juniors before you leave?	  Our architecture also takes inspiration from recent work by Kulkarni et al.~\cite{kulkarni2016hierarchical} and Vezhnevets et al.~\cite{vezhnevets2017feudal}. These works outline hierarchical architectures that comprise a subgoal-selecting meta-controller and a sub-controller that tries to achieve the subgoal. The main feature that sets our model apart is the design of the subgoals.  Kulkarni et al. pre-define a set of discrete subgoals specific to the tasks at hand, while Vezhnevets et al. construct subgoals as a large continuous set of embedded states. We construct two discrete sets of subgoals, which are discussed in more detail in Section~\ref{sec:measure_control}. One of them is fixed but is designed to be generically applicable in visual domains; the other can be automatically learned such that the subgoals are useful for solving the task at hand.	  Our architecture also takes inspiration from recent work by Kulkarni et al.~\cite{kulkarni2016hierarchical} and Vezhnevets et al.~\cite{vezhnevets2017feudal}. These works outline hierarchical architectures that comprise a subgoal-selecting meta-controller and a sub-controller that tries to achieve the subgoal. The main feature that sets our model apart is the design of the subgoals.  Kulkarni et al. pre-define a set of discrete subgoals specific to the tasks at hand, while Vezhnevets et al. construct subgoals as a large continuous set of embedded states. We construct two discrete sets of subgoals, which are discussed in more detail in Section~\ref{sec:measure_control}. One of them is fixed but is designed to be generically applicable in visual domains; the other can be automatically learned such that the subgoals are useful for solving the task at hand.	score:438
National Institute of Technology Karnataka (NITK) , Surathkal: To the graduating batch: What lessons would you want to give to your juniors before you leave?	 Prototypes are selected members of a data set so as to attain various tasks including reducing, condensing or summarizing a data set. Many learning methods aim to carry out more than one of these tasks, thereby building efficient learning algorithms \citep{pekalska2006,bien2011}. A desirable prototype set reduces the data set in order to decrease running time, condenses the data set to preserve information, and summarizes the data set for better exploration and understanding.  The methods we discuss in this work are considered as decision boundary generators where decisions are made based on class conditional regions, or \emph{class covers}, that are composed of a collection of convex sets, each associated with a prototype \citep{toussaint2002}. The union of such convex sets constitute a region for the class of interest, estimating the support of this class \citep{scholkopf2001}.	 Prototypes are selected members of a data set so as to attain various tasks including reducing, condensing or summarizing a data set. Many learning methods aim to carry out more than one of these tasks, thereby building efficient learning algorithms \citep{pekalska2006,bien2011}. A desirable prototype set reduces the data set in order to decrease running time, condenses the data set to preserve information, and summarizes the data set for better exploration and understanding.  The methods we discuss in this work are considered as decision boundary generators where decisions are made based on class conditional regions, or \emph{class covers}, that are composed of a collection of convex sets, each associated with a prototype \citep{toussaint2002}. The union of such convex sets constitute a region for the class of interest, estimating the support of this class \citep{scholkopf2001}.	score:439
What is the alternative to machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:327
What is the alternative to machine learning?	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	score:374
What is the alternative to machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:376
What is the alternative to machine learning?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:380
What is the alternative to machine learning?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:387
How do I work with machine learning researchers after graduation?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:381
How do I work with machine learning researchers after graduation?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:382
How do I work with machine learning researchers after graduation?	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	score:403
How do I work with machine learning researchers after graduation?	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	score:405
How do I work with machine learning researchers after graduation?	 Our work is therefore a step towards fully automated machine learning.  	As the performance of machine learning algorithms has skyrocketed over recent years the often unspoken relationship between the human data scientist and the machines they run has evolved significantly.  A great deal of work has been put into new state-of-the-art methods, and researchers are constantly optimising the various aspects of machine learning algorithms.  Such efforts include proposing algorithms for optimising hyperparameters and network architectures \cite{real:2017:large} and the latest trends show increasing emphasis on algorithms that require less human intervention. Consider the automatic statistician project \footnote{https://www.automaticstatistician.com/index/} which aims at removing the data scientist from the process of understanding data by using Bayesian model selection.	 Our work is therefore a step towards fully automated machine learning.  	As the performance of machine learning algorithms has skyrocketed over recent years the often unspoken relationship between the human data scientist and the machines they run has evolved significantly.  A great deal of work has been put into new state-of-the-art methods, and researchers are constantly optimising the various aspects of machine learning algorithms.  Such efforts include proposing algorithms for optimising hyperparameters and network architectures \cite{real:2017:large} and the latest trends show increasing emphasis on algorithms that require less human intervention. Consider the automatic statistician project \footnote{https://www.automaticstatistician.com/index/} which aims at removing the data scientist from the process of understanding data by using Bayesian model selection.	score:413
How can undergraduate help with machine learning research?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:438
How can undergraduate help with machine learning research?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:447
How can undergraduate help with machine learning research?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:478
How can undergraduate help with machine learning research?	 Consider a researcher seeking a summer intern to work on a machine learning project on deep learning who searches for, say, ``linkedin graduate student machine learning neural networks.'' Now, a word embedding's semantic knowledge can improve relevance in the sense that a LinkedIn web page containing terms such as ``PhD student,'' ``embeddings,'' and ``deep learning,'' which are related to but different from the query terms, may be ranked highly in the results.  However, word embeddings also rank CS research related terms closer to male names than female names. The consequence would be, between two pages that differed in the names Mary and John but were otherwise identical, the search engine would rank John's higher than Mary. In this hypothetical example, the usage of word embedding makes it even harder for women to be recognized as computer scientists and would contribute to widening the existing gender gap in computer science.	 Consider a researcher seeking a summer intern to work on a machine learning project on deep learning who searches for, say, ``linkedin graduate student machine learning neural networks.'' Now, a word embedding's semantic knowledge can improve relevance in the sense that a LinkedIn web page containing terms such as ``PhD student,'' ``embeddings,'' and ``deep learning,'' which are related to but different from the query terms, may be ranked highly in the results.  However, word embeddings also rank CS research related terms closer to male names than female names. The consequence would be, between two pages that differed in the names Mary and John but were otherwise identical, the search engine would rank John's higher than Mary. In this hypothetical example, the usage of word embedding makes it even harder for women to be recognized as computer scientists and would contribute to widening the existing gender gap in computer science.	score:480
How can undergraduate help with machine learning research?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:483
Is graph theory counted as a prerequisite for machine learning?	 Machine learning on graphs is an important and ubiquitous task with  applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e. g., degree statistics or kernel functions).  However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction.  Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks.	  However, graphs are not only useful as structured knowledge repositories: they also play a key role in modern machine learning.  Many machine learning applications seek to make predictions or discover new patterns using graph-structured data as feature information. For example, one might wish to classify the role of a protein in a biological interaction graph, predict the role of a person in a collaboration network, recommend new friends to a user in a social network, or predict new therapeutic applications of existing drug molecules, whose structure can be represented as a graph.	score:296
Is graph theory counted as a prerequisite for machine learning?	  However, graphs are not only useful as structured knowledge repositories: they also play a key role in modern machine learning.  Many machine learning applications seek to make predictions or discover new patterns using graph-structured data as feature information. For example, one might wish to classify the role of a protein in a biological interaction graph, predict the role of a person in a collaboration network, recommend new friends to a user in a social network, or predict new therapeutic applications of existing drug molecules, whose structure can be represented as a graph.	  However, graphs are not only useful as structured knowledge repositories: they also play a key role in modern machine learning.  Many machine learning applications seek to make predictions or discover new patterns using graph-structured data as feature information. For example, one might wish to classify the role of a protein in a biological interaction graph, predict the role of a person in a collaboration network, recommend new friends to a user in a social network, or predict new therapeutic applications of existing drug molecules, whose structure can be represented as a graph.	score:303
Is graph theory counted as a prerequisite for machine learning?	  Real-world examples are molecules or proteins, image annotated with semantic information, text documents reflecting complex content dependencies, and manifold data modeling objects and scenes in robotics.  The goal of learning with graphs is to exploit the rich information contained in graphs representing structured data. The main challenge is to efficiently exploit the graph structure for machine-learning tasks such as classification or retrieval.   A popular approach to learning from structured data is to design graph kernels measuring the similarity between graphs.  For classification or regression problems, the graph kernel can then be plugged into a kernel machine, such as a support vector machine or a Gaussian process, for efficient learning and prediction.  Several graph kernels have been proposed in the literature, but they often make strong assumptions as to the nature and availability of information related to the graphs at hand.	  A popular approach to learning from structured data is to design graph kernels measuring the similarity between graphs.  For classification or regression problems, the graph kernel can then be plugged into a kernel machine, such as a support vector machine or a Gaussian process, for efficient learning and prediction.  Several graph kernels have been proposed in the literature, but they often make strong assumptions as to the nature and availability of information related to the graphs at hand.	score:305
Is graph theory counted as a prerequisite for machine learning?	 These representations can then be utilised as input to secondary supervised models for downstream prediction tasks, including classification \cite{Perozzi2014} or link prediction \cite{Grover2016}. Thus, graph embeddings are becoming a key area of research as they act as a translation layer between the raw graph and some desired machine learning model.   However, to date, there has been little research performed into why graph embedding approaches have been so successful. They all aim to capture as much topological information as possible during the embedding process, but how this is achieved, or even exactly what structure is being captured, is currently not known. In previous work \cite{bonner2017embedding}, we provided a framework which could be used to directly measure the ability of graph embeddings to capture a good representation of a graph's topology.	 One reason for the popularity is that the structure or topology of the resulting graph can reveal important and unique insights into the data it represents. Recently, analysing and making predictions about graph using machine learning has shown significant advances in a range of commonly performed tasks over traditional approaches \cite{Goyal2017}.  Such tasks include predicting the formation of new edges within the graph and the classification of vertices \cite{Moyano2017}. However, graph are inherently complex structures and do not naturally lend themselves as input into existing machine learning methods, many of which operate on vectors of real numbers.  Graph embeddings \footnote{In this work, we focus on vertex representation learning approaches.	score:307
Is graph theory counted as a prerequisite for machine learning?	     Graph classification has recently received a lot of attention from various fields of machine learning e.g. kernel methods, sequential modeling or graph embedding. All these approaches offer promising results with different respective strengths and weaknesses. However, most of them rely on complex mathematics and require heavy computational power to achieve their best performance.  We propose a simple and fast algorithm based on the spectral decomposition of graph Laplacian to perform graph classification and get a first reference score for a dataset. We show that this method obtains competitive results compared to state-of-the-art algorithms.  	Graph classification methods can schematically be divided into three categories: graph kernels, sequential methods and embedding methods.	     Graph classification has recently received a lot of attention from various fields of machine learning e.g. kernel methods, sequential modeling or graph embedding. All these approaches offer promising results with different respective strengths and weaknesses. However, most of them rely on complex mathematics and require heavy computational power to achieve their best performance.  We propose a simple and fast algorithm based on the spectral decomposition of graph Laplacian to perform graph classification and get a first reference score for a dataset. We show that this method obtains competitive results compared to state-of-the-art algorithms.  	Graph classification methods can schematically be divided into three categories: graph kernels, sequential methods and embedding methods.	score:307
Is Graph Theory a nice-to-have course to have for machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:366
Is Graph Theory a nice-to-have course to have for machine learning?	  However, all current deep learning frameworks still require detailed knowledge of the underlying machine learning algorithms and libraries as well as programming experience or tedious editing of text configuration files to set a vast amount of parameters.  Although there exist graphical interfaces such as NVIDIA's DIGITS\footnote{\url{https://github. com/NVIDIA/DIGITS}}, Intel's Deep Learning SDK\footnote{\url{https://software.intel.com/en-us/deep-learning-sdk}}, Caffe Gui Tool\footnote{\url{https://github.com/Chasvortex/caffe-gui-tool}}, or Expresso~\citep{Dholakiya2015}, none of the products currently available offer a graphical interface for the complete pipeline of deep learning in various applications.	  However, all current deep learning frameworks still require detailed knowledge of the underlying machine learning algorithms and libraries as well as programming experience or tedious editing of text configuration files to set a vast amount of parameters.  Although there exist graphical interfaces such as NVIDIA's DIGITS\footnote{\url{https://github. com/NVIDIA/DIGITS}}, Intel's Deep Learning SDK\footnote{\url{https://software.intel.com/en-us/deep-learning-sdk}}, Caffe Gui Tool\footnote{\url{https://github.com/Chasvortex/caffe-gui-tool}}, or Expresso~\citep{Dholakiya2015}, none of the products currently available offer a graphical interface for the complete pipeline of deep learning in various applications.	score:379
Is Graph Theory a nice-to-have course to have for machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:380
Is Graph Theory a nice-to-have course to have for machine learning?	 Machine learning on graphs is an important and ubiquitous task with  applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e. g., degree statistics or kernel functions).  However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction.  Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks.	 Machine learning on graphs is an important and ubiquitous task with  applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e. g., degree statistics or kernel functions).  However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction.  Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks.	score:388
Is Graph Theory a nice-to-have course to have for machine learning?	~\cite{Benedetti-2016} and shown to perform well for training restricted Boltzmann machines. More recently, generalizations and alternative techniques have been introduced in Ref.~\cite{Raymond-DWave-2016}. In the context of machine learning, these techniques need to estimate temperature at each iteration, implying a computational overhead.  \begin{figure} \includegraphics[width=. 50\textwidth]{fig_design-eps-converted-to.pdf} \caption{{\it Quantum-assisted unsupervised learning.} (a) During the training phase, samples generated by the quantum annealer are compared with samples from the data set of, say, black-and-white images. The control parameters are then modified according to a learning rule (see Sec. III). This process is iterated a given number of times, also known as epochs.	~\cite{Benedetti-2016} and shown to perform well for training restricted Boltzmann machines. More recently, generalizations and alternative techniques have been introduced in Ref.~\cite{Raymond-DWave-2016}. In the context of machine learning, these techniques need to estimate temperature at each iteration, implying a computational overhead.  \begin{figure} \includegraphics[width=. 50\textwidth]{fig_design-eps-converted-to.pdf} \caption{{\it Quantum-assisted unsupervised learning.} (a) During the training phase, samples generated by the quantum annealer are compared with samples from the data set of, say, black-and-white images. The control parameters are then modified according to a learning rule (see Sec. III). This process is iterated a given number of times, also known as epochs.	score:391
Why did Infosys reject a mass number of candidates from the 2014 passed out batches?	 ADL specifically addresses catastrophic forgetting by having a different-depth structure which is capable of achieving a trade-off between plasticity and stability. Network significance (NS) formula is proposed to drive the hidden nodes growing and pruning mechanism. Drift detection scenario (DDS) is put forward to signal distributional changes in data streams which induce the creation of a new hidden layer.  Maximum information compression index (MICI) method plays an important role as a complexity reduction module eliminating redundant layers. The efficacy of ADL is numerically validated under the prequential test-then-train procedure in lifelong environments using nine popular data stream problems. The numerical results demonstrate that ADL consistently outperforms recent continual learning methods while characterizing the automatic construction of network structures.	 ADL specifically addresses catastrophic forgetting by having a different-depth structure which is capable of achieving a trade-off between plasticity and stability. Network significance (NS) formula is proposed to drive the hidden nodes growing and pruning mechanism. Drift detection scenario (DDS) is put forward to signal distributional changes in data streams which induce the creation of a new hidden layer.  Maximum information compression index (MICI) method plays an important role as a complexity reduction module eliminating redundant layers. The efficacy of ADL is numerically validated under the prequential test-then-train procedure in lifelong environments using nine popular data stream problems. The numerical results demonstrate that ADL consistently outperforms recent continual learning methods while characterizing the automatic construction of network structures.	score:434
Why did Infosys reject a mass number of candidates from the 2014 passed out batches?	 Thus, the choice of appropriate problem-specific summary statistics is of crucial importance for the quality of posterior inference in ABC.  Several methods exist in the literature for the selection or construction of summary statistics. A number of these methods can be assembled around the idea of constructing summary statistics by linear or non-linear regression from the full dataset or a set of candidate statistics.  In addition to considerations about the sufficiency of summary statistics, all of these methods require either expert knowledge for the selection of the set of candidate statistics, e.g. \citet{nakagome13}, or perform complex and high-dimensional regression by using the full dataset, e.g. \citet{Fearnhead2012}. Other examples of this approach include \citet{blum2010non}, \citet{boulesteix2007partial} and \citet{wegmann2009efficient}.	 Thus, the choice of appropriate problem-specific summary statistics is of crucial importance for the quality of posterior inference in ABC.  Several methods exist in the literature for the selection or construction of summary statistics. A number of these methods can be assembled around the idea of constructing summary statistics by linear or non-linear regression from the full dataset or a set of candidate statistics.  In addition to considerations about the sufficiency of summary statistics, all of these methods require either expert knowledge for the selection of the set of candidate statistics, e.g. \citet{nakagome13}, or perform complex and high-dimensional regression by using the full dataset, e.g. \citet{Fearnhead2012}. Other examples of this approach include \citet{blum2010non}, \citet{boulesteix2007partial} and \citet{wegmann2009efficient}.	score:435
Why did Infosys reject a mass number of candidates from the 2014 passed out batches?	    In this paper, we address the problem of identifying anomalous networks from a database of multiple network samples while at the same time investigating \textit{why} a network is exceptional. An outlier is defined at the global level of an entire network sample but we use local subnetworks to explain its exceptionality. Although the outlierness of a network sample can be quantified via the outlier degree, such a single measure only bears limited explanatory information~\cite{Barbora13,Dang14} since it lacks the capability of showing in what data view, i. e. local subnetworks, an anomalous network is most exceptional.  Moreover, although two networks may have similar outlier degrees, the local subnetworks that make them abnormal might be quite different since the anomalous networks themselves are usually not homogeneous. For example, exploring a database of gene networks for outliers can lead to the isolation of subjects suffering from cancer.	    In this paper, we address the problem of identifying anomalous networks from a database of multiple network samples while at the same time investigating \textit{why} a network is exceptional. An outlier is defined at the global level of an entire network sample but we use local subnetworks to explain its exceptionality. Although the outlierness of a network sample can be quantified via the outlier degree, such a single measure only bears limited explanatory information~\cite{Barbora13,Dang14} since it lacks the capability of showing in what data view, i. e. local subnetworks, an anomalous network is most exceptional.  Moreover, although two networks may have similar outlier degrees, the local subnetworks that make them abnormal might be quite different since the anomalous networks themselves are usually not homogeneous. For example, exploring a database of gene networks for outliers can lead to the isolation of subjects suffering from cancer.	score:443
Why did Infosys reject a mass number of candidates from the 2014 passed out batches?	 However, once initiated, dendrite growth is extremely hard to mitigate as pointed out by several studies\cite{diggle1969zinc,monroe2003-dendrite,Monroe2004Effect}. Therefore, it is best to prevent dendrites from initiating to ensure smooth electrodeposition throughout cycling of the battery.  In recent years, high-throughput computational materials design has emerged as a major driver of discovery of novel materials for various applications \cite{curtarolo2013highthroughput,saal2013oqmd}.	 However, once initiated, dendrite growth is extremely hard to mitigate as pointed out by several studies\cite{diggle1969zinc,monroe2003-dendrite,Monroe2004Effect}. Therefore, it is best to prevent dendrites from initiating to ensure smooth electrodeposition throughout cycling of the battery.  In recent years, high-throughput computational materials design has emerged as a major driver of discovery of novel materials for various applications \cite{curtarolo2013highthroughput,saal2013oqmd}.	score:448
Why did Infosys reject a mass number of candidates from the 2014 passed out batches?	 In the real world, many online shopping websites or service provider have single email-id where customers can send their query, concern etc. At the back-end service provider receive million of emails every week, how they can identify which email is belonged of a particular department? This paper presents an artificial neural network (ANN) model that is used to solve this problem and experiments are carried out on user personal Gmail emails datasets.  This problem can be generalised as typical Text Classification or Categorization \cite{sebastiani2002machine}.  \textbf{Keywords}: Artificial Neural Network, Email Classification, Natural Computing, Text Categorization 	Electronic mail or e-mail is a method of electronic communication between two or more users using the Internet. Nowadays emails are not just used for communication but also used for managing the task, solving customer queries.	 In the real world, many online shopping websites or service provider have single email-id where customers can send their query, concern etc. At the back-end service provider receive million of emails every week, how they can identify which email is belonged of a particular department? This paper presents an artificial neural network (ANN) model that is used to solve this problem and experiments are carried out on user personal Gmail emails datasets.  This problem can be generalised as typical Text Classification or Categorization \cite{sebastiani2002machine}.  \textbf{Keywords}: Artificial Neural Network, Email Classification, Natural Computing, Text Categorization 	Electronic mail or e-mail is a method of electronic communication between two or more users using the Internet. Nowadays emails are not just used for communication but also used for managing the task, solving customer queries.	score:451
When is the next batch of Infosys after 9 Jan 2016?	 Till now, BP is very competitive in both speed and accuracy for topic modeling~\citep{Zeng:11}. Similar BP ideas have also been discussed as the zero-order approximation of the collapsed VB (CVB0) algorithm within the mean-field framework~\citep{Asuncion:09,Asuncion:10}.  However, the message passing techniques often require storing previous messages for updating and passing, which leads to the high memory usage increasing linearly with the number of documents or the number of topics.	 Till now, BP is very competitive in both speed and accuracy for topic modeling~\citep{Zeng:11}. Similar BP ideas have also been discussed as the zero-order approximation of the collapsed VB (CVB0) algorithm within the mean-field framework~\citep{Asuncion:09,Asuncion:10}.  However, the message passing techniques often require storing previous messages for updating and passing, which leads to the high memory usage increasing linearly with the number of documents or the number of topics.	score:498
When is the next batch of Infosys after 9 Jan 2016?	 Till now, BP is very competitive in both speed and accuracy for topic modeling~\citep{Zeng:11}. Similar BP ideas have also been discussed as the zero-order approximation of the collapsed VB (CVB0) algorithm within the mean-field framework~\citep{Asuncion:09,Asuncion:10}.  However, the message passing techniques often require storing previous messages for updating and passing, which leads to the high memory usage increasing linearly with the number of documents or the number of topics.	 Till now, BP is very competitive in both speed and accuracy for topic modeling~\citep{Zeng:11}. Similar BP ideas have also been discussed as the zero-order approximation of the collapsed VB (CVB0) algorithm within the mean-field framework~\citep{Asuncion:09,Asuncion:10}.  However, the message passing techniques often require storing previous messages for updating and passing, which leads to the high memory usage increasing linearly with the number of documents or the number of topics.	score:498
When is the next batch of Infosys after 9 Jan 2016?	 On the other hand, we model PTC to learn discontinuous temporal coherence in long-term time series. Temporal coherence is often influenced by periodic events or human activities. For example, according to the survey in 2016~\cite{Ellering2016}, the best time periods to make a post on Facebook are Saturday and Sunday around 12 a.m. to 1 p.m., with periodic peaking time from 9 a. m. to 3 p.m. And weekly peak time for Pinterest is on Saturday from 8 p.m. to 11 p.m. These findings motivate us to consider both of the temporal and sequential characteristics for predicting popularity more precisely.   In this paper, therefore, we propose a novel deep prediction framework called Deep Temporal Context Networks (DTCN) by exploring both temporal contexts and temporal attention at different time-scales jointly (such as days of a week, hours of a day).	 On the other hand, we model PTC to learn discontinuous temporal coherence in long-term time series. Temporal coherence is often influenced by periodic events or human activities. For example, according to the survey in 2016~\cite{Ellering2016}, the best time periods to make a post on Facebook are Saturday and Sunday around 12 a.m. to 1 p.m., with periodic peaking time from 9 a. m. to 3 p.m. And weekly peak time for Pinterest is on Saturday from 8 p.m. to 11 p.m. These findings motivate us to consider both of the temporal and sequential characteristics for predicting popularity more precisely.   In this paper, therefore, we propose a novel deep prediction framework called Deep Temporal Context Networks (DTCN) by exploring both temporal contexts and temporal attention at different time-scales jointly (such as days of a week, hours of a day).	score:498
When is the next batch of Infosys after 9 Jan 2016?	 This can sometimes happen for discrete predictor variables and will always happen for continuous observations. An interpolated prediction which is close to the split point location is illustrated in the following example. Consider data with only one predictor variable $X$ that can get integer values from 1 to 10, and a deterministic dependent variable $Y$ that gets 0 if $X >= 9$ and 1 otherwise.  Suppose that the data used for training the decision tree had only observations with $X$-values 1, 2, and 10 (and $Y$-values 1, 1, and 0). If the model would be used to predict a new observation with $X=1$ it would correctly classify it as 1. But what should the model give when used to interpolate a prediction for an observation that is close to the split point 9, such as $X=8$?  There are three methods used in practice by decision tree learners for making such interpolated predictions. The first method, "Sweep Left", classifies any observation above (but not including) 2 as 0 ($X>2 \Rightarrow  Y=0$), and estimates the cutoff quantile ($p$) of the decision tree as $\hat p_{SL}$. Under the second method, "Sweep Right", only observations with the values $X=10$ would be classified as 0 ($X=10 \Rightarrow  Y=0$), with cutoff $\hat p_{SR}$.	 Suppose that the data used for training the decision tree had only observations with $X$-values 1, 2, and 10 (and $Y$-values 1, 1, and 0). If the model would be used to predict a new observation with $X=1$ it would correctly classify it as 1. But what should the model give when used to interpolate a prediction for an observation that is close to the split point 9, such as $X=8$?	score:505
When is the next batch of Infosys after 9 Jan 2016?	 These have achieved state-of-the-art performance on a few benchmarks using a depth of 10 layers. However, the performance of this architecture suffers from a bottleneck, and ceases to improve when an attempt is made to add more layers. In this work, we analyze the causes for this, and postulate that the main source is the way that the information flows through time.  We introduce a novel and simple variation for the RHN cell, called Highway State Gating (HSG), which allows adding more layers, while continuing to improve performance. By using a gating mechanism for the state, we allow the net to "choose" whether to pass information directly through time, or to gate it. This mechanism also allows the gradient to back-propagate directly through time and, therefore, results in a slightly faster convergence.	 These have achieved state-of-the-art performance on a few benchmarks using a depth of 10 layers. However, the performance of this architecture suffers from a bottleneck, and ceases to improve when an attempt is made to add more layers. In this work, we analyze the causes for this, and postulate that the main source is the way that the information flows through time.  We introduce a novel and simple variation for the RHN cell, called Highway State Gating (HSG), which allows adding more layers, while continuing to improve performance. By using a gating mechanism for the state, we allow the net to "choose" whether to pass information directly through time, or to gate it. This mechanism also allows the gradient to back-propagate directly through time and, therefore, results in a slightly faster convergence.	score:508
What is the sigmoid function, and what is its use in machine learning's neural networks?	 	Stochastic gradient algorithms provide powerful and iterative techniques for the solution of optimization problems \cite{Polyak}. In many situations of interest, the objective function is in the form of the expectation of a convex loss function over the distribution of the input data. Such situations arise in machine learning applications, where the input data are features to a classifier and their associated class labels.  For example, the goal of a binary classifier is to predict the label ($\pm 1$) given a vector of features that describes an observation (or, equivalently, to separate two classes based on their feature vector descriptions). The classifier achieves this goal by learning a classification rule based on a cost function that penalizes incorrect classification according to some criterion.	 	Stochastic gradient algorithms provide powerful and iterative techniques for the solution of optimization problems \cite{Polyak}. In many situations of interest, the objective function is in the form of the expectation of a convex loss function over the distribution of the input data. Such situations arise in machine learning applications, where the input data are features to a classifier and their associated class labels.  For example, the goal of a binary classifier is to predict the label ($\pm 1$) given a vector of features that describes an observation (or, equivalently, to separate two classes based on their feature vector descriptions). The classifier achieves this goal by learning a classification rule based on a cost function that penalizes incorrect classification according to some criterion.	score:328
What is the sigmoid function, and what is its use in machine learning's neural networks?	 In machine learning and applied statistics, a convex function such as the objective function of support vector machines (SVMs) is generally preferred, since it can leverage the high-performance algorithms and rigorous guarantees established in the extensive literature on convex optimization. One may thus wonder if there exists a meaningful convex objective function for the inference problem in human computation.  In this paper, we investigate this convexity issue for human computation. We take an axiomatic approach by formulating a set of axioms that impose two mild and natural assumptions on the objective function for the inference. Under these axioms, we show that it is unfortunately impossible to ensure convexity of the inference problem. On the other hand, we show that interestingly, in the absence of a requirement to model ``spammers'', one can construct reasonable objective functions for crowdsourcing that guarantee convex inference.	 In machine learning and applied statistics, a convex function such as the objective function of support vector machines (SVMs) is generally preferred, since it can leverage the high-performance algorithms and rigorous guarantees established in the extensive literature on convex optimization. One may thus wonder if there exists a meaningful convex objective function for the inference problem in human computation.  In this paper, we investigate this convexity issue for human computation. We take an axiomatic approach by formulating a set of axioms that impose two mild and natural assumptions on the objective function for the inference. Under these axioms, we show that it is unfortunately impossible to ensure convexity of the inference problem. On the other hand, we show that interestingly, in the absence of a requirement to model ``spammers'', one can construct reasonable objective functions for crowdsourcing that guarantee convex inference.	score:333
What is the sigmoid function, and what is its use in machine learning's neural networks?	    Deep learning, in the form of artificial neural networks, has achieved remarkable practical success in recent years, for a variety of difficult machine learning applications. However, a theoretical explanation for this remains a major open problem, since training neural networks involves optimizing a highly non-convex objective function, and is known to be computationally hard in the worst case.  In this work, we study the \emph{geometric} structure of the associated non-convex objective function, in the context of ReLU networks and starting from a random initialization of the network parameters. We identify some conditions under which it becomes more favorable to optimization, in the sense of (i) High probability of initializing at a point from which there is a monotonically decreasing path to a global minimum; and (ii) High probability of initializing at a basin (suitably defined) with a small minimal objective value.	    Deep learning, in the form of artificial neural networks, has achieved remarkable practical success in recent years, for a variety of difficult machine learning applications. However, a theoretical explanation for this remains a major open problem, since training neural networks involves optimizing a highly non-convex objective function, and is known to be computationally hard in the worst case.  In this work, we study the \emph{geometric} structure of the associated non-convex objective function, in the context of ReLU networks and starting from a random initialization of the network parameters. We identify some conditions under which it becomes more favorable to optimization, in the sense of (i) High probability of initializing at a point from which there is a monotonically decreasing path to a global minimum; and (ii) High probability of initializing at a basin (suitably defined) with a small minimal objective value.	score:337
What is the sigmoid function, and what is its use in machine learning's neural networks?	  We also introduce several novel applications that are enabled by the atomic-norm framework, including tensor completion, moment problems in signal processing, and graph deconvolution. 	Minimization of a convex loss function with a constraint on the ``simplicity" of the solution has found widespread applications in communications, machine learning, image processing, genetics, and other fields.  While exact formulations of the simplicity requirement are often intractable, it is sometimes possible to devise tractable formulations via convex relaxation that are (nearly) equivalent.                       Since these formulations differ so markedly across applications, a principled and unified convex heuristic for different notions of simplicity has been proposed using notions of {\em atoms} and {\em   atomic norms} \cite{venkat}.	  We also introduce several novel applications that are enabled by the atomic-norm framework, including tensor completion, moment problems in signal processing, and graph deconvolution. 	Minimization of a convex loss function with a constraint on the ``simplicity" of the solution has found widespread applications in communications, machine learning, image processing, genetics, and other fields.  While exact formulations of the simplicity requirement are often intractable, it is sometimes possible to devise tractable formulations via convex relaxation that are (nearly) equivalent.                       Since these formulations differ so markedly across applications, a principled and unified convex heuristic for different notions of simplicity has been proposed using notions of {\em atoms} and {\em   atomic norms} \cite{venkat}.	score:339
What is the sigmoid function, and what is its use in machine learning's neural networks?	 In addition, we assume that the optimization is done given a particular amount of computational resources, for example the number of CPUs we can use and the amount of time we are willing to use them. An example that fits this scenario, from the field of machine learning, is the training of neural networks where the parameters to optimize are usually the various learning rates of the learning algorithm, number of neurons and connections, etc.   What would be the requirements for a good search strategy in this scenario? First, in high dimensions exhaustive searches are unreasonable, thus the search must be efficient and find local extrema of the function as quickly as possible. Second, our strategy must allow for as many parallel evaluations of the function as our resources allow. In most scenarios, the duration of function evaluations is variable.	 In addition, we assume that the optimization is done given a particular amount of computational resources, for example the number of CPUs we can use and the amount of time we are willing to use them. An example that fits this scenario, from the field of machine learning, is the training of neural networks where the parameters to optimize are usually the various learning rates of the learning algorithm, number of neurons and connections, etc.   What would be the requirements for a good search strategy in this scenario? First, in high dimensions exhaustive searches are unreasonable, thus the search must be efficient and find local extrema of the function as quickly as possible. Second, our strategy must allow for as many parallel evaluations of the function as our resources allow. In most scenarios, the duration of function evaluations is variable.	score:339
How is machine learning used in finance?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:453
How is machine learning used in finance?	  Continuing the above example, suppose that in a particular protected community $S$, on average, individuals are financially disadvantaged and are unlikely to repay a loan.  A machine-learning algorithm that aims to optimize the institution's returns might devote resources to learning outside of $S$ -- where there is more opportunity for gains in utility -- and assign a fixed, low probability to all $i \in S$.   Such an algorithm would discriminate against the {\em qualified} members of $S$. If $S$ is an underrepresented subpopulation, this form of discrimination has the potential to amplify $S$'s underrepresentation by refusing to approve members that are capable of repaying the loan.    \subsection{Overview of our contributions} Focusing on such concerns, we investigate a notion we call \emph{multicalibration}.	  Continuing the above example, suppose that in a particular protected community $S$, on average, individuals are financially disadvantaged and are unlikely to repay a loan.  A machine-learning algorithm that aims to optimize the institution's returns might devote resources to learning outside of $S$ -- where there is more opportunity for gains in utility -- and assign a fixed, low probability to all $i \in S$.   Such an algorithm would discriminate against the {\em qualified} members of $S$. If $S$ is an underrepresented subpopulation, this form of discrimination has the potential to amplify $S$'s underrepresentation by refusing to approve members that are capable of repaying the loan.    \subsection{Overview of our contributions} Focusing on such concerns, we investigate a notion we call \emph{multicalibration}.	score:457
How is machine learning used in finance?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	score:458
How is machine learning used in finance?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:460
How is machine learning used in finance?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:460
What is exactly the attention mechanism introduced to RNN (recurrent neural network)? It would be nice if you could make it easy to understand!	 \\  Recently, Attention Mechanisms \cite{bahdanau2014neural} have been proven quite useful for Neural Machine Translation (NMT) when paired with RNNs. However, Attention Mechanisms are so good that they recently have been used alone without any kind of RNN nor Convolutional Neural Network (CNN). This marks the apparition of the Transformer Network \cite{vaswani2017attention}, claiming by the title of the paper that Attention Is All You Need.  Since then, people started to believe that RNNs could be discarded in favor of attention mechanisms. \\  On the other side, there is Yoshua Bengio with his paper about The Consciousness Prior \cite{bengio2017consciousness}, in which he express consciousness as being recurrent through time, introducing the consciousness RNN which, by any means, may retain fragments of the input representations through time in the RNN cell as if it was attention.	 \\  Recently, Attention Mechanisms \cite{bahdanau2014neural} have been proven quite useful for Neural Machine Translation (NMT) when paired with RNNs. However, Attention Mechanisms are so good that they recently have been used alone without any kind of RNN nor Convolutional Neural Network (CNN). This marks the apparition of the Transformer Network \cite{vaswani2017attention}, claiming by the title of the paper that Attention Is All You Need.  Since then, people started to believe that RNNs could be discarded in favor of attention mechanisms. \\  On the other side, there is Yoshua Bengio with his paper about The Consciousness Prior \cite{bengio2017consciousness}, in which he express consciousness as being recurrent through time, introducing the consciousness RNN which, by any means, may retain fragments of the input representations through time in the RNN cell as if it was attention.	score:267
What is exactly the attention mechanism introduced to RNN (recurrent neural network)? It would be nice if you could make it easy to understand!	 What is the best brand and what's reliable?","Weight and dimensions are important if you're planning to travel with the laptop. Get something with at least 512 mb of RAM. [..] is a good brand, and has an easy to use site where you can build a custom laptop."\\\hline   \end{tabular}     \caption{Examples of text samples and their labels.}       \label{figresults} \end{table*} The goal of natural language processing (NLP) is to process text with computers in order to analyze it, to extract information and eventually to represent the same information differently.	 What is the best brand and what's reliable?","Weight and dimensions are important if you're planning to travel with the laptop. Get something with at least 512 mb of RAM. [..] is a good brand, and has an easy to use site where you can build a custom laptop."\\\hline   \end{tabular}     \caption{Examples of text samples and their labels.}       \label{figresults} \end{table*} The goal of natural language processing (NLP) is to process text with computers in order to analyze it, to extract information and eventually to represent the same information differently.	score:271
What is exactly the attention mechanism introduced to RNN (recurrent neural network)? It would be nice if you could make it easy to understand!	 Moreover, RNN based handwriting recognition~\cite{graves2009novel}, speech recognition~\cite{kombrink2011recurrent}, and machine translation~\cite{auli2013joint} systems have also led to much improvement in the corresponding tasks. Compared to traditional feedforward neural networks, RNN has demonstrated its strong capability to exploit dependencies in the sequence due to its specific recurrent network structure.   In this work, we propose to leverage RNN to model sequential dependency into predicting ad click probability. We consider each user's ad browsing history as one sequence which yields the intrinsic internal dependency. In the training process of RNN model, features of each ad impression will be feedforwarded into the hidden layer, together with previously accumulated hidden state.  In this way, the dependency among impressions will be embedded into the recurrent network structure. Our experiments on the large scale data from a commercial search engine reveal that, such RNN structure can give rise to a significant improvement on the click prediction accuracy compared with the state-of-the-art dependency-free models such as Neural Networks and Logistic Regression.   The main contributions of this paper are in three folds: \begin{itemize}   \item We investigate the sequential dependency among particular user's ad impressions, and identify several important sequential dependency relationships.   \item We use Recurrent Neural Networks to model user's click sequence, and successfully incorporate sequential dependency into enhancing the accuracy of click prediction.	 Moreover, RNN based handwriting recognition~\cite{graves2009novel}, speech recognition~\cite{kombrink2011recurrent}, and machine translation~\cite{auli2013joint} systems have also led to much improvement in the corresponding tasks. Compared to traditional feedforward neural networks, RNN has demonstrated its strong capability to exploit dependencies in the sequence due to its specific recurrent network structure.   In this work, we propose to leverage RNN to model sequential dependency into predicting ad click probability. We consider each user's ad browsing history as one sequence which yields the intrinsic internal dependency. In the training process of RNN model, features of each ad impression will be feedforwarded into the hidden layer, together with previously accumulated hidden state.  In this way, the dependency among impressions will be embedded into the recurrent network structure. Our experiments on the large scale data from a commercial search engine reveal that, such RNN structure can give rise to a significant improvement on the click prediction accuracy compared with the state-of-the-art dependency-free models such as Neural Networks and Logistic Regression.   The main contributions of this paper are in three folds: \begin{itemize}   \item We investigate the sequential dependency among particular user's ad impressions, and identify several important sequential dependency relationships.   \item We use Recurrent Neural Networks to model user's click sequence, and successfully incorporate sequential dependency into enhancing the accuracy of click prediction.	score:272
What is exactly the attention mechanism introduced to RNN (recurrent neural network)? It would be nice if you could make it easy to understand!	 Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers.  We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs.  Experimental results have shown that the proposed IndRNN is able to process very long sequences (over $5000$ time steps), can be used to construct very deep networks ($21$ layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM.	 We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs.	score:283
What is exactly the attention mechanism introduced to RNN (recurrent neural network)? It would be nice if you could make it easy to understand!	   However, they still differ fundamentally: the dependency structure of an RNN is \emph{acyclic}, while that of an MRF is \emph{cyclic}.  Consequently, the hidden states cannot be inferred in a single \emph{feed-forward} manner as in a RNN.  This posts a significant challenge -- how can one derive the back-propagation procedure without a well-defined forward function?   Our strategy to tackle this difficulty is to \emph{unroll an iterative inference procedure into a feed-forward function}. This is motivated by the observation that while the inference is iterative, each cycle of updates is still a feed-forward procedure.  Following a carefully devised scheduling policy, which we call the \emph{Coupled Acyclic Passes (CAP)}, the inference can be unrolled into multiple RNNs operating along opposite directions that are coupled together.	   However, they still differ fundamentally: the dependency structure of an RNN is \emph{acyclic}, while that of an MRF is \emph{cyclic}.  Consequently, the hidden states cannot be inferred in a single \emph{feed-forward} manner as in a RNN.  This posts a significant challenge -- how can one derive the back-propagation procedure without a well-defined forward function?   Our strategy to tackle this difficulty is to \emph{unroll an iterative inference procedure into a feed-forward function}. This is motivated by the observation that while the inference is iterative, each cycle of updates is still a feed-forward procedure.  Following a carefully devised scheduling policy, which we call the \emph{Coupled Acyclic Passes (CAP)}, the inference can be unrolled into multiple RNNs operating along opposite directions that are coupled together.	score:288
If 2 is an eigen value of scalar matrix A 3x3 , then eigen value of adj A is?	   Despite the many  applications where both SC and PI are  an integral part of the learning process, the study of  online learning with SC has been limited only to the MAB and Expert settings. In the MAB setting, it has been shown that the expected regret of any player is at least $\tilde{\Omega}(c^{1/3} K^{1/3}T^{2/3})$ \cite{dekel2014bandits},  and that Batch EXP3  is an order optimal algorithm  \cite{arora2012online}.  In the Expert setting, it has been shown that the expected regret is at least $\tilde{\Omega}(\sqrt{\log(K)T})$  \cite{cesa2006prediction}, and order optimal algorithms   have been proposed in \cite{geulen2010regret,gyorgy2014near}.   The PI setup has been investigated only in the absence of SC, and for any fixed feedback system $G_{t}=G$ with independence number  $\alpha(G)>1$,  it has been shown that  the expected regret is at least $\tilde{\Omega}(\sqrt{\alpha(G)T})$~\cite{mannor2011bandits}.	e. the feedback graph $G_{t}=G$ is fixed and un-directed, and MAB. In these two cases,   $\beta(G_{1:T})$ equals $\alpha(G)$ and $K$ respectively. The  state-of-art algorithm EXP3 SET in PI setting without SC  is known to be order optimal only for these cases as well  \cite{alon2017nonstochastic}.   Threshold Based EXP3 is order optimal in the SC $c$ as well, while EXP3. SC  has an additional factor of $c$ in its expected regret. In the time-varying case, for sequence   $G_{1:T}$, the expected regret  is dependent on the worst $t^{*}$ and $n^{*}$ instances of the ratio of $\mbox{mas}(G_{t})$   and  $\mbox{mas}(G_{(T)})$, where  $\{\mbox{mas}(G_{(1)}),   \mbox{mas}(G_{(2)}),  \ldots, \mbox{mas}(G_{(T)}) \}$ are the sizes of the maximal acyclic subgraphs of  $G_{1:T}$   arranged in non-increasing order, $t^{*}={\ceil{T^{2/3}c^{-2/3}\mbox{mas}^{1/3}(G_{(T)})}}$ and $n^*= 0. 5\mbox{mas}^{1/3}(G_{(T)})T^{2/3}c^{1/3}$. Finally, Table \ref{contribution} also provides the performance in  the equi-informational setting, namely when  $G_{t}$ is undirected and  all the maximal acyclic subgraphs in $G_{1:T}$ have  the same size. The proofs of all these results are available online \cite{rangi2018online}.   Numerical comparison  shows that Threshold Based EXP3 outperforms   EXP3 SET   in the presence of SCs.  Threshold Based EXP3 also outperforms Batch EXP3, which is another order optimal algorithm for the MAB setting with SC~\cite{arora2012online}.                     \subsection{Related Work} In the absence of SC, the lower bound on the expected regret  is known for all   three categories of online learning problems. In the MAB setting, the expected regret is at least $\tilde{\Omega}(\sqrt{KT})$~\cite{auer2002nonstochastic,cesa2006prediction,rangi2018unifying}.	score:339
If 2 is an eigen value of scalar matrix A 3x3 , then eigen value of adj A is?	 If, for example, the algorithm converges to an equilibrium from a randomly chosen initial point with step size $0.3$, then this equilibrium is almost surely $x=1$.} \end{figure}  {\bf Example 3. Dependence of convergence on the initialization:} Consider the function $f_3(x) = x^L$ where $L \in \mathbb N$ is an even number larger than 2. The gradient descent results in the system \[ x[k+1] = x[k] - \delta Lx[k]^{L-1}.	 If, for example, the algorithm converges to an equilibrium from a randomly chosen initial point with step size $0.3$, then this equilibrium is almost surely $x=1$.} \end{figure}  {\bf Example 3. Dependence of convergence on the initialization:} Consider the function $f_3(x) = x^L$ where $L \in \mathbb N$ is an even number larger than 2. The gradient descent results in the system \[ x[k+1] = x[k] - \delta Lx[k]^{L-1}.	score:351
If 2 is an eigen value of scalar matrix A 3x3 , then eigen value of adj A is?	 Since the Schatten-${p}$ quasi-norm is equivalent to the $\ell_{p}$ quasi-norm on the singular values, it is natural to ask the following question: can we design an equivalent matrix factorization form to some cases of the Schatten-${p}$ quasi-norm, e.g., $p\!=\!1/2$ or $1/3$?  In this paper we first define two tractable Schatten norms, the bi-trace (Bi-tr) and tri-trace (Tri-tr) norms.  We then prove that they are in essence the Schatten-${1/2}$ and ${1/3}$ quasi-norms, respectively, for solving whose minimization we only need to perform SVDs on much smaller factor matrices to replace the large matrices in the algorithms mentioned above. Then we design two efficient linearized alternating minimization algorithms with guaranteed convergence to solve our problems. Finally, we provide the sufficient condition for exact recovery, and the restricted strong convexity (RSC) based and MC error bounds.	 Since the Schatten-${p}$ quasi-norm is equivalent to the $\ell_{p}$ quasi-norm on the singular values, it is natural to ask the following question: can we design an equivalent matrix factorization form to some cases of the Schatten-${p}$ quasi-norm, e.g., $p\!=\!1/2$ or $1/3$?  In this paper we first define two tractable Schatten norms, the bi-trace (Bi-tr) and tri-trace (Tri-tr) norms.  We then prove that they are in essence the Schatten-${1/2}$ and ${1/3}$ quasi-norms, respectively, for solving whose minimization we only need to perform SVDs on much smaller factor matrices to replace the large matrices in the algorithms mentioned above. Then we design two efficient linearized alternating minimization algorithms with guaranteed convergence to solve our problems. Finally, we provide the sufficient condition for exact recovery, and the restricted strong convexity (RSC) based and MC error bounds.	score:370
If 2 is an eigen value of scalar matrix A 3x3 , then eigen value of adj A is?	  Then any estimation algorithm for NPR with bit-flip noise must use at least the following number of samples:    \[     \begin{cases}         \exp\left(\Theta\left(n^{1/3} \cdot \ln^{2/3}(1/\epsilon)/\err^{2/3} \right)\right) & \quad \text{if } \frac{\ln(1/\eps)}{n} \leq \err \leq 1/2, \\         \exp\left(\Theta\left(n^{1/3} \cdot \ln^{2/3}(1/\epsilon) \cdot (1-\err)^{1/3}\right)\right) & \quad\text{if } 1/2 \leq \err \leq 1-\frac{\ln(1/\eps)}{n}.  \\     \end{cases}     \]     Furthermore, there is an algorithm for the full NPR problem with bit-flip noise having running time and samples equal to the above times $\poly(n,1/\eps)$. \end{theorem}   Prior to this work and the very recent and independent work of \cite{PTW17}, no nontrivial upper or lower bounds were known even for the sample complexity of the general bit-flip noise population recovery problem.	  Then any estimation algorithm for NPR with bit-flip noise must use at least the following number of samples:    \[     \begin{cases}         \exp\left(\Theta\left(n^{1/3} \cdot \ln^{2/3}(1/\epsilon)/\err^{2/3} \right)\right) & \quad \text{if } \frac{\ln(1/\eps)}{n} \leq \err \leq 1/2, \\         \exp\left(\Theta\left(n^{1/3} \cdot \ln^{2/3}(1/\epsilon) \cdot (1-\err)^{1/3}\right)\right) & \quad\text{if } 1/2 \leq \err \leq 1-\frac{\ln(1/\eps)}{n}.  \\     \end{cases}     \]     Furthermore, there is an algorithm for the full NPR problem with bit-flip noise having running time and samples equal to the above times $\poly(n,1/\eps)$. \end{theorem}   Prior to this work and the very recent and independent work of \cite{PTW17}, no nontrivial upper or lower bounds were known even for the sample complexity of the general bit-flip noise population recovery problem.	score:371
If 2 is an eigen value of scalar matrix A 3x3 , then eigen value of adj A is?	   In this paper, we provide numerical comparisons of the performance of  three state-of-the-art algorithms, namely, {\sc OptSpace}, {\sc ADMiRA} and  FPCA, and show that these efficient algorithms can be used to reconstruct real data matrices, as well as randomly generated matrices, accurately.     \subsection{Outline}\label{sec:outline}   The organization of this paper is as follows. In Section 2, we describe the matrix completion problem and  efficient algorithms to solve the matrix completion problem  when the observations are corrupted by noise.  Section 3 discusses the results of numerical simulations  and compares the performance of  three matrix completion algorithms with  respect to speed and accuracy.	   In this paper, we provide numerical comparisons of the performance of  three state-of-the-art algorithms, namely, {\sc OptSpace}, {\sc ADMiRA} and  FPCA, and show that these efficient algorithms can be used to reconstruct real data matrices, as well as randomly generated matrices, accurately.     \subsection{Outline}\label{sec:outline}   The organization of this paper is as follows. In Section 2, we describe the matrix completion problem and  efficient algorithms to solve the matrix completion problem  when the observations are corrupted by noise.  Section 3 discusses the results of numerical simulations  and compares the performance of  three matrix completion algorithms with  respect to speed and accuracy.	score:371
I have an intracluster correlation matrix, symmetric matrix capturing correlation between clusters (based on k means). What is a nice way to represent these clusters graphically using this matrix and the post intra-cluster correlation of each variable?	 By tuning this scaling matrix, the network can reach an appropriate orthogonal matrix using a relatively simple gradient descent update step. The resulting method achieves superior performance on various sequential data tasks.    The method we present in this paper works entirely with real matrices, and as such, our results deal only with orthogonal and skew-symmetric matrices. However, the method and all related theory remain valid for unitary and skew-Hermitian matrices in the complex case. The experimental results in this paper indicate that state of the art performance can be achieved without using complex matrices to optimize along the Stiefel manifold.	 By tuning this scaling matrix, the network can reach an appropriate orthogonal matrix using a relatively simple gradient descent update step. The resulting method achieves superior performance on various sequential data tasks.    The method we present in this paper works entirely with real matrices, and as such, our results deal only with orthogonal and skew-symmetric matrices. However, the method and all related theory remain valid for unitary and skew-Hermitian matrices in the complex case. The experimental results in this paper indicate that state of the art performance can be achieved without using complex matrices to optimize along the Stiefel manifold.	score:183
I have an intracluster correlation matrix, symmetric matrix capturing correlation between clusters (based on k means). What is a nice way to represent these clusters graphically using this matrix and the post intra-cluster correlation of each variable?	  By exploiting the special matrix properties and graph characteristics of a graph Laplacian matrix, we propose an efficient method for computing the $(K+1)$-th eigenpair given all of the $K$ smallest eigenpairs, which we call the Incremental method of Increasing Orders (Incremental-IO). For each increment, given the previously computed smallest eigenpairs, we show that computing the next smallest eigenpair is equivalent to computing a leading eigenpair of a particular matrix, which transforms potentially tedious numerical computation (such as the iterative tridiagonalization and eigen-decomposition steps in the Lanczos algorithm \cite{lanczos1950iteration}) to simple matrix power iterations of known computational efficiency \cite{kuczynski1992estimating}.	  By exploiting the special matrix properties and graph characteristics of a graph Laplacian matrix, we propose an efficient method for computing the $(K+1)$-th eigenpair given all of the $K$ smallest eigenpairs, which we call the Incremental method of Increasing Orders (Incremental-IO). For each increment, given the previously computed smallest eigenpairs, we show that computing the next smallest eigenpair is equivalent to computing a leading eigenpair of a particular matrix, which transforms potentially tedious numerical computation (such as the iterative tridiagonalization and eigen-decomposition steps in the Lanczos algorithm \cite{lanczos1950iteration}) to simple matrix power iterations of known computational efficiency \cite{kuczynski1992estimating}.	score:201
I have an intracluster correlation matrix, symmetric matrix capturing correlation between clusters (based on k means). What is a nice way to represent these clusters graphically using this matrix and the post intra-cluster correlation of each variable?	 Through extensive simulation studies and a real application to a call center data, we have demonstrated the fine performance of our method compared with existing alternatives. 	Covariance matrix and precision matrix (inverse of the covariance matrix) are among the most fundamental quantities in Statistics as they describe the dependence between different variables (components) of a multivariate observation.	 Through extensive simulation studies and a real application to a call center data, we have demonstrated the fine performance of our method compared with existing alternatives. 	Covariance matrix and precision matrix (inverse of the covariance matrix) are among the most fundamental quantities in Statistics as they describe the dependence between different variables (components) of a multivariate observation.	score:202
I have an intracluster correlation matrix, symmetric matrix capturing correlation between clusters (based on k means). What is a nice way to represent these clusters graphically using this matrix and the post intra-cluster correlation of each variable?	  \item We demonstrate that our methods are relevant for various applications whose practical success is made possible by our algorithmic tools and efficient implementations. First, we introduce a new CUR matrix factorization technique exploiting structured sparse regularization,  built upon the links drawn by~\citet{bien} between CUR decomposition~\citep{mahoney2009cur} and sparse regularization.  Then, we illustrate our algorithms with different tasks: video background subtraction, estimation of hierarchical structures for dictionary learning of natural image patches~\citep{jenatton3,jenatton4}, wavelet image denoising with a structured sparse prior, and topographic dictionary learning of natural image patches~\citep{hyvarinen2,kavukcuoglu2,garrigues}.	  \item We demonstrate that our methods are relevant for various applications whose practical success is made possible by our algorithmic tools and efficient implementations. First, we introduce a new CUR matrix factorization technique exploiting structured sparse regularization,  built upon the links drawn by~\citet{bien} between CUR decomposition~\citep{mahoney2009cur} and sparse regularization.  Then, we illustrate our algorithms with different tasks: video background subtraction, estimation of hierarchical structures for dictionary learning of natural image patches~\citep{jenatton3,jenatton4}, wavelet image denoising with a structured sparse prior, and topographic dictionary learning of natural image patches~\citep{hyvarinen2,kavukcuoglu2,garrigues}.	score:203
I have an intracluster correlation matrix, symmetric matrix capturing correlation between clusters (based on k means). What is a nice way to represent these clusters graphically using this matrix and the post intra-cluster correlation of each variable?	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	score:204
How can we choose a "good" K for K-means clustering?	 \item We extend Hartigan's method to  kernel spaces, which leads to a new clustering method that we call \emph{kernel k-groups}. \end{itemize} Furthermore, we show that kernel k-groups has the same complexity as kernel k-means, however, our numerical results provide compelling evidence that kernel k-groups is more accurate and robust, especially in high dimensions.	 A well-known kernel based clustering method is kernel k-means, which is precisely k-means formulated in the feature space \cite{Girolami}. Furthermore, kernel k-means algorithm \cite{Dhillon2,Dhillon} is still based on Loyd's heuristic. We refer the reader to \cite{Filippone} for a survey of clustering methods.  Besides Lloyd's approach to clustering  there is an old heuristic due to Hartigan \cite{Hartigan1975,Hartigan1979} that goes as follows: for each data point, simply assign it to a cluster in an optimal way such that a loss function is minimized.  While Lloyd's method only iterates if some cluster contains a point that is closer to the mean of another cluster, Hartigan's method may iterate even if that is not the case, and moreover, it takes into account the motion of the means resulting from the reassignments. In this sense, Hartigan's method may potentially escape local minima of Lloyd's method.	score:341
How can we choose a "good" K for K-means clustering?	 In Section~\ref{sec:msc}, we describe our novel heuristic merge-split-cluster $k$-means and report on its performances with respect to Hartigan's heuristic. In Section~\ref{sec:klmeans}, we present a generalization of the $k$-means objective function where each point is associated to its $l$ closest clusters: the $(k,l)$-means clustering. We show how to directl convert or iteratively relax a sequence of $(k,l)$-means to a $k$-means and compare experimentally those solutions with a direct $k$-means. Finally, Section~\ref{sec:dis} wrap ups the contributions and discusses further perspectives.	  Those events tend to increasingly occur when $k$ or $d$ increases, or when performing several restarts of the $k$-means heuristic with a different seeding  at each round in order to keep the best clustering in the lot. We show that those special events are a blessing because they allow to partially re-seed some cluster centers while further   minimizing the $k$-means objective function.   Second, we describe a novel heuristic, called merge-and-split $k$-means, that consists in merging two clusters and splitting this merged cluster again with two new centers provided it improves the $k$-means objective.  Hartigan's heuristic can improve a Lloyd's heuristic when it reaches a local minimum, and similarly this novel heuristic can improve Hartigan's $k$-means when it has converged to a local minimum.	score:342
How can we choose a "good" K for K-means clustering?	 Instead, our analysis relies on basic tools, such as the Markov inequality and the triangle inequality. In that sense one can view our work as ``aligning'' Kumar and Kannan's work with the rest of clustering-under-center-separation literature -- we show that the bulk of Kannan and Kumar's analysis can be simplified to rely merely on center-separation.     \paragraph{Our results.} We improve upon the results of~\cite{KumarK10} along several axes. In addition to the weaker condition of Equation~\eqref{eq:means_separation}, we also weaken the Kumar-Kannan proximity condition by a factor of $k$, and still retrieve the target clustering, if all points satisfy the ($k$-weaker) proximity condition. Secondly, if at most $\epsilon n$ points do not satisfy the $k$-weaker proximity condition, we show that we can correctly classify all but a $(\epsilon + O(1/c^4))$-fraction of the points, improving over the bound of~\cite{KumarK10} of $O(k^2\epsilon)$.  Note that our bound is meaningful even if $\epsilon$ is a constant whereas $k = \omega(1)$. Furthermore, we prove that the $k$-means cost of the clustering we output is a $(1+O(1/c))$-approximation of the $k$-means cost of the target clustering.  Once we have improved on the main theorem of Kumar and Kannan, we derive immediate improvements on its applications.  In Section~\ref{subsec:ORSS-improve} we show our analysis subsumes the work of Ostrovsky et al~\cite{Ostrovsky06}, and applies also to non-constant $k$. Using the fact that Equation~\eqref{eq:means_separation} ``shaves off'' a $\sqrt{k}$ factor from the separation condition of Kumar and Kannan, we obtain a separation condition of $\Omega(\sigma_{\max} \sqrt{k})$ for learning a mixture of Gaussians, and we also match the separation results of the Planted Partition model of McSherry~\cite{McSherry01}.	 Instead, our analysis relies on basic tools, such as the Markov inequality and the triangle inequality. In that sense one can view our work as ``aligning'' Kumar and Kannan's work with the rest of clustering-under-center-separation literature -- we show that the bulk of Kannan and Kumar's analysis can be simplified to rely merely on center-separation.     \paragraph{Our results.} We improve upon the results of~\cite{KumarK10} along several axes. In addition to the weaker condition of Equation~\eqref{eq:means_separation}, we also weaken the Kumar-Kannan proximity condition by a factor of $k$, and still retrieve the target clustering, if all points satisfy the ($k$-weaker) proximity condition. Secondly, if at most $\epsilon n$ points do not satisfy the $k$-weaker proximity condition, we show that we can correctly classify all but a $(\epsilon + O(1/c^4))$-fraction of the points, improving over the bound of~\cite{KumarK10} of $O(k^2\epsilon)$.  Note that our bound is meaningful even if $\epsilon$ is a constant whereas $k = \omega(1)$. Furthermore, we prove that the $k$-means cost of the clustering we output is a $(1+O(1/c))$-approximation of the $k$-means cost of the target clustering.  Once we have improved on the main theorem of Kumar and Kannan, we derive immediate improvements on its applications.  In Section~\ref{subsec:ORSS-improve} we show our analysis subsumes the work of Ostrovsky et al~\cite{Ostrovsky06}, and applies also to non-constant $k$. Using the fact that Equation~\eqref{eq:means_separation} ``shaves off'' a $\sqrt{k}$ factor from the separation condition of Kumar and Kannan, we obtain a separation condition of $\Omega(\sigma_{\max} \sqrt{k})$ for learning a mixture of Gaussians, and we also match the separation results of the Planted Partition model of McSherry~\cite{McSherry01}.	score:371
How can we choose a "good" K for K-means clustering?	  Our objective is therefore to obtain a good clustering of random variables based on an appropriate and simple enough distance for being used with basic clustering algorithms, e.g. Ward hierarchical clustering \citep{Inchoate:Ward63}, $k$-means++ \citep{kmeanspp}, affinity propagation \citep{frey2007clustering}.   By clustering we mean the task of grouping sets of objects in such a way that objects in the same cluster are more similar to each other than those in different clusters.  More specifically, a cluster of random variables should gather random variables with common dependence between them and with a common distribution. Two clusters should differ either in the dependency between their random variables or in their distributions.  A good clustering is a partition of the data that must be stable to small perturbations of the dataset.	  Our objective is therefore to obtain a good clustering of random variables based on an appropriate and simple enough distance for being used with basic clustering algorithms, e.g. Ward hierarchical clustering \citep{Inchoate:Ward63}, $k$-means++ \citep{kmeanspp}, affinity propagation \citep{frey2007clustering}.   By clustering we mean the task of grouping sets of objects in such a way that objects in the same cluster are more similar to each other than those in different clusters.  More specifically, a cluster of random variables should gather random variables with common dependence between them and with a common distribution. Two clusters should differ either in the dependency between their random variables or in their distributions.  A good clustering is a partition of the data that must be stable to small perturbations of the dataset.	score:380
How can we choose a "good" K for K-means clustering?	 Kernel k-groups is cheaper than spectral clustering and has the same computational cost as kernel k-means (which is based on Lloyd's heuristic) but our numerical results show an improved performance, especially in higher dimensions. Moreover, we verify the efficiency of kernel k-groups in community detection in sparse stochastic block models which has fascinating applications in several areas of science.  	}                             \IEEEPARstart{E}{nergy Statistics} \cite{Szkely2013,Szkely2017} is based on a notion of statistical potential energy between probability distributions, in close analogy to Newton's gravitational potential in classical mechanics. When probability distributions are different, the ``statistical potential energy'' diverges as sample size increases, while tends to a nondegenerate limit distribution when probability distributions are equal.	 A well-known kernel based clustering method is kernel k-means, which is precisely k-means formulated in the feature space \cite{Girolami}. Furthermore, kernel k-means algorithm \cite{Dhillon2,Dhillon} is still based on Loyd's heuristic. We refer the reader to \cite{Filippone} for a survey of clustering methods.  Besides Lloyd's approach to clustering  there is an old heuristic due to Hartigan \cite{Hartigan1975,Hartigan1979} that goes as follows: for each data point, simply assign it to a cluster in an optimal way such that a loss function is minimized.  While Lloyd's method only iterates if some cluster contains a point that is closer to the mean of another cluster, Hartigan's method may iterate even if that is not the case, and moreover, it takes into account the motion of the means resulting from the reassignments. In this sense, Hartigan's method may potentially escape local minima of Lloyd's method.	score:384
Should a machine learning beginner go straight for deep learning?	 Over the last decade, deep learning has become a disruptive advance in machine learning, giving new live to the long-standing connectionist paradigm in artificial intelligence. Deep learning methods are ideally suited to large-scale data and, therefore, they should be ideally suited to knowledge discovery in bioinformatics and biomedicine at large.  In this brief paper, we review key aspects of the application of deep learning in bioinformatics and medicine, drawing from the themes covered by the contributions to an ESANN 2018 special session devoted to this topic. 	Deep Learning (DL) \cite{Goodfellow-et-al-2016} has become an increasingly popular Machine Learning (ML) approach in the last decade, as shown in Fig.	 Over the last decade, deep learning has become a disruptive advance in machine learning, giving new live to the long-standing connectionist paradigm in artificial intelligence. Deep learning methods are ideally suited to large-scale data and, therefore, they should be ideally suited to knowledge discovery in bioinformatics and biomedicine at large.  In this brief paper, we review key aspects of the application of deep learning in bioinformatics and medicine, drawing from the themes covered by the contributions to an ESANN 2018 special session devoted to this topic. 	Deep Learning (DL) \cite{Goodfellow-et-al-2016} has become an increasingly popular Machine Learning (ML) approach in the last decade, as shown in Fig.	score:303
Should a machine learning beginner go straight for deep learning?	 In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input.  This paper reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.  	Reinforcement Learning (RL) \cite{kaelbling1996reinforcement,sutton1998introduction} is a branch of machine learning in which an agent learns from interacting with an environment.	 In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input.  This paper reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.  	Reinforcement Learning (RL) \cite{kaelbling1996reinforcement,sutton1998introduction} is a branch of machine learning in which an agent learns from interacting with an environment.	score:308
Should a machine learning beginner go straight for deep learning?	 \keywords{relational learning, deep learning, unsupervised representation learning, clustering} 	Latent representations created by deep learning approaches \cite{Goodfellow2016} have proven to be a powerful tool in machine learning. Traditional machine learning algorithms learn a function that directly maps data to the target concept. In contrast, deep learning creates several layers of latent features between the original data and the target concept.  This results in a multi-step procedure that simplifies a given task before solving it.    The progress in learning such latent representations has predominantly focused on vectorized data representations. Likewise, their utility has been recognized in the relational learning community \cite{Nickel0TG16} in which models are learned not only from instances but from their relationships as well \cite{Getoor2007,Muggleton94}.	 \keywords{relational learning, deep learning, unsupervised representation learning, clustering} 	Latent representations created by deep learning approaches \cite{Goodfellow2016} have proven to be a powerful tool in machine learning. Traditional machine learning algorithms learn a function that directly maps data to the target concept. In contrast, deep learning creates several layers of latent features between the original data and the target concept.  This results in a multi-step procedure that simplifies a given task before solving it.    The progress in learning such latent representations has predominantly focused on vectorized data representations. Likewise, their utility has been recognized in the relational learning community \cite{Nickel0TG16} in which models are learned not only from instances but from their relationships as well \cite{Getoor2007,Muggleton94}.	score:319
Should a machine learning beginner go straight for deep learning?	 	Machine learning is progressing quickly due to deep learning. The key tool for deep learning is crowd-sourcing, i.e., to the exploitation of human intelligence. Success stories demonstrate that superhuman performance can be reached this way \citep{schmidhuber2015deep}. Still, the groundbreaking deep network approach seems limited as `each application requires years of focused research and careful unique construction' \citep{AI2030}.  However, if we take a look at human information processing, for example, we learn that it has two basic routes: (i)  holistic recognition \citep{tanaka2011features} and (ii) recognition by components \citep{biederman1987recognition}. These processing methodologies are competing and also complementing each other. Deep learning methods, on the other hand, tend to favor end-to-end learning, which corresponds to holistic recognition and are missing the advantages of the component based approach.	 	Machine learning is progressing quickly due to deep learning. The key tool for deep learning is crowd-sourcing, i.e., to the exploitation of human intelligence. Success stories demonstrate that superhuman performance can be reached this way \citep{schmidhuber2015deep}. Still, the groundbreaking deep network approach seems limited as `each application requires years of focused research and careful unique construction' \citep{AI2030}.  However, if we take a look at human information processing, for example, we learn that it has two basic routes: (i)  holistic recognition \citep{tanaka2011features} and (ii) recognition by components \citep{biederman1987recognition}. These processing methodologies are competing and also complementing each other. Deep learning methods, on the other hand, tend to favor end-to-end learning, which corresponds to holistic recognition and are missing the advantages of the component based approach.	score:325
Should a machine learning beginner go straight for deep learning?	  \keywords{Deep Learning, Neural Network, Ensemble Learning} 	Deep learning is a machine learning method that uses layers of processing units where the output of a layer cascades to be the input of the next layer and can be applied to either supervised or unsupervised learning problems~\cite{Bengio2013RepresentationPerspectives}~\cite{Langkvist14areview}.  Deep neural networks (DNN) is an architecture of deep learning that typically has many connected units arranged in layers of varying sizes with information being fed forward through the network. DNN have been successfully applied to fields such as computer vision and natural language processing, having achieved accuracy rates similar or superior to humans in classification~\cite{Schmidhuber:2012:MDN:2354409.	  \keywords{Deep Learning, Neural Network, Ensemble Learning} 	Deep learning is a machine learning method that uses layers of processing units where the output of a layer cascades to be the input of the next layer and can be applied to either supervised or unsupervised learning problems~\cite{Bengio2013RepresentationPerspectives}~\cite{Langkvist14areview}.  Deep neural networks (DNN) is an architecture of deep learning that typically has many connected units arranged in layers of varying sizes with information being fed forward through the network. DNN have been successfully applied to fields such as computer vision and natural language processing, having achieved accuracy rates similar or superior to humans in classification~\cite{Schmidhuber:2012:MDN:2354409.	score:327
Why is deep learning so important in machine learning?	 The emergence of deep learning speeded up the development of machine learning and artificial intelligence. Consequently, deep learning has become a research hot spot in research organizations \cite{nature}. In general, deep learning uses a multi-layer neural network model to extract high-level features which are a combination of low-level abstractions to find the distributed data features, in order to solve complex problems in machine learning.	 As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks.  In order to improve the performance as well to maintain the low power cost, in this paper we design DLAU, which is a scalable accelerator architecture for large-scale deep learning networks using FPGA as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications.	score:399
Why is deep learning so important in machine learning?	  Recently, Deep Learning has been showing promising results in various Artificial Intelligence applications like image recognition, natural language processing, language modeling, neural machine translation, etc. Although, in general, it is computationally more expensive as compared to classical machine learning techniques, their results are found to be more effective in some cases.  Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 \& 7 layers architectures with the four different feature sets, and found that  irrespective of the features inputs, the classical RF accuracy outperforms the DNN.	  Recently, Deep Learning has been showing promising results in various Artificial Intelligence applications like image recognition, natural language processing, language modeling, neural machine translation, etc. Although, in general, it is computationally more expensive as compared to classical machine learning techniques, their results are found to be more effective in some cases.  Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 \& 7 layers architectures with the four different feature sets, and found that  irrespective of the features inputs, the classical RF accuracy outperforms the DNN.	score:401
Why is deep learning so important in machine learning?	 \keywords{relational learning, deep learning, unsupervised representation learning, clustering} 	Latent representations created by deep learning approaches \cite{Goodfellow2016} have proven to be a powerful tool in machine learning. Traditional machine learning algorithms learn a function that directly maps data to the target concept. In contrast, deep learning creates several layers of latent features between the original data and the target concept.  This results in a multi-step procedure that simplifies a given task before solving it.    The progress in learning such latent representations has predominantly focused on vectorized data representations. Likewise, their utility has been recognized in the relational learning community \cite{Nickel0TG16} in which models are learned not only from instances but from their relationships as well \cite{Getoor2007,Muggleton94}.	 \keywords{relational learning, deep learning, unsupervised representation learning, clustering} 	Latent representations created by deep learning approaches \cite{Goodfellow2016} have proven to be a powerful tool in machine learning. Traditional machine learning algorithms learn a function that directly maps data to the target concept. In contrast, deep learning creates several layers of latent features between the original data and the target concept.  This results in a multi-step procedure that simplifies a given task before solving it.    The progress in learning such latent representations has predominantly focused on vectorized data representations. Likewise, their utility has been recognized in the relational learning community \cite{Nickel0TG16} in which models are learned not only from instances but from their relationships as well \cite{Getoor2007,Muggleton94}.	score:411
Why is deep learning so important in machine learning?	 Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation.  In this pioneering paper, we propose the ``coding criterion'' to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations.	 Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation.  In this pioneering paper, we propose the ``coding criterion'' to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations.	score:411
Why is deep learning so important in machine learning?	 Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning \emph{can make mistakes} and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems.  We show that in limited contexts the required number of training samples can be low and self-improvement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications.	 Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning \emph{can make mistakes} and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems.  We show that in limited contexts the required number of training samples can be low and self-improvement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications.	score:416
How important is it to have frequent reunions with your batch mates from schools and colleges? Is organising a reunion easy?	  If we compare this to the way in which many problems in machine learning are phrased, there is a novel element here. Usually, a loss function is designed with a specific problem in mind, and so errors in the performance of that problem are de-facto important. But if we wish to construct an unsupervised technique, it should somehow decide on its own in a way inspired on the dependencies within the data itself what is asymptotically important and what errors are irrelevant.  For example, recent advances in image synthesis such as style transfer use error functions constructed out of intermediate layer activations of an object classifier network rather than working at the pixel level, with the result of minimizing perceptually meaningful inconsistencies rather than errors in the raw pixel values \cite{gatys2015neural,johnson2016perceptual}.	  If we compare this to the way in which many problems in machine learning are phrased, there is a novel element here. Usually, a loss function is designed with a specific problem in mind, and so errors in the performance of that problem are de-facto important. But if we wish to construct an unsupervised technique, it should somehow decide on its own in a way inspired on the dependencies within the data itself what is asymptotically important and what errors are irrelevant.  For example, recent advances in image synthesis such as style transfer use error functions constructed out of intermediate layer activations of an object classifier network rather than working at the pixel level, with the result of minimizing perceptually meaningful inconsistencies rather than errors in the raw pixel values \cite{gatys2015neural,johnson2016perceptual}.	score:298
How important is it to have frequent reunions with your batch mates from schools and colleges? Is organising a reunion easy?	 A learning process can be localized and performed over a holarchic structure in a recursive way without changing the core learning logic and without employing additional mechanisms to reconfigure the network. This is the proposed \emph{self-adaption approach} to decentralized learning that is by design highly reactive and cost-effective as it maximizes the utilization of the available communication and computational resources, in contrast to a complementary and more proactive \emph{self-organization approach} that requires additional interactions between agents and therefore can increase communication and computational cost.	 A learning process can be localized and performed over a holarchic structure in a recursive way without changing the core learning logic and without employing additional mechanisms to reconfigure the network. This is the proposed \emph{self-adaption approach} to decentralized learning that is by design highly reactive and cost-effective as it maximizes the utilization of the available communication and computational resources, in contrast to a complementary and more proactive \emph{self-organization approach} that requires additional interactions between agents and therefore can increase communication and computational cost.	score:300
How important is it to have frequent reunions with your batch mates from schools and colleges? Is organising a reunion easy?	 What is the best brand and what's reliable?","Weight and dimensions are important if you're planning to travel with the laptop. Get something with at least 512 mb of RAM. [..] is a good brand, and has an easy to use site where you can build a custom laptop."\\\hline   \end{tabular}     \caption{Examples of text samples and their labels.}       \label{figresults} \end{table*} The goal of natural language processing (NLP) is to process text with computers in order to analyze it, to extract information and eventually to represent the same information differently.	 What is the best brand and what's reliable?","Weight and dimensions are important if you're planning to travel with the laptop. Get something with at least 512 mb of RAM. [..] is a good brand, and has an easy to use site where you can build a custom laptop."\\\hline   \end{tabular}     \caption{Examples of text samples and their labels.}       \label{figresults} \end{table*} The goal of natural language processing (NLP) is to process text with computers in order to analyze it, to extract information and eventually to represent the same information differently.	score:301
How important is it to have frequent reunions with your batch mates from schools and colleges? Is organising a reunion easy?	 Further more, many regularization tricks such as weight decay~\cite{weightdecay}, dropout~\cite{dropout} or batch normalization~\cite{batchnorm} seem crucial in the training of DNN, whereas they lake for theoretical foundations enabling to interpret their effect.            In this article, we study the possibility to design a regularization scheme that can be applied efficiently to deep learning and that has theoretical motivations.  Our approach requires to define a regularization criterion, which is our first contribution. We claim that, for any model, the entropy of its intermediate representation, conditionally to the class variable, is a good criterion to apprehend the generalization potential of the model. More formally let's note $X \in \X$ the input variable, $C \in \C$ the class variable, $w$ the model parameters with $Y=h(w,X)$ the (deep) representation of the input leading to the class prediction.	 Further more, many regularization tricks such as weight decay~\cite{weightdecay}, dropout~\cite{dropout} or batch normalization~\cite{batchnorm} seem crucial in the training of DNN, whereas they lake for theoretical foundations enabling to interpret their effect.            In this article, we study the possibility to design a regularization scheme that can be applied efficiently to deep learning and that has theoretical motivations.  Our approach requires to define a regularization criterion, which is our first contribution. We claim that, for any model, the entropy of its intermediate representation, conditionally to the class variable, is a good criterion to apprehend the generalization potential of the model. More formally let's note $X \in \X$ the input variable, $C \in \C$ the class variable, $w$ the model parameters with $Y=h(w,X)$ the (deep) representation of the input leading to the class prediction.	score:304
How important is it to have frequent reunions with your batch mates from schools and colleges? Is organising a reunion easy?	 We formulate the problem as the learning of structural representations of social activities describing how the agents and their body-parts move. Such representation must contain a sufficient amount of information to execute the activity (e.g., how should it be decomposed? what body-parts are important? how should the body-parts move?), allowing its social affordance at each time frame to be computed by inferring the status of the activity and by computing the most appropriate motion to make the overall activity successful (Figure~\ref{fig:intro}).	 We formulate the problem as the learning of structural representations of social activities describing how the agents and their body-parts move. Such representation must contain a sufficient amount of information to execute the activity (e.g., how should it be decomposed? what body-parts are important? how should the body-parts move?), allowing its social affordance at each time frame to be computed by inferring the status of the activity and by computing the most appropriate motion to make the overall activity successful (Figure~\ref{fig:intro}).	score:308
Should I study formal methods for artificial intelligence & machine learning?	 To address this gap between computational modeling and behavioral psychology, we introduce here a mathematical framework for studying how behavior effects learning and develop a novel model of learning-driven exploration.  In Computational Neuroscience,  machine learning techniques have been successfully applied towards modeling how the brain might learn the structure underlying sensory signals, e. g., \cite{olshausen1996emergence,rehn2007network,lewicki2002efficient,5766096,1556155,serre2007robust,crutchfield1987equations}. Generally, these methods focus on passive learning where the learning system can not directly effect the sensory input it receives. Exploration, in contrast, is inherently active, and can only occur in the context of a closed-action perception loop.	 To address this gap between computational modeling and behavioral psychology, we introduce here a mathematical framework for studying how behavior effects learning and develop a novel model of learning-driven exploration.  In Computational Neuroscience,  machine learning techniques have been successfully applied towards modeling how the brain might learn the structure underlying sensory signals, e. g., \cite{olshausen1996emergence,rehn2007network,lewicki2002efficient,5766096,1556155,serre2007robust,crutchfield1987equations}. Generally, these methods focus on passive learning where the learning system can not directly effect the sensory input it receives. Exploration, in contrast, is inherently active, and can only occur in the context of a closed-action perception loop.	score:314
Should I study formal methods for artificial intelligence & machine learning?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:319
Should I study formal methods for artificial intelligence & machine learning?	  In~\cite{ostaszewski18approximation} we suggested that recurrent neural  networks can be used as a method for constructing such approximation. In this paper we scrutinize this approach in the context of classical machine  learning methods. We compare recurrent neural networks with geometrical  methods, and study the features and limitations of both approximation techniques.  In the first approach, the pulses are treated as time-series. For harnessing this representation we use an artificial neural network designed for time-series as the corresponding method~\cite{hochreiter1997long}. In the  second approach, we treat the control pulse vectors as points in space,  without assigning any meaning to the dimensions of the space.	  In~\cite{ostaszewski18approximation} we suggested that recurrent neural  networks can be used as a method for constructing such approximation. In this paper we scrutinize this approach in the context of classical machine  learning methods. We compare recurrent neural networks with geometrical  methods, and study the features and limitations of both approximation techniques.  In the first approach, the pulses are treated as time-series. For harnessing this representation we use an artificial neural network designed for time-series as the corresponding method~\cite{hochreiter1997long}. In the  second approach, we treat the control pulse vectors as points in space,  without assigning any meaning to the dimensions of the space.	score:321
Should I study formal methods for artificial intelligence & machine learning?	 In this paper we use machine learning (specifically a support vector machine) to select between heuristics for choosing a variable ordering, outperforming each of the separate heuristics.  \keywords{machine learning, support vector machine, symbolic computation, cylindrical algebraic decomposition, problem formulation}  	Cylindrical algebraic decomposition (CAD) is a key tool in real algebraic geometry.   It was first introduced by Collins \cite{Collins1975} to implement quantifier elimination over the reals, but has since been applied to applications including robot motion planning \cite{WDEB13}, programming with complex valued functions \cite{DBEW12}, optimisation \cite{FPM05} and epidemic modelling \cite{BENW06}.  Decision methods for real closed fields are of great use in theorem proving \cite{DSW98b}.	 In this paper we use machine learning (specifically a support vector machine) to select between heuristics for choosing a variable ordering, outperforming each of the separate heuristics.  \keywords{machine learning, support vector machine, symbolic computation, cylindrical algebraic decomposition, problem formulation}  	Cylindrical algebraic decomposition (CAD) is a key tool in real algebraic geometry.   It was first introduced by Collins \cite{Collins1975} to implement quantifier elimination over the reals, but has since been applied to applications including robot motion planning \cite{WDEB13}, programming with complex valued functions \cite{DBEW12}, optimisation \cite{FPM05} and epidemic modelling \cite{BENW06}.  Decision methods for real closed fields are of great use in theorem proving \cite{DSW98b}.	score:328
Should I study formal methods for artificial intelligence & machine learning?	  In essence, learning the relationship between an entity and its opposite entity for a given problem is a special case of \textit{a-priori knowledge}, which can be beneficial for computationally intelligent algorithms in stochastic setups. In context of machine learning algorithm, one may ask, why should effort be spent on extraction of the opposite relations when input-output relationship itself is not well defined?  However, various research on this topic has shown that simultaneous analysis of entities and their opposites can accelerate the task in focus -- since it allows the algorithm to harness the knowledge about symmetry in the solution domain thus allowing a better exploration of the solutions. \textit{Opposition-based Differential Evolution (ODE)}, however, seems to be the most successful oppositional inspired algorithm so far \cite{rahnamayan2008opposition}.	  In essence, learning the relationship between an entity and its opposite entity for a given problem is a special case of \textit{a-priori knowledge}, which can be beneficial for computationally intelligent algorithms in stochastic setups. In context of machine learning algorithm, one may ask, why should effort be spent on extraction of the opposite relations when input-output relationship itself is not well defined?  However, various research on this topic has shown that simultaneous analysis of entities and their opposites can accelerate the task in focus -- since it allows the algorithm to harness the knowledge about symmetry in the solution domain thus allowing a better exploration of the solutions. \textit{Opposition-based Differential Evolution (ODE)}, however, seems to be the most successful oppositional inspired algorithm so far \cite{rahnamayan2008opposition}.	score:333
What are the best recipes for small batch chocolate chip cookies?	 However, for large-scale data, such an approach  becomes inefficient and very costly. Thanks to the advent of online crowdsourcing services such as Amazon Mechanical Turk, a much more efficient way is to post unlabeled data to a crowdsourcing marketplace, where a big crowd of low-paid workers can be hired instantaneously to perform labeling tasks.    Despite of its high efficiency and immediate availability, crowd labeling raises many new challenges.	 However, for large-scale data, such an approach  becomes inefficient and very costly. Thanks to the advent of online crowdsourcing services such as Amazon Mechanical Turk, a much more efficient way is to post unlabeled data to a crowdsourcing marketplace, where a big crowd of low-paid workers can be hired instantaneously to perform labeling tasks.    Despite of its high efficiency and immediate availability, crowd labeling raises many new challenges.	score:418
What are the best recipes for small batch chocolate chip cookies?	  Hence, existing benchmarks are small; they mostly cover the head of the distributions of facts, and are restricted in their question types and their syntactic and lexical variations. As such, it is still unknown how much the existing systems perform outside the range of the specific question templates of a few, small benchmark datasets, and it is also unknown whether learning on a single dataset transfers well on other ones, and whether such systems can learn from different training sources, which we believe is necessary to capture the whole range of possible questions.   Besides, the actual need for reasoning, i.e. constructing the answer from more than a single fact from the KB, depends on the actual structure of the KB. As we shall see, for instance, a simple preprocessing of \fb tremendously increases the coverage of simple QA in terms of possible questions that can be answered with a single fact, including list questions that expect more than a single answer.	  Hence, existing benchmarks are small; they mostly cover the head of the distributions of facts, and are restricted in their question types and their syntactic and lexical variations. As such, it is still unknown how much the existing systems perform outside the range of the specific question templates of a few, small benchmark datasets, and it is also unknown whether learning on a single dataset transfers well on other ones, and whether such systems can learn from different training sources, which we believe is necessary to capture the whole range of possible questions.   Besides, the actual need for reasoning, i.e. constructing the answer from more than a single fact from the KB, depends on the actual structure of the KB. As we shall see, for instance, a simple preprocessing of \fb tremendously increases the coverage of simple QA in terms of possible questions that can be answered with a single fact, including list questions that expect more than a single answer.	score:422
What are the best recipes for small batch chocolate chip cookies?	 Therefore, the perplexity is much easier to choose by the user and much more robust to produce good embedding performance. We further use noise contrastive samples to avoid comparing data points with all exemplars, which further reduces computational/memory complexity and increases scalability. Although comparing training data points only with representative exemplars indirectly preserves similarities between pairwise data points in each local neighborhood, it is much better than randomly sampling small mini-batches in pt-SNE whose coverages are too small to capture all pairwise similarities on a large dataset.	 Fortunately, owing to the explicit mapping function defined by the DNN, optimization methods such as stochastic gradient descent or conjugate gradient descent based on mini-batches can be deployed when pt-SNE is applied to large-scale datasets.   However, on one hand, the objective function of pt-SNE is a sum of a quadratic number of terms over pairwise data points, which requires mini-batches with fairly large batch sizes to achieve a reasonably good approximation to the original objective; On the other hand, optimizing the parameters of the DNN in pt-SNE also requires careful choices of batch sizes, which is often best served with small batch sizes to avoid being stuck in a bad local minimum.	score:422
What are the best recipes for small batch chocolate chip cookies?	    Given that DPP-MAX is too greedy, it may be desirable to allow for uncertainty in the observations. Thus, we define DPP-SAMPLE which selects the batches via sampling subsets from DPPs, and show that the expected regret is smaller than that of DPP-MAX. To provide a fair comparison with an existing method, BUCB, we also derive regret bounds for B-EST [Theorem \ref{BESTThm}].  Finally, for all methods with known regret bounds, the key quantity is the information gain. In the appendix, we also provide a simpler proof of the information gain for the widely-used RBF kernel which also improves the bound from $\mathcal{O}((\log T)^{d+1})$~\cite{Seeger,Srinivas} to $\mathcal{O}((\log T)^{d})$. We conclude with experiments on synthetic and real-world robotics and hyper-parameter optimization for extreme multi-label classification tasks which demonstrate that our DPP-based methods, especially the sampling based ones are superior or competitive with the existing baselines.	    Given that DPP-MAX is too greedy, it may be desirable to allow for uncertainty in the observations. Thus, we define DPP-SAMPLE which selects the batches via sampling subsets from DPPs, and show that the expected regret is smaller than that of DPP-MAX. To provide a fair comparison with an existing method, BUCB, we also derive regret bounds for B-EST [Theorem \ref{BESTThm}].  Finally, for all methods with known regret bounds, the key quantity is the information gain. In the appendix, we also provide a simpler proof of the information gain for the widely-used RBF kernel which also improves the bound from $\mathcal{O}((\log T)^{d+1})$~\cite{Seeger,Srinivas} to $\mathcal{O}((\log T)^{d})$. We conclude with experiments on synthetic and real-world robotics and hyper-parameter optimization for extreme multi-label classification tasks which demonstrate that our DPP-based methods, especially the sampling based ones are superior or competitive with the existing baselines.	score:423
What are the best recipes for small batch chocolate chip cookies?	 Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project.  In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels.	 Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project.  In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels.	score:427
Is CNN biased against Trump?	 (\expandafter{\romannumeral2}) CNN has the ability to capture the frequency shift in human speech signal.  Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks \cite{cnn7,cnn8,cnn9,cnn10}. Most of previous application of CNNs in speech recognition only used fewer convolutional layers.  For example, Abdel-Hamid~et~al. \cite{cnn1} used one convolutional layer, one pooling layer and a few full-connected layers. Amodei~et~al. \cite{rnn2} also used only three convolutional layers as the feature preprocessing layers. Some researcher has shown that CNN-based speech recognition which uses raw speech as input can be more robust \cite{cnn11}, and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks \cite{cnn2,cnn3,cnn9,cnn14}.	 (\expandafter{\romannumeral2}) CNN has the ability to capture the frequency shift in human speech signal.  Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks \cite{cnn7,cnn8,cnn9,cnn10}. Most of previous application of CNNs in speech recognition only used fewer convolutional layers.  For example, Abdel-Hamid~et~al. \cite{cnn1} used one convolutional layer, one pooling layer and a few full-connected layers. Amodei~et~al. \cite{rnn2} also used only three convolutional layers as the feature preprocessing layers. Some researcher has shown that CNN-based speech recognition which uses raw speech as input can be more robust \cite{cnn11}, and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks \cite{cnn2,cnn3,cnn9,cnn14}.	score:502
Is CNN biased against Trump?	 The proposed NDDR layer combines existing CNN components in a novel way, which possesses clear mathematical interpretability as discriminative dimensionality reduction. Moreover, the use of the existing CNN components is desirable to guarantee the extensibility of our method to various state-of-the-art CNN architectures, where the proposed NDDR layer can be used in a ``plug-and-play'' manner.  The rest of this paper is organized as follows. First, we describe the NDDR layer and propose a novel NDDR-CNN as well as its variant NDDR-CNN-Shortcut for MTL in Sect. \ref{Sect:methods}. After that, we discuss the related works in Sect. \ref{Sect:related_works}, where we show that our method can generalize several state-of-the-art methods, which can be treated as our special cases.	 Then, we show that the discriminative dimensionality reduction can be fulfilled by 1 $\times$ 1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use of existing CNN components ensures the end-to-end training and the extensibility of the proposed NDDR layer to various state-of-the-art CNN architectures in a ``plug-and-play'' manner.  The detailed ablation analysis shows that the proposed NDDR layer is easy to train and also robust to different hyperparameters. Experiments on different task sets with various base network architectures demonstrate the promising performance and desirable generalizability of our proposed method. The code of our paper is available at \url{https://github.	score:506
Is CNN biased against Trump?	 Most representative progress was made by \newcite{zeng14}, who proposed a CNN-based approach that can deliver quite competitive results without any extra knowledge resource and NLP modules. Following the success of CNN, there are some valuable models  such as multi-window CNN\cite{nguyenrelation}, CR-CNN\cite{dos2015classifying} and NS-depLCNN\cite{xu2015semantic} been proposed recently,  which are all based on CNN structure.  Though some models also based on other structures like MV-RNN\cite{SocherEtAl2012:MVRNN}, FCM\cite{yufactor14} and SDP-LSTM\cite{xu2015classifying}, CNN occupies a leading position.  Despite the success obtained so far, most of the current CNN-based learning approaches to relation learning and classification are weak in modeling temporal patterns. Though SDP-LSTM algorithm utilize recurrent structure on dependency parsing, no further analysis has been shown to compare RNN with CNN models.	 Most representative progress was made by \newcite{zeng14}, who proposed a CNN-based approach that can deliver quite competitive results without any extra knowledge resource and NLP modules. Following the success of CNN, there are some valuable models  such as multi-window CNN\cite{nguyenrelation}, CR-CNN\cite{dos2015classifying} and NS-depLCNN\cite{xu2015semantic} been proposed recently,  which are all based on CNN structure.  Though some models also based on other structures like MV-RNN\cite{SocherEtAl2012:MVRNN}, FCM\cite{yufactor14} and SDP-LSTM\cite{xu2015classifying}, CNN occupies a leading position.  Despite the success obtained so far, most of the current CNN-based learning approaches to relation learning and classification are weak in modeling temporal patterns. Though SDP-LSTM algorithm utilize recurrent structure on dependency parsing, no further analysis has been shown to compare RNN with CNN models.	score:506
Is CNN biased against Trump?	  To overcome the windowing issue, we propose to use a convolutional neural network (CNN) which can label the properties of each amino acid in the entire target sequence all at once. CNNs have been used successfully in computer vision \cite{pinheiro_recurrent_2013,szegedy_going_2014} and natural language processing \cite{kim_convolutional_2014,collobert_unified_2008}.  In addition to parameter sharing and pooling, which reduce computation, CNNs are also highly parallelizable.  Thus, CNNs can achieve a much greater speedup compared to a windowed MLP approach. The issue when trying to label each position in an input sequence with a CNN is that pooling leads to a decreased output resolution. To handle this issue, we propose a new multilayer shift-and-stitch method which allows us to efficiently label each target input at full resolution in a  computationally efficient manner.	  To overcome the windowing issue, we propose to use a convolutional neural network (CNN) which can label the properties of each amino acid in the entire target sequence all at once. CNNs have been used successfully in computer vision \cite{pinheiro_recurrent_2013,szegedy_going_2014} and natural language processing \cite{kim_convolutional_2014,collobert_unified_2008}.  In addition to parameter sharing and pooling, which reduce computation, CNNs are also highly parallelizable.  Thus, CNNs can achieve a much greater speedup compared to a windowed MLP approach. The issue when trying to label each position in an input sequence with a CNN is that pooling leads to a decreased output resolution. To handle this issue, we propose a new multilayer shift-and-stitch method which allows us to efficiently label each target input at full resolution in a  computationally efficient manner.	score:508
Is CNN biased against Trump?	 \citet{invert-cnn} used upsampling-deconvolutional architectures to invert the hidden activations of feedforward CNNs to the input domain.  In another related work, \citet{what-where} proposed a stacked what-where autoencoder network and demonstrated its promise in unsupervised and semi-supervised settings.  \citet{deconv-recon} showed that CNNs discriminately trained for image classification (e. g., VGGNet~\citep{vggnet}) are almost fully invertible using pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why CNNs are invertible yet.  \textgray{ Our setup is also similar to recent work of Zhang et al. (http://bit.ly/2a6sR) which empirically shows that CNNs are almost fully invertible using pooling switches (it also shows that encouraging this property improves large-scale image classification).	 \citet{invert-cnn} used upsampling-deconvolutional architectures to invert the hidden activations of feedforward CNNs to the input domain.  In another related work, \citet{what-where} proposed a stacked what-where autoencoder network and demonstrated its promise in unsupervised and semi-supervised settings.  \citet{deconv-recon} showed that CNNs discriminately trained for image classification (e. g., VGGNet~\citep{vggnet}) are almost fully invertible using pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why CNNs are invertible yet.  \textgray{ Our setup is also similar to recent work of Zhang et al. (http://bit.ly/2a6sR) which empirically shows that CNNs are almost fully invertible using pooling switches (it also shows that encouraging this property improves large-scale image classification).	score:514
Is Donald Trump right that CNN is biased against him?	 (\expandafter{\romannumeral2}) CNN has the ability to capture the frequency shift in human speech signal.  Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks \cite{cnn7,cnn8,cnn9,cnn10}. Most of previous application of CNNs in speech recognition only used fewer convolutional layers.  For example, Abdel-Hamid~et~al. \cite{cnn1} used one convolutional layer, one pooling layer and a few full-connected layers. Amodei~et~al. \cite{rnn2} also used only three convolutional layers as the feature preprocessing layers. Some researcher has shown that CNN-based speech recognition which uses raw speech as input can be more robust \cite{cnn11}, and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks \cite{cnn2,cnn3,cnn9,cnn14}.	 (\expandafter{\romannumeral2}) CNN has the ability to capture the frequency shift in human speech signal.  Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks \cite{cnn7,cnn8,cnn9,cnn10}. Most of previous application of CNNs in speech recognition only used fewer convolutional layers.  For example, Abdel-Hamid~et~al. \cite{cnn1} used one convolutional layer, one pooling layer and a few full-connected layers. Amodei~et~al. \cite{rnn2} also used only three convolutional layers as the feature preprocessing layers. Some researcher has shown that CNN-based speech recognition which uses raw speech as input can be more robust \cite{cnn11}, and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks \cite{cnn2,cnn3,cnn9,cnn14}.	score:456
Is Donald Trump right that CNN is biased against him?	 \citet{invert-cnn} used upsampling-deconvolutional architectures to invert the hidden activations of feedforward CNNs to the input domain.  In another related work, \citet{what-where} proposed a stacked what-where autoencoder network and demonstrated its promise in unsupervised and semi-supervised settings.  \citet{deconv-recon} showed that CNNs discriminately trained for image classification (e. g., VGGNet~\citep{vggnet}) are almost fully invertible using pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why CNNs are invertible yet.  \textgray{ Our setup is also similar to recent work of Zhang et al. (http://bit.ly/2a6sR) which empirically shows that CNNs are almost fully invertible using pooling switches (it also shows that encouraging this property improves large-scale image classification).	 \citet{invert-cnn} used upsampling-deconvolutional architectures to invert the hidden activations of feedforward CNNs to the input domain.  In another related work, \citet{what-where} proposed a stacked what-where autoencoder network and demonstrated its promise in unsupervised and semi-supervised settings.  \citet{deconv-recon} showed that CNNs discriminately trained for image classification (e. g., VGGNet~\citep{vggnet}) are almost fully invertible using pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why CNNs are invertible yet.  \textgray{ Our setup is also similar to recent work of Zhang et al. (http://bit.ly/2a6sR) which empirically shows that CNNs are almost fully invertible using pooling switches (it also shows that encouraging this property improves large-scale image classification).	score:470
Is Donald Trump right that CNN is biased against him?	 Thanks to the condition, even with the bad control-knob that keeps oscillating the probabilistic accuracy up and down, we can preserve the privacy of CNNs in a certain level that the condition designates.  This paper is organized as follows: Section~\ref{sec:problem_description} describes the problem that controls the privacy loss of CNN with the IFMs of layers.  In Section~\ref{sec:proposed}, the degree of sanitization is introduced as the boundary condition that the method of decreasing the probabilistic accuracy should satisfy. Also, the IFM approximation scheme that reduces the accuracy and its network-wise control method are proposed. Section~\ref{sec:evaluation} evaluates the proposed scheme on the layers of AlexNet in Caffe~\cite{caffe} CNN framework. Finally, Section~\ref{sec:conclusion} concludes with the summary of our contribution.	 Also, some representation that does not make any sense for human may work for CNN~\cite{DNN_fool}. One thing obvious is that CNN learns all the things from the data fed into it. This implies what CNN recognizes as a noise comes from the data set. In this paper, we mainly focus on how to add the effective noise that disrupts CNN's recognition for the purpose of privacy protection.   Differential privacy~\cite{cynthia} is the mathematical definition that has been introduced to measure the privacy loss for the domains that handle massive user data such as data mining applications.  Google~\cite{rappor} and Apple~\cite{apple} have adopted the differential privacy techniques into their crowdsourced applications with some data sketching technique to approximate users' private data.	score:479
Is Donald Trump right that CNN is biased against him?	 Most representative progress was made by \newcite{zeng14}, who proposed a CNN-based approach that can deliver quite competitive results without any extra knowledge resource and NLP modules. Following the success of CNN, there are some valuable models  such as multi-window CNN\cite{nguyenrelation}, CR-CNN\cite{dos2015classifying} and NS-depLCNN\cite{xu2015semantic} been proposed recently,  which are all based on CNN structure.  Though some models also based on other structures like MV-RNN\cite{SocherEtAl2012:MVRNN}, FCM\cite{yufactor14} and SDP-LSTM\cite{xu2015classifying}, CNN occupies a leading position.  Despite the success obtained so far, most of the current CNN-based learning approaches to relation learning and classification are weak in modeling temporal patterns. Though SDP-LSTM algorithm utilize recurrent structure on dependency parsing, no further analysis has been shown to compare RNN with CNN models.	 Most representative progress was made by \newcite{zeng14}, who proposed a CNN-based approach that can deliver quite competitive results without any extra knowledge resource and NLP modules. Following the success of CNN, there are some valuable models  such as multi-window CNN\cite{nguyenrelation}, CR-CNN\cite{dos2015classifying} and NS-depLCNN\cite{xu2015semantic} been proposed recently,  which are all based on CNN structure.  Though some models also based on other structures like MV-RNN\cite{SocherEtAl2012:MVRNN}, FCM\cite{yufactor14} and SDP-LSTM\cite{xu2015classifying}, CNN occupies a leading position.  Despite the success obtained so far, most of the current CNN-based learning approaches to relation learning and classification are weak in modeling temporal patterns. Though SDP-LSTM algorithm utilize recurrent structure on dependency parsing, no further analysis has been shown to compare RNN with CNN models.	score:483
Is Donald Trump right that CNN is biased against him?	 The proposed NDDR layer combines existing CNN components in a novel way, which possesses clear mathematical interpretability as discriminative dimensionality reduction. Moreover, the use of the existing CNN components is desirable to guarantee the extensibility of our method to various state-of-the-art CNN architectures, where the proposed NDDR layer can be used in a ``plug-and-play'' manner.  The rest of this paper is organized as follows. First, we describe the NDDR layer and propose a novel NDDR-CNN as well as its variant NDDR-CNN-Shortcut for MTL in Sect. \ref{Sect:methods}. After that, we discuss the related works in Sect. \ref{Sect:related_works}, where we show that our method can generalize several state-of-the-art methods, which can be treated as our special cases.	 In order to tackle this issue, Misra \etal used trainable \emph{scalars} to weighted-sum the features from different tasks at multiple CNN levels and achieved state-of-the-art performance \cite{misra2016cross}.  We consider this problem in another way, \ie, by leveraging all the hierarchical features from different tasks. This is because that the CNN layers trained by different tasks can be treated as different feature descriptors, therefore the features learned from them can be treated as different representations/views of input data.	score:486
Are convolutional neural networks also deep networks?	 In recent years, deep neural networks have achieved great success in the field of computer vision. However, it is still a big challenge to deploy these deep models on resource-constrained embedded devices such as mobile robots, smart phones and so on. Therefore, network compression for such platforms is a reasonable solution to reduce memory consumption and computation complexity.  In this paper, a novel channel pruning method based on genetic algorithm is proposed to compress very deep Convolution Neural Networks (CNNs). Firstly, a pre-trained CNN model is pruned layer by layer according to the sensitivity of each layer. After that, the pruned model is fine-tuned based on knowledge distillation framework. These two improvements significantly decrease the model redundancy with less accuracy drop.	 In recent years, deep neural networks have achieved great success in the field of computer vision. However, it is still a big challenge to deploy these deep models on resource-constrained embedded devices such as mobile robots, smart phones and so on. Therefore, network compression for such platforms is a reasonable solution to reduce memory consumption and computation complexity.  In this paper, a novel channel pruning method based on genetic algorithm is proposed to compress very deep Convolution Neural Networks (CNNs). Firstly, a pre-trained CNN model is pruned layer by layer according to the sensitivity of each layer. After that, the pruned model is fine-tuned based on knowledge distillation framework. These two improvements significantly decrease the model redundancy with less accuracy drop.	score:369
Are convolutional neural networks also deep networks?	  Deep neural network algorithms are difficult to analyze because they lack structure allowing to understand the properties of underlying transforms and invariants.  Multiscale hierarchical convolutional networks are structured deep convolutional networks where layers are indexed by progressively higher dimensional attributes, which are learned from training data.  Each new layer is computed with multidimensional convolutions along spatial and attribute variables. We introduce an efficient implementation of such networks where the dimensionality is progressively reduced by averaging intermediate layers along attribute indices. Hierarchical networks are tested on CIFAR image data bases where they obtain comparable precisions to state of the art networks, with much fewer parameters.	  Deep neural network algorithms are difficult to analyze because they lack structure allowing to understand the properties of underlying transforms and invariants.  Multiscale hierarchical convolutional networks are structured deep convolutional networks where layers are indexed by progressively higher dimensional attributes, which are learned from training data.  Each new layer is computed with multidimensional convolutions along spatial and attribute variables. We introduce an efficient implementation of such networks where the dimensionality is progressively reduced by averaging intermediate layers along attribute indices. Hierarchical networks are tested on CIFAR image data bases where they obtain comparable precisions to state of the art networks, with much fewer parameters.	score:394
Are convolutional neural networks also deep networks?	 \item We introduce a normalized data representation, which can be used in Neural Network algorithms, or other methods.  \item We present a Deep Convolutional Network, which can be able to learn traffic flow of different traffic points. \item Then, we present a Recurrent Neural Network, which, apart from its structure, can do the same as Convolutional Network.  \item Both of these models are able to predict \textit{n}-level traffic prediction for different points of the traffic (e.g. Quiet, light traffic, heavy traffic, congested, etc.) \item They also put up the predicted average speeds on different points of the traffic network based on the speed limits in that point (e.g. 0.65 of speed limit). \item Then, we present a Recurrent Neural Network \end{enumerate}  In the rest of this paper, we start our work by some preliminaries in section \ref{prelim}.	 \item We introduce a normalized data representation, which can be used in Neural Network algorithms, or other methods.  \item We present a Deep Convolutional Network, which can be able to learn traffic flow of different traffic points. \item Then, we present a Recurrent Neural Network, which, apart from its structure, can do the same as Convolutional Network.  \item Both of these models are able to predict \textit{n}-level traffic prediction for different points of the traffic (e.g. Quiet, light traffic, heavy traffic, congested, etc.) \item They also put up the predicted average speeds on different points of the traffic network based on the speed limits in that point (e.g. 0.65 of speed limit). \item Then, we present a Recurrent Neural Network \end{enumerate}  In the rest of this paper, we start our work by some preliminaries in section \ref{prelim}.	score:397
Are convolutional neural networks also deep networks?	 This discovery reveals the limitations of many existing deep learning architectures for inverse problems, and leads us to propose a novel theory for {\em deep convolutional framelets}  neural network. Using numerical experiments with various inverse problems,  we demonstrated that our deep convolution framelets network  shows consistent improvement over   existing deep architectures.	 This discovery reveals the limitations of many existing deep learning architectures for inverse problems, and leads us to propose a novel theory for {\em deep convolutional framelets}  neural network. Using numerical experiments with various inverse problems,  we demonstrated that our deep convolution framelets network  shows consistent improvement over   existing deep architectures.	score:399
Are convolutional neural networks also deep networks?	 This report will show the history of deep learning evolves. It will trace back as far as the initial belief of connectionism modelling of brain, and come back to look at its early stage realization: neural networks. With the background of neural network, we will gradually introduce how convolutional neural network, as a representative of deep discriminative models, is developed from neural networks, together with many practical techniques that can help in optimization of neural networks.   On the other hand, we will also trace back to see the evolution history of deep generative models, to see how researchers balance the representation power and computation complexity to reach Restricted Boltzmann Machine and eventually reach Deep Belief Nets.  Further, we will also look into the development history of modelling time series data with neural networks.	 This report will show the history of deep learning evolves. It will trace back as far as the initial belief of connectionism modelling of brain, and come back to look at its early stage realization: neural networks. With the background of neural network, we will gradually introduce how convolutional neural network, as a representative of deep discriminative models, is developed from neural networks, together with many practical techniques that can help in optimization of neural networks.   On the other hand, we will also trace back to see the evolution history of deep generative models, to see how researchers balance the representation power and computation complexity to reach Restricted Boltzmann Machine and eventually reach Deep Belief Nets.  Further, we will also look into the development history of modelling time series data with neural networks.	score:400
I am a beginner in the area of machine learning. What papers can I go through to get started? Also, what projects can I take up on my own and complete quickly so that I can approach professors?	 Similar ideas have recently been explored in the context of convex optimization \cite{donti2017task}, but to our knowledge ours is the first attempt to train machine learning systems for performance on \emph{combinatorial} decision-making problems. Combinatorial settings raise new technical challenges because the optimization problem is discrete. However, machine learning systems (e. g., deep neural networks) are often trained via gradient descent.     Our first contribution is a general framework for training machine learning models via their performance on combinatorial problems. The starting point is to relax the combinatorial problem to a continuous one. Then, we analytically differentiate the optimal solution to the continuous problem as a function of the model's predictions.	 Similar ideas have recently been explored in the context of convex optimization \cite{donti2017task}, but to our knowledge ours is the first attempt to train machine learning systems for performance on \emph{combinatorial} decision-making problems. Combinatorial settings raise new technical challenges because the optimization problem is discrete. However, machine learning systems (e. g., deep neural networks) are often trained via gradient descent.     Our first contribution is a general framework for training machine learning models via their performance on combinatorial problems. The starting point is to relax the combinatorial problem to a continuous one. Then, we analytically differentiate the optimal solution to the continuous problem as a function of the model's predictions.	score:318
I am a beginner in the area of machine learning. What papers can I go through to get started? Also, what projects can I take up on my own and complete quickly so that I can approach professors?	  Recently, it has been shown that the property that the same vertices tend to appear many times in the training data can be exploited in order to develop much faster Kronecker product kernel method training algorithms.  Several efficient machine learning algorithms have been proposed for the special case, where the training sets consists of a complete bipartite graph, meaning that each possible start-end vertex pair appears exactly once, and a ridge regression loss is minimized.  Specifically \cite{Raymond2010scalable,pahikkala2010conditional,pahikkala2013efficient,pahikkala2014twostep,bernardino2015emabarrassing} derive closed form solutions based on Kronecker algebraic optimization (see also \cite{martin2006shiftedkron} for the basic mathematical results underlying these studies). Further, iterative methods based on Kronecker product kernel matrix - vector multiplications, have been proposed (see e.	  	This work concerns the problem of learning supervised machine learning models from labeled bipartite graphs.  Given a training set $(\bm{d}_i,\bm{t}_j,y_h)_{h=1}^\tsize$ of edges $(\bm{d}_i,\bm{t}_j)$, where $\bm{d}_i$ is the start vertex, and $\bm{t}_j$ the end vertex and $y_h$ the label, the goal is to learn to predict labels for new edges with unknown labels.  We assume that both the start and end vertices have their own feature representations.  Further, we assume that the same vertices tend to appear as start or end vertices in multiple edges (for example, $(\bm{d}_t,\bm{t}_v)$, $(\bm{d}_u,\bm{t}_w)$, $(\bm{d}_t,\bm{t}_w)$ and $(\bm{d}_u,\bm{t}_v)$ might all belong to the same training set). This latter property is what differentiates this learning setting from the standard supervised learning setting, as the data violates the very basic i.	score:319
I am a beginner in the area of machine learning. What papers can I go through to get started? Also, what projects can I take up on my own and complete quickly so that I can approach professors?	 To create local targets for the model's higher-level objectives, we show that simple fixed projection functions can be used to create special error units that can generate local targets.      \item To evaluate our model and learning algorithm, we experiment with a video modeling problem and discover promising results with our learning approach. \end{itemize} This work can also be viewed as another contribution towards the long-term goal of finding more biologically plausible machine learning approaches to the credit assignment problem \cite{bengio2015towards}. Specifically, we offer a rather simple approach to implementing and training sequential predictive neural models without back-propagation through time.	 To create local targets for the model's higher-level objectives, we show that simple fixed projection functions can be used to create special error units that can generate local targets.      \item To evaluate our model and learning algorithm, we experiment with a video modeling problem and discover promising results with our learning approach. \end{itemize} This work can also be viewed as another contribution towards the long-term goal of finding more biologically plausible machine learning approaches to the credit assignment problem \cite{bengio2015towards}. Specifically, we offer a rather simple approach to implementing and training sequential predictive neural models without back-propagation through time.	score:320
I am a beginner in the area of machine learning. What papers can I go through to get started? Also, what projects can I take up on my own and complete quickly so that I can approach professors?	 However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks.	 However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks.	score:327
I am a beginner in the area of machine learning. What papers can I go through to get started? Also, what projects can I take up on my own and complete quickly so that I can approach professors?	  Instead of the conventional method of teaching computers how to play chess by giving them hardcoded rules, this project is an attempt to use machine learning to figure out how to play chess through self-play, and have them derive their own rules from the games.  Using multiple deep artificial neural networks trained in a temporal-difference reinforcement learning framework, we use machine learning to assist the engine in making decisions in a few places - \begin{itemize} \item Statically evaluating positions - estimating how good a position is without looking further \item Deciding which branches are most "interesting" in any given position, and should be searched further, as well as which branches to discard \item Ordering moves - determining which moves to search before others, which significantly affects efficiency of searches \end{itemize}  Using artificial neural networks as a substitute for "intuition", we hope to create a machine that can play chess more efficiently.	  Instead of the conventional method of teaching computers how to play chess by giving them hardcoded rules, this project is an attempt to use machine learning to figure out how to play chess through self-play, and have them derive their own rules from the games.  Using multiple deep artificial neural networks trained in a temporal-difference reinforcement learning framework, we use machine learning to assist the engine in making decisions in a few places - \begin{itemize} \item Statically evaluating positions - estimating how good a position is without looking further \item Deciding which branches are most "interesting" in any given position, and should be searched further, as well as which branches to discard \item Ordering moves - determining which moves to search before others, which significantly affects efficiency of searches \end{itemize}  Using artificial neural networks as a substitute for "intuition", we hope to create a machine that can play chess more efficiently.	score:327
Which are best machine learning courses which gives free certification?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:361
Which are best machine learning courses which gives free certification?	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	score:362
Which are best machine learning courses which gives free certification?	 In this paper we describe the environment with ToriLLE's capabilities and limitations, and experimentally show its applicability as a learning environment with baseline and human experiments. The source code of the environment and conducted experiments can be found at \url{https://github.com/Miffyli/ToriLLE}. 	Video games provide a rich and complex environments for training machine learning agents, without limiting them to real-time progression like with robotics.  Popularity of such learning environments can be seen from the number of different video games used for such purpose, such as Atari games \cite{gym}, Doom \cite{vizdoom}, Quake \cite{dmlab} and Starcraft \cite{torchcraft,sc2}. These environments provide challenges for agents to complete in single-agent scenarios, which allows comparing performance of different learning methods, for example.	 In this paper we describe the environment with ToriLLE's capabilities and limitations, and experimentally show its applicability as a learning environment with baseline and human experiments. The source code of the environment and conducted experiments can be found at \url{https://github.com/Miffyli/ToriLLE}. 	Video games provide a rich and complex environments for training machine learning agents, without limiting them to real-time progression like with robotics.  Popularity of such learning environments can be seen from the number of different video games used for such purpose, such as Atari games \cite{gym}, Doom \cite{vizdoom}, Quake \cite{dmlab} and Starcraft \cite{torchcraft,sc2}. These environments provide challenges for agents to complete in single-agent scenarios, which allows comparing performance of different learning methods, for example.	score:365
Which are best machine learning courses which gives free certification?	 However, as machine learning systems are deployed in increasingly large-scale, autonomous, open-domain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.   There has been a great deal of public discussion around accidents.  To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents \cite{bostrom2014superintelligence}.  However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics \cite{davis2015ethical,lawrence2016discussion}.	 However, as machine learning systems are deployed in increasingly large-scale, autonomous, open-domain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.   There has been a great deal of public discussion around accidents.  To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents \cite{bostrom2014superintelligence}.  However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics \cite{davis2015ethical,lawrence2016discussion}.	score:372
Which are best machine learning courses which gives free certification?	 This paradigm creates a bottleneck where the knowledge accumulated by machines depends on highly trained human experts to interpret, and to conveying to humans. This remains a barrier to the broader usefulness of machine learning. While machines can communicate perfectly among themselves by exchanging bits, humans communicate with data---they teach.  The purposeful selection of data plays a featured role in theories of cognition \citep{sperber1986relevance}, cognitive development \citep{Gergely2007}, and culture \citep{Tomasello2005}. In each of these cases, teaching is conceived of as purposeful, rather than random, selection of small set of examples, with the goal of facilitating accurate inferences about a body of knowledge.	 This paradigm creates a bottleneck where the knowledge accumulated by machines depends on highly trained human experts to interpret, and to conveying to humans. This remains a barrier to the broader usefulness of machine learning. While machines can communicate perfectly among themselves by exchanging bits, humans communicate with data---they teach.  The purposeful selection of data plays a featured role in theories of cognition \citep{sperber1986relevance}, cognitive development \citep{Gergely2007}, and culture \citep{Tomasello2005}. In each of these cases, teaching is conceived of as purposeful, rather than random, selection of small set of examples, with the goal of facilitating accurate inferences about a body of knowledge.	score:373
What are some good books or references to get started with machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:392
What are some good books or references to get started with machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:401
What are some good books or references to get started with machine learning?	 In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model.  We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process.	 In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model.  We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process.	score:415
What are some good books or references to get started with machine learning?	 However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks.	 However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks.	score:416
What are some good books or references to get started with machine learning?	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:418
How can I understand machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:421
How can I understand machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:438
How can I understand machine learning?	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	score:440
How can I understand machine learning?	 However, as machine learning systems are deployed in increasingly large-scale, autonomous, open-domain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.   There has been a great deal of public discussion around accidents.  To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents \cite{bostrom2014superintelligence}.  However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics \cite{davis2015ethical,lawrence2016discussion}.	 However, as machine learning systems are deployed in increasingly large-scale, autonomous, open-domain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.   There has been a great deal of public discussion around accidents.  To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents \cite{bostrom2014superintelligence}.  However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics \cite{davis2015ethical,lawrence2016discussion}.	score:451
How can I understand machine learning?	  Deep learning frameworks such as TensorFlow \citep{tensorflow} support distributed training, making large scale machine learning systems easier to implement and deploy. Despite this, much current research in deep reinforcement learning concerns itself with improving performance within the computational budget of a single machine, and the question of how to best harness more resources is comparatively underexplored.   In this paper we describe an approach to scaling up deep reinforcement learning by generating more data and selecting from it in a prioritized fashion \citep{prioritized-replay}. Standard approaches to distributed training of neural networks focus on parallelizing the computation of gradients, to more rapidly optimize the parameters \citep{Dean:2012:LSD:2999134.	  Deep learning frameworks such as TensorFlow \citep{tensorflow} support distributed training, making large scale machine learning systems easier to implement and deploy. Despite this, much current research in deep reinforcement learning concerns itself with improving performance within the computational budget of a single machine, and the question of how to best harness more resources is comparatively underexplored.   In this paper we describe an approach to scaling up deep reinforcement learning by generating more data and selecting from it in a prioritized fashion \citep{prioritized-replay}. Standard approaches to distributed training of neural networks focus on parallelizing the computation of gradients, to more rapidly optimize the parameters \citep{Dean:2012:LSD:2999134.	score:453
As a first time dropper which batch should i choose alchievers last batch or leader last batch at allen kota for neet 2017?	 However, in domains where the sampled data is unpredictable or changes quickly, such as what is seen by a cell phone camera, the value of a static deep network may be quite limited.   One mechanism the brain has maintained in selective regions such as the hippocampus is the permissive birth of new neurons throughout one's lifetime, a process known as adult neurogenesis~\cite{aimone2014regulation}.  While the specific function of neurogenesis in memory is still debated, it clearly provides the hippocampus with a unique form of plasticity that is not present in other regions less exposed to concept drift. The process of biological neurogenesis is complex, but two key observations are that new neurons are preferentially recruited in response to behavioral novelty and that new neurons gradually learn to encode information (e.	 However, in domains where the sampled data is unpredictable or changes quickly, such as what is seen by a cell phone camera, the value of a static deep network may be quite limited.   One mechanism the brain has maintained in selective regions such as the hippocampus is the permissive birth of new neurons throughout one's lifetime, a process known as adult neurogenesis~\cite{aimone2014regulation}.  While the specific function of neurogenesis in memory is still debated, it clearly provides the hippocampus with a unique form of plasticity that is not present in other regions less exposed to concept drift. The process of biological neurogenesis is complex, but two key observations are that new neurons are preferentially recruited in response to behavioral novelty and that new neurons gradually learn to encode information (e.	score:432
As a first time dropper which batch should i choose alchievers last batch or leader last batch at allen kota for neet 2017?	  For example, on Amazon's Mechanical Turk,  a worker cannot repeat the same task more than once.  However, it is difficult to guarantee that a worker completes all the tasks in a batch she started on.  In practice, there are simple ways to ensure this by, for instance, conditioning  the payment on completing all the tasks in a batch.  A problem with restricting the number of tasks assigned to each worker (as we propose in Section~\ref{sec:algorithm})  is that it might take a long time to have all the batches completed.   Letting the workers choose how many tasks they want to complete  allows a few eager workers to complete enormous amount of tasks.  However, if we restrict the number of tasks assigned to each worker,  we might need to recruit more workers to complete all the tasks.  This problem of tasks taking long time to finish is not  just restricted to our model, but is a very common problem  in open crowdsourcing platforms.	  For example, on Amazon's Mechanical Turk,  a worker cannot repeat the same task more than once.  However, it is difficult to guarantee that a worker completes all the tasks in a batch she started on.  In practice, there are simple ways to ensure this by, for instance, conditioning  the payment on completing all the tasks in a batch.  A problem with restricting the number of tasks assigned to each worker (as we propose in Section~\ref{sec:algorithm})  is that it might take a long time to have all the batches completed.   Letting the workers choose how many tasks they want to complete  allows a few eager workers to complete enormous amount of tasks.  However, if we restrict the number of tasks assigned to each worker,  we might need to recruit more workers to complete all the tasks.  This problem of tasks taking long time to finish is not  just restricted to our model, but is a very common problem  in open crowdsourcing platforms.	score:435
As a first time dropper which batch should i choose alchievers last batch or leader last batch at allen kota for neet 2017?	                   A new algorithm is proposed which accelerates the mini-batch k-means algorithm of~\cite{Sculley:2010:WKC:1772690.1772862} by using the distance bounding approach of~\cite{elkan_2003_kmeansicml}. We argue that, when incorporating distance bounds into a mini-batch algorithm,  already used data should preferentially be reused. To this end we propose using \emph{nested} mini-batches, whereby data in a mini-batch at iteration $t$ is automatically reused at iteration $t+1$.	                   A new algorithm is proposed which accelerates the mini-batch k-means algorithm of~\cite{Sculley:2010:WKC:1772690.1772862} by using the distance bounding approach of~\cite{elkan_2003_kmeansicml}. We argue that, when incorporating distance bounds into a mini-batch algorithm,  already used data should preferentially be reused. To this end we propose using \emph{nested} mini-batches, whereby data in a mini-batch at iteration $t$ is automatically reused at iteration $t+1$.	score:435
As a first time dropper which batch should i choose alchievers last batch or leader last batch at allen kota for neet 2017?	 However, research for scenarios in which task boundaries are unknown during training has been lacking. In this paper we present, for the first time, a method for preventing catastrophic forgetting (BGD) for scenarios with task boundaries that are unknown during training --- task-agnostic continual learning. Code of our algorithm is available at \href{https://github. com/igolan/bgd}{https://github.com/igolan/bgd}. 	A \emph{continual learning} algorithm is one that is faced with sequentially-arriving tasks, with no access to samples from previous or future tasks. Special measures are needed to prevent a deep neural network (DNN) from adapting only to the latest task and forgetting past knowledge --- a phenomenon known as catastrophic forgetting~\citep{mccloskey1989catastrophic}.	 However, research for scenarios in which task boundaries are unknown during training has been lacking. In this paper we present, for the first time, a method for preventing catastrophic forgetting (BGD) for scenarios with task boundaries that are unknown during training --- task-agnostic continual learning. Code of our algorithm is available at \href{https://github. com/igolan/bgd}{https://github.com/igolan/bgd}. 	A \emph{continual learning} algorithm is one that is faced with sequentially-arriving tasks, with no access to samples from previous or future tasks. Special measures are needed to prevent a deep neural network (DNN) from adapting only to the latest task and forgetting past knowledge --- a phenomenon known as catastrophic forgetting~\citep{mccloskey1989catastrophic}.	score:436
As a first time dropper which batch should i choose alchievers last batch or leader last batch at allen kota for neet 2017?	 Yet $\varepsilon$-greedy is a poor exploration strategy and for environments with sparse rewards it is quite ineffective (for example the Atari game `Montezuma's Revenge'): it just takes too long until the agent randomwalks into the first reward.   More sophisticated exploration strategies have been proposed: using information gain about the environment ~\citep{SGS:2011infogain,OLH:2013ksa,HCDSDA:2016explore} or pseudo-count~\citep{BSOSSM:2016explore}.  In practice, these exploration strategies are employed by adding an exploration bonus (`intrinsic motivation') to the reward signal~\citep{Schmidhuber:2010everything}. While the methods above require the agent to have a model of its environment and formalize the strategy `explore by going to where the model has high uncertainty,' there are also model-free strategies like the automatic discovery of options proposed by \citet{MB:2016options}.	 Yet $\varepsilon$-greedy is a poor exploration strategy and for environments with sparse rewards it is quite ineffective (for example the Atari game `Montezuma's Revenge'): it just takes too long until the agent randomwalks into the first reward.   More sophisticated exploration strategies have been proposed: using information gain about the environment ~\citep{SGS:2011infogain,OLH:2013ksa,HCDSDA:2016explore} or pseudo-count~\citep{BSOSSM:2016explore}.  In practice, these exploration strategies are employed by adding an exploration bonus (`intrinsic motivation') to the reward signal~\citep{Schmidhuber:2010everything}. While the methods above require the agent to have a model of its environment and formalize the strategy `explore by going to where the model has high uncertainty,' there are also model-free strategies like the automatic discovery of options proposed by \citet{MB:2016options}.	score:437
Iam in 9 th grade. Iam studying in fiitjee.iam selected to the top batch for iit preparation. How can i plan the day for better iit rank.	e. $r_t[i]$ denote the  the ratio of the closing price of stock $i$ on day $t$ to the closing price on day $t-1$,  then an interesting function is the logarithmic growth ratio, i.e. $\sum_{t=1}^T \log(\mathbf x_t^{\top}\mathbf r_t)$, which is a concave function need to be maximized. Similar to~\citep{Hazan-2008-extract,  DBLP:journals/ml/HazanK10}, we aim to develop algorithms for online convex optimization with regrets bounded by the variation in the cost functions.	e. $r_t[i]$ denote the  the ratio of the closing price of stock $i$ on day $t$ to the closing price on day $t-1$,  then an interesting function is the logarithmic growth ratio, i.e. $\sum_{t=1}^T \log(\mathbf x_t^{\top}\mathbf r_t)$, which is a concave function need to be maximized. Similar to~\citep{Hazan-2008-extract,  DBLP:journals/ml/HazanK10}, we aim to develop algorithms for online convex optimization with regrets bounded by the variation in the cost functions.	score:372
Iam in 9 th grade. Iam studying in fiitjee.iam selected to the top batch for iit preparation. How can i plan the day for better iit rank.	25 to 256 on the log$_2$ scale, i.e. $\Theta = \left\{2^{-2}, 2^{-1.5} \ldots, 2^{8}\right\}$.  Simple bootstrap resampling \citep{Efron:1983ul} was used to tune the model where, on average, the number of samples held out at each iteration of resampling was 1598. To evaluate how well the model performed within each resampling iteration, an ROC curve \citep{Altman:1994uv,Fawcett:2006gr,Brown:2006wp} is created by applying $\hat{f}_{ij}(R_i, \theta_j)$ to $T_i$.  The area under the ROC curve is then used to quantify model fitness.  The results of this process are shown in Figure \ref{F:qsar_svm_cv}. When the cost value is small, the model has poor performance due to under--fitting. After a peak in performance is reached,  the model begins to become too complex and over--fit. A simple ``pick--the--winner'' strategy would select a model with a cost value of $2^{1.	25 to 256 on the log$_2$ scale, i.e. $\Theta = \left\{2^{-2}, 2^{-1.5} \ldots, 2^{8}\right\}$.  Simple bootstrap resampling \citep{Efron:1983ul} was used to tune the model where, on average, the number of samples held out at each iteration of resampling was 1598. To evaluate how well the model performed within each resampling iteration, an ROC curve \citep{Altman:1994uv,Fawcett:2006gr,Brown:2006wp} is created by applying $\hat{f}_{ij}(R_i, \theta_j)$ to $T_i$.  The area under the ROC curve is then used to quantify model fitness.  The results of this process are shown in Figure \ref{F:qsar_svm_cv}. When the cost value is small, the model has poor performance due to under--fitting. After a peak in performance is reached,  the model begins to become too complex and over--fit. A simple ``pick--the--winner'' strategy would select a model with a cost value of $2^{1.	score:395
Iam in 9 th grade. Iam studying in fiitjee.iam selected to the top batch for iit preparation. How can i plan the day for better iit rank.	 From a macroscopic perspective, it provides the commonly believed result that mobile Internet will witness a 1000-folded traffic growth in the next 10 years \cite{cisco_cisco_2013}, which is acting as a crucial anchor for the design of next-generation cellular network architecture and embedded algorithms. On the other hand, the fine traffic prediction on a daily, hourly or even minutely basis could contribute to the optimization and management of cellular networks like energy savings \cite{li_energy_2014}, opportunistic scheduling \cite{paul_opportunistic_2012}, and network anomaly detection \cite{romirer-maierhofer_device-specific_2015} .	 From a macroscopic perspective, it provides the commonly believed result that mobile Internet will witness a 1000-folded traffic growth in the next 10 years \cite{cisco_cisco_2013}, which is acting as a crucial anchor for the design of next-generation cellular network architecture and embedded algorithms. On the other hand, the fine traffic prediction on a daily, hourly or even minutely basis could contribute to the optimization and management of cellular networks like energy savings \cite{li_energy_2014}, opportunistic scheduling \cite{paul_opportunistic_2012}, and network anomaly detection \cite{romirer-maierhofer_device-specific_2015} .	score:402
Iam in 9 th grade. Iam studying in fiitjee.iam selected to the top batch for iit preparation. How can i plan the day for better iit rank.	 This {\it chord recognition} task can often be time consuming and cognitively demanding, hence the utility of computer-based implementations. Reflecting historical trends in artificial intelligence, automatic approaches to harmonic analysis have evolved from purely grammar-based and rule-based systems \citep{winograd:jmt68,maxwell:chapter92}, to systems employing weighted rules and optimization algorithms \citep{temperley:cmj99,pardo:cmj02,scholz:ismir08,rocher:icmc09}, to data driven approaches based on supervised machine learning (ML) \citep{raphael:ismir03,radicioni:amir10}.  Due to their requirements for annotated data, ML approaches have also led to the development of music analysis datasets containing a large number of manually annotated harmonic structures, such as the 60 Bach chorales introduced in \citep{radicioni:amir10}, and the 27 themes and variations of TAVERN \citep{devaney:ismir15}.  \begin{figure}[t] \centering \includegraphics[width=\columnwidth]{segments-small.	 This {\it chord recognition} task can often be time consuming and cognitively demanding, hence the utility of computer-based implementations. Reflecting historical trends in artificial intelligence, automatic approaches to harmonic analysis have evolved from purely grammar-based and rule-based systems \citep{winograd:jmt68,maxwell:chapter92}, to systems employing weighted rules and optimization algorithms \citep{temperley:cmj99,pardo:cmj02,scholz:ismir08,rocher:icmc09}, to data driven approaches based on supervised machine learning (ML) \citep{raphael:ismir03,radicioni:amir10}.  Due to their requirements for annotated data, ML approaches have also led to the development of music analysis datasets containing a large number of manually annotated harmonic structures, such as the 60 Bach chorales introduced in \citep{radicioni:amir10}, and the 27 themes and variations of TAVERN \citep{devaney:ismir15}.  \begin{figure}[t] \centering \includegraphics[width=\columnwidth]{segments-small.	score:403
Iam in 9 th grade. Iam studying in fiitjee.iam selected to the top batch for iit preparation. How can i plan the day for better iit rank.	 The theoretical outcomes can be used to motivate algorithm design, select models or give insights on the effects and behaviors of some interesting quantities. For example, the well-known large margin principle in support vector machines (SVMs) is well supported by various SLT bounds~\citep{Vapnik98SLT,Bartlett02,Sun10JMLR}. Different from early bounds that often rely on the complexity measures of the considered function classes, the recent PAC-Bayes bounds~\citep{McAllester99,Seeger02,Langford05} give the tightest predictions of the generalization performance, for which the prior and posterior distributions of learners are involved on top of the PAC (Probably Approximately Correct) learning setting~\citep{Catoni07,Germain09}.	 The theoretical outcomes can be used to motivate algorithm design, select models or give insights on the effects and behaviors of some interesting quantities. For example, the well-known large margin principle in support vector machines (SVMs) is well supported by various SLT bounds~\citep{Vapnik98SLT,Bartlett02,Sun10JMLR}. Different from early bounds that often rely on the complexity measures of the considered function classes, the recent PAC-Bayes bounds~\citep{McAllester99,Seeger02,Langford05} give the tightest predictions of the generalization performance, for which the prior and posterior distributions of learners are involved on top of the PAC (Probably Approximately Correct) learning setting~\citep{Catoni07,Germain09}.	score:404
How can Google Analytics data and machine learning help an online business?	 At the same time, machine learning---one of the primary tools of the modern data scientist---has experienced a revitalization as academics, businesses, and governments alike discovered new applications for automated algorithms that can learn from data. As a consequence, there has been a growing demand for tools that make machine learning more accessible, scalable, and flexible.  Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.   As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?	 Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.	score:366
How can Google Analytics data and machine learning help an online business?	  Mobile networks possess information about the users as well as the network. Such information is useful for making the network end-to-end visible and intelligent. Big data analytics can efficiently analyze user and network information, unearth meaningful insights with the help of machine learning tools. Utilizing big data analytics and machine learning, this work contributes in three ways.  First, we utilize the \emph{call detail records} (CDR) data to detect anomalies in the network. For authentication and verification of anomalies, we use k-means clustering, an unsupervised machine learning algorithm. Through effective detection of anomalies, we can proceed to suitable design for resource distribution as well as fault detection and avoidance.	  Mobile networks possess information about the users as well as the network. Such information is useful for making the network end-to-end visible and intelligent. Big data analytics can efficiently analyze user and network information, unearth meaningful insights with the help of machine learning tools. Utilizing big data analytics and machine learning, this work contributes in three ways.  First, we utilize the \emph{call detail records} (CDR) data to detect anomalies in the network. For authentication and verification of anomalies, we use k-means clustering, an unsupervised machine learning algorithm. Through effective detection of anomalies, we can proceed to suitable design for resource distribution as well as fault detection and avoidance.	score:373
How can Google Analytics data and machine learning help an online business?	 However, most knowledge bases use a linked network to organize the knowledge. For example, a {\it CEO} is connected to an {\it IT company}, and the {\it IT company} is a {\it company}. Thus, the structure of the knowledge also provides rich information about the connections of entities and relations. Therefore, we should also carefully consider the best way to represent the world knowledge for machine learning algorithms.  Third, given the world knowledge about entities and their relations, as well as the types of entities and relations, we should consider an effective algorithm that can propagate the knowledge about entity and relation categories to the categories of data that contain the entities and relations. This is a non-trivial task because we should consider both the data representation and the structural representation of the world knowledge.	 However, most knowledge bases use a linked network to organize the knowledge. For example, a {\it CEO} is connected to an {\it IT company}, and the {\it IT company} is a {\it company}. Thus, the structure of the knowledge also provides rich information about the connections of entities and relations. Therefore, we should also carefully consider the best way to represent the world knowledge for machine learning algorithms.  Third, given the world knowledge about entities and their relations, as well as the types of entities and relations, we should consider an effective algorithm that can propagate the knowledge about entity and relation categories to the categories of data that contain the entities and relations. This is a non-trivial task because we should consider both the data representation and the structural representation of the world knowledge.	score:377
How can Google Analytics data and machine learning help an online business?	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	score:389
How can Google Analytics data and machine learning help an online business?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:397
What is the reputation of the machine learning group in UC Santa Cruz?	 The key approaches for machine learning, especially learning in unknown probabilistic environments are new representations and computation mechanisms. In this paper, a novel quantum reinforcement learning (QRL) method is proposed by combining quantum theory and reinforcement learning (RL). Inspired by the state superposition principle and quantum parallelism, a framework of value updating algorithm is introduced.  The state (action) in traditional RL is identified as the eigen state (eigen action) in QRL. The state (action) set can be represented with a quantum superposition state and the eigen state (eigen action) can be obtained by randomly observing the simulated quantum state according to the collapse postulate of quantum measurement. The probability of the eigen action is determined by the probability amplitude, which is parallelly updated according to rewards.	 The Grover algorithm can achieve a square speedup over classical algorithms in unsorted database searching and its experimental implementations have also been demonstrated using NMR \cite{Chuang et al 1998}-\cite{Jones et al 1998} and quantum optics \cite{Kwiat et al 2000}, \cite{Scully and Zubairy 2001} for a system with four states. Some methods have also been explored to connect quantum computation and machine learning.  For example, the quantum computing version of artificial neural network has been studied from the pure theory to the simple simulated and experimental implementation \cite{Ventura and Martinez 2000}-\cite{Behrman et al 2000}. Rigatos and Tzafestas \cite{Rigatos and Tzafestas 2002} used quantum computation for the parallelization of a fuzzy logic control algorithm to speed up the fuzzy inference.	score:492
What is the reputation of the machine learning group in UC Santa Cruz?	  However, recent studies have shown that quantum computing is a competitive alternative when generating such networks (\cite{2016arXiv160606123D, PhysRevX.7.041052, Benedetti2016, arXiv:1510.06356, Benedetti2017}).  Quantum machine learning (QML) is a blossoming field.  As summarized in the comprehensive review of  QML in \cite{biamonte2017quantum}, machine learning applications from support vector machines to principal component analysis are being reimagined on various quantum devices.   One of the most exciting research areas within QML is deep quantum learning, which focuses on the impact quantum devices and algorithms can have on classical deep neural networks (DNNs) and graphical models.  A particular class of DNNs is the Boltzmann machine (BM), which is an incredibly powerful fully-connected graphical model that can be trained to learn arbitrary probability distributions.	  However, recent studies have shown that quantum computing is a competitive alternative when generating such networks (\cite{2016arXiv160606123D, PhysRevX.7.041052, Benedetti2016, arXiv:1510.06356, Benedetti2017}).  Quantum machine learning (QML) is a blossoming field.  As summarized in the comprehensive review of  QML in \cite{biamonte2017quantum}, machine learning applications from support vector machines to principal component analysis are being reimagined on various quantum devices.   One of the most exciting research areas within QML is deep quantum learning, which focuses on the impact quantum devices and algorithms can have on classical deep neural networks (DNNs) and graphical models.  A particular class of DNNs is the Boltzmann machine (BM), which is an incredibly powerful fully-connected graphical model that can be trained to learn arbitrary probability distributions.	score:494
What is the reputation of the machine learning group in UC Santa Cruz?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:497
What is the reputation of the machine learning group in UC Santa Cruz?	 \item These four boosting algorithms (especially {\em abc-logitboost})  outperform SVM on many datasets. \item Compared to the best deep learning methods, these four boosting algorithms (especially {\em abc-logitboost}) are competitive. \end{enumerate}  	Boosting algorithms \cite{Article:Schapire_ML90,Article:Freund_95,Article:Freund_JCSS97,Article:Bartlett_AS98,Article:Schapire_ML99,Article:FHT_AS00,Proc:Mason_NIPS00,Article:Friedman_AS01} have become very successful in machine learning.   In this paper, we provide an empirical evaluation of \textbf{four} tree-based  boosting algorithms for multi-class classification: \textbf{\em mart}\cite{Article:Friedman_AS01}, \textbf{\em abc-mart}\cite{Proc:ABC_ICML09}, \textbf{\em robust logitboost}\cite{Report:Li_Robust-LogitBoost}, and \textbf{\em abc-logitboost}\cite{Report:Li_ABC-LogitBoost}, on a wide range of datasets.	 \item These four boosting algorithms (especially {\em abc-logitboost})  outperform SVM on many datasets. \item Compared to the best deep learning methods, these four boosting algorithms (especially {\em abc-logitboost}) are competitive. \end{enumerate}  	Boosting algorithms \cite{Article:Schapire_ML90,Article:Freund_95,Article:Freund_JCSS97,Article:Bartlett_AS98,Article:Schapire_ML99,Article:FHT_AS00,Proc:Mason_NIPS00,Article:Friedman_AS01} have become very successful in machine learning.   In this paper, we provide an empirical evaluation of \textbf{four} tree-based  boosting algorithms for multi-class classification: \textbf{\em mart}\cite{Article:Friedman_AS01}, \textbf{\em abc-mart}\cite{Proc:ABC_ICML09}, \textbf{\em robust logitboost}\cite{Report:Li_Robust-LogitBoost}, and \textbf{\em abc-logitboost}\cite{Report:Li_ABC-LogitBoost}, on a wide range of datasets.	score:497
What is the reputation of the machine learning group in UC Santa Cruz?	 Here we review the literature in quantum machine learning and discuss perspectives for a mixed readership of classical machine learning and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems.  Learning in the presence of noise and certain computationally hard problems in machine learning are identified as promising directions for the field. Practical questions, like how to upload classical data into quantum form, will also be addressed.\\ 	In the last twenty years, thanks to increased computational power and the availability of vast amounts of data, \textit{machine learning} (ML) algorithms have achieved remarkable successes in tasks ranging from computer vision~\cite{krizhevsky2012imagenet} to playing complex games such as Go~\cite{silver2016mastering}.	 We do not aim for completeness but rather discuss only the most relevant results in quantum algorithms for learning. For the interested reader there is now a number of resources covering quantum machine learning in the broader sense of the term~\cite{adcock2015advances,biamonte2016quantum}. For an introduction to quantum algorithms we refer to the reviews of Montanaro~\cite{montanaro2016quantum} and Bacon~\cite{bacon2010recent}, while for machine learning to the books by Bishop~\cite{bishop2006pattern} and Murphy~\cite{murphy2012machine}. \\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:497
Is Go the best programming language for next generation A.I. and machine learning applications?	                         The choice of a method depends on the goal of the task for which it is used. One goal for processing natural language is to develop useful applications that help humans in their daily life, for example machine translation and speech recognition.  In application scenarios where a rough analysis is acceptable (e.g., a translation that provides the gist of the message) and large annotated and structured corpora are available, machine learning is the methodology of choice to address this goal.  However, where precise analysis is required or where there is a scarcity of data, a machine learning approach may not be suitable. Furthermore, if the goal of processing language is rather motivated by the desire to better understand its cognitive foundations, than a machine learning methodology, particularly one based on an unconstrained, fully connected deep neural network, is not appropriate.	                         The choice of a method depends on the goal of the task for which it is used. One goal for processing natural language is to develop useful applications that help humans in their daily life, for example machine translation and speech recognition.  In application scenarios where a rough analysis is acceptable (e.g., a translation that provides the gist of the message) and large annotated and structured corpora are available, machine learning is the methodology of choice to address this goal.  However, where precise analysis is required or where there is a scarcity of data, a machine learning approach may not be suitable. Furthermore, if the goal of processing language is rather motivated by the desire to better understand its cognitive foundations, than a machine learning methodology, particularly one based on an unconstrained, fully connected deep neural network, is not appropriate.	score:323
Is Go the best programming language for next generation A.I. and machine learning applications?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:328
Is Go the best programming language for next generation A.I. and machine learning applications?	 A very powerful machine (e.g., deep neural network) learns the mapping from natural language of problem descriptions to source code. During development, users express their intention by natural language (similar to some in the repository);  the learning machine automatically output the desired code as the solution.   A more compelling feature is that the above process works in an ``end-to-end'' manner, which requires little, if any, human knowledge, and is completely language independent---the only thing needed is to represent sentences and programs as characters.	 A very powerful machine (e.g., deep neural network) learns the mapping from natural language of problem descriptions to source code. During development, users express their intention by natural language (similar to some in the repository);  the learning machine automatically output the desired code as the solution.   A more compelling feature is that the above process works in an ``end-to-end'' manner, which requires little, if any, human knowledge, and is completely language independent---the only thing needed is to represent sentences and programs as characters.	score:336
Is Go the best programming language for next generation A.I. and machine learning applications?	 Machine learning is often used in competitive scenarios: Participants learn and fit static models, and those models compete in a shared platform. The common assumption is that in order to win a competition one has to have the best predictive model, i.e., the model with the smallest out-sample error. Is that necessarily true?  Does the best theoretical predictive model for a target always yield the best reward in a competition?  If not, can one take the best model and purposefully change it into a theoretically inferior model which in practice results in a higher competitive edge? How does that modification look like? And finally, if all participants modify their prediction models towards the  best practical performance, who benefits the most? players with  inferior models, or those with theoretical superiority?	 Machine learning is often used in competitive scenarios: Participants learn and fit static models, and those models compete in a shared platform. The common assumption is that in order to win a competition one has to have the best predictive model, i.e., the model with the smallest out-sample error. Is that necessarily true?  Does the best theoretical predictive model for a target always yield the best reward in a competition?  If not, can one take the best model and purposefully change it into a theoretically inferior model which in practice results in a higher competitive edge? How does that modification look like? And finally, if all participants modify their prediction models towards the  best practical performance, who benefits the most? players with  inferior models, or those with theoretical superiority?	score:338
Is Go the best programming language for next generation A.I. and machine learning applications?	  With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them.  This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they  will be used,  within the context of stochastic programming.  We present three experimental evaluations of the proposed approach: a classical inventory stock problem,  a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task.  We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.	  With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them.  This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they  will be used,  within the context of stochastic programming.  We present three experimental evaluations of the proposed approach: a classical inventory stock problem,  a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task.  We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.	score:351
How do i get started on machine learning?	 However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks.	 However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks.	score:475
How do i get started on machine learning?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:488
How do i get started on machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:490
How do i get started on machine learning?	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:496
How do i get started on machine learning?	 In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model.  We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process.	 In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model.  We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process.	score:496
Algorithm for matrix multiplication?	 There is usually a massive amount of such user-item interaction available for any web applications. Algorithms like PLSI or Matrix Factorization runs several iterations through the dataset, and may prove very expensive for large datasets. Here we propose a recommendation algorithm based on Method of Moment, which involves factorization of second and third order moments of the dataset.  Our algorithm can be proven to be globally convergent using PAC learning theory. Further, we show how to extract the parameters using only three passes through the entire dataset. This results in a highly scalable algorithm that scales up to million of users even on a machine with a single-core processor and 8 GB RAM and produces competitive performance in comparison with existing algorithms.	 There is usually a massive amount of such user-item interaction available for any web applications. Algorithms like PLSI or Matrix Factorization runs several iterations through the dataset, and may prove very expensive for large datasets. Here we propose a recommendation algorithm based on Method of Moment, which involves factorization of second and third order moments of the dataset.  Our algorithm can be proven to be globally convergent using PAC learning theory. Further, we show how to extract the parameters using only three passes through the entire dataset. This results in a highly scalable algorithm that scales up to million of users even on a machine with a single-core processor and 8 GB RAM and produces competitive performance in comparison with existing algorithms.	score:421
Algorithm for matrix multiplication?	e., trying to develop delicate ways of matricization to keep the multi-way properties of a tensor object while using matrix-based completion algorithms. Second, the high-order characteristics introduce even higher space and computational complexity, which may prevent the usage of traditional matrix completion algorithms.         \begin{figure} \centering \vspace{-0. 1cm} \includegraphics[height=3.7cm, width=13cm]{outline2.pdf}  \vspace{-0.2cm}  \caption{Outline of this survey.} \label{fig:outline}   \vspace{-0.4cm}  \end{figure}     In some early works~\cite{bro1998multi,tomasi2005parafac,andersson1998improving,smilde2005multi}, tensor completion is often considered as a byproduct when dealing with missing data in the tensor decomposition problem.	e., trying to develop delicate ways of matricization to keep the multi-way properties of a tensor object while using matrix-based completion algorithms. Second, the high-order characteristics introduce even higher space and computational complexity, which may prevent the usage of traditional matrix completion algorithms.         \begin{figure} \centering \vspace{-0. 1cm} \includegraphics[height=3.7cm, width=13cm]{outline2.pdf}  \vspace{-0.2cm}  \caption{Outline of this survey.} \label{fig:outline}   \vspace{-0.4cm}  \end{figure}     In some early works~\cite{bro1998multi,tomasi2005parafac,andersson1998improving,smilde2005multi}, tensor completion is often considered as a byproduct when dealing with missing data in the tensor decomposition problem.	score:424
Algorithm for matrix multiplication?	  Our main application is to multinomial regression. In Section \ref{sec:sgl} we introduce the general sparse group lasso optimization problem with a convex loss function. Part \ref{sec:multi} investigates the performance of the multinomial sparse group lasso classifier. In Part \ref{sec:algo} we present the general sparse group lasso algorithm and establish convergence.   The formulation of an efficient and robust sparse group lasso algorithm is not straight forward due to non-differentiability of the penalty. Firstly, the sparse group lasso penalty is not completely separable, which is problematic when using a standard coordinate descent scheme. To obtain a robust algorithm an adjustment is necessary. Our solution is a minor modification of the coordinate descent algorithm; it efficiently treats the singularity at zero that cannot be separated out.	  Our main application is to multinomial regression. In Section \ref{sec:sgl} we introduce the general sparse group lasso optimization problem with a convex loss function. Part \ref{sec:multi} investigates the performance of the multinomial sparse group lasso classifier. In Part \ref{sec:algo} we present the general sparse group lasso algorithm and establish convergence.   The formulation of an efficient and robust sparse group lasso algorithm is not straight forward due to non-differentiability of the penalty. Firstly, the sparse group lasso penalty is not completely separable, which is problematic when using a standard coordinate descent scheme. To obtain a robust algorithm an adjustment is necessary. Our solution is a minor modification of the coordinate descent algorithm; it efficiently treats the singularity at zero that cannot be separated out.	score:431
Algorithm for matrix multiplication?	 We present an algorithm for sampling useful column blocks and provide novel guarantees for the quality of the approximation. This algorithm has application in problems as diverse as biometric data analysis to distributed computing. We demonstrate the effectiveness of the proposed algorithms for computing the Block CUR decomposition of large matrices in a distributed setting with multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes.  In the biometric setting, the rows correspond to different users and columns correspond to users' biometric reaction to external stimuli, {\em e.g.,}~watching video content, at a particular time instant. There is significant cost in acquiring each user's reaction to lengthy content so we sample a few important scenes to approximate the biometric response.	 We present an algorithm for sampling useful column blocks and provide novel guarantees for the quality of the approximation. This algorithm has application in problems as diverse as biometric data analysis to distributed computing. We demonstrate the effectiveness of the proposed algorithms for computing the Block CUR decomposition of large matrices in a distributed setting with multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes.  In the biometric setting, the rows correspond to different users and columns correspond to users' biometric reaction to external stimuli, {\em e.g.,}~watching video content, at a particular time instant. There is significant cost in acquiring each user's reaction to lengthy content so we sample a few important scenes to approximate the biometric response.	score:434
Algorithm for matrix multiplication?	  This paper studies precision matrix estimation for multiple related Gaussian graphical models from a dataset consisting of different classes, based upon the formulation of this problem as group graphical lasso. In particular, this paper proposes a novel hybrid covariance thresholding algorithm that can effectively identify zero entries in the precision matrices and split a large joint graphical lasso problem into many small subproblems.  Our hybrid covariance thresholding method is superior to existing uniform thresholding methods in that our method can split the precision matrix of each individual class using different partition schemes and thus, split group graphical lasso into much smaller subproblems, each of which can be solved very fast.  This paper also establishes necessary and sufficient conditions for our hybrid covariance thresholding algorithm.	  This paper studies precision matrix estimation for multiple related Gaussian graphical models from a dataset consisting of different classes, based upon the formulation of this problem as group graphical lasso. In particular, this paper proposes a novel hybrid covariance thresholding algorithm that can effectively identify zero entries in the precision matrices and split a large joint graphical lasso problem into many small subproblems.  Our hybrid covariance thresholding method is superior to existing uniform thresholding methods in that our method can split the precision matrix of each individual class using different partition schemes and thus, split group graphical lasso into much smaller subproblems, each of which can be solved very fast.  This paper also establishes necessary and sufficient conditions for our hybrid covariance thresholding algorithm.	score:435
I'm 25 and majoring in industrial engineering in grad school. I know basic statistics and limited knowledge of programming. I am a year from graduation. Is it possible I can pursue my career in data mining/machine learning?	   	Machine learning is based on sample data. Sometimes, these data are labeled and, thus, models to solve a certain task (e.g., a classification or regression task) can be built using targets assigned to the input data of a classification or regression model. In other cases, data are unlabeled (e.g., for clustering) or only partially labeled. Correspondingly, we distinguish the areas of \textit{supervised}, \textit{unsupervised}, and \textit{semi-supervised} learning.  In many application areas (e.g., industrial quality monitoring processes~\cite{Sic98}, intrusion detection in computer networks~\cite{HSS03}, speech recognition~\cite{FHYA12}, or drug discovery~\cite{MME14}) it is rather easy to collect unlabeled data, but quite difficult, time-consuming, or expensive to gather the corresponding targets. That is, labeling is in principal possible, but the costs are enormous.	   	Machine learning is based on sample data. Sometimes, these data are labeled and, thus, models to solve a certain task (e.g., a classification or regression task) can be built using targets assigned to the input data of a classification or regression model. In other cases, data are unlabeled (e.g., for clustering) or only partially labeled. Correspondingly, we distinguish the areas of \textit{supervised}, \textit{unsupervised}, and \textit{semi-supervised} learning.  In many application areas (e.g., industrial quality monitoring processes~\cite{Sic98}, intrusion detection in computer networks~\cite{HSS03}, speech recognition~\cite{FHYA12}, or drug discovery~\cite{MME14}) it is rather easy to collect unlabeled data, but quite difficult, time-consuming, or expensive to gather the corresponding targets. That is, labeling is in principal possible, but the costs are enormous.	score:380
I'm 25 and majoring in industrial engineering in grad school. I know basic statistics and limited knowledge of programming. I am a year from graduation. Is it possible I can pursue my career in data mining/machine learning?	  Adapting data mining and machine learning methods to complex data is possible, but time consuming and complex, both at the theoretical level (e.g., consistency of the algorithms is generally proved only in the Euclidean case) and on a practical point of view (new implementations are needed). Therefore, it is tempting to build generic methods that use only properties that are shared by all types of data.    Two such generic approaches have been used successfully: the dissimilarity based approach and the kernel based approach \cite{ShaweTaylorChristianini2004KernelMethods}. Both are based on fairly generic assumptions: the analyst is given a data set on which either a dissimilarity or a kernel is defined. A dissimilarity measures how much two objects differs, while a kernel can be seen as a form a similarity measure, at least in the correlation sense.	  Adapting data mining and machine learning methods to complex data is possible, but time consuming and complex, both at the theoretical level (e.g., consistency of the algorithms is generally proved only in the Euclidean case) and on a practical point of view (new implementations are needed). Therefore, it is tempting to build generic methods that use only properties that are shared by all types of data.    Two such generic approaches have been used successfully: the dissimilarity based approach and the kernel based approach \cite{ShaweTaylorChristianini2004KernelMethods}. Both are based on fairly generic assumptions: the analyst is given a data set on which either a dissimilarity or a kernel is defined. A dissimilarity measures how much two objects differs, while a kernel can be seen as a form a similarity measure, at least in the correlation sense.	score:390
I'm 25 and majoring in industrial engineering in grad school. I know basic statistics and limited knowledge of programming. I am a year from graduation. Is it possible I can pursue my career in data mining/machine learning?	 Traditional event extraction approaches train the machine learning models based on the annotation on specific domains, e.g., 33 event types in ACE 2005~\cite{NIST05} or 38 event types in TAC KBP 2015~\cite{mitamura2015event}. Consequently, the supervised learning systems easily overfit these domains. However, there are many more types of events. When generalizing the trained models to other domains, more annotation should be used.	 Traditional event extraction approaches train the machine learning models based on the annotation on specific domains, e.g., 33 event types in ACE 2005~\cite{NIST05} or 38 event types in TAC KBP 2015~\cite{mitamura2015event}. Consequently, the supervised learning systems easily overfit these domains. However, there are many more types of events. When generalizing the trained models to other domains, more annotation should be used.	score:395
I'm 25 and majoring in industrial engineering in grad school. I know basic statistics and limited knowledge of programming. I am a year from graduation. Is it possible I can pursue my career in data mining/machine learning?	 	Classifying objects based on their features (e.g.: color, magnitude or any statistical descriptor) dates back in the 19th century \citep{Rosenberg:1910}. Recently automatic classification methods have become much more sophisticated and necessary due to the exponential growth of astronomical data. In time-domain astronomy, where data is in the form of light-curves, a typical classification method uses features\footnote{we use the term \lq\lq features'' for all the descriptors we may use to represent a light-curve with a numerical vector} of the light-curves and applies sophisticated machine learning to classify objects in a multidimensional features space, provided there are enough examples to learn from (training).	 	Classifying objects based on their features (e.g.: color, magnitude or any statistical descriptor) dates back in the 19th century \citep{Rosenberg:1910}. Recently automatic classification methods have become much more sophisticated and necessary due to the exponential growth of astronomical data. In time-domain astronomy, where data is in the form of light-curves, a typical classification method uses features\footnote{we use the term \lq\lq features'' for all the descriptors we may use to represent a light-curve with a numerical vector} of the light-curves and applies sophisticated machine learning to classify objects in a multidimensional features space, provided there are enough examples to learn from (training).	score:395
I'm 25 and majoring in industrial engineering in grad school. I know basic statistics and limited knowledge of programming. I am a year from graduation. Is it possible I can pursue my career in data mining/machine learning?	  In the traditional machine learning paradigm, a teacher will typically construct a batch set of examples, and provide them to a learning algorithm in one shot; then the learning algorithm will work on this batch dataset trying to learn the target concept. Thus, many research work under this topic try to construct the smallest such dataset, or characterize the size of of such dataset, called the teaching dimension of the student model~\cite{zhu2013machine,zhu2015machine}.  There are also many seminal theory work on analyzing the teaching dimension of different models~\cite{shinohara1991teachability,goldman1995complexity,doliwa2014recursive,liu2016teaching}.   \begin{figure}[t]   \centering   \footnotesize   \includegraphics[width=2.5in]{fig1_compv3.pdf}   \vspace{-2.7mm}   \caption{Comparison between iterative machine teaching and the other learning paradigms.	  In the traditional machine learning paradigm, a teacher will typically construct a batch set of examples, and provide them to a learning algorithm in one shot; then the learning algorithm will work on this batch dataset trying to learn the target concept. Thus, many research work under this topic try to construct the smallest such dataset, or characterize the size of of such dataset, called the teaching dimension of the student model~\cite{zhu2013machine,zhu2015machine}.  There are also many seminal theory work on analyzing the teaching dimension of different models~\cite{shinohara1991teachability,goldman1995complexity,doliwa2014recursive,liu2016teaching}.   \begin{figure}[t]   \centering   \footnotesize   \includegraphics[width=2.5in]{fig1_compv3.pdf}   \vspace{-2.7mm}   \caption{Comparison between iterative machine teaching and the other learning paradigms.	score:398
How can I apply for off-campus for MNCs of 2017 batch?	                      Much of the success of single agent deep reinforcement learning (DRL) in recent years can be attributed to the use of experience replay memories (ERM), which allow Deep Q-Networks (DQNs) to be trained efficiently through sampling stored state transitions. However, care is required when using ERMs for multi-agent deep reinforcement learning (MA-DRL), as stored transitions can become outdated because agents update their policies in parallel {\cite{foerster2017stabilising}}.  In this work we apply \emph{leniency} {\cite{panait2006lenient}} to MA-DRL. Lenient agents map state-action pairs to decaying temperature values that control the amount of leniency applied towards negative policy updates that are sampled from the ERM. This introduces optimism in the value-function update, and has been shown to facilitate cooperation in tabular fully-cooperative multi-agent reinforcement learning problems.	                      Much of the success of single agent deep reinforcement learning (DRL) in recent years can be attributed to the use of experience replay memories (ERM), which allow Deep Q-Networks (DQNs) to be trained efficiently through sampling stored state transitions. However, care is required when using ERMs for multi-agent deep reinforcement learning (MA-DRL), as stored transitions can become outdated because agents update their policies in parallel {\cite{foerster2017stabilising}}.  In this work we apply \emph{leniency} {\cite{panait2006lenient}} to MA-DRL. Lenient agents map state-action pairs to decaying temperature values that control the amount of leniency applied towards negative policy updates that are sampled from the ERM. This introduces optimism in the value-function update, and has been shown to facilitate cooperation in tabular fully-cooperative multi-agent reinforcement learning problems.	score:473
How can I apply for off-campus for MNCs of 2017 batch?	 For learning an unknown quantum measurement, we apply a sequence of quantum states through the measurement apparatus and obtain the statistics of each measurement outcome. Our goal is to infer the most likely quantum measurement from the hypothesis set, which `behaves' like the target measurement on the collected data. In this paper, we mainly focus on learning an unknown  two-outcome measurement, which resembles a `yes-no' instrument.  For multi-outcome measurements, the results can easily be generalised\footnote{In the scenario of learning multi-outcome measurements, each POVM element can be considered as a two-outcome POVM. Hence, the learnability of each POVM element can be derived by following the proposed paradigm. We note that this problem can be tackled by the \emph{multi-label learning} algorithms (also called \emph{multi-target prediction} or \emph{multivariate regression}.	 Suppose that the outcome statistics of the set of quantum states are known. Can we infer the unknown quantum measurement from the quantum states at hand? How many samples of quantum states are needed for the learning machine to decide an optimal quantum measurement from the hypothesis set? Can the chosen candidate approximate the target measurement with the desired accuracy?  These questions are typical sample complexity problems in statistical learning theory, and the answer lies in a proper quantification of the ``effective size" of the hypothesis set. In this paper, we propose a framework (see Section \ref{Framework}) to connect the problems of learning two-outcome measurements with the tasks of learning real-valued linear functional on quantum states.	score:486
How can I apply for off-campus for MNCs of 2017 batch?	 Transfer learning has received a lot of attention in recent years and has successfully been used in several applications, such as indoor localization \cite{pan_transfer_2008}, image processing \cite{hinton2007using}, land-mine detection \cite{grubinger2015domain} and biological applications \cite{muandet2013domain}.  Most transfer learning approaches can be referred to as \emph{offline approaches}, since learning is performed offline.  However, in the context of climate control in residential buildings, data are usually collected continuously and knowledge transfer needs to be implemented in an \emph{online} fashion. Thus, it is necessary to develop an online transfer learning methodology that will be particularly appropriate for knowledge transfer between different houses. To the best of our knowledge, only the \emph{Online Transfer Learning (OTL)}~\cite{zhao2010otl} method addresses an online learning case.	 Transfer learning has received a lot of attention in recent years and has successfully been used in several applications, such as indoor localization \cite{pan_transfer_2008}, image processing \cite{hinton2007using}, land-mine detection \cite{grubinger2015domain} and biological applications \cite{muandet2013domain}.  Most transfer learning approaches can be referred to as \emph{offline approaches}, since learning is performed offline.  However, in the context of climate control in residential buildings, data are usually collected continuously and knowledge transfer needs to be implemented in an \emph{online} fashion. Thus, it is necessary to develop an online transfer learning methodology that will be particularly appropriate for knowledge transfer between different houses. To the best of our knowledge, only the \emph{Online Transfer Learning (OTL)}~\cite{zhao2010otl} method addresses an online learning case.	score:487
How can I apply for off-campus for MNCs of 2017 batch?	 \citet{bjm2014aistats} showed that the problem of learning bounded treewidth Bayesian networks can be reduced to a weighted maximum satisfiability problem, and subsequently solved by weighted MAX-SAT solvers. They report experimental results showing that their approach outperforms \citeauthor{korhonen2exact}'s dynamic programming approach. In the same year, \citet{pfl2014aistats} showed that the problem can be reduced to a mixed-integer linear program (MILP), and then solved by off-the-shelf MILP optimizers (e. g.~CPLEX). Their reduced MILP problem however has exponentially many constraints in the number of variables. Following the work of \citet{cussens2}, the authors avoid creating such large programs by a cutting plane generation mechanism, which iteratively includes a new constraint while the optimum is not found. The generation of each new constraint (cutting plane) requires solving another MILP problem.	 \citet{bjm2014aistats} showed that the problem of learning bounded treewidth Bayesian networks can be reduced to a weighted maximum satisfiability problem, and subsequently solved by weighted MAX-SAT solvers. They report experimental results showing that their approach outperforms \citeauthor{korhonen2exact}'s dynamic programming approach. In the same year, \citet{pfl2014aistats} showed that the problem can be reduced to a mixed-integer linear program (MILP), and then solved by off-the-shelf MILP optimizers (e. g.~CPLEX). Their reduced MILP problem however has exponentially many constraints in the number of variables. Following the work of \citet{cussens2}, the authors avoid creating such large programs by a cutting plane generation mechanism, which iteratively includes a new constraint while the optimum is not found. The generation of each new constraint (cutting plane) requires solving another MILP problem.	score:492
How can I apply for off-campus for MNCs of 2017 batch?	 Suppose that the outcome statistics of the set of quantum states are known. Can we infer the unknown quantum measurement from the quantum states at hand? How many samples of quantum states are needed for the learning machine to decide an optimal quantum measurement from the hypothesis set? Can the chosen candidate approximate the target measurement with the desired accuracy?  These questions are typical sample complexity problems in statistical learning theory, and the answer lies in a proper quantification of the ``effective size" of the hypothesis set. In this paper, we propose a framework (see Section \ref{Framework}) to connect the problems of learning two-outcome measurements with the tasks of learning real-valued linear functional on quantum states.	 Suppose that the outcome statistics of the set of quantum states are known. Can we infer the unknown quantum measurement from the quantum states at hand? How many samples of quantum states are needed for the learning machine to decide an optimal quantum measurement from the hypothesis set? Can the chosen candidate approximate the target measurement with the desired accuracy?  These questions are typical sample complexity problems in statistical learning theory, and the answer lies in a proper quantification of the ``effective size" of the hypothesis set. In this paper, we propose a framework (see Section \ref{Framework}) to connect the problems of learning two-outcome measurements with the tasks of learning real-valued linear functional on quantum states.	score:497
What would be the Wipro joining dates for campus placed 2017 batch?	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	score:385
What would be the Wipro joining dates for campus placed 2017 batch?	 However, the training phase continues to be a bottleneck, where the training data must be processed serially over thousands of individual training runs, thus leading to long training times.  This situation is only exacerbated by recent computer architecture trends where clock speeds are stagnant, and future speedups are available only through increased concurrency.     This situation repeats itself for the time-dependent PDE simulation community, where it has been identified as a sequential time-stepping bottleneck \cite{Fa2014}.  In response, interest in parallel-in-time methods, that can parallelize over the time domain, has grown considerably in the last decade.  One of the most well known parallel-in-time methods is Parareal \cite{LiMaTu2001}, which is equivalent to a two-level multigrid method \cite{GaVa2007}.	 However, the training phase continues to be a bottleneck, where the training data must be processed serially over thousands of individual training runs, thus leading to long training times.  This situation is only exacerbated by recent computer architecture trends where clock speeds are stagnant, and future speedups are available only through increased concurrency.     This situation repeats itself for the time-dependent PDE simulation community, where it has been identified as a sequential time-stepping bottleneck \cite{Fa2014}.  In response, interest in parallel-in-time methods, that can parallelize over the time domain, has grown considerably in the last decade.  One of the most well known parallel-in-time methods is Parareal \cite{LiMaTu2001}, which is equivalent to a two-level multigrid method \cite{GaVa2007}.	score:394
What would be the Wipro joining dates for campus placed 2017 batch?	 The working principle behind \emph{approximate SCD} methods is to trade-off exactness of the greedy direction against the time spent to decide the steepest direction (e.g. \citep{stich_approximate_2017}). For smooth problems, \citet{DhillonNearestNeighborbased2011} show that \emph{approximate nearest neighbor search} algorithms can be used to provide in \emph{sublinear time} an approximate steepest descent direction.  We build upon these ideas and extend the framework to non-smooth composite problems, thereby capturing a significantly larger class of input problems. In particular we show how to efficiently map the {\steepsub} rule to an instance of maximum inner product search~($\MIPS$).            \paragraph{Contributions.} We analyze and advocate the use of the {\steepsub} greedy rule to compute the update direction for composite problems.	 The working principle behind \emph{approximate SCD} methods is to trade-off exactness of the greedy direction against the time spent to decide the steepest direction (e.g. \citep{stich_approximate_2017}). For smooth problems, \citet{DhillonNearestNeighborbased2011} show that \emph{approximate nearest neighbor search} algorithms can be used to provide in \emph{sublinear time} an approximate steepest descent direction.  We build upon these ideas and extend the framework to non-smooth composite problems, thereby capturing a significantly larger class of input problems. In particular we show how to efficiently map the {\steepsub} rule to an instance of maximum inner product search~($\MIPS$).            \paragraph{Contributions.} We analyze and advocate the use of the {\steepsub} greedy rule to compute the update direction for composite problems.	score:412
What would be the Wipro joining dates for campus placed 2017 batch?	 One major assumption that guarantees this successful translation is that data are accurately labeled. However, collecting true labels for large-scale datasets is often expensive, time-consuming, and sometimes impossible. For this reason, some weak but cheap supervision information has been exploited to boost learning performance. Such supervision includes side information \cite{xing2003distance}, privileged information \cite{vapnik2009new}, and weakly supervised information \cite{Law_2017_CVPR} based on semi-supervised data \cite{zhu2005semi,haeusser2017learning,Abbasnejad_2017_CVPR}, positive and unlabeled data \cite{du2014analysis}, or noisy labeled data \cite{misra2016seeing,Veit_2017_CVPR,han2018progressive,han2018co,cheng2017learning,gong2017learning}.	 One major assumption that guarantees this successful translation is that data are accurately labeled. However, collecting true labels for large-scale datasets is often expensive, time-consuming, and sometimes impossible. For this reason, some weak but cheap supervision information has been exploited to boost learning performance. Such supervision includes side information \cite{xing2003distance}, privileged information \cite{vapnik2009new}, and weakly supervised information \cite{Law_2017_CVPR} based on semi-supervised data \cite{zhu2005semi,haeusser2017learning,Abbasnejad_2017_CVPR}, positive and unlabeled data \cite{du2014analysis}, or noisy labeled data \cite{misra2016seeing,Veit_2017_CVPR,han2018progressive,han2018co,cheng2017learning,gong2017learning}.	score:412
What would be the Wipro joining dates for campus placed 2017 batch?	 He will thus reproduce different possible network conditions during each of the year's months, in terms of likely nodal wind generation and demand during that month. For each of the reproduced conditions, a UC problem will be solved given the specific future topology of the grid under the outages planned for this month. The planner will conduct this using simulation, iterating many times for each of the year's months, per each of the optional outage schedules.  The more accurate he wishes the result to be, the more wind and demand samples he should feed to his UC solver. Each resulting UC solution can be used to evaluate the outage schedule in multiple ways: counting the number of feasible UC programs; averaging UC cost; averaging load lost amount; used as a reference for calculating  costs in finer-grained hourly simulation, such as re-dispatch and re-commitment of generators.	 He will thus reproduce different possible network conditions during each of the year's months, in terms of likely nodal wind generation and demand during that month. For each of the reproduced conditions, a UC problem will be solved given the specific future topology of the grid under the outages planned for this month. The planner will conduct this using simulation, iterating many times for each of the year's months, per each of the optional outage schedules.  The more accurate he wishes the result to be, the more wind and demand samples he should feed to his UC solver. Each resulting UC solution can be used to evaluate the outage schedule in multiple ways: counting the number of feasible UC programs; averaging UC cost; averaging load lost amount; used as a reference for calculating  costs in finer-grained hourly simulation, such as re-dispatch and re-commitment of generators.	score:413
What does matrix multiplication do?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	}   In this paper, we extend the theory of low-rank matrix completion to a collection of multiple and heterogeneous matrices. We first consider general matrix completion setting where we assume that for each matrix its  entries are sampled from natural exponential distributions~\citep{lehmCase98}. In this setting, we may have Gaussian distribution for continuous data; Bernoulli for binary data; Poisson for count-data, etc.  In a second part, we relax the assumption of exponential family distribution for the noise and we do not assume any specific model for the observations.  This approach is more popular and widely used in machine learning.  The proposed estimation procedure is based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix.	score:360
What does matrix multiplication do?	 Section \ref{Fully connected (FC) layer} shows the details of matrix multiplication in fully connected layer. Then, Section \ref{Common explanation on convolutional (CONV) layer} introduces the common explanations about convolutional operations. Section \ref{Converting convolutional operation to matrix multiplication} demonstrates how to convert the convolutional operation to matrix multiplication.  Section \ref{Experiments} shows the result of a simple experiment on training two equivalent networks, a fully connected network and a convolutional neural network.  \textbf{Notation}: In the rest of the note, scalar variables are denoted as non-bold font lowercases, e.g., $c$ and $s$ are scalar values. Matrix and vectors are denoted  by bold font capitals and lowercases respectively. For example, $\mathbf{W} \in \mathbb{R}^{a \times b}$ means a matrix of the shape $a \times b$ and $\mathbf{x} \in \mathbb{R}^{d\times 1}$ means a column vector with $d$ dimensions.	 Section \ref{Fully connected (FC) layer} shows the details of matrix multiplication in fully connected layer. Then, Section \ref{Common explanation on convolutional (CONV) layer} introduces the common explanations about convolutional operations. Section \ref{Converting convolutional operation to matrix multiplication} demonstrates how to convert the convolutional operation to matrix multiplication.  Section \ref{Experiments} shows the result of a simple experiment on training two equivalent networks, a fully connected network and a convolutional neural network.  \textbf{Notation}: In the rest of the note, scalar variables are denoted as non-bold font lowercases, e.g., $c$ and $s$ are scalar values. Matrix and vectors are denoted  by bold font capitals and lowercases respectively. For example, $\mathbf{W} \in \mathbb{R}^{a \times b}$ means a matrix of the shape $a \times b$ and $\mathbf{x} \in \mathbb{R}^{d\times 1}$ means a column vector with $d$ dimensions.	score:361
What does matrix multiplication do?	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	score:404
What does matrix multiplication do?	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	score:405
What does matrix multiplication do?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:411
Linear Algebra: Why is matrix multiplication defined the way it is?	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	score:299
Linear Algebra: Why is matrix multiplication defined the way it is?	 Section \ref{Fully connected (FC) layer} shows the details of matrix multiplication in fully connected layer. Then, Section \ref{Common explanation on convolutional (CONV) layer} introduces the common explanations about convolutional operations. Section \ref{Converting convolutional operation to matrix multiplication} demonstrates how to convert the convolutional operation to matrix multiplication.  Section \ref{Experiments} shows the result of a simple experiment on training two equivalent networks, a fully connected network and a convolutional neural network.  \textbf{Notation}: In the rest of the note, scalar variables are denoted as non-bold font lowercases, e.g., $c$ and $s$ are scalar values. Matrix and vectors are denoted  by bold font capitals and lowercases respectively. For example, $\mathbf{W} \in \mathbb{R}^{a \times b}$ means a matrix of the shape $a \times b$ and $\mathbf{x} \in \mathbb{R}^{d\times 1}$ means a column vector with $d$ dimensions.	 Section \ref{Fully connected (FC) layer} shows the details of matrix multiplication in fully connected layer. Then, Section \ref{Common explanation on convolutional (CONV) layer} introduces the common explanations about convolutional operations. Section \ref{Converting convolutional operation to matrix multiplication} demonstrates how to convert the convolutional operation to matrix multiplication.  Section \ref{Experiments} shows the result of a simple experiment on training two equivalent networks, a fully connected network and a convolutional neural network.  \textbf{Notation}: In the rest of the note, scalar variables are denoted as non-bold font lowercases, e.g., $c$ and $s$ are scalar values. Matrix and vectors are denoted  by bold font capitals and lowercases respectively. For example, $\mathbf{W} \in \mathbb{R}^{a \times b}$ means a matrix of the shape $a \times b$ and $\mathbf{x} \in \mathbb{R}^{d\times 1}$ means a column vector with $d$ dimensions.	score:313
Linear Algebra: Why is matrix multiplication defined the way it is?	 Intuitively, the tensor completion problem could be solved with matrix completion algorithms by downgrading the problem into a matrix level, typically by either slicing a tensor into multiple small matrices or unfolding it into one big matrix. However, several problems distinguish tensor completion from being treated as a straightforward extension of the matrix completion problem.  First, it has been shown that matrix completion algorithms may break the multi-way structure of a tensor and lose the redundancy among modes to improve the imputation accuracy~\cite{signoretto2011tensor}. While many tensor-based algorithms directly build upon matrix completion algorithms~\cite{mu2014square,xu2013parallel}, their key focus is out of the context of matrix level, i. e., trying to develop delicate ways of matricization to keep the multi-way properties of a tensor object while using matrix-based completion algorithms. Second, the high-order characteristics introduce even higher space and computational complexity, which may prevent the usage of traditional matrix completion algorithms.         \begin{figure} \centering \vspace{-0. 1cm} \includegraphics[height=3.7cm, width=13cm]{outline2.pdf}  \vspace{-0.2cm}  \caption{Outline of this survey.} \label{fig:outline}   \vspace{-0.4cm}  \end{figure}     In some early works~\cite{bro1998multi,tomasi2005parafac,andersson1998improving,smilde2005multi}, tensor completion is often considered as a byproduct when dealing with missing data in the tensor decomposition problem.	e., trying to develop delicate ways of matricization to keep the multi-way properties of a tensor object while using matrix-based completion algorithms. Second, the high-order characteristics introduce even higher space and computational complexity, which may prevent the usage of traditional matrix completion algorithms.         \begin{figure} \centering \vspace{-0.	score:313
Linear Algebra: Why is matrix multiplication defined the way it is?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	  In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	score:321
Linear Algebra: Why is matrix multiplication defined the way it is?	 The covariance matrix of the model is approximated by a block-diagonal matrix. The structure of this matrix is detected by thresholding the sample covariance matrix, where the threshold is selected using the slope heuristic. Based on the block-diagonal structure of the covariance matrix, the estimation problem is divided into several independent problems: subsequently, the network of dependencies between variables is inferred using the graphical lasso algorithm in each block.  The performance of the procedure is illustrated on simulated data. An application to a real gene expression dataset with a limited sample size is also presented: the dimension reduction allows attention to be objectively focused on interactions among smaller subsets of genes, leading to a more parsimonious and interpretable modular network.        		 The covariance matrix of the model is approximated by a block-diagonal matrix. The structure of this matrix is detected by thresholding the sample covariance matrix, where the threshold is selected using the slope heuristic. Based on the block-diagonal structure of the covariance matrix, the estimation problem is divided into several independent problems: subsequently, the network of dependencies between variables is inferred using the graphical lasso algorithm in each block.  The performance of the procedure is illustrated on simulated data. An application to a real gene expression dataset with a limited sample size is also presented: the dimension reduction allows attention to be objectively focused on interactions among smaller subsets of genes, leading to a more parsimonious and interpretable modular network.        		score:332
What is the difference between image classification and image annotation?	  A typical image labeling task begins with a set of instructions to the annotator, showing them example images from the classes of interest.  The annotator is then asked to assign class labels to new images where the ground truth is unknown.   But what happens if the annotator is unsure?  This is a real problem when annotators are incorrectly assumed to have prior knowledge of the classes of interest from which they can generalize, either from everyday life or from specialized training.   For many problems, highly specialized, domain specific knowledge, acquired through extensive training, is needed before someone can differentiate between potentially multiple, highly self-similar object categories.    \begin{figure}[t!]   \centering   \includegraphics[width=\linewidth]{teaching_overview_new}   \caption{In Interactive Machine Teaching the computer teaches the human learner, one image at a time.	  A typical image labeling task begins with a set of instructions to the annotator, showing them example images from the classes of interest.  The annotator is then asked to assign class labels to new images where the ground truth is unknown.   But what happens if the annotator is unsure?  This is a real problem when annotators are incorrectly assumed to have prior knowledge of the classes of interest from which they can generalize, either from everyday life or from specialized training.   For many problems, highly specialized, domain specific knowledge, acquired through extensive training, is needed before someone can differentiate between potentially multiple, highly self-similar object categories.    \begin{figure}[t!]   \centering   \includegraphics[width=\linewidth]{teaching_overview_new}   \caption{In Interactive Machine Teaching the computer teaches the human learner, one image at a time.	score:315
What is the difference between image classification and image annotation?	 \end{itemize}  In general, the algorithm can be applied to many different tasks (namely, binary and multi-class semantic segmentation, and also with trivial adjustments to classification and regression) where there are several annotators labeling each image.  	With exponential data growth it becomes possible to construct high-performance systems based on neural networks.  Unfortunately, one cannot simply collect a lot of data and feed it into neural network. Supervised methods of machine learning require labeled data which are scarce and not infrequently are far from being perfect.  To make annotation more accurate, a dataset can be labeled by several annotators but that is where new problems arise. For instance, different annotators can annotate the same image in different ways and very often their decisions are not similar and sometimes even mutually exclusive.	 \end{itemize}  In general, the algorithm can be applied to many different tasks (namely, binary and multi-class semantic segmentation, and also with trivial adjustments to classification and regression) where there are several annotators labeling each image.  	With exponential data growth it becomes possible to construct high-performance systems based on neural networks.  Unfortunately, one cannot simply collect a lot of data and feed it into neural network. Supervised methods of machine learning require labeled data which are scarce and not infrequently are far from being perfect.  To make annotation more accurate, a dataset can be labeled by several annotators but that is where new problems arise. For instance, different annotators can annotate the same image in different ways and very often their decisions are not similar and sometimes even mutually exclusive.	score:320
What is the difference between image classification and image annotation?	 	How would you describe why the image in Figure \ref{fig:introbird} looks like a clay colored sparrow? Perhaps the bird's head and wing bars look like those of a prototypical clay colored sparrow.  When we describe how we classify images, we might focus on parts of the image and compare them with prototypical parts of images from a given class. This method of reasoning is commonly used in difficult identification tasks: e. g., radiologists compare suspected tumors in X-ray scans with prototypical tumor images for diagnosis of cancer \cite{HoltEtAl2005}. The question is whether we can ask a machine learning model to imitate this way of thinking, and to explain its reasoning process in a human-understandable way.   \begin{figure}[t]   \centering     \includegraphics[scale=0.	 In this work, we introduce a network architecture -- \textit{prototypical part network} (ProtoPNet), that accommodates this definition of interpretability, where comparison of image parts to learned prototypes is integral to the way our network reasons about new examples.  Given a new bird image as in Figure \ref{fig:introbird}, our model is able to identify several parts of the image where it thinks that \textit{this} part of the image looks like \textit{that} prototypical part of some class, and makes its prediction based on a weighted combination of the similarity scores between parts of the image and the learned prototypes.	score:324
What is the difference between image classification and image annotation?	 Such typical application is clustering of images and documents \cite{cai2011graph,liu2012constrained}. However, in real world supervised or semi-supervised classification applications, the  class labels of the training data samples are usually available, which is ignored by most existed NMF methods. If the class label information could be unutilized during the representation procedure, the discriminative ability of the representation could be improved significantly.  To this end, some supervised and semi-supervised NMF are proposed. For example, Wang and Jia \cite{wang2004fisher} proposed the Fisher nonnegative matrix factorization (FNMF) method to encode discrimination information for a classification problem by imposing Fisher constraints on the NMF algorithm. Lee et al. \cite{lee2010semi} proposed the semi-supervised nonnegative matrix factorization (SSNMF) by jointly incorporating the data matrix and the (partial) class label matrix into NMF.	 Such typical application is clustering of images and documents \cite{cai2011graph,liu2012constrained}. However, in real world supervised or semi-supervised classification applications, the  class labels of the training data samples are usually available, which is ignored by most existed NMF methods. If the class label information could be unutilized during the representation procedure, the discriminative ability of the representation could be improved significantly.  To this end, some supervised and semi-supervised NMF are proposed. For example, Wang and Jia \cite{wang2004fisher} proposed the Fisher nonnegative matrix factorization (FNMF) method to encode discrimination information for a classification problem by imposing Fisher constraints on the NMF algorithm. Lee et al. \cite{lee2010semi} proposed the semi-supervised nonnegative matrix factorization (SSNMF) by jointly incorporating the data matrix and the (partial) class label matrix into NMF.	score:335
What is the difference between image classification and image annotation?	 Convolutional neural networks have been shown to develop internal representations, which correspond closely to semantically meaningful objects and parts, although trained solely on class labels. Class Activation Mapping (CAM) is a recent method that makes it possible to easily highlight the image regions contributing to a network's classification decision.  We build upon these two developments to enable a network to re-examine informative image regions, which we term \emph{introspection}. We propose a weakly-supervised iterative scheme, which shifts its center of attention to increasingly discriminative regions as it progresses, by alternating stages of classification and introspection. We evaluate our method and show its effectiveness over a range of several datasets, where we obtain competitive or state-of-the-art results: on Stanford-40 Actions, we set a new state-of the art of 81.	 Convolutional neural networks have been shown to develop internal representations, which correspond closely to semantically meaningful objects and parts, although trained solely on class labels. Class Activation Mapping (CAM) is a recent method that makes it possible to easily highlight the image regions contributing to a network's classification decision.  We build upon these two developments to enable a network to re-examine informative image regions, which we term \emph{introspection}. We propose a weakly-supervised iterative scheme, which shifts its center of attention to increasingly discriminative regions as it progresses, by alternating stages of classification and introspection. We evaluate our method and show its effectiveness over a range of several datasets, where we obtain competitive or state-of-the-art results: on Stanford-40 Actions, we set a new state-of the art of 81.	score:336
What is so special about VMC extreme x batch?	  The vMF distribution is analogous to a \emph{symmetric} Gaussian distribution, wrapped around a unit sphere. As such, it is useful for modelling directional data that is symmetrically distributed with respect to a mean direction. The modelling of asymmetrically distributed directional data, however, requires distributions which generalize the vMF distribution.  A generalization of vMF is called the Fisher-Bingham distribution  \citep{mardia1975statistics} which takes the form: \begin{equation} f(\boldx;\boldtheta) \propto \exp\{\kappa \mean\trans\boldx\                          + \beta_2 (\major\trans\boldx)^2                         + \beta_3 (\minor\trans\boldx)^2\}  \label{eqn:fb_density} \end{equation} where the parameters $\mean,\major,\minor$ are unit vectors with $\major$ and $\minor$ being orthogonal to each other, the parameters   $\beta_2$ and $\beta_3$ are real values with $\beta_2 \ge \beta_3$.	   The importance of vMF and \fb~distributions in mixture modelling tasks has been well established: vMF mixtures have been used in large-scale text clustering \citep{Banerjee:generative-clustering,gopal2014mises},  clustering of protein dihedral angles  \citep{dowe1996circular,mardia2007protein}, and gene expression  analyses \citep{Banerjee:clustering-hypersphere}.  Mixtures of \fb~distributions have been employed by \citet{peel2001fitting} to identify joint sets in rock masses, and by \citet{hamelryck2006sampling} to sample random protein conformations. The \fb~distribution has increasingly found support in machine learning tasks in structural bioinformatics \citep{kent2005using,boomsma2006graphical,hamelryck2009probabilistic}.	score:456
What is so special about VMC extreme x batch?	 	Large hierarchical classification with more than 10000 categories is a challenging task (\cite{partalas2015lshtc}). Traditionally, hierarchical classification can be done by training a classifier on the flattened labels (\cite{babbar2013flat}) or by training a classifier at each hierarchical node (\cite{silla2011survey}), whereby each hierarchical node is a decision maker of which subsequent node to route to.	 	Large hierarchical classification with more than 10000 categories is a challenging task (\cite{partalas2015lshtc}). Traditionally, hierarchical classification can be done by training a classifier on the flattened labels (\cite{babbar2013flat}) or by training a classifier at each hierarchical node (\cite{silla2011survey}), whereby each hierarchical node is a decision maker of which subsequent node to route to.	score:463
What is so special about VMC extreme x batch?	   The importance of vMF and \fb~distributions in mixture modelling tasks has been well established: vMF mixtures have been used in large-scale text clustering \citep{Banerjee:generative-clustering,gopal2014mises},  clustering of protein dihedral angles  \citep{dowe1996circular,mardia2007protein}, and gene expression  analyses \citep{Banerjee:clustering-hypersphere}.  Mixtures of \fb~distributions have been employed by \citet{peel2001fitting} to identify joint sets in rock masses, and by \citet{hamelryck2006sampling} to sample random protein conformations. The \fb~distribution has increasingly found support in machine learning tasks in structural bioinformatics \citep{kent2005using,boomsma2006graphical,hamelryck2009probabilistic}.	   The importance of vMF and \fb~distributions in mixture modelling tasks has been well established: vMF mixtures have been used in large-scale text clustering \citep{Banerjee:generative-clustering,gopal2014mises},  clustering of protein dihedral angles  \citep{dowe1996circular,mardia2007protein}, and gene expression  analyses \citep{Banerjee:clustering-hypersphere}.  Mixtures of \fb~distributions have been employed by \citet{peel2001fitting} to identify joint sets in rock masses, and by \citet{hamelryck2006sampling} to sample random protein conformations. The \fb~distribution has increasingly found support in machine learning tasks in structural bioinformatics \citep{kent2005using,boomsma2006graphical,hamelryck2009probabilistic}.	score:464
What is so special about VMC extreme x batch?	 Our proposed scheme, called ``Deep Packet,'' can handle both \emph{traffic characterization} in which the network traffic is categorized into major classes (\eg, FTP and P2P) and \emph{application identification} in which end-user applications (\eg, BitTorrent and Skype) identification is desired. Contrary to most of the current methods, Deep Packet can identify encrypted traffic and also distinguishes between VPN and non-VPN  network traffic.  After an initial pre-processing phase on data, packets are fed into Deep Packet framework that embeds stacked autoencoder  and convolution neural network in order to classify network traffic. Deep packet with CNN as its  classification model achieved recall of $0.98$ in application identification task and $0.94$ in traffic categorization task. To the best of our knowledge, Deep Packet outperforms all of the proposed classification methods on UNB ISCX VPN-nonVPN dataset.	 Our proposed scheme, called ``Deep Packet,'' can handle both \emph{traffic characterization} in which the network traffic is categorized into major classes (\eg, FTP and P2P) and \emph{application identification} in which end-user applications (\eg, BitTorrent and Skype) identification is desired. Contrary to most of the current methods, Deep Packet can identify encrypted traffic and also distinguishes between VPN and non-VPN  network traffic.  After an initial pre-processing phase on data, packets are fed into Deep Packet framework that embeds stacked autoencoder  and convolution neural network in order to classify network traffic. Deep packet with CNN as its  classification model achieved recall of $0.98$ in application identification task and $0.94$ in traffic categorization task. To the best of our knowledge, Deep Packet outperforms all of the proposed classification methods on UNB ISCX VPN-nonVPN dataset.	score:468
What is so special about VMC extreme x batch?	  	As public databases of chemical compounds, like PubChem \cite{kim2015pubchem}\cite{wang2016pubchem} or ChEMBL \cite{bento2014chembl}, and private databases owned pharmaceutical companies are developed, there is growing demand to apply them to improve estimation of molecular characteristics or molecular design in medicinal and material science.  One of the most difficult obstacles to achieve this goal is that it can be almost impossible to collect annotated labels.	  	As public databases of chemical compounds, like PubChem \cite{kim2015pubchem}\cite{wang2016pubchem} or ChEMBL \cite{bento2014chembl}, and private databases owned pharmaceutical companies are developed, there is growing demand to apply them to improve estimation of molecular characteristics or molecular design in medicinal and material science.  One of the most difficult obstacles to achieve this goal is that it can be almost impossible to collect annotated labels.	score:471
What skills are needed for machine learning jobs?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:427
What skills are needed for machine learning jobs?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:440
What skills are needed for machine learning jobs?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:446
What skills are needed for machine learning jobs?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:452
What skills are needed for machine learning jobs?	 One would hope for much stronger theoretical guarantees of success, but no widely used machine learning methods provide them, so it is difficult to know when they will work.  One key question faced by any machine learning system is: what kind of models will it generate? In neural networks one asks whether the architecture is feed-forward or recurrent and how many layers it has.  In genetic programming one asks for the program representation and which functions are built-in. In general, each machine learning system must make this choice. If the class of generated models or programs is too broad, it might be impossible to learn them efficiently. If it is too narrow, it might not suffice for the task at hand. To solve a different task, one might need a different kind of model.	 One would hope for much stronger theoretical guarantees of success, but no widely used machine learning methods provide them, so it is difficult to know when they will work.  One key question faced by any machine learning system is: what kind of models will it generate? In neural networks one asks whether the architecture is feed-forward or recurrent and how many layers it has.  In genetic programming one asks for the program representation and which functions are built-in. In general, each machine learning system must make this choice. If the class of generated models or programs is too broad, it might be impossible to learn them efficiently. If it is too narrow, it might not suffice for the task at hand. To solve a different task, one might need a different kind of model.	score:454
Who or what labs are doing the most advanced research in machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:354
Who or what labs are doing the most advanced research in machine learning?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:386
Who or what labs are doing the most advanced research in machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:392
Who or what labs are doing the most advanced research in machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:400
Who or what labs are doing the most advanced research in machine learning?	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	score:402
Andrew Ng: Who or what lab is doing the most advanced research in machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:343
Andrew Ng: Who or what lab is doing the most advanced research in machine learning?	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	score:367
Andrew Ng: Who or what lab is doing the most advanced research in machine learning?	 \textbf{Introduction:} Machine learning provides fundamental tools both for scientific research and for the development of technologies with significant impact on society. It provides methods that facilitate the discovery of regularities in data and that give predictions without explicit knowledge of the rules governing a system. However, a price is paid for exploiting such flexibility: machine learning methods are typically black-boxes where it is difficult to fully understand what the machine is doing or how it is operating.  This poses constraints on the applicability and explainability of such methods. \textbf{Methods:} Our research aims to open the black-box of recurrent neural networks, an important family of neural networks used for processing sequential data. We propose a novel methodology that provides a mechanistic interpretation of behaviour when solving a computational task.	 \textbf{Introduction:} Machine learning provides fundamental tools both for scientific research and for the development of technologies with significant impact on society. It provides methods that facilitate the discovery of regularities in data and that give predictions without explicit knowledge of the rules governing a system. However, a price is paid for exploiting such flexibility: machine learning methods are typically black-boxes where it is difficult to fully understand what the machine is doing or how it is operating.  This poses constraints on the applicability and explainability of such methods. \textbf{Methods:} Our research aims to open the black-box of recurrent neural networks, an important family of neural networks used for processing sequential data. We propose a novel methodology that provides a mechanistic interpretation of behaviour when solving a computational task.	score:380
Andrew Ng: Who or what lab is doing the most advanced research in machine learning?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:380
Andrew Ng: Who or what lab is doing the most advanced research in machine learning?	  Arguments in recent debates have claimed that it is only the accuracy of machine learning models that matters, not their interpretability.  However, taking this view ignores the fact that the overall system in high-stakes settings is a machine learning model communicating with a human who makes the final decision, and thus it is the accuracy of the overall system that is of relevance.   Interpretable machine learning models are an appropriate means for communication between AI and human \cite{DhurandharILS2017}; the contribution of this paper is to abstractly model the overall system and theoretically show the system performance advantage of interpretable machine learning models over black box machine learning models.  In this paper, we consider the population setting (the limit as the number of samples goes to infinity, allowing access to the probability distributions of the data) and appeal to the theory of distributed detection and data fusion \cite{Varshney1997}.	  Arguments in recent debates have claimed that it is only the accuracy of machine learning models that matters, not their interpretability.  However, taking this view ignores the fact that the overall system in high-stakes settings is a machine learning model communicating with a human who makes the final decision, and thus it is the accuracy of the overall system that is of relevance.   Interpretable machine learning models are an appropriate means for communication between AI and human \cite{DhurandharILS2017}; the contribution of this paper is to abstractly model the overall system and theoretically show the system performance advantage of interpretable machine learning models over black box machine learning models.  In this paper, we consider the population setting (the limit as the number of samples goes to infinity, allowing access to the probability distributions of the data) and appeal to the theory of distributed detection and data fusion \cite{Varshney1997}.	score:384
What data science and machine learning career opportunities are there at Apple?	 APPLE efficiently computes the solution path for the penalized likelihood estimator using a hybrid of the modified predictor-corrector method and the coordinate-descent algorithm. APPLE is compared with several well-known packages via simulation and analysis of two gene expression data sets.  \keywords{APPLE \and LASSO \and MCP \and penalized likelihood estimator \and solution path} 	 Variable selection is a vital tool in statistical analysis of high-dimensional data. Typically, a large number of potential predictors are included during the first stage of modeling, in order to avoid missing important links between a predictor and the outcome. This practice has become more popular in recent years for two primary reasons. First, in many recently promising fields, such as bioinformatics, genetics and finance, more and more high-throughput and high-dimensional data are being generated.	 APPLE efficiently computes the solution path for the penalized likelihood estimator using a hybrid of the modified predictor-corrector method and the coordinate-descent algorithm. APPLE is compared with several well-known packages via simulation and analysis of two gene expression data sets.  \keywords{APPLE \and LASSO \and MCP \and penalized likelihood estimator \and solution path} 	 Variable selection is a vital tool in statistical analysis of high-dimensional data. Typically, a large number of potential predictors are included during the first stage of modeling, in order to avoid missing important links between a predictor and the outcome. This practice has become more popular in recent years for two primary reasons. First, in many recently promising fields, such as bioinformatics, genetics and finance, more and more high-throughput and high-dimensional data are being generated.	score:351
What data science and machine learning career opportunities are there at Apple?	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	score:363
What data science and machine learning career opportunities are there at Apple?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:366
What data science and machine learning career opportunities are there at Apple?	  We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines.  We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets.   Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.	Privacy has become a growing concern, due to the massive increase in personal information stored in electronic databases, such as medical records, financial records, web search histories, and social network data.   Machine learning can be employed to discover novel population-wide patterns, however the results of such algorithms may reveal certain individuals' sensitive information, thereby violating their privacy. Thus, an emerging challenge for machine learning is how to learn from datasets that contain sensitive personal information.   At the first glance, it may appear that simple anonymization of private information is enough to preserve privacy.  However, this is often not the case; even if obvious identifiers, such as names and addresses, are removed from the data, the remaining fields can still form unique ``signatures'' that can help re-identify individuals. Such attacks have been demonstrated by  various works, and are possible in many realistic settings, such as when an adversary has side information \citep{Sweeney:97weaving,netflix,GKS08}, and when the data has structural properties \citep{livejournal}, among others.	  We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines.  We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets.   Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.	Privacy has become a growing concern, due to the massive increase in personal information stored in electronic databases, such as medical records, financial records, web search histories, and social network data.   Machine learning can be employed to discover novel population-wide patterns, however the results of such algorithms may reveal certain individuals' sensitive information, thereby violating their privacy. Thus, an emerging challenge for machine learning is how to learn from datasets that contain sensitive personal information.   At the first glance, it may appear that simple anonymization of private information is enough to preserve privacy.  However, this is often not the case; even if obvious identifiers, such as names and addresses, are removed from the data, the remaining fields can still form unique ``signatures'' that can help re-identify individuals. Such attacks have been demonstrated by  various works, and are possible in many realistic settings, such as when an adversary has side information \citep{Sweeney:97weaving,netflix,GKS08}, and when the data has structural properties \citep{livejournal}, among others.	score:369
What data science and machine learning career opportunities are there at Apple?	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	score:375
What data science and machine learning career opportunities are there at Fitbit?	       	Data volumes are growing at a faster rate than available computing power, storage space, or network bandwidth.  This has fueled interest in distributed approaches to machine learning. However, the substantial jump in communication costs between a multicore and a multicomputer system currently confines many popular techniques to the single computer regime.  Consequently, even those machine learning workflows that today originate with distributed data in a clustered environment often terminate with the learning problem being solved on a single machine.  Furthermore, because of computation, storage, or network limitations there is often a subsampling step between the data store and the single machine.  Our concern here is to make the subsampling step as statistically efficient as possible, knowing that the data will subsequently be given to an empirical risk minimization (ERM) algorithm.	       	Data volumes are growing at a faster rate than available computing power, storage space, or network bandwidth.  This has fueled interest in distributed approaches to machine learning. However, the substantial jump in communication costs between a multicore and a multicomputer system currently confines many popular techniques to the single computer regime.  Consequently, even those machine learning workflows that today originate with distributed data in a clustered environment often terminate with the learning problem being solved on a single machine.  Furthermore, because of computation, storage, or network limitations there is often a subsampling step between the data store and the single machine.  Our concern here is to make the subsampling step as statistically efficient as possible, knowing that the data will subsequently be given to an empirical risk minimization (ERM) algorithm.	score:363
What data science and machine learning career opportunities are there at Fitbit?	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	score:387
What data science and machine learning career opportunities are there at Fitbit?	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	score:391
What data science and machine learning career opportunities are there at Fitbit?	 It relies on several research areas including statistics and machine learning. Usually, machine learning techniques are developed around flat data representation (i.e., matrix form) and are known as propositional learning approaches. However, due to the development of communication and storage technologies, data management practices have taken further aspects.  Data can present a very large number of dimensions, with several different types of entities. With the growing interest in extracting patterns from such data representation, relational data mining approaches have emerged with the interest of finding patterns in a given relational database~\cite{datamining01} and Statistical Relational Learning (SRL) has emerged as an area of machine learning that enables effective and robust reasoning about relational data structures~\cite{SL07}.	 It relies on several research areas including statistics and machine learning. Usually, machine learning techniques are developed around flat data representation (i.e., matrix form) and are known as propositional learning approaches. However, due to the development of communication and storage technologies, data management practices have taken further aspects.  Data can present a very large number of dimensions, with several different types of entities. With the growing interest in extracting patterns from such data representation, relational data mining approaches have emerged with the interest of finding patterns in a given relational database~\cite{datamining01} and Statistical Relational Learning (SRL) has emerged as an area of machine learning that enables effective and robust reasoning about relational data structures~\cite{SL07}.	score:396
What data science and machine learning career opportunities are there at Fitbit?	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	score:400
My best friend rejected me saying she wanted to finish college first. Few months later, she started dating someone in my batch. Why do I feel bad?	 Our method first performs a deterministic step (computation of the gradient of the objective function  at the starting point), followed by a large number of stochastic steps. The process is repeated a few times with the last iterate becoming the new starting point. The novelty of our method is in introduction of mini-batching into the computation of stochastic steps.  In each step, instead of choosing a single function, we sample $b$ functions, compute their gradients, and compute the direction based on this. We analyze the complexity of the method and show that it benefits from two speedup effects. First,  we prove that as long as $b$ is below a certain threshold, we can reach any predefined accuracy with less overall work than without mini-batching.	 Our method first performs a deterministic step (computation of the gradient of the objective function  at the starting point), followed by a large number of stochastic steps. The process is repeated a few times with the last iterate becoming the new starting point. The novelty of our method is in introduction of mini-batching into the computation of stochastic steps.  In each step, instead of choosing a single function, we sample $b$ functions, compute their gradients, and compute the direction based on this. We analyze the complexity of the method and show that it benefits from two speedup effects. First,  we prove that as long as $b$ is below a certain threshold, we can reach any predefined accuracy with less overall work than without mini-batching.	score:381
My best friend rejected me saying she wanted to finish college first. Few months later, she started dating someone in my batch. Why do I feel bad?	 In the first part of this paper, we provide a detailed description of a large and varied set of large-scale experiments. We will discuss various trade-offs encountered during the training procedure. Certain settings of training parameters can lead to initially good results, but later lead to a \emph{catastrophic} degradation in performance. This phenomenon is highly undesirable and we will provide guidelines on how to avoid it, as well as an explanation of such phenomena.   In the second part of this paper, we show that surprisingly, it is possible to gain insight into the operating principle or inner workings of an MLP trained on image denoising. This is the least difficult for MLPs with a single hidden layer, but we will show that MLPs with more hidden layers are also interpretable through analysis of the activation patterns of the hidden units.	 In the first part of this paper, we provide a detailed description of a large and varied set of large-scale experiments. We will discuss various trade-offs encountered during the training procedure. Certain settings of training parameters can lead to initially good results, but later lead to a \emph{catastrophic} degradation in performance. This phenomenon is highly undesirable and we will provide guidelines on how to avoid it, as well as an explanation of such phenomena.   In the second part of this paper, we show that surprisingly, it is possible to gain insight into the operating principle or inner workings of an MLP trained on image denoising. This is the least difficult for MLPs with a single hidden layer, but we will show that MLPs with more hidden layers are also interpretable through analysis of the activation patterns of the hidden units.	score:393
My best friend rejected me saying she wanted to finish college first. Few months later, she started dating someone in my batch. Why do I feel bad?	  First we consider the expert's predictions when she observes all data. During the first block, our expert will increase her probability of a one from $\half$ to nearly one. Then during the second block it will go down to $\half$ again. During the third block it will increase from $\half$ up, but slower. Thus, for block two the expert is extremely bad, while for block three she is at best mediocre.  (See \figref{fig:example}.)  Compare this to the expert's predictions on the subsequences. During the subsequence of ones (first and third block), our expert will increase her probability of a one from $\half$ to almost one, while during the subsequence of zeroes she will decrease her probability from $\half$ to nearly zero. Thus, the expert is much better on the subsequences in isolation.	  First we consider the expert's predictions when she observes all data. During the first block, our expert will increase her probability of a one from $\half$ to nearly one. Then during the second block it will go down to $\half$ again. During the third block it will increase from $\half$ up, but slower. Thus, for block two the expert is extremely bad, while for block three she is at best mediocre.  (See \figref{fig:example}.)  Compare this to the expert's predictions on the subsequences. During the subsequence of ones (first and third block), our expert will increase her probability of a one from $\half$ to almost one, while during the subsequence of zeroes she will decrease her probability from $\half$ to nearly zero. Thus, the expert is much better on the subsequences in isolation.	score:393
My best friend rejected me saying she wanted to finish college first. Few months later, she started dating someone in my batch. Why do I feel bad?	  However, as the following example will illustrate, it may be beneficial if experts learn only from the segment for which they are selected, because they may get confused by data in other segments that follow a different pattern. We call this the \emph{local learners} interpretation of tracking the best expert (LL-TBE). As a slight complication, it will turn out that in LL-TBE we have a further choice: whether to tell a learning expert the timing of its segment or not, which generally makes a difference.  When segment timing is preserved, we call the resulting reference scheme \emph{sleeping LL-TBE}; when segment timing is \emph{not} preserved we call the reference scheme \emph{freezing LL-TBE}. The next example demonstrates that S-TBE and the two variants of LL-TBE are really different reference schemes.  \paragraph{Example: Drifting Mean}  In applications one would usually build up complicated prediction strategies from simpler ones in a hierarchical fashion.	 These are the learning experts that can be described in EHMM form. Although this excludes learning experts that for example implement follow-the-perturbed-leader, the class of EHMMs is still rich enough to be of interest, if only because it includes all ordinary HMMs.  In the interpretation of the two LL-TBE reference schemes for learning experts in EHMM form, we do need to be careful if the base experts in the EHMMs are learning themselves: because we make no assumptions about the base experts, they always learn from all the data.   \paragraph{Main Result: Achieving LL-TBE Efficiently}  We present two new algorithms: $\fs^\sl$ for sleeping LL-TBE and $\fs^\fr$ for freezing LL-TBE, which both generalise $\fs$. We show that these algorithms achieve the same regret bound compared to their respective LL-TBE reference schemes as $\fs$ achieves compared to the S-TBE reference scheme.	score:393
My best friend rejected me saying she wanted to finish college first. Few months later, she started dating someone in my batch. Why do I feel bad?	g., using a different decoder for each task. The later is the one we investigate in this paper. However, our setup differs from previous works in that we focus an information-theoretic formulation of the MTL problem. We should also mention that we restrict our setup to MTL scenarios  where the inputs are common to all tasks. Although this can be mathematically equivalent to the problem of multi-label learning (MLL), there are some important differences (see~\cite{zhang_review_2014} for further details).    \subsection{Our contribution}  We first introduce an information-theoretic paradigm which provides the fundamental tradeoff between the \emph{log-loss} (average risk) and the information rate of the features (statistical model complexity). We derive an iterative Arimoto-Blahut like algorithm to address the non-convex optimization problem of the IB method in presence of side information available only at the decoder~\cite{wyner_ziv76, our_isit15}, as described in Fig.	g., using a different decoder for each task. The later is the one we investigate in this paper. However, our setup differs from previous works in that we focus an information-theoretic formulation of the MTL problem. We should also mention that we restrict our setup to MTL scenarios  where the inputs are common to all tasks. Although this can be mathematically equivalent to the problem of multi-label learning (MLL), there are some important differences (see~\cite{zhang_review_2014} for further details).    \subsection{Our contribution}  We first introduce an information-theoretic paradigm which provides the fundamental tradeoff between the \emph{log-loss} (average risk) and the information rate of the features (statistical model complexity). We derive an iterative Arimoto-Blahut like algorithm to address the non-convex optimization problem of the IB method in presence of side information available only at the decoder~\cite{wyner_ziv76, our_isit15}, as described in Fig.	score:394
Can machine learning be used for financial time-series forecasting?	 At the same time, machine learning---one of the primary tools of the modern data scientist---has experienced a revitalization as academics, businesses, and governments alike discovered new applications for automated algorithms that can learn from data. As a consequence, there has been a growing demand for tools that make machine learning more accessible, scalable, and flexible.  Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.	 At the same time, machine learning---one of the primary tools of the modern data scientist---has experienced a revitalization as academics, businesses, and governments alike discovered new applications for automated algorithms that can learn from data. As a consequence, there has been a growing demand for tools that make machine learning more accessible, scalable, and flexible.  Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.	score:388
Can machine learning be used for financial time-series forecasting?	   Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance.   Most of these applications need to be able to compare these structured data.   In this context, dynamic time warping (DTW) is probably the most common comparison measure.   However, not much research effort has been put into improving it by learning.    In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification.   Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification.   The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers.	   Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance.   Most of these applications need to be able to compare these structured data.   In this context, dynamic time warping (DTW) is probably the most common comparison measure.   However, not much research effort has been put into improving it by learning.    In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification.   Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification.   The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers.	score:391
Can machine learning be used for financial time-series forecasting?	   Transductive learning is a natural choice for learning problems where the locations of the test samples are known at training time.  For instance, consider the task of predicting where a particular person is named during one thousand hours of speech. Because of time, financial, or technical constraints, it may be feasible to manually label only a small fraction of the speech frames, to be used as training set.  Since the speech frames for both training and test samples are known, this would be a learning problem well suited for transduction.  More generally, transductive learning has found a wide and diverse range of successful applications, including text categorization, image colorization, image compression, image segmentation, reconstruction of protein interaction networks, speech tagging, and statistical machine translation; all of these discussed and referenced in \citep[Section 1.	   Transductive learning is a natural choice for learning problems where the locations of the test samples are known at training time.  For instance, consider the task of predicting where a particular person is named during one thousand hours of speech. Because of time, financial, or technical constraints, it may be feasible to manually label only a small fraction of the speech frames, to be used as training set.  Since the speech frames for both training and test samples are known, this would be a learning problem well suited for transduction.  More generally, transductive learning has found a wide and diverse range of successful applications, including text categorization, image colorization, image compression, image segmentation, reconstruction of protein interaction networks, speech tagging, and statistical machine translation; all of these discussed and referenced in \citep[Section 1.	score:392
Can machine learning be used for financial time-series forecasting?	 Deep Learning methods are capable of modeling highly non-linear, very complex data, making them suitable for application to financial data \cite{langkvist2014review}, as well as time series forecasting \cite{rahman2016layered}.  Furthermore, ML techniques which perform feature extraction may uncover robust features, better-suited to the specific task at hand.  Autoencoders \cite{vincent2008extracting}, are Neural Networks which learn new features extracted from the original input space, which can be used to enhance the performance of various tasks, such as classification or regression. Bag-of-Features (BoF) models comprise another feature extraction method that can be used to extract representations of objects described by multiple feature vectors, such as time-series \cite{baydogan2013bag,iosifidis2013multidimensional}.	 	Forecasting of financial time series is a very challenging problem and has attracted scientific interest in the past few decades. Due to the inherently noisy and non-stationary nature of financial time series, statistical models are unsuitable for the task of modeling and forecasting such data. Thus, the nature of financial data necessitates the utilization of more sophisticated methods, capable of modeling complex non-linear relationships between data, such as Machine Learning (ML) algorithms.   Early Machine Learning approaches to this problem included shallow Neural Networks (NNs) \cite{kaastra1995forecasting,kaastra1996designing}, and Support Vector Machines (SVMs) \cite{tay2001application,cao2003support,lu2009financial}. However, the lack of appropriate training and regularization algorithms for Neural Networks at the time, such as the dropout technique \cite{srivastava2014dropout}, rendered them susceptible to over fitting the training data.	score:419
Can machine learning be used for financial time-series forecasting?	 It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, ``learning to learn'' as they do so.  In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal.   Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm.  Our aim is to learn new internal representations as the algorithm learns new target functions,  that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data.	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	score:422
Delaware Registered Agents: When we get the result of Y combinator 2014 winter batch application? Did anyone got a mail from YC?	 The approach is resilient to catastrophic forgetting \citep{mccloskey89, ratcliff90}; on the contrary, results show that performance continues to improve on earlier tasks even when only training on later tasks. In total, the result is a method that can gather knowledge from a variety of weak supervision, distill it into a cumulative, re-usable library, and use the library within induced algorithms to exhibit strong generalization.  } \begin{figure*}[t] \centering                                                                           \includegraphics[width=\textwidth]{resources/navigation.pdf} \caption{\small Components of an illustrative \neuralterpretshort program for learning loopy programs that measure path length (\texttt{path\_len}) through a maze of street sign images.	 The approach is resilient to catastrophic forgetting \citep{mccloskey89, ratcliff90}; on the contrary, results show that performance continues to improve on earlier tasks even when only training on later tasks. In total, the result is a method that can gather knowledge from a variety of weak supervision, distill it into a cumulative, re-usable library, and use the library within induced algorithms to exhibit strong generalization.  } \begin{figure*}[t] \centering                                                                           \includegraphics[width=\textwidth]{resources/navigation.pdf} \caption{\small Components of an illustrative \neuralterpretshort program for learning loopy programs that measure path length (\texttt{path\_len}) through a maze of street sign images.	score:345
Delaware Registered Agents: When we get the result of Y combinator 2014 winter batch application? Did anyone got a mail from YC?	 Heterogeneity may arise from the interaction structure, the information available to the agents as well as from heterogeneous agent strategies as an effect of learning in finite populations, and we will concentrate on the latter here. In particular, we will show that heterogeneity can by accounted for  in a macroscopic model formulation by the correction term introduced in \cite{Olbrich2012}.	 However, in order to consume the nut and derive utility $y$ from this consumption agents have to find a trading partner, that is, another agent with a nut. Therefore, the agents have to base their harvest decision \emph{now} (by setting $c_i$) on their expectation to find a trading partner \emph{in the future}. Or, less metaphorically, agents are faced with production decisions that have to be evaluated based on their expectations about the future utility of the produced entity which in turn depends on the global production level via a trading mechanism.	score:346
Delaware Registered Agents: When we get the result of Y combinator 2014 winter batch application? Did anyone got a mail from YC?	 It deals with the setup where an agent interacts with an environment and at each time step it sees the current \textit{state} (or just an \textit{observation}) and takes one \textit{action}, which will determine the immediate \textit{reward} $R_t$ it will get in the next state.  Typically, the agent is interested in maximising the \textit{expected value} of the return $G_t$, which is often defined as the discounted sum of all rewards in the trajectory.   \[ G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \]   A policy $\pi(a|s)$ is a probability distribution over actions, conditioned on the current state. A \textit{state-value function}, $v^{\pi}$, is a function mapping from states to real numbers, defined as the expected return, when the agent starts in a given state $s$ and then behaves according to the policy $\pi$.	 It deals with the setup where an agent interacts with an environment and at each time step it sees the current \textit{state} (or just an \textit{observation}) and takes one \textit{action}, which will determine the immediate \textit{reward} $R_t$ it will get in the next state.  Typically, the agent is interested in maximising the \textit{expected value} of the return $G_t$, which is often defined as the discounted sum of all rewards in the trajectory.   \[ G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \]   A policy $\pi(a|s)$ is a probability distribution over actions, conditioned on the current state. A \textit{state-value function}, $v^{\pi}$, is a function mapping from states to real numbers, defined as the expected return, when the agent starts in a given state $s$ and then behaves according to the policy $\pi$.	score:354
Delaware Registered Agents: When we get the result of Y combinator 2014 winter batch application? Did anyone got a mail from YC?	        Recent results indicate that significant improvements can be made when better approximate inference methods are used: \citet{Salimans-et-al-arxiv2014} for example  presented an iterative inference procedure that improves the samples from $q$ by employing a learned MCMC transition operator. In \citep{hjelm2015iterative} the authors propose an iterative, gradient based procedure to refine the approximate posterior.	        Recent results indicate that significant improvements can be made when better approximate inference methods are used: \citet{Salimans-et-al-arxiv2014} for example  presented an iterative inference procedure that improves the samples from $q$ by employing a learned MCMC transition operator. In \citep{hjelm2015iterative} the authors propose an iterative, gradient based procedure to refine the approximate posterior.	score:362
Delaware Registered Agents: When we get the result of Y combinator 2014 winter batch application? Did anyone got a mail from YC?	  In this approach, an agent considers what the collaborator, the opposite agent cooperating in dialog, will respond by using an explicit approximated model of the collaborator. If one wishes to efficiently convey information to the other, it is best to converse in a way that maximizes the other's understanding \cite{bruner1981}.  For our method, we consider the mind to be beyond a part of mental states (e. g., belief, intent, knowledge). The mind is the probabilistic distribution of the model of the collaborator itself.   \begin{figure}[t]  \centering \includegraphics[width=0.48\textwidth]{icml18-fig1-k} \hspace{0.4cm} \includegraphics[width=0.45\textwidth]{cvpr18-fig2-c} \caption{(Left) Illustration of an AQM algorithm for goal-oriented visual dialog.	  In this approach, an agent considers what the collaborator, the opposite agent cooperating in dialog, will respond by using an explicit approximated model of the collaborator. If one wishes to efficiently convey information to the other, it is best to converse in a way that maximizes the other's understanding \cite{bruner1981}.  For our method, we consider the mind to be beyond a part of mental states (e. g., belief, intent, knowledge). The mind is the probabilistic distribution of the model of the collaborator itself.   \begin{figure}[t]  \centering \includegraphics[width=0.48\textwidth]{icml18-fig1-k} \hspace{0.4cm} \includegraphics[width=0.45\textwidth]{cvpr18-fig2-c} \caption{(Left) Illustration of an AQM algorithm for goal-oriented visual dialog.	score:370
What is machine learning and how it is linked to Big Data/Data Mining?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	score:322
What is machine learning and how it is linked to Big Data/Data Mining?	       	Data volumes are growing at a faster rate than available computing power, storage space, or network bandwidth.  This has fueled interest in distributed approaches to machine learning. However, the substantial jump in communication costs between a multicore and a multicomputer system currently confines many popular techniques to the single computer regime.  Consequently, even those machine learning workflows that today originate with distributed data in a clustered environment often terminate with the learning problem being solved on a single machine.  Furthermore, because of computation, storage, or network limitations there is often a subsampling step between the data store and the single machine.  Our concern here is to make the subsampling step as statistically efficient as possible, knowing that the data will subsequently be given to an empirical risk minimization (ERM) algorithm.	       	Data volumes are growing at a faster rate than available computing power, storage space, or network bandwidth.  This has fueled interest in distributed approaches to machine learning. However, the substantial jump in communication costs between a multicore and a multicomputer system currently confines many popular techniques to the single computer regime.  Consequently, even those machine learning workflows that today originate with distributed data in a clustered environment often terminate with the learning problem being solved on a single machine.  Furthermore, because of computation, storage, or network limitations there is often a subsampling step between the data store and the single machine.  Our concern here is to make the subsampling step as statistically efficient as possible, knowing that the data will subsequently be given to an empirical risk minimization (ERM) algorithm.	score:340
What is machine learning and how it is linked to Big Data/Data Mining?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	score:343
What is machine learning and how it is linked to Big Data/Data Mining?	 One of the most vital uses of this data is in modeling, visualizing, and analyzing large data sets through probabilistic tools. Statistical machine learning is at the core of numerous such applications in what is becoming known as Internet of Things (IoT). Such iterative learning mechanisms help in better control performance for many cyber-physical-systems in which estimation of parameters and system-identification is required.  Probabilistic graphical modeling is one key research area that has helped in data analysis in inference and prediction tasks, \cite{koller2009probabilistic}. These models visually express assumptions about data and its hidden structure. Posterior inference algorithms have been proven to exploit such models in explaining this hidden structure while being adaptive, robust, parallelizable, and scalable.	 One of the most vital uses of this data is in modeling, visualizing, and analyzing large data sets through probabilistic tools. Statistical machine learning is at the core of numerous such applications in what is becoming known as Internet of Things (IoT). Such iterative learning mechanisms help in better control performance for many cyber-physical-systems in which estimation of parameters and system-identification is required.  Probabilistic graphical modeling is one key research area that has helped in data analysis in inference and prediction tasks, \cite{koller2009probabilistic}. These models visually express assumptions about data and its hidden structure. Posterior inference algorithms have been proven to exploit such models in explaining this hidden structure while being adaptive, robust, parallelizable, and scalable.	score:349
What is machine learning and how it is linked to Big Data/Data Mining?	 To extract useful knowledge and make appropriate decisions from big data, machine learning techniques have been regarded as a powerful solution. As the hottest subfield of machine learning, deep learning is able to act as a bridge connecting big machinery data and intelligent machine health monitoring.   As a branch of machine learning, deep learning attempts to model high level representations behind data and classify(predict) patterns via stacking multiple layers of information processing modules in hierarchical architectures.  Recently, deep learning has been successfully adopted in various areas such as computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics \cite{ren2015faster,collobert2008unified,hinton2012deep,leung2014deep}. In fact, deep learning is not a new idea, which even dates back to the 1940s \cite{888,lecun2015deep}.	 To extract useful knowledge and make appropriate decisions from big data, machine learning techniques have been regarded as a powerful solution. As the hottest subfield of machine learning, deep learning is able to act as a bridge connecting big machinery data and intelligent machine health monitoring.   As a branch of machine learning, deep learning attempts to model high level representations behind data and classify(predict) patterns via stacking multiple layers of information processing modules in hierarchical architectures.  Recently, deep learning has been successfully adopted in various areas such as computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics \cite{ren2015faster,collobert2008unified,hinton2012deep,leung2014deep}. In fact, deep learning is not a new idea, which even dates back to the 1940s \cite{888,lecun2015deep}.	score:352
Is it relatively easy to get into the MSCS program at University of Chicago, as compared to other top universities? I have been reading a lot and from what I have read I believe their Theory and NLP Groups are strong.	 However, one cannot afford the cost of communications. This paper aims to find an algorithm with acceptable cost of communications and maximum factor of speed-up.  Due to the cost of communications, we can find that traditional methods for $\X$-armed bandits such as StoSOO algorithm \cite{valko2013stochastic} and HOO algorithm \cite{bubeck2011x} can hardly be put into a distributed framework.  These methods all operates in many traversals of a covering tree from the root down to leaves. If two players traverse the covering tree without communication, they cannot know whether they are visiting the same node. Thus, the whole process may be inefficient and uncoordinated unless large number of communication rounds are set in the algorithm.  Motivated by this, we propose a novel distributed algorithm for the $\X$-armed bandit problem with multiple communication rounds.	 However, one cannot afford the cost of communications. This paper aims to find an algorithm with acceptable cost of communications and maximum factor of speed-up.  Due to the cost of communications, we can find that traditional methods for $\X$-armed bandits such as StoSOO algorithm \cite{valko2013stochastic} and HOO algorithm \cite{bubeck2011x} can hardly be put into a distributed framework.  These methods all operates in many traversals of a covering tree from the root down to leaves. If two players traverse the covering tree without communication, they cannot know whether they are visiting the same node. Thus, the whole process may be inefficient and uncoordinated unless large number of communication rounds are set in the algorithm.  Motivated by this, we propose a novel distributed algorithm for the $\X$-armed bandit problem with multiple communication rounds.	score:313
Is it relatively easy to get into the MSCS program at University of Chicago, as compared to other top universities? I have been reading a lot and from what I have read I believe their Theory and NLP Groups are strong.	 We show that the derived algorithms are simple to implement, provably convergent under the ADMM theory, and that they effectively solve complex robust CS formulations.  The paper is organized follows. Section II gives some background on robust CS, whilst Section III describes the FISTA and ADMM algorithms for solving the robust CS formulation. Section IV presents four extensions of the robust CS formulation and derive computationally efficient algorithms for solving them. Section V contains numerical experiments to demonstrate the computational efficiency of the proposed algorithms. Finally, Section VI concludes the paper.  All Matlab code to implement our methods described in this paper and reproduce our results is readily available at the following website \url{http://www.computing.edu.au/~dsp/code.php}.	 We show that the derived algorithms are simple to implement, provably convergent under the ADMM theory, and that they effectively solve complex robust CS formulations.  The paper is organized follows. Section II gives some background on robust CS, whilst Section III describes the FISTA and ADMM algorithms for solving the robust CS formulation. Section IV presents four extensions of the robust CS formulation and derive computationally efficient algorithms for solving them. Section V contains numerical experiments to demonstrate the computational efficiency of the proposed algorithms. Finally, Section VI concludes the paper.  All Matlab code to implement our methods described in this paper and reproduce our results is readily available at the following website \url{http://www.computing.edu.au/~dsp/code.php}.	score:326
Is it relatively easy to get into the MSCS program at University of Chicago, as compared to other top universities? I have been reading a lot and from what I have read I believe their Theory and NLP Groups are strong.	 However, both their methodology and theory are still within the $\ell_1$ regularization framework, and their conditions (especially their C-Ridge and C-OLS conditions) are overly strong and can be easily violated in practice. \citet{jain2014iterative} proposed an iterative hard thresholding  algorithm for sparse regression, which shares a similar spirit of hard thresholding as our algorithm.  Nevertheless, their motivation is completely different, their algorithm lacks theoretical guarantees for consistent support recovery, and they require an iterative estimation procedure.  \subsection{Our Contributions} We provide a generalized form of OLS for fitting  high dimensional data motivated by ridge regression, and develop two algorithms that can consistently fit linear models on weakly sparse coefficients.	 However, both their methodology and theory are still within the $\ell_1$ regularization framework, and their conditions (especially their C-Ridge and C-OLS conditions) are overly strong and can be easily violated in practice. \citet{jain2014iterative} proposed an iterative hard thresholding  algorithm for sparse regression, which shares a similar spirit of hard thresholding as our algorithm.  Nevertheless, their motivation is completely different, their algorithm lacks theoretical guarantees for consistent support recovery, and they require an iterative estimation procedure.  \subsection{Our Contributions} We provide a generalized form of OLS for fitting  high dimensional data motivated by ridge regression, and develop two algorithms that can consistently fit linear models on weakly sparse coefficients.	score:327
Is it relatively easy to get into the MSCS program at University of Chicago, as compared to other top universities? I have been reading a lot and from what I have read I believe their Theory and NLP Groups are strong.	 Recently, it is of great interest to cluster the subspaces while the observations w.r.t. some coordinates are missing. To resolve the issue, as an application in this paper, our theorem relates robust MC to a certain subspace clustering model -- the so-called extended robust Low-Rank Representation (LRR). Thus one could hope to correctly recover the structure of multiple subspaces, if robust MC is able to complete the unavailable values and remove the outlier samples at an overwhelming probability.  This is guaranteed by our paper.  \subsection{Related Work} Suppose that $L_0$ is an $m\times n$ data matrix of rank $r$ whose columns are sample points, and entries are partially observed among the set $\mathcal{K}_{obs}$. The MC problem aims at exactly recovering $L_0$, or the range space of $L_0$, from the measured elements. Probably the most well-known MC model was proposed by Cand\`{e}s et al.	 Recently, it is of great interest to cluster the subspaces while the observations w.r.t. some coordinates are missing. To resolve the issue, as an application in this paper, our theorem relates robust MC to a certain subspace clustering model -- the so-called extended robust Low-Rank Representation (LRR). Thus one could hope to correctly recover the structure of multiple subspaces, if robust MC is able to complete the unavailable values and remove the outlier samples at an overwhelming probability.  This is guaranteed by our paper.  \subsection{Related Work} Suppose that $L_0$ is an $m\times n$ data matrix of rank $r$ whose columns are sample points, and entries are partially observed among the set $\mathcal{K}_{obs}$. The MC problem aims at exactly recovering $L_0$, or the range space of $L_0$, from the measured elements. Probably the most well-known MC model was proposed by Cand\`{e}s et al.	score:334
Is it relatively easy to get into the MSCS program at University of Chicago, as compared to other top universities? I have been reading a lot and from what I have read I believe their Theory and NLP Groups are strong.	 This algorithm tries to directly approximate the optimal Bayesian policy using a one-step lookahead heuristic, but unfortunately there are no performance guarantees for this method. \citet{deshpande2012linear} consider a linear bandit problem with dimension that is too large relative to the desired horizon. They propose an algorithm that limits exploration and learns something useful within this short time frame.  \citet{berry1997bandit, wang2009algorithms} and \citet{bonald2013two} study an infinitely-armed bandit problem in which it's impossible to identify an optimal action and propose algorithms to minimizes the asymptotic growth rate of regret. While we will instantiate our general regret bound for STS on the infinitely-armed bandit problem, we use this example mostly to provide a simple analytic illustration. We hope that the flexibility of STS and our analysis framework allow this work to be applied to more complicated time-sensitive learning problems.	 \citet{berry1997bandit, wang2009algorithms} and \citet{bonald2013two} study an infinitely-armed bandit problem in which it's impossible to identify an optimal action and propose algorithms to minimizes the asymptotic growth rate of regret. While we will instantiate our general regret bound for STS on the infinitely-armed bandit problem, we use this example mostly to provide a simple analytic illustration. We hope that the flexibility of STS and our analysis framework allow this work to be applied to more complicated time-sensitive learning problems.	score:335
How do I learn mathematics for machine learning?	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:418
How do I learn mathematics for machine learning?	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.    \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.	score:422
How do I learn mathematics for machine learning?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:431
How do I learn mathematics for machine learning?	 To address this gap between computational modeling and behavioral psychology, we introduce here a mathematical framework for studying how behavior effects learning and develop a novel model of learning-driven exploration.  In Computational Neuroscience,  machine learning techniques have been successfully applied towards modeling how the brain might learn the structure underlying sensory signals, e. g., \cite{olshausen1996emergence,rehn2007network,lewicki2002efficient,5766096,1556155,serre2007robust,crutchfield1987equations}. Generally, these methods focus on passive learning where the learning system can not directly effect the sensory input it receives. Exploration, in contrast, is inherently active, and can only occur in the context of a closed-action perception loop.	 To address this gap between computational modeling and behavioral psychology, we introduce here a mathematical framework for studying how behavior effects learning and develop a novel model of learning-driven exploration.  In Computational Neuroscience,  machine learning techniques have been successfully applied towards modeling how the brain might learn the structure underlying sensory signals, e. g., \cite{olshausen1996emergence,rehn2007network,lewicki2002efficient,5766096,1556155,serre2007robust,crutchfield1987equations}. Generally, these methods focus on passive learning where the learning system can not directly effect the sensory input it receives. Exploration, in contrast, is inherently active, and can only occur in the context of a closed-action perception loop.	score:433
How do I learn mathematics for machine learning?	 \item Showing how machine learning techniques can be integrated into   this framework, and demonstrating how training models on   simpler expressions can help which the discovery of more complex ones. \item A novel application of a recursive neural-network to learn a continuous representation of    mathematical structures, making the symbolic domain accessible to   many other learning approaches.  \item The discovery of many new mathematical identities which offer a significant reduction   in computational complexity for certain expressions. \end{compactitem}   \begin{minipage}{\linewidth} \begin{framed} \begin{flushleft}  \vspace{0mm}  {\bf Example 1:} Assume we are given matrices $A \in \mathbb{R}^{n \times m}$, $B \in \mathbb{R}^{m \times p}$.	   In this paper we explore how machine learning techniques can be   applied to the discovery of efficient mathematical identities. We   introduce an attribute grammar framework for representing symbolic   expressions. Given a grammar of math operators, we build trees that   combine them in different ways, looking for compositions that are analytically equivalent to a target   expression but of lower computational complexity.  However, as the   space of trees grows exponentially with the complexity of the   target expression, brute force search is impractical for all but the   simplest of expressions. Consequently, we introduce two novel   learning approaches that are able to learn from simpler expressions   to guide the tree search. The first of these is a simple $n$-gram   model, the other being a recursive neural-network.	score:436
Should I use powers of 2 when choosing the size of a batch size when training my Neural Network?	 This scheme also yields a training dataset of  $\sum_{i=1}^m {k_i \choose 2}$ pairs of examples for training and testing the deep siamese network, which is far more likely to be of sufficient size for training a generalizable model. We evaluate our  neural ranking approach against a standard weighted aggregation algorithm in which weights are based on forecasters' average past performance.	 This scheme also yields a training dataset of  $\sum_{i=1}^m {k_i \choose 2}$ pairs of examples for training and testing the deep siamese network, which is far more likely to be of sufficient size for training a generalizable model. We evaluate our  neural ranking approach against a standard weighted aggregation algorithm in which weights are based on forecasters' average past performance.	score:336
Should I use powers of 2 when choosing the size of a batch size when training my Neural Network?	  \item For 2-layer neural networks with ReLU activations, we obtain an explicit relationship between the step size of the gradient descent algorithm and the output of the solution that the algorithm can converge to.  \end{enumerate}  \subsection{Related work}     It is a well-known problem that the gradient of the training cost function can become disproportionate for different parameters when training a neural network.  Several works in the literature tried to~address this problem. For example, changing the geometry of optimization was proposed in (Neyshabur et al., 2017) and a regularized descent algorithm was proposed to prevent the gradients from exploding and vanishing during training.     Deep residual networks, which is a specific class of neural networks, yielded exceptional results in practice with their peculiar structure (He et al.	  \item For 2-layer neural networks with ReLU activations, we obtain an explicit relationship between the step size of the gradient descent algorithm and the output of the solution that the algorithm can converge to.  \end{enumerate}  \subsection{Related work}     It is a well-known problem that the gradient of the training cost function can become disproportionate for different parameters when training a neural network.  Several works in the literature tried to~address this problem. For example, changing the geometry of optimization was proposed in (Neyshabur et al., 2017) and a regularized descent algorithm was proposed to prevent the gradients from exploding and vanishing during training.     Deep residual networks, which is a specific class of neural networks, yielded exceptional results in practice with their peculiar structure (He et al.	score:394
Should I use powers of 2 when choosing the size of a batch size when training my Neural Network?	 We were able to obtain $80\%$ accuracy training with $-2\%,+2\%$ deviations, and the same trained model can generalize for more relaxed deviations with increasing performance. We also show that the model is capable of generalizing for larger problem sizes. Finally, we provide a method for predicting the optimal route cost within $2\%$ deviation from the ground truth.	 We were able to obtain $80\%$ accuracy training with $-2\%,+2\%$ deviations, and the same trained model can generalize for more relaxed deviations with increasing performance. We also show that the model is capable of generalizing for larger problem sizes. Finally, we provide a method for predicting the optimal route cost within $2\%$ deviation from the ground truth.	score:402
Should I use powers of 2 when choosing the size of a batch size when training my Neural Network?	 We propose a new method to efficiently compute load-flows (the steady-state of the power-grid for given productions, consumptions and grid topology), substituting conventional simulators based on differential equation solvers. We use a deep feed-forward neural network trained with load-flows precomputed by simulation. Our architecture permits to train a network on so-called ``n-1'' problems, in which load flows are evaluated for every possible line disconnection, then generalize to ``n-2'' problems without re-training (a clear advantage because of the combinatorial nature of the problem). To that end, we developed a technique bearing similarity with ``dropout'', which we named ``guided dropout''.	 We propose a new method to efficiently compute load-flows (the steady-state of the power-grid for given productions, consumptions and grid topology), substituting conventional simulators based on differential equation solvers. We use a deep feed-forward neural network trained with load-flows precomputed by simulation. Our architecture permits to train a network on so-called ``n-1'' problems, in which load flows are evaluated for every possible line disconnection, then generalize to ``n-2'' problems without re-training (a clear advantage because of the combinatorial nature of the problem). To that end, we developed a technique bearing similarity with ``dropout'', which we named ``guided dropout''.	score:404
Should I use powers of 2 when choosing the size of a batch size when training my Neural Network?	 More precisely, the parameters are constrained to $2^s\times\{0,\pm 2^{1-2^{b-2}}, \pm 2^{2-2^{b-2}}, \dots, \pm 1\}$ associated with a layerwise scaling factor $2^s$, $s$ an integer depending only on the weight maximum in the layer. At inference time, the original floating-point multiplication operations can be replaced by faster and cheaper binary bit shifting.  The quantization scheme of \cite{inq_17} is however heuristic.  In this paper, we present the exact solution of the general $b$-bit approximation problem of a real weight vector $W^f$ in the least squares sense. If $b=2$ and the dimension of $W^f$ is $N$, the computational complexity of the 2 bit solution is $O(N\log N)$. At $b\geq 3$, the combinatorial nature of the solution renders direct computation too expensive for large scale tasks.	 More precisely, the parameters are constrained to $2^s\times\{0,\pm 2^{1-2^{b-2}}, \pm 2^{2-2^{b-2}}, \dots, \pm 1\}$ associated with a layerwise scaling factor $2^s$, $s$ an integer depending only on the weight maximum in the layer. At inference time, the original floating-point multiplication operations can be replaced by faster and cheaper binary bit shifting.  The quantization scheme of \cite{inq_17} is however heuristic.  In this paper, we present the exact solution of the general $b$-bit approximation problem of a real weight vector $W^f$ in the least squares sense. If $b=2$ and the dimension of $W^f$ is $N$, the computational complexity of the 2 bit solution is $O(N\log N)$. At $b\geq 3$, the combinatorial nature of the solution renders direct computation too expensive for large scale tasks.	score:410
What are the best machine learning online courses?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:378
What are the best machine learning online courses?	  Arguments in recent debates have claimed that it is only the accuracy of machine learning models that matters, not their interpretability.  However, taking this view ignores the fact that the overall system in high-stakes settings is a machine learning model communicating with a human who makes the final decision, and thus it is the accuracy of the overall system that is of relevance.   Interpretable machine learning models are an appropriate means for communication between AI and human \cite{DhurandharILS2017}; the contribution of this paper is to abstractly model the overall system and theoretically show the system performance advantage of interpretable machine learning models over black box machine learning models.  In this paper, we consider the population setting (the limit as the number of samples goes to infinity, allowing access to the probability distributions of the data) and appeal to the theory of distributed detection and data fusion \cite{Varshney1997}.	  Arguments in recent debates have claimed that it is only the accuracy of machine learning models that matters, not their interpretability.  However, taking this view ignores the fact that the overall system in high-stakes settings is a machine learning model communicating with a human who makes the final decision, and thus it is the accuracy of the overall system that is of relevance.   Interpretable machine learning models are an appropriate means for communication between AI and human \cite{DhurandharILS2017}; the contribution of this paper is to abstractly model the overall system and theoretically show the system performance advantage of interpretable machine learning models over black box machine learning models.  In this paper, we consider the population setting (the limit as the number of samples goes to infinity, allowing access to the probability distributions of the data) and appeal to the theory of distributed detection and data fusion \cite{Varshney1997}.	score:421
What are the best machine learning online courses?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:422
What are the best machine learning online courses?	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	score:426
What are the best machine learning online courses?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:427
What are some great online courses on machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:392
What are some great online courses on machine learning?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:393
What are some great online courses on machine learning?	 Most machine learning and deep neural network algorithms rely on certain iterative algorithms to optimise their utility/cost functions, e.g. Stochastic Gradient Descent (SGD). In distributed learning, the networked nodes have to work collaboratively to update the model parameters, and the way how they proceed is referred to as synchronous parallel design (or barrier control).  Synchronous parallel protocol is practically the building block of all distributed learning frameworks, and its design has direct impact on the performance and scalability of the system.  In this paper, we propose a new barrier control technique - Probabilistic Synchronous Parallel (PSP). Comparing to the previous Bulk Synchronous Parallel (BSP), Stale Synchronous Parallel (SSP), and (Asynchronous Parallel) ASP, the proposed solution effectively improves both the convergence speed and the scalability of the SGD algorithm by introducing a sampling primitive into the system.	 Most machine learning and deep neural network algorithms rely on certain iterative algorithms to optimise their utility/cost functions, e.g. Stochastic Gradient Descent (SGD). In distributed learning, the networked nodes have to work collaboratively to update the model parameters, and the way how they proceed is referred to as synchronous parallel design (or barrier control).  Synchronous parallel protocol is practically the building block of all distributed learning frameworks, and its design has direct impact on the performance and scalability of the system.  In this paper, we propose a new barrier control technique - Probabilistic Synchronous Parallel (PSP). Comparing to the previous Bulk Synchronous Parallel (BSP), Stale Synchronous Parallel (SSP), and (Asynchronous Parallel) ASP, the proposed solution effectively improves both the convergence speed and the scalability of the SGD algorithm by introducing a sampling primitive into the system.	score:396
What are some great online courses on machine learning?	 This paradigm creates a bottleneck where the knowledge accumulated by machines depends on highly trained human experts to interpret, and to conveying to humans. This remains a barrier to the broader usefulness of machine learning. While machines can communicate perfectly among themselves by exchanging bits, humans communicate with data---they teach.  The purposeful selection of data plays a featured role in theories of cognition \citep{sperber1986relevance}, cognitive development \citep{Gergely2007}, and culture \citep{Tomasello2005}. In each of these cases, teaching is conceived of as purposeful, rather than random, selection of small set of examples, with the goal of facilitating accurate inferences about a body of knowledge.	 This paradigm creates a bottleneck where the knowledge accumulated by machines depends on highly trained human experts to interpret, and to conveying to humans. This remains a barrier to the broader usefulness of machine learning. While machines can communicate perfectly among themselves by exchanging bits, humans communicate with data---they teach.  The purposeful selection of data plays a featured role in theories of cognition \citep{sperber1986relevance}, cognitive development \citep{Gergely2007}, and culture \citep{Tomasello2005}. In each of these cases, teaching is conceived of as purposeful, rather than random, selection of small set of examples, with the goal of facilitating accurate inferences about a body of knowledge.	score:396
What are some great online courses on machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:403
Why do we write A=IA for row operations and A=AI for column operation to find inverse of a matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	  In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	score:308
Why do we write A=IA for row operations and A=AI for column operation to find inverse of a matrix?	    Our main contribution is to survey and compare  techniques to solve this mathematical difficulty: \begin{itemize} \item We may  require $M$ to be a diagonal matrix. \item We can use a pseudoinverse. \item We can apply covariance shrinkage. \end{itemize} Moreover, we can either learn one such distance measure for the entire data set, or one distance measure per class.  To our knowledge, there was no attempt  to compare these alternatives in the context of time series.  After comparing these alternatives, we present two main findings: \begin{itemize} \item   We get significantly poorer classification accuracy  when using pseudoinverses. Indeed, the pseudoinverse approach generates twice the error rate of  the covariance shrinkage or diagonal-matrix approach.	    Our main contribution is to survey and compare  techniques to solve this mathematical difficulty: \begin{itemize} \item We may  require $M$ to be a diagonal matrix. \item We can use a pseudoinverse. \item We can apply covariance shrinkage. \end{itemize} Moreover, we can either learn one such distance measure for the entire data set, or one distance measure per class.  To our knowledge, there was no attempt  to compare these alternatives in the context of time series.  After comparing these alternatives, we present two main findings: \begin{itemize} \item   We get significantly poorer classification accuracy  when using pseudoinverses. Indeed, the pseudoinverse approach generates twice the error rate of  the covariance shrinkage or diagonal-matrix approach.	score:317
Why do we write A=IA for row operations and A=AI for column operation to find inverse of a matrix?	   One can attempt to solve  the robust tensor problem  in \eqref{eqn:robust-def}  using matrix methods. In other words, robust matrix PCA can be applied either to  each matrix slice of the tensor, or to the matrix obtained by flattening the tensor. However, such matrix methods ignore the  tensor algebraic constraints or the CP  rank constraints, which differ from the matrix rank constraints.  There are however a number of challenges to incorporating the tensor CP rank   constraints. Enforcing a given  tensor rank   is NP-hard~\cite{hillar2013most}, unlike the matrix case, where low rank projections can be computed efficiently. Moreover, finding the best convex relaxation of the tensor CP rank is also NP-hard~\cite{hillar2013most}, unlike   the matrix case, where the convex relaxation of the rank, \viz the nuclear norm,  can be computed efficiently.	 We compare our method with natural baselines, \viz which apply   robust  matrix   PCA either to the {\em flattened} tensor, or to the matrix slices of the tensor. Our method can provably handle a far greater level of perturbation  when the sparse  tensor is  block-structured.  Thus, we establish that tensor methods can tolerate  a higher level of gross corruptions compared to matrix methods.  	In this paper, we develop a robust tensor decomposition  method, which  recovers a low rank  tensor   subject to gross corruptions.  Given an input tensor $T=L^*+S^*$, we aim to recover both $L^*$ and $S^*$, where $\Lo$ is a low rank tensor and $\So$ is a sparse tensor \begin{equation}\label{eqn:robust-def} \M = \Lo+\So,\quad \Lo=\sum_{i=1}^r \sigma_i^* u_i\otimes u_i \otimes u_i\end{equation} and $\M, \Lo, \So\in \Rbb^{n \times n \times n}$.	score:321
Why do we write A=IA for row operations and A=AI for column operation to find inverse of a matrix?	 A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined \emph{blocks} of columns (or rows) from the matrix.	 A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined \emph{blocks} of columns (or rows) from the matrix.	score:327
Why do we write A=IA for row operations and A=AI for column operation to find inverse of a matrix?	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	 The problem of completing a low-rank matrix from a subset of its entries is often encountered in the analysis of incomplete data sets exhibiting an underlying factor model with applications in collaborative filtering, computer vision and control. Most recent work had been focused on constructing efficient algorithms for exact or approximate recovery of the missing matrix entries and proving lower bounds for the number of known entries that guarantee a successful recovery with high probability.	score:328
What is the determinant of the inverse of a matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	 The basic principle of matrix completion  consists in recovering all the entries of an unknown data matrix from incomplete and noisy observations of its entries.  To address the high-dimensionality in matrix completion problem, statistical inference based on low-rank constraint is now an ubiquitous technique for recovering the underlying data matrix.  Thus, matrix completion can be formulated as minimizing the rank of the matrix given a random sample of its entries. However, this rank minimization problem is in general NP-hard due to the combinatorial nature of the rank function~\citep{fazel2001,fazelPhD-2000}. To alleviate this problem and make it tractable, convex relaxation strategies were proposed, e.	score:406
What is the determinant of the inverse of a matrix?	 In mathematics, and in particular linear algebra, a pseudoinverse A+ of a matrix A is a generalization of the inverse matrix. The most widely known type of matrix pseudoinverse is the Moore–Penrose inverse, which was independently described by E. H. Moore in 1920, Arne Bjerhammar in 1951, and Roger Penrose in 1955. A common use of the pseudoinverse is to compute a `best fit' (least squares) solution to a system of linear equations that lacks a unique solution.	 In mathematics, and in particular linear algebra, a pseudoinverse A+ of a matrix A is a generalization of the inverse matrix. The most widely known type of matrix pseudoinverse is the Moore–Penrose inverse, which was independently described by E. H. Moore in 1920, Arne Bjerhammar in 1951, and Roger Penrose in 1955. A common use of the pseudoinverse is to compute a `best fit' (least squares) solution to a system of linear equations that lacks a unique solution.	score:421
What is the determinant of the inverse of a matrix?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:424
What is the determinant of the inverse of a matrix?	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	score:431
What is the determinant of the inverse of a matrix?	 A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined \emph{blocks} of columns (or rows) from the matrix.	 A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined \emph{blocks} of columns (or rows) from the matrix.	score:431
What is the salary range for classification levels H and F in Stanford University?	 The performance of the classifier is then evaluated based on the whole curve using a specific operating point (i.e., a desired FPR level) or by considering the area under the curve (AUC). The area under the curve is an interesting measure since it as a probabilistic interpretation. The area under the curve of a classifier $h(x), \mathbb{R}^n \to \mathbb{R} $ is the probability that for a random positive sample $x^+$ and a  random negative sample $x^-$ the classifier will produce $h(x^+)>h(x^-)$.  In this paper we show that the thresholding approach can be refined such that performance can be improved  {\em without retraining the classifier}.              Our approach is based on two observations. The first is that even after conditioning on the true class of the sample, the score is often correlated with some features (we will refer to them as auxiliary features).	 The performance of the classifier is then evaluated based on the whole curve using a specific operating point (i.e., a desired FPR level) or by considering the area under the curve (AUC). The area under the curve is an interesting measure since it as a probabilistic interpretation. The area under the curve of a classifier $h(x), \mathbb{R}^n \to \mathbb{R} $ is the probability that for a random positive sample $x^+$ and a  random negative sample $x^-$ the classifier will produce $h(x^+)>h(x^-)$.  In this paper we show that the thresholding approach can be refined such that performance can be improved  {\em without retraining the classifier}.              Our approach is based on two observations. The first is that even after conditioning on the true class of the sample, the score is often correlated with some features (we will refer to them as auxiliary features).	score:430
What is the salary range for classification levels H and F in Stanford University?	 In this paper we exploit this property and introduce stochastic Hessian-free optimization (SHF), a variation of HF that operates on gradient and curvature mini-batches independent of the dataset size. Our goal in developing SHF is to combine the generalization advantages of SGD with second-order information from HF. SHF can adapt its behaviour through the choice of batch size and number of conjugate gradient iterations, for which its behaviour either becomes more characteristic of SGD or HF.  Additionally we integrate dropout, as a means of preventing co-adaptation of feature detectors. We perform experimental evaluation on both classification and deep autoencoder tasks. For classification, dropout SHF is competitive with dropout SGD on all tasks considered while for autoencoders SHF performs comparably to HF and momentum-based methods. Moreover, no tuning of learning rates needs to be done.	  Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with gradient and curvature mini-batches independent of the dataset size.  We modify Martens' HF for these settings and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. Stochastic Hessian-free optimization gives an intermediary between SGD and HF that achieves competitive performance on both classification and deep autoencoder experiments.  	Stochastic gradient descent (SGD) has become the most popular algorithm for training neural networks.	score:445
What is the salary range for classification levels H and F in Stanford University?	 However, these consideration extend beyond the scope of this paper and instead, standard single channel feature compensation methods for additive noise and linear filtering such as VTS and CMVN compensation are used throughout this paper.  Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC front-end on a phoneme classification task; this task remains important in comparing different methods and representations \cite{hlmgmm,hiddencrf2,shasaul,rls2,halberstadt97,clarkson,hiddencrf,vaseghinew,galesita,classificationref1,classificationref2,classificationref3}.	 However, these consideration extend beyond the scope of this paper and instead, standard single channel feature compensation methods for additive noise and linear filtering such as VTS and CMVN compensation are used throughout this paper.  Robustness of the proposed front-end to additive noise and linear filtering is demonstrated by its comparison with the MFCC front-end on a phoneme classification task; this task remains important in comparing different methods and representations \cite{hlmgmm,hiddencrf2,shasaul,rls2,halberstadt97,clarkson,hiddencrf,vaseghinew,galesita,classificationref1,classificationref2,classificationref3}.	score:466
What is the salary range for classification levels H and F in Stanford University?	  Given a subject's eye fixation sequences, a subject's HMM is learned using a variational Bayesian approach.  If the HMM is well estimated, then the subject's HMM can then be interpreted as the overall eye gaze strategy of the person.   In this paper we run a simulation study to investigate the estimation error for learning HMMs with respect to the number of sequences and the sequence lengths.   We then make recommendations about how many fixations are required to make the interpretation that the subject's HMM is representative of their underlying eye gaze strategy.  Note that if fewer fixations are used, then it is still okay to estimate HMMs from the data. In this case, the estimated HMM represents the subject's eye gaze pattern on the particular stimuli, not the subject's underlying strategy.	  Given a subject's eye fixation sequences, a subject's HMM is learned using a variational Bayesian approach.  If the HMM is well estimated, then the subject's HMM can then be interpreted as the overall eye gaze strategy of the person.   In this paper we run a simulation study to investigate the estimation error for learning HMMs with respect to the number of sequences and the sequence lengths.   We then make recommendations about how many fixations are required to make the interpretation that the subject's HMM is representative of their underlying eye gaze strategy.  Note that if fewer fixations are used, then it is still okay to estimate HMMs from the data. In this case, the estimated HMM represents the subject's eye gaze pattern on the particular stimuli, not the subject's underlying strategy.	score:467
What is the salary range for classification levels H and F in Stanford University?	 We compare the performance of Sylvester normalizing flows against planar flows and inverse autoregressive flows and demonstrate that they compare favorably on several datasets.  The code of our model is publicly available at \url{https://github.com/riannevdberg/sylvester-flows}. 	Stochastic variational inference \citep{Hoffman13a} allows for posterior inference in increasingly large and complex problems using stochastic gradient ascent.	 We compare the performance of Sylvester normalizing flows against planar flows and inverse autoregressive flows and demonstrate that they compare favorably on several datasets.  The code of our model is publicly available at \url{https://github.com/riannevdberg/sylvester-flows}. 	Stochastic variational inference \citep{Hoffman13a} allows for posterior inference in increasingly large and complex problems using stochastic gradient ascent.	score:467
What is the salary range for classification levels D and E at Stanford University?	 However, it is still challenging to reliably and economically operate power systems under large penetrations of solar energy due to the uncertainty and variability in solar power. Improving solar forecasting accuracy could partially address the challenges of foreseeable large solar penetrations faced by independent system operators and independent power producers.   For short-term solar forecasting, statistical methods are popularly used due to their learning power and high computational efficiency. Based on the algorithm complexity, statistical methods can be categorized into traditional time series methods (e.g., autoregressive integrated moving average (ARIMA) method~\cite{david2016probabilistic}), machine learning methods (e.	 However, it is still challenging to reliably and economically operate power systems under large penetrations of solar energy due to the uncertainty and variability in solar power. Improving solar forecasting accuracy could partially address the challenges of foreseeable large solar penetrations faced by independent system operators and independent power producers.   For short-term solar forecasting, statistical methods are popularly used due to their learning power and high computational efficiency. Based on the algorithm complexity, statistical methods can be categorized into traditional time series methods (e.g., autoregressive integrated moving average (ARIMA) method~\cite{david2016probabilistic}), machine learning methods (e.	score:491
What is the salary range for classification levels D and E at Stanford University?	  A subset of research is focused on \emph{binary classification}, e.g. discriminating between normal vs. tumor samples \cite{Li:2004cx, Bhat:2016us, MAHDIEH:2016wi, Yang:2016ha}. Stacked autoencoders are used for binary classification between glioma grades III vs. IV, and evaluated on 185 samples \cite{patil2018stacked}. These methods, however, can have limited clinical applications since most of the molecular pathology problems are \emph{multiclass}, e. g. assigning each sample to one of the different cancer types. Reaching high accuracy in classification problems usually becomes harder as the number of classes increases. Even a random assignment of samples to two classes will achieve 50\% accuracy if the classes are \emph{balanced} (i.e. there are an equal number of samples in each class), but a random classification will be around 3\% accurate if there are 33 balanced classes.	  A subset of research is focused on \emph{binary classification}, e.g. discriminating between normal vs. tumor samples \cite{Li:2004cx, Bhat:2016us, MAHDIEH:2016wi, Yang:2016ha}. Stacked autoencoders are used for binary classification between glioma grades III vs. IV, and evaluated on 185 samples \cite{patil2018stacked}. These methods, however, can have limited clinical applications since most of the molecular pathology problems are \emph{multiclass}, e. g. assigning each sample to one of the different cancer types. Reaching high accuracy in classification problems usually becomes harder as the number of classes increases. Even a random assignment of samples to two classes will achieve 50\% accuracy if the classes are \emph{balanced} (i.e. there are an equal number of samples in each class), but a random classification will be around 3\% accurate if there are 33 balanced classes.	score:492
What is the salary range for classification levels D and E at Stanford University?	 \textit{Is there a current DNN approach that reaches state-of-the-art performance for TSC and is less complex than HIVE-COTE}?  \textit{What type of DNN architectures works best for the TSC task}?  \textit{How does the random initialization affect the performance of deep learning classifiers}? And finally: \textit{Could the black-box effect of DNNs be avoided to provide interpretability}?   Given that the latter questions have not been addressed by the TSC community, it is surprising how much recent papers have neglected the possibility that TSC problems could be solved using a pure feature learning algorithm~\citep{neamtu2018generalized,bagnall2017the,lines2016hive}.  In fact, a recent empirical study~\citep{bagnall2017the} evaluated 18 TSC algorithms on 85 time series datasets, none of which was a deep learning model.	 \textit{Is there a current DNN approach that reaches state-of-the-art performance for TSC and is less complex than HIVE-COTE}?  \textit{What type of DNN architectures works best for the TSC task}?  \textit{How does the random initialization affect the performance of deep learning classifiers}? And finally: \textit{Could the black-box effect of DNNs be avoided to provide interpretability}?   Given that the latter questions have not been addressed by the TSC community, it is surprising how much recent papers have neglected the possibility that TSC problems could be solved using a pure feature learning algorithm~\citep{neamtu2018generalized,bagnall2017the,lines2016hive}.  In fact, a recent empirical study~\citep{bagnall2017the} evaluated 18 TSC algorithms on 85 time series datasets, none of which was a deep learning model.	score:494
What is the salary range for classification levels D and E at Stanford University?	  What is common to all these applications is uncertainty and then that they have to deal with decision and statistical inference.  Ranking is perhaps the most crucial task performed by the data management systems which have to deal with uncertainty. In many applications, ranking aims at deciding or inferring, for example, the class assigned to a unit or the order by relevance, usefulness, or utility of the units delivered to another application or to an end user.	  What is common to all these applications is uncertainty and then that they have to deal with decision and statistical inference.  Ranking is perhaps the most crucial task performed by the data management systems which have to deal with uncertainty. In many applications, ranking aims at deciding or inferring, for example, the class assigned to a unit or the order by relevance, usefulness, or utility of the units delivered to another application or to an end user.	score:496
What is the salary range for classification levels D and E at Stanford University?	 This is important to the behavior scientists for the state (feature) design. (b) From the perspective of optimization, the actor-critic algorithm has great properties of quick convergence with low variance\ \cite{Grondman_2012_IEEEts_surveyOfActorCriticRL}.  However, Lei's method assumes that the states at different decision points are i.i.d. and the current action only influences the immediate reward\ \cite{huitian_2016_PhdThesis_actCriticAlgorithm}.  This assumption is infeasible in real situations. Taking the delayed effect in the SDM or mHealth for example, the current action influences not only the immediate reward but also the next state and through that, all the subsequent rewards\ \cite{Sutton_2012_MitPress_RLintroduction}. Accordingly, Lei proposed a new method\ \cite{huitian_2016_PhdThesis_actCriticAlgorithm} by emphasizing on explorations and reducing exploitations.	 This is important to the behavior scientists for the state (feature) design. (b) From the perspective of optimization, the actor-critic algorithm has great properties of quick convergence with low variance\ \cite{Grondman_2012_IEEEts_surveyOfActorCriticRL}.  However, Lei's method assumes that the states at different decision points are i.i.d. and the current action only influences the immediate reward\ \cite{huitian_2016_PhdThesis_actCriticAlgorithm}.  This assumption is infeasible in real situations. Taking the delayed effect in the SDM or mHealth for example, the current action influences not only the immediate reward but also the next state and through that, all the subsequent rewards\ \cite{Sutton_2012_MitPress_RLintroduction}. Accordingly, Lei proposed a new method\ \cite{huitian_2016_PhdThesis_actCriticAlgorithm} by emphasizing on explorations and reducing exploitations.	score:504
How do I learn to code probability in modelling machine learning solutions?	 More details are provided in the Approach section below. \\  \item \textbf{Model Flexibility} Existing AutoML frameworks generate a full pipeline that includes data preprocessing, feature engineering, and model selection. Model selection usually involves the optimization of a single machine learning primitive, such as a Support Vector Machine (SVM) \cite{cortes1995support}, or a traditional ensemble method, such as Boosting \cite{kearns1988learning}\cite{schapire1990strength}\cite{freund1995boosting}.  Autostacker allows for flexible combinations of many machine learning primitives, resulting in a larger search space. \\  \item \textbf{Evolutionary Search Algorithm} EAs allow us to tractably find good solutions in a large space of variables \cite{eiben}. Such variables include the type of primitive machine learning models, the configuration settings of the framework (for instance, the number of primitive models in each stacking layer) and the hyperparameters in each primitive model.	 More details are provided in the Approach section below. \\  \item \textbf{Model Flexibility} Existing AutoML frameworks generate a full pipeline that includes data preprocessing, feature engineering, and model selection. Model selection usually involves the optimization of a single machine learning primitive, such as a Support Vector Machine (SVM) \cite{cortes1995support}, or a traditional ensemble method, such as Boosting \cite{kearns1988learning}\cite{schapire1990strength}\cite{freund1995boosting}.  Autostacker allows for flexible combinations of many machine learning primitives, resulting in a larger search space. \\  \item \textbf{Evolutionary Search Algorithm} EAs allow us to tractably find good solutions in a large space of variables \cite{eiben}. Such variables include the type of primitive machine learning models, the configuration settings of the framework (for instance, the number of primitive models in each stacking layer) and the hyperparameters in each primitive model.	score:335
How do I learn to code probability in modelling machine learning solutions?	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:337
How do I learn to code probability in modelling machine learning solutions?	 To address this gap between computational modeling and behavioral psychology, we introduce here a mathematical framework for studying how behavior effects learning and develop a novel model of learning-driven exploration.  In Computational Neuroscience,  machine learning techniques have been successfully applied towards modeling how the brain might learn the structure underlying sensory signals, e. g., \cite{olshausen1996emergence,rehn2007network,lewicki2002efficient,5766096,1556155,serre2007robust,crutchfield1987equations}. Generally, these methods focus on passive learning where the learning system can not directly effect the sensory input it receives. Exploration, in contrast, is inherently active, and can only occur in the context of a closed-action perception loop.	  In the model we propose here, an agent moving between discrete states in a world has to learn how its actions influence its state transitions. The underlying transition dynamics are governed by a Controllable Markov Chain (CMC). Within this simple framework, various utility functions for guiding exploratory behaviors will be studied, as well as several methods for coordinating actions over time. The different exploratory strategies are compared in their rate of learning.	score:340
How do I learn to code probability in modelling machine learning solutions?	 Many machine learning models, such as logistic regression~(LR) and support vector machine~(SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization~(DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods.  However, most of these DSO methods might not be scalable enough. In this paper, we propose a novel DSO method, called \underline{s}calable \underline{c}omposite \underline{op}timization for l\underline{e}arning~({SCOPE}), and implement it on the fault-tolerant distributed platform \mbox{Spark}. SCOPE is both computation-efficient and communication-efficient.	 Many machine learning models, such as logistic regression~(LR) and support vector machine~(SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization~(DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods.  However, most of these DSO methods might not be scalable enough. In this paper, we propose a novel DSO method, called \underline{s}calable \underline{c}omposite \underline{op}timization for l\underline{e}arning~({SCOPE}), and implement it on the fault-tolerant distributed platform \mbox{Spark}. SCOPE is both computation-efficient and communication-efficient.	score:345
How do I learn to code probability in modelling machine learning solutions?	 Codes are widely used in many engineering applications to offer {\it robustness} against {\it noise}. In large-scale systems there are several types of noise that can affect the performance of distributed machine learning algorithms -- straggler nodes, system failures, or communication bottlenecks -- but there has been little interaction cutting across codes, machine learning, and distributed systems.  In this work, we provide theoretical insights on how {\it coded} solutions can achieve significant gains compared to uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: {\it matrix multiplication} and {\it data shuffling}. For matrix multiplication, we use codes to alleviate the effect of stragglers, and show that if the number of homogeneous workers is $n$, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of $\log n$.	 Codes are widely used in many engineering applications to offer {\it robustness} against {\it noise}. In large-scale systems there are several types of noise that can affect the performance of distributed machine learning algorithms -- straggler nodes, system failures, or communication bottlenecks -- but there has been little interaction cutting across codes, machine learning, and distributed systems.  In this work, we provide theoretical insights on how {\it coded} solutions can achieve significant gains compared to uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: {\it matrix multiplication} and {\it data shuffling}. For matrix multiplication, we use codes to alleviate the effect of stragglers, and show that if the number of homogeneous workers is $n$, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of $\log n$.	score:346
Do AI and machine learning involve a lot of coding?	   Due to the complexity of the power grid, it is difficult to come up with a verbatim standard depicting the potential stability after reconnection of a subnetwork. With advances in the artificial intelligence community, we can make use of machine learning algorithms in order to explore vast combinations of sensor inputs, states, and control actions.   This can be done in a similar fashion to successful techniques applied to other power system problems as seen in the research literature \cite{Paper0064,Paper0082,Paper0094,Paper0095}. In this paper we propose to use a machine learning algorithm, specifically a Support Vector Machine, to predict safe times to reconnect a portion of a grid. The Support Vector Machines allow one to build a classifier predicated upon training data by determining a linear separator in a specific feature dimension \cite{Paper0065}.	   Due to the complexity of the power grid, it is difficult to come up with a verbatim standard depicting the potential stability after reconnection of a subnetwork. With advances in the artificial intelligence community, we can make use of machine learning algorithms in order to explore vast combinations of sensor inputs, states, and control actions.   This can be done in a similar fashion to successful techniques applied to other power system problems as seen in the research literature \cite{Paper0064,Paper0082,Paper0094,Paper0095}. In this paper we propose to use a machine learning algorithm, specifically a Support Vector Machine, to predict safe times to reconnect a portion of a grid. The Support Vector Machines allow one to build a classifier predicated upon training data by determining a linear separator in a specific feature dimension \cite{Paper0065}.	score:362
Do AI and machine learning involve a lot of coding?	 Machine learning is a powerful tool of drugs virtual screening that can help drug developer eliminate unqualified candidate molecules quickly.   A molecule can be of arbitrary size and shape. However, most machine learning methods can only handle inputs of a fixed size. A wide-used conventional proposal is to utilize hand-crafted feature like ECFP\cite{rogers2010extended}, Coulomb Matrix\cite{rupp2012fast} as input and feed it into conventional classifier like random forest and multi-layer perceptron.	 Machine learning is a powerful tool of drugs virtual screening that can help drug developer eliminate unqualified candidate molecules quickly.   A molecule can be of arbitrary size and shape. However, most machine learning methods can only handle inputs of a fixed size. A wide-used conventional proposal is to utilize hand-crafted feature like ECFP\cite{rogers2010extended}, Coulomb Matrix\cite{rupp2012fast} as input and feed it into conventional classifier like random forest and multi-layer perceptron.	score:363
Do AI and machine learning involve a lot of coding?	  	Machine learning (ML) is a subfield of Artificial Intelligence (AI) discipline. This branch of AI involves the computer applications and/or systems design that based on the simple concept: get data inputs, try some outputs, build a prediction. Nowadays, ML has  driven advances in many different fields~\cite{Kochura} like pedestrian detection, object recognition, visual-semantic embedding, language identification, acoustic modeling in speech recognition, video classification, fatigue estimation~\cite{Gordienko}, generation of alphabet of symbols for multimodal human-computer interfaces~\cite{Hamotskyi}, etc.	  	Machine learning (ML) is a subfield of Artificial Intelligence (AI) discipline. This branch of AI involves the computer applications and/or systems design that based on the simple concept: get data inputs, try some outputs, build a prediction. Nowadays, ML has  driven advances in many different fields~\cite{Kochura} like pedestrian detection, object recognition, visual-semantic embedding, language identification, acoustic modeling in speech recognition, video classification, fatigue estimation~\cite{Gordienko}, generation of alphabet of symbols for multimodal human-computer interfaces~\cite{Hamotskyi}, etc.	score:364
Do AI and machine learning involve a lot of coding?	  Training robots to perceive, act and communicate using multiple modalities still represents a challenging problem, particularly if robots are expected to learn efficiently from small sets of example interactions. We describe a learning approach as a step in this direction, where we teach a humanoid robot how to play the game of noughts and crosses.  Given that multiple multimodal skills can be trained to play this game, we focus our attention to training the robot to perceive the game, and to interact in this game. Our multimodal deep reinforcement learning agent perceives multimodal features and exhibits verbal and non-verbal actions while playing. Experimental results using simulations show that the robot can learn to win or draw up to 98\% of the games.	  Training robots to perceive, act and communicate using multiple modalities still represents a challenging problem, particularly if robots are expected to learn efficiently from small sets of example interactions. We describe a learning approach as a step in this direction, where we teach a humanoid robot how to play the game of noughts and crosses.  Given that multiple multimodal skills can be trained to play this game, we focus our attention to training the robot to perceive the game, and to interact in this game. Our multimodal deep reinforcement learning agent perceives multimodal features and exhibits verbal and non-verbal actions while playing. Experimental results using simulations show that the robot can learn to win or draw up to 98\% of the games.	score:369
Do AI and machine learning involve a lot of coding?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:370
Have any of the postgraduate students in the 2015 batch gotten their offer letters from Infosys, if they got their intent in the past two months?	 We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions.  We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy.  For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule.	 We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy.	score:366
Have any of the postgraduate students in the 2015 batch gotten their offer letters from Infosys, if they got their intent in the past two months?	 To begin, the first two semesters of courses completed by a student are used to predict if they will obtain an undergraduate degree. Secondly, for the students that completed a program, their major is predicted using once again the first few courses they have registered to. A classification tree is an intuitive and powerful classifier and building a random forest of trees improves this classifier.  Random forests also allow for reliable variable importance measurements. These measures explain what variables are useful to the classifiers and can be used to better understand what is statistically related to the students' situation. The results are two accurate classifiers and a variable importance analysis that provides useful information to university administrations.   \bigskip  \noindent  \textbf{Keywords} : Higher Education, Student Retention, Academic Success, Machine Learning, Classification Tree, Random Forest, Variable Importance   	Being able to predict if a student is at risk of not completing its program is valuable for universities that would like to intervene and help those students move forward. Predicting the major that will be completed by students is also important in order to understand as soon as possible which program attracts more students and allocate resources accordingly.	 To begin, the first two semesters of courses completed by a student are used to predict if they will obtain an undergraduate degree. Secondly, for the students that completed a program, their major is predicted using once again the first few courses they have registered to. A classification tree is an intuitive and powerful classifier and building a random forest of trees improves this classifier.  Random forests also allow for reliable variable importance measurements. These measures explain what variables are useful to the classifiers and can be used to better understand what is statistically related to the students' situation. The results are two accurate classifiers and a variable importance analysis that provides useful information to university administrations.   \bigskip  \noindent  \textbf{Keywords} : Higher Education, Student Retention, Academic Success, Machine Learning, Classification Tree, Random Forest, Variable Importance   	Being able to predict if a student is at risk of not completing its program is valuable for universities that would like to intervene and help those students move forward. Predicting the major that will be completed by students is also important in order to understand as soon as possible which program attracts more students and allocate resources accordingly.	score:379
Have any of the postgraduate students in the 2015 batch gotten their offer letters from Infosys, if they got their intent in the past two months?	 For researchers, such information is one of the most considered factors for hiring, promotion, funding decisions, award consideration and professional recognition. For academic journals, it is an indicator of a journal's stature among its peers, which is valuable for various reasons, from being considered by prospective authors for paper submission to being sought after by readers who need authoritative opinions on a topic.   Due to the lack of a unified definition, the quality and importance of scientific articles are often judged based on the journal in which they are published \citep{Simons2008}. In particular, a journal's {\it impact factor} \citep[see][]{Garfield2006}, defined using a journal's average number of citations per article, is frequently used as an indicator of the ``quality" of its articles and a means of evaluating the research output of individuals and institutions \citep{Casadevall2014}.	 For researchers, such information is one of the most considered factors for hiring, promotion, funding decisions, award consideration and professional recognition. For academic journals, it is an indicator of a journal's stature among its peers, which is valuable for various reasons, from being considered by prospective authors for paper submission to being sought after by readers who need authoritative opinions on a topic.   Due to the lack of a unified definition, the quality and importance of scientific articles are often judged based on the journal in which they are published \citep{Simons2008}. In particular, a journal's {\it impact factor} \citep[see][]{Garfield2006}, defined using a journal's average number of citations per article, is frequently used as an indicator of the ``quality" of its articles and a means of evaluating the research output of individuals and institutions \citep{Casadevall2014}.	score:392
Have any of the postgraduate students in the 2015 batch gotten their offer letters from Infosys, if they got their intent in the past two months?	 As a metaphor, they are twins who live in the same environment and have the same education, yet they could have different learning skills and acquire different knowledge in certain aspects.    The work in this paper  can be easily generalized to the comparison of any two neural networks with the same inputs. In contrast to existing approaches like \cite{Raghu2017}, our framework focus on studying the intrinsic geometry of manifolds, namely, curvature information.   The remainder of this paper is organized as follows. Section \ref{manifoldandSVD} presents the geometric structure of neural network, and specifies the reason for taking curvature into account. Then a new SVD-based close data generating method is proposed, based on which we give the dimension estimation of manifolds in every fully connected layer in each AlexNet.	 As a metaphor, they are twins who live in the same environment and have the same education, yet they could have different learning skills and acquire different knowledge in certain aspects.    The work in this paper  can be easily generalized to the comparison of any two neural networks with the same inputs. In contrast to existing approaches like \cite{Raghu2017}, our framework focus on studying the intrinsic geometry of manifolds, namely, curvature information.   The remainder of this paper is organized as follows. Section \ref{manifoldandSVD} presents the geometric structure of neural network, and specifies the reason for taking curvature into account. Then a new SVD-based close data generating method is proposed, based on which we give the dimension estimation of manifolds in every fully connected layer in each AlexNet.	score:393
Have any of the postgraduate students in the 2015 batch gotten their offer letters from Infosys, if they got their intent in the past two months?	 We show that whenever   any ``reasonable'' mechanism can do so, a simple linear mechanism suffices. 	One of the fundamental insights in the economics of information is the way in which assessing people (students, job applicants, employees) can serve two purposes simultaneously: it can identify the strongest performers, and it can also motivate people to invest effort in improving their performance \cite{spence1973signaling}.  This principle has only grown in importance with the rise in algorithmic methods for predicting individual performance across a wide range of domains, including education,  employment, and finance.  A key challenge is that we do not generally have access to the  true underlying properties that we need for an assessment; rather, they are encoded by an intermediate layer of {\em features}, so that the true properties determine the features, and the features then determine our assessment.	 We show that whenever   any ``reasonable'' mechanism can do so, a simple linear mechanism suffices. 	One of the fundamental insights in the economics of information is the way in which assessing people (students, job applicants, employees) can serve two purposes simultaneously: it can identify the strongest performers, and it can also motivate people to invest effort in improving their performance \cite{spence1973signaling}.  This principle has only grown in importance with the rise in algorithmic methods for predicting individual performance across a wide range of domains, including education,  employment, and finance.  A key challenge is that we do not generally have access to the  true underlying properties that we need for an assessment; rather, they are encoded by an intermediate layer of {\em features}, so that the true properties determine the features, and the features then determine our assessment.	score:395
When is the IBM off campus drive for 2016 batch?	 It is well known that GHZ states have several practical applications, including quantum machine learning. We illustrate our experience in implementing and querying a uniform quantum example oracle based on the GHZ circuit, for solving the classically hard problem of learning parity with noise. 	Since 2016, IBM offers hands-on, cloud-based access to its experimental quantum computing platform, denoted as IBM Q Experience \cite{IBMQE}.  Such a platform comprises a 5 qubit device, denoted as QX4, and a 16 qubit device, named QX5. All devices are based on transmon qubits \cite{Koch2007}, i.e., superconducting qubits that are insensitive to charge noise and have $\sim 100\mu$sec decoherence time.   IBM Q Experience is an effective experimental platform for testing quantum algorithms.	} \cite{Cross2015} proved that learning parity with noise can be performed with superpolynomial quantum computational speedup. The experimental demonstration on IBM QX2 was recently presented by Rist\`e \textit{et al.}\cite{Riste2017} In this work, we illustrate similar experiments we have performed on IBM QX5. GHZ circuits play a major role in these quantum computations.	score:485
When is the IBM off campus drive for 2016 batch?	  Fog computing promises dramatic reduction in latency and mobile energy consumption by offloading computation tasks \cite{Mao2017}. Recently, many works have been carried out addressing the task offloading in fog computing \cite{2017_Pu_D2D_fog,2017_Yang_DEBTS,2017_Mao_MobileEdge,2015_Kwak_JSAC_DREAM, 2017_You_MobileEdge,2017_Yang_MEETS}. Among these works, some considered the energy issues and formulated the task offloading as deterministic optimization problems \cite{2017_You_MobileEdge,2017_Yang_MEETS}.  Considering the real-time states of users and servers, the task offloading problem is a typical stochastic optimization problem. To make this problem tractable, in \cite{2017_Pu_D2D_fog,2017_Yang_DEBTS,2017_Mao_MobileEdge,2015_Kwak_JSAC_DREAM}, the Lyapunov optimization method was applied to transform the challenging stochastic optimization problem to a sequential decision problem.	  Fog computing promises dramatic reduction in latency and mobile energy consumption by offloading computation tasks \cite{Mao2017}. Recently, many works have been carried out addressing the task offloading in fog computing \cite{2017_Pu_D2D_fog,2017_Yang_DEBTS,2017_Mao_MobileEdge,2015_Kwak_JSAC_DREAM, 2017_You_MobileEdge,2017_Yang_MEETS}. Among these works, some considered the energy issues and formulated the task offloading as deterministic optimization problems \cite{2017_You_MobileEdge,2017_Yang_MEETS}.  Considering the real-time states of users and servers, the task offloading problem is a typical stochastic optimization problem. To make this problem tractable, in \cite{2017_Pu_D2D_fog,2017_Yang_DEBTS,2017_Mao_MobileEdge,2015_Kwak_JSAC_DREAM}, the Lyapunov optimization method was applied to transform the challenging stochastic optimization problem to a sequential decision problem.	score:521
When is the IBM off campus drive for 2016 batch?	 Simply combining a convolutional net with a sequential MCTS algorithm, such as UCT \citep{Kocsis+Szepesvari_2006}, is impractical as inference in a convolutional net has too great a computation latency, even when executed on a high throughput GPU (ignoring the added communication latency between CPU and GPU) to perform Monte Carlo backups rapidly enough in MCTS.	 Simply combining a convolutional net with a sequential MCTS algorithm, such as UCT \citep{Kocsis+Szepesvari_2006}, is impractical as inference in a convolutional net has too great a computation latency, even when executed on a high throughput GPU (ignoring the added communication latency between CPU and GPU) to perform Monte Carlo backups rapidly enough in MCTS.	score:523
When is the IBM off campus drive for 2016 batch?	 Therefore, the transfer speed between processes severely impacts the overall data throughput speedup\footnote{We define data throughput   speedup as the change in total time taken to process a certain   amount of examples. It includes both training and communication time.}. Since the parameters to be transferred are  computed on GPUs, a GPU-to-GPU transfer is required.	 Therefore, the transfer speed between processes severely impacts the overall data throughput speedup\footnote{We define data throughput   speedup as the change in total time taken to process a certain   amount of examples. It includes both training and communication time.}. Since the parameters to be transferred are  computed on GPUs, a GPU-to-GPU transfer is required.	score:537
When is the IBM off campus drive for 2016 batch?	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	score:538
When is the deloitte off campus drive for 2016 batch?	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	score:482
When is the deloitte off campus drive for 2016 batch?	 However, the training phase continues to be a bottleneck, where the training data must be processed serially over thousands of individual training runs, thus leading to long training times.  This situation is only exacerbated by recent computer architecture trends where clock speeds are stagnant, and future speedups are available only through increased concurrency.     This situation repeats itself for the time-dependent PDE simulation community, where it has been identified as a sequential time-stepping bottleneck \cite{Fa2014}.  In response, interest in parallel-in-time methods, that can parallelize over the time domain, has grown considerably in the last decade.  One of the most well known parallel-in-time methods is Parareal \cite{LiMaTu2001}, which is equivalent to a two-level multigrid method \cite{GaVa2007}.	 However, the training phase continues to be a bottleneck, where the training data must be processed serially over thousands of individual training runs, thus leading to long training times.  This situation is only exacerbated by recent computer architecture trends where clock speeds are stagnant, and future speedups are available only through increased concurrency.     This situation repeats itself for the time-dependent PDE simulation community, where it has been identified as a sequential time-stepping bottleneck \cite{Fa2014}.  In response, interest in parallel-in-time methods, that can parallelize over the time domain, has grown considerably in the last decade.  One of the most well known parallel-in-time methods is Parareal \cite{LiMaTu2001}, which is equivalent to a two-level multigrid method \cite{GaVa2007}.	score:502
When is the deloitte off campus drive for 2016 batch?	 For example, the next destination people want to go to often depends on what other places they have gone to and how long they have stayed in each place in the past. When the next clinical visit \citep{Choi16} will occur for a patient depends on the time of the most recent visits and the respective duration between them. Therefore, the temporal information of events and the interval between them are crucial to the event sequence prediction in general.  However, how to effectively use and represent time in sequence prediction still largely remains under explored.  A natural and straightforward solution is to bring time as an additional input into an existing sequence model (e.g., recurrent neural networks). However, it is notoriously challenging for recurrent neural networks to directly handle continuous input that has a wide value range, as what is shown in our experiments.	 For example, the next destination people want to go to often depends on what other places they have gone to and how long they have stayed in each place in the past. When the next clinical visit \citep{Choi16} will occur for a patient depends on the time of the most recent visits and the respective duration between them. Therefore, the temporal information of events and the interval between them are crucial to the event sequence prediction in general.  However, how to effectively use and represent time in sequence prediction still largely remains under explored.  A natural and straightforward solution is to bring time as an additional input into an existing sequence model (e.g., recurrent neural networks). However, it is notoriously challenging for recurrent neural networks to directly handle continuous input that has a wide value range, as what is shown in our experiments.	score:507
When is the deloitte off campus drive for 2016 batch?	 We would also like to briefly mention \cite{OkamotoBerntorpEtAl2017} as a recent nonparametric learning method that compares human driver trajectories to a form of nearest neighbors from an interaction dataset in order to predict future behavior, however, the authors of \cite{OkamotoBerntorpEtAl2017} note that this comparison procedure is difficult to scale at run time for online policy construction.                        \emph{Statement of Contributions}: The primary contribution of this paper is the synthesis and demonstration of a data-driven framework for HRI that, to the best of our knowledge, uniquely meets all five aforementioned conditions for desirable model-based policy construction in rapidly evolving multimodal interaction scenarios.	 We would also like to briefly mention \cite{OkamotoBerntorpEtAl2017} as a recent nonparametric learning method that compares human driver trajectories to a form of nearest neighbors from an interaction dataset in order to predict future behavior, however, the authors of \cite{OkamotoBerntorpEtAl2017} note that this comparison procedure is difficult to scale at run time for online policy construction.                        \emph{Statement of Contributions}: The primary contribution of this paper is the synthesis and demonstration of a data-driven framework for HRI that, to the best of our knowledge, uniquely meets all five aforementioned conditions for desirable model-based policy construction in rapidly evolving multimodal interaction scenarios.	score:508
When is the deloitte off campus drive for 2016 batch?	 For example, a beacon, which is a fixed type of traffic sensor, can steadily collect the traffic data of the road where it is located in a shot time period; however, the detection area is narrow. A probe vehicle, which is a GPS-equipped vehicle, can collect the traffic data of a comprehensive area, but cannot steadily collect the data and needs a long time period to acquire comprehensive traffic data.  Therefore, the fusion of various data collected from different sensors for traffic prediction has recently attracted much attention \cite{Faouzi_Leung_Kurian_InformationFusion_2011}.  Traffic prediction is a major research topic in the machine learning field \cite{Bishop_Book_2006}. In fact, the analysis of freeway traffic has been researched since the 70s \cite{Ahmed_Cook_TransportationReserchRecord_1979}.	 For example, a beacon, which is a fixed type of traffic sensor, can steadily collect the traffic data of the road where it is located in a shot time period; however, the detection area is narrow. A probe vehicle, which is a GPS-equipped vehicle, can collect the traffic data of a comprehensive area, but cannot steadily collect the data and needs a long time period to acquire comprehensive traffic data.  Therefore, the fusion of various data collected from different sensors for traffic prediction has recently attracted much attention \cite{Faouzi_Leung_Kurian_InformationFusion_2011}.  Traffic prediction is a major research topic in the machine learning field \cite{Bishop_Book_2006}. In fact, the analysis of freeway traffic has been researched since the 70s \cite{Ahmed_Cook_TransportationReserchRecord_1979}.	score:508
What's the best software to use to develop a variance-covariance matrix with BEKK model?	 Bayesian inference allows us to do so in a principled way that accounts for parameter and model uncertainty by computing the posterior distribution over parameters and the model evidence, also known as marginal likelihood or Bayes factor. However, Bayesian inference is generally analytically intractable, and the statistical tools of approximate inference, such as Markov Chain Monte Carlo (MCMC) or variational inference, generally require knowledge about the model (e.	 Bayesian inference allows us to do so in a principled way that accounts for parameter and model uncertainty by computing the posterior distribution over parameters and the model evidence, also known as marginal likelihood or Bayes factor. However, Bayesian inference is generally analytically intractable, and the statistical tools of approximate inference, such as Markov Chain Monte Carlo (MCMC) or variational inference, generally require knowledge about the model (e.	score:272
What's the best software to use to develop a variance-covariance matrix with BEKK model?	 Our task amounts to estimating the transition matrix, which is assumed to be sparse. In such a scenario, where covariates are highly interdependent and partially missing, new theoretical challenges arise. While transition matrix estimation in vector autoregressive models has been studied previously, the missing data scenario requires separate efforts.  Moreover, while transition matrix estimation can be studied from a high-dimensional sparse linear regression perspective, the covariates are highly dependent and existing results on regularized estimation with missing data from i.i.d.~covariates are not applicable. At the heart of our analysis lies 1) a novel concentration result when the innovation noise satisfies the convex concentration property, as well as 2) a new quantity for characterizing the interactions of the time-varying observation process with the underlying dynamical system.	 Our task amounts to estimating the transition matrix, which is assumed to be sparse. In such a scenario, where covariates are highly interdependent and partially missing, new theoretical challenges arise. While transition matrix estimation in vector autoregressive models has been studied previously, the missing data scenario requires separate efforts.  Moreover, while transition matrix estimation can be studied from a high-dimensional sparse linear regression perspective, the covariates are highly dependent and existing results on regularized estimation with missing data from i.i.d.~covariates are not applicable. At the heart of our analysis lies 1) a novel concentration result when the innovation noise satisfies the convex concentration property, as well as 2) a new quantity for characterizing the interactions of the time-varying observation process with the underlying dynamical system.	score:279
What's the best software to use to develop a variance-covariance matrix with BEKK model?	 This involves back-propagating curvatures through the computational graph of the function~\cite{becker-lecun-89,lecun-98b, chapelle-erhan-2011,   martens-2012}, which is particularly easy when computing diagonal approximations or Hessians of recurrent neural nets. Others have attempted to identify a few dominant eigen-directions in which the curvatures are larger than the others, which can be seen as a low-rank Hessian approximation~\cite{lecun-simard-pearlmutter-93}  More recently, decompositions of covariance matrices using products of rotations on pairs of coordinates have been explored in~\cite{bouman-2011}.  The method greedily selects the best pairwise rotations to approximate the diagonalizing basis of the matrix, and then finds the best corresponding eigenvalues.	 It can be used to approximate covariance matrix of Gaussian models in order to speed up inference, or to estimate and track the inverse Hessian of an objective function by relating changes in parameters to changes in gradient along the trajectory followed by the optimization procedure.  Experiments were conducted to approximate synthetic matrices, covariance matrices of real data, and Hessian matrices of objective functions involved in machine learning problems.  	Covariance matrices, Hessian matrices and, more generally speaking, symmetric matrices, play a major role in machine learning. In density models containing covariance matrices (\emph{e.g.} mixtures of Gaussians), estimation and inference involves computing the inverse of such matrices and computing products of such matrices with vectors. The size of these matrices grow quadratically with the dimension of the space, and some of the computations grow cubically.	score:287
What's the best software to use to develop a variance-covariance matrix with BEKK model?	 The models are usually developed with machine   learning algorithms, such as Bayesian networks, clustering models   and latent semantic models. Complex preference patterns can be   recognized by these models, allowing model-based CF to perform better for    preference prediction tasks.  Among all these models,   matrix factorization is most popular and successful, c. f.   \citep{koren2009matrix,salakhutdinov2008bayesian,mackey2011divide,gopalan2013scalable}.      With the recent development of deep   learning~\citep{krizhevsky2012imagenet,szegedy2014going,he2015deep},   neural network based CF, a subclass of model-based CF, has gained   enormous attention. A prominent example is RBM-based CF   (RBM-CF)~\citep{salakhutdinov2007restricted}.	 The models are usually developed with machine   learning algorithms, such as Bayesian networks, clustering models   and latent semantic models. Complex preference patterns can be   recognized by these models, allowing model-based CF to perform better for    preference prediction tasks.  Among all these models,   matrix factorization is most popular and successful, c. f.   \citep{koren2009matrix,salakhutdinov2008bayesian,mackey2011divide,gopalan2013scalable}.      With the recent development of deep   learning~\citep{krizhevsky2012imagenet,szegedy2014going,he2015deep},   neural network based CF, a subclass of model-based CF, has gained   enormous attention. A prominent example is RBM-based CF   (RBM-CF)~\citep{salakhutdinov2007restricted}.	score:287
What's the best software to use to develop a variance-covariance matrix with BEKK model?	  In this work, we focus on matrix factorization based co-clustering, which models the sample-feature relationship from the data reconstruction perspective. A heuristic method is to use singular value decomposition (SVD), whose low rank singular vectors constitute a compact representation. However, its factorized matrices contain negative values, which lacks a nice interpretation for co-clustering on documents or facial images.  Therefore, we take advantage of a popularized tool, namely nonnegative matrix tri-factorization \cite{ding2006onmtf}, as the foundation of our approach. This method imposes the nonnegative constraints on the decomposed matrices, which leads to a parts-based representation \cite{lee1999nmf}. On the other hand, previous studies have shown human generated data (\textit{e.	  In this work, we focus on matrix factorization based co-clustering, which models the sample-feature relationship from the data reconstruction perspective. A heuristic method is to use singular value decomposition (SVD), whose low rank singular vectors constitute a compact representation. However, its factorized matrices contain negative values, which lacks a nice interpretation for co-clustering on documents or facial images.  Therefore, we take advantage of a popularized tool, namely nonnegative matrix tri-factorization \cite{ding2006onmtf}, as the foundation of our approach. This method imposes the nonnegative constraints on the decomposed matrices, which leads to a parts-based representation \cite{lee1999nmf}. On the other hand, previous studies have shown human generated data (\textit{e.	score:289
When is it important to use convolutions cross channels and when is it not?	         	Most of the decisions that network operators make depend on how the traffic flows in their network. However, although it is very important to accurately estimate traffic parameters, current routers and network devices do not provide the possibility for real-time  monitoring, hence network operators cannot react effectively to  the traffic changes.  To cope with this problem, prediction techniques have been  applied to predict network parameters and therefore be able  to react to network changes in near real-time.   The predictability of network traffic parameters is mainly determined by  their statistical characteristics and the fact that they present a strong correlation between chronologically ordered values.	         	Most of the decisions that network operators make depend on how the traffic flows in their network. However, although it is very important to accurately estimate traffic parameters, current routers and network devices do not provide the possibility for real-time  monitoring, hence network operators cannot react effectively to  the traffic changes.  To cope with this problem, prediction techniques have been  applied to predict network parameters and therefore be able  to react to network changes in near real-time.   The predictability of network traffic parameters is mainly determined by  their statistical characteristics and the fact that they present a strong correlation between chronologically ordered values.	score:373
When is it important to use convolutions cross channels and when is it not?	  Desirable properties of clustering algorithms come from practitioners who have intuitive notions of what is a good clustering: ``they know it when they see it''. This is of course not satisfactory and a theoretical understanding needs to be developed. However, one thing this intuition reflects is the fact that density needs to be incorporated in the clustering procedures.  Single linkage clustering, a procedure that enjoys several nice theoretical properties, is notorious for its insensitivity to density, which is manifested in the so called \emph{chaining effect}  \cite{lance67general}.    Other methods such as average linkage, complete linkage \cite[Chapter 3]{clusteringref} and $k$-means share the property that they exhibit some sort of sensitivity to density, but are unstable in a sense which has been made theoretically precise \cite{clust-um} and are therefore not well supported by theory.	  Desirable properties of clustering algorithms come from practitioners who have intuitive notions of what is a good clustering: ``they know it when they see it''. This is of course not satisfactory and a theoretical understanding needs to be developed. However, one thing this intuition reflects is the fact that density needs to be incorporated in the clustering procedures.  Single linkage clustering, a procedure that enjoys several nice theoretical properties, is notorious for its insensitivity to density, which is manifested in the so called \emph{chaining effect}  \cite{lance67general}.    Other methods such as average linkage, complete linkage \cite[Chapter 3]{clusteringref} and $k$-means share the property that they exhibit some sort of sensitivity to density, but are unstable in a sense which has been made theoretically precise \cite{clust-um} and are therefore not well supported by theory.	score:381
When is it important to use convolutions cross channels and when is it not?	 Encouragement can have positive effects on learning and engagement even when coming from software \cite{brown_positive_2014}, suggesting that it is the content and context of encouragement, and not who it comes from, that is important.  Offering encouragement in coding tutorials poses many challenges. If encouragement is not timed well, it may interrupt learners' thoughts, causing them to lose their places in a difficult coding problem or exercise.  Moreover, learners who are already engaged and successfully learning may perceive encouragement as annoying, condescending, or as a lack of confidence in their abilities, making them question whether they really are succeeding. This leaves designers of online coding tutorials between two extremes: if they always provide encouragement, they may annoy the majority of learners who are engaged; if they never provide encouragement, they fail to help learners who are likely to abandon the tutorial.	 Encouragement can have positive effects on learning and engagement even when coming from software \cite{brown_positive_2014}, suggesting that it is the content and context of encouragement, and not who it comes from, that is important.  Offering encouragement in coding tutorials poses many challenges. If encouragement is not timed well, it may interrupt learners' thoughts, causing them to lose their places in a difficult coding problem or exercise.  Moreover, learners who are already engaged and successfully learning may perceive encouragement as annoying, condescending, or as a lack of confidence in their abilities, making them question whether they really are succeeding. This leaves designers of online coding tutorials between two extremes: if they always provide encouragement, they may annoy the majority of learners who are engaged; if they never provide encouragement, they fail to help learners who are likely to abandon the tutorial.	score:381
When is it important to use convolutions cross channels and when is it not?	 Designing adaptive classifiers for an evolving data stream is a challenging task due to the data size and its dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is a well-known solution. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments.	 Designing adaptive classifiers for an evolving data stream is a challenging task due to the data size and its dynamically changing nature. Combining individual classifiers in an online setting, the ensemble approach, is a well-known solution. It is possible that a subset of classifiers in the ensemble outperforms others in a time-varying fashion. However, optimum weight assignment for component classifiers is a problem which is not yet fully addressed in online evolving environments.	score:383
When is it important to use convolutions cross channels and when is it not?	 This allows the use of statistical methods, such as least squares and maximum likelihood based methods, for the tasks of analysis and prediction. However, when different loss functions are considered these assumptions do not hold in general, and the aforementioned methods are not applicable. We are not aware of a previous approach that tries to relax these assumptions for general convex loss functions.  We note that there has been previous work which tries to relax such assumptions for the squared loss, usually under additional modelling assumptions such as \emph{t}-distribution of the noise (e.g., \cite{DaElS89,TiWoVaBi00}). We emphasize that the independence assumption is rather strict and previous works that relax this assumption usually offer specific  dependency model, e.	 This allows the use of statistical methods, such as least squares and maximum likelihood based methods, for the tasks of analysis and prediction. However, when different loss functions are considered these assumptions do not hold in general, and the aforementioned methods are not applicable. We are not aware of a previous approach that tries to relax these assumptions for general convex loss functions.  We note that there has been previous work which tries to relax such assumptions for the squared loss, usually under additional modelling assumptions such as \emph{t}-distribution of the noise (e.g., \cite{DaElS89,TiWoVaBi00}). We emphasize that the independence assumption is rather strict and previous works that relax this assumption usually offer specific  dependency model, e.	score:390
What is the salary range for classification level L in Stanford University?	 There are tremendous amount of research on improving the classification performance in such cases. One highly investigated field for this problem is ensemble learning \cite{kuncheva}, where multiple prediction are fused the produce a more efficient classification approach. One fundamental requirement for the creation of classifier ensembles is diversity among them \cite{Brown2005}, that is, the classifiers included in the ensemble need to complement each other to provide more generalization capabilities than a single learner.	 There are tremendous amount of research on improving the classification performance in such cases. One highly investigated field for this problem is ensemble learning \cite{kuncheva}, where multiple prediction are fused the produce a more efficient classification approach. One fundamental requirement for the creation of classifier ensembles is diversity among them \cite{Brown2005}, that is, the classifiers included in the ensemble need to complement each other to provide more generalization capabilities than a single learner.	score:485
What is the salary range for classification level L in Stanford University?	 Empirical studies partially demonstrate the resistance of \(k\)-nearest neighbor to noise \citep{Kusner:Tyree:Weinberger:Agrawal2014,Tarlow:Swersky:Swersky:Charlin:Sutskever:Zemel2013}, whereas there is a paucity of deep understanding.  This work focuses on binary classification in the presence of random classification noises \citep{Angluin:Laird1988}, that is, each observed label has been flipped with certain probability instead of seeing the ground-truth label, and training data of each class are contaminated by samples from the other class.	 Empirical studies partially demonstrate the resistance of \(k\)-nearest neighbor to noise \citep{Kusner:Tyree:Weinberger:Agrawal2014,Tarlow:Swersky:Swersky:Charlin:Sutskever:Zemel2013}, whereas there is a paucity of deep understanding.  This work focuses on binary classification in the presence of random classification noises \citep{Angluin:Laird1988}, that is, each observed label has been flipped with certain probability instead of seeing the ground-truth label, and training data of each class are contaminated by samples from the other class.	score:488
What is the salary range for classification level L in Stanford University?	   There has been attempts in applying the second order method Limited memory BFGS (LBFGS) to optimizing DNNs in the famous deep learning framework PyTorch. However, when optimizing DNNs, the Hessian of LBFGS may become negative semi-definite and thus the objective loss will increase to a very large value. Moreover, the optimization process of LBFGS is highly sensitive to the learning rate.  If the learning rate is too large, the training process is unstable. Otherwise, the objective function does not decrease.  Recently, Wang et al \cite{wang2017stochastic} propose a second order algorithm, Stochastic damped LBFGS (SdLBFGS), to optimize non-convex functions. The Hessian in each step of SdLBFGS is guaranteed to be PSD. However, the convergence is not fully proved and we observed non-convergence and crashes in experiments.	   There has been attempts in applying the second order method Limited memory BFGS (LBFGS) to optimizing DNNs in the famous deep learning framework PyTorch. However, when optimizing DNNs, the Hessian of LBFGS may become negative semi-definite and thus the objective loss will increase to a very large value. Moreover, the optimization process of LBFGS is highly sensitive to the learning rate.  If the learning rate is too large, the training process is unstable. Otherwise, the objective function does not decrease.  Recently, Wang et al \cite{wang2017stochastic} propose a second order algorithm, Stochastic damped LBFGS (SdLBFGS), to optimize non-convex functions. The Hessian in each step of SdLBFGS is guaranteed to be PSD. However, the convergence is not fully proved and we observed non-convergence and crashes in experiments.	score:493
What is the salary range for classification level L in Stanford University?	  Classifying programming languages of source code files using ML and NLP methods has been well explored in the research community (cf. \cite{Kennedy}\cite{S.Gilda}\cite{Khasnabish}). It has been established that the programming language of a source code file can be identified with high accuracy. However, most of the previous work that study the classification of programming languages use the GitHub dataset in which the size of source code files is typically large.  Applying ML and NLP methods to classify a large source code file provides a very high accuracy as the large sample contains many features that help the machine learning model to learn better. In this paper, we are interested in a tool that can classify a code snippet which is a small block reusable code with at least two lines of code, a much more challenging task.	  Classifying programming languages of source code files using ML and NLP methods has been well explored in the research community (cf. \cite{Kennedy}\cite{S.Gilda}\cite{Khasnabish}). It has been established that the programming language of a source code file can be identified with high accuracy. However, most of the previous work that study the classification of programming languages use the GitHub dataset in which the size of source code files is typically large.  Applying ML and NLP methods to classify a large source code file provides a very high accuracy as the large sample contains many features that help the machine learning model to learn better. In this paper, we are interested in a tool that can classify a code snippet which is a small block reusable code with at least two lines of code, a much more challenging task.	score:496
What is the salary range for classification level L in Stanford University?	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	score:500
What is the salary range for classification levels E in Stanford University?	  What is common to all these applications is uncertainty and then that they have to deal with decision and statistical inference.  Ranking is perhaps the most crucial task performed by the data management systems which have to deal with uncertainty. In many applications, ranking aims at deciding or inferring, for example, the class assigned to a unit or the order by relevance, usefulness, or utility of the units delivered to another application or to an end user.	  What is common to all these applications is uncertainty and then that they have to deal with decision and statistical inference.  Ranking is perhaps the most crucial task performed by the data management systems which have to deal with uncertainty. In many applications, ranking aims at deciding or inferring, for example, the class assigned to a unit or the order by relevance, usefulness, or utility of the units delivered to another application or to an end user.	score:473
What is the salary range for classification levels E in Stanford University?	 Empirical studies partially demonstrate the resistance of \(k\)-nearest neighbor to noise \citep{Kusner:Tyree:Weinberger:Agrawal2014,Tarlow:Swersky:Swersky:Charlin:Sutskever:Zemel2013}, whereas there is a paucity of deep understanding.  This work focuses on binary classification in the presence of random classification noises \citep{Angluin:Laird1988}, that is, each observed label has been flipped with certain probability instead of seeing the ground-truth label, and training data of each class are contaminated by samples from the other class.	 Empirical studies partially demonstrate the resistance of \(k\)-nearest neighbor to noise \citep{Kusner:Tyree:Weinberger:Agrawal2014,Tarlow:Swersky:Swersky:Charlin:Sutskever:Zemel2013}, whereas there is a paucity of deep understanding.  This work focuses on binary classification in the presence of random classification noises \citep{Angluin:Laird1988}, that is, each observed label has been flipped with certain probability instead of seeing the ground-truth label, and training data of each class are contaminated by samples from the other class.	score:478
What is the salary range for classification levels E in Stanford University?	  Listwise large margin surrogates form an important sub-class of listwise surrogates. Their use is motivated by the success of large margin surrogates in supervised classification problems. However, existing popular listwise large margin surrogates in the learning to rank literature are derived using the structured prediction framework \cite{chapelle2007, yue2007, chakrabarti2008}.  In standard structured prediction, the supervision space is the same as the output space of the function being learned. To fit the structured prediction framework to the learning to rank problem (where the supervision is in form of relevance vectors but the output space consists of \emph{full rankings} of the documents associated with a query), the relevance vectors are \emph{arbitrarily} mapped to full rankings.	  Listwise large margin surrogates form an important sub-class of listwise surrogates. Their use is motivated by the success of large margin surrogates in supervised classification problems. However, existing popular listwise large margin surrogates in the learning to rank literature are derived using the structured prediction framework \cite{chapelle2007, yue2007, chakrabarti2008}.  In standard structured prediction, the supervision space is the same as the output space of the function being learned. To fit the structured prediction framework to the learning to rank problem (where the supervision is in form of relevance vectors but the output space consists of \emph{full rankings} of the documents associated with a query), the relevance vectors are \emph{arbitrarily} mapped to full rankings.	score:485
What is the salary range for classification levels E in Stanford University?	 There are tremendous amount of research on improving the classification performance in such cases. One highly investigated field for this problem is ensemble learning \cite{kuncheva}, where multiple prediction are fused the produce a more efficient classification approach. One fundamental requirement for the creation of classifier ensembles is diversity among them \cite{Brown2005}, that is, the classifiers included in the ensemble need to complement each other to provide more generalization capabilities than a single learner.	 There are tremendous amount of research on improving the classification performance in such cases. One highly investigated field for this problem is ensemble learning \cite{kuncheva}, where multiple prediction are fused the produce a more efficient classification approach. One fundamental requirement for the creation of classifier ensembles is diversity among them \cite{Brown2005}, that is, the classifiers included in the ensemble need to complement each other to provide more generalization capabilities than a single learner.	score:489
What is the salary range for classification levels E in Stanford University?	 Adversarial robustness has become an important research topic given empirical demonstrations on the lack of robustness of deep neural networks. Unfortunately, recent theoretical results suggest that adversarial training induces a strict tradeoff between classification accuracy and adversarial robustness. In this paper, we propose and then study a new regularization for any margin classifier or deep neural network.  We motivate this regularization by a novel generalization bound that shows a tradeoff in classifier accuracy between maximizing its margin and average margin. We thus call our approach an average margin (AM) regularization, and it consists of a linear term added to the objective. We theoretically show that for certain distributions AM regularization can both improve classifier accuracy and robustness to adversarial attacks.	 Adversarial robustness has become an important research topic given empirical demonstrations on the lack of robustness of deep neural networks. Unfortunately, recent theoretical results suggest that adversarial training induces a strict tradeoff between classification accuracy and adversarial robustness. In this paper, we propose and then study a new regularization for any margin classifier or deep neural network.  We motivate this regularization by a novel generalization bound that shows a tradeoff in classifier accuracy between maximizing its margin and average margin. We thus call our approach an average margin (AM) regularization, and it consists of a linear term added to the objective. We theoretically show that for certain distributions AM regularization can both improve classifier accuracy and robustness to adversarial attacks.	score:494
What are the skills or knowledge that a NLP (Natural Language Processing) developer should have after 1 year of his job?	  \item What forms of knowledge will be used to help future learning?  \item How does the system obtain the knowledge?  \item How does the system use the knowledge to help future learning?  \end{enumerate}  Motivated by these questions, we present the following definition of \textit{lifelong learning} (LL).   \vspace{3pt} \noindent \textbf{Definition (Lifelong Learning)}: A learner has performed learning on a sequence of tasks, from 1 to $N-1$.	  \item What forms of knowledge will be used to help future learning?  \item How does the system obtain the knowledge?  \item How does the system use the knowledge to help future learning?  \end{enumerate}  Motivated by these questions, we present the following definition of \textit{lifelong learning} (LL).   \vspace{3pt} \noindent \textbf{Definition (Lifelong Learning)}: A learner has performed learning on a sequence of tasks, from 1 to $N-1$.	score:413
What are the skills or knowledge that a NLP (Natural Language Processing) developer should have after 1 year of his job?	 The main result of \citet{kitagawa2015should} is that, if treatment propensities $e(X_i)$ are known, then a variant of inverse-propensity weighted policy learning achieves regret on the order of \smash{$\sqrt{\VC(\Pi)/n}$}. However, in observational studies where the treatment propensities are unknown, the bounds of \citet{kitagawa2015should} depend on the rate at which we can estimate $e(\cdot)$, and will generally decay slower than $1/\sqrt{n}$.  The only other available $1/\sqrt{n}$-bounds for policy learning in observational studies with a binary treatment that we are aware of are a result of \citet*{van2006cross} for the case where $\Pi$ consists of a finite set of policies whose cardinality grows with $n$, and a result of \citet{kallus2017balanced} in the special case $m(\cdot, \, w)$ is assumed to belong to a reproducing kernel Hilbert space.	 The main result of \citet{kitagawa2015should} is that, if treatment propensities $e(X_i)$ are known, then a variant of inverse-propensity weighted policy learning achieves regret on the order of \smash{$\sqrt{\VC(\Pi)/n}$}. However, in observational studies where the treatment propensities are unknown, the bounds of \citet{kitagawa2015should} depend on the rate at which we can estimate $e(\cdot)$, and will generally decay slower than $1/\sqrt{n}$.  The only other available $1/\sqrt{n}$-bounds for policy learning in observational studies with a binary treatment that we are aware of are a result of \citet*{van2006cross} for the case where $\Pi$ consists of a finite set of policies whose cardinality grows with $n$, and a result of \citet{kallus2017balanced} in the special case $m(\cdot, \, w)$ is assumed to belong to a reproducing kernel Hilbert space.	score:415
What are the skills or knowledge that a NLP (Natural Language Processing) developer should have after 1 year of his job?	 Therefore, it is necessary to compare and analyze the result of different models according to different criteria to be able to choose the best possible for each task. However, in practice, this type of comparisons cannot be done as they require extra resources and take more time.  The main goal of natural language processing (NLP) based in machine learning is to obtain a high level of accuracy and efficiency.  Unfortunately, obtaining high accuracy often comes at the cost of slow computation \cite{Jiang:2012}. While there is a lot of research to improve accuracy, few consider time and accuracy together, as with big data we need NLP systems to be fast as well as accurate, seeking a reasonable trade-off between speed and accuracy. However, ``what is reasonable for one person might not be reasonable for another'' \cite{Jiang:2012}.	 Therefore, it is necessary to compare and analyze the result of different models according to different criteria to be able to choose the best possible for each task. However, in practice, this type of comparisons cannot be done as they require extra resources and take more time.  The main goal of natural language processing (NLP) based in machine learning is to obtain a high level of accuracy and efficiency.  Unfortunately, obtaining high accuracy often comes at the cost of slow computation \cite{Jiang:2012}. While there is a lot of research to improve accuracy, few consider time and accuracy together, as with big data we need NLP systems to be fast as well as accurate, seeking a reasonable trade-off between speed and accuracy. However, ``what is reasonable for one person might not be reasonable for another'' \cite{Jiang:2012}.	score:425
What are the skills or knowledge that a NLP (Natural Language Processing) developer should have after 1 year of his job?	   	In this work, we consider a seller offering $N$ products, where $N$ is large, and the pricing of certain products may influence the demand for others in unknown ways. We let $\mathbf{p}_t \in \mathbb{R}^N$ denote the vector of selected prices at which each product is sold during time period $t \in \{1,\dots, T\}$, which results in total demands for the products over this period represented in the vector $\mathbf{q}_t \in \mathbb{R}^N$.   Note that $\mathbf{q}_t$ represents a (noisy) evaluation of the aggregate demand curve at the chosen prices $\mathbf{p}_t$, but we never observe the counterfactual demand that would have resulted had we selected a different price-point.  This is referred to as \emph{bandit} feedback in the online optimization literature \cite{Dani07}.  Our goal is find a setting of the prices for each time period to maximize the \emph{total revenue} of the seller (over all rounds).	   	In this work, we consider a seller offering $N$ products, where $N$ is large, and the pricing of certain products may influence the demand for others in unknown ways. We let $\mathbf{p}_t \in \mathbb{R}^N$ denote the vector of selected prices at which each product is sold during time period $t \in \{1,\dots, T\}$, which results in total demands for the products over this period represented in the vector $\mathbf{q}_t \in \mathbb{R}^N$.   Note that $\mathbf{q}_t$ represents a (noisy) evaluation of the aggregate demand curve at the chosen prices $\mathbf{p}_t$, but we never observe the counterfactual demand that would have resulted had we selected a different price-point.  This is referred to as \emph{bandit} feedback in the online optimization literature \cite{Dani07}.  Our goal is find a setting of the prices for each time period to maximize the \emph{total revenue} of the seller (over all rounds).	score:425
What are the skills or knowledge that a NLP (Natural Language Processing) developer should have after 1 year of his job?	 Even subsequent offerings of a course within a year can have a different population of students, mentors, and - in some cases - instructors. Also, due to the nascent nature of the online learning platforms, many other aspects of a course evolve quickly so that students are frequently being exposed to experimental content modalities or workflow refinements.  In this world of MOOCs, an automated machine which reliably forecasts students' performance early in their coursework would be a valuable tool for making smart decisions about when (and with whom) to make live educational interventions, with the aim of increasing engagement, providing motivation, and empowering students to succeed.   In the last three years, a few deep learning based neural network models for predicting students' future performance have been explored within data mining and learning analytic communities \cite{Mi15, Piech15, Whitehill17, Wang17, Kim18}.	 Even subsequent offerings of a course within a year can have a different population of students, mentors, and - in some cases - instructors. Also, due to the nascent nature of the online learning platforms, many other aspects of a course evolve quickly so that students are frequently being exposed to experimental content modalities or workflow refinements.  In this world of MOOCs, an automated machine which reliably forecasts students' performance early in their coursework would be a valuable tool for making smart decisions about when (and with whom) to make live educational interventions, with the aim of increasing engagement, providing motivation, and empowering students to succeed.   In the last three years, a few deep learning based neural network models for predicting students' future performance have been explored within data mining and learning analytic communities \cite{Mi15, Piech15, Whitehill17, Wang17, Kim18}.	score:426
Is computer vision advancing more rapidly than natural language processing (NLP) right now? If so, why?	  Following great success in the image processing field, the idea of adversarial training has been applied to tasks in the natural language processing (NLP) field.  One promising approach directly applies adversarial training developed in the image processing field to the input word embedding space instead of the discrete input space of texts.  However, this approach abandons such {\it interpretability} as generating adversarial texts to significantly improve the performance of NLP tasks.   This paper restores interpretability to such methods by  restricting the directions of perturbations toward the existing words in the input embedding space.  As a result, we can straightforwardly reconstruct each input with perturbations to an actual text by considering the perturbations to be the replacement of words in the sentence while maintaining or even improving the task performance\footnote{Our code for reproducing our experiments is available at \url{https://github.	  Following great success in the image processing field, the idea of adversarial training has been applied to tasks in the natural language processing (NLP) field.  One promising approach directly applies adversarial training developed in the image processing field to the input word embedding space instead of the discrete input space of texts.  However, this approach abandons such {\it interpretability} as generating adversarial texts to significantly improve the performance of NLP tasks.   This paper restores interpretability to such methods by  restricting the directions of perturbations toward the existing words in the input embedding space.  As a result, we can straightforwardly reconstruct each input with perturbations to an actual text by considering the perturbations to be the replacement of words in the sentence while maintaining or even improving the task performance\footnote{Our code for reproducing our experiments is available at \url{https://github.	score:308
Is computer vision advancing more rapidly than natural language processing (NLP) right now? If so, why?	   It can also be shown that even-symmetric functions, like the square, applied to  filter responses, show strong dependencies.  So they seem to capture a lot of what we are missing after performing a single  layer of feature learning \cite{NaturalImageStatistics}.  Another motivation is that Gabor features are local Fourier components, so computing  squares is like computing spectral energies.    Finally, it can be shown that in the presence of a upstream square-root  non-linearity, using squared features generalizes standard feature learning,  since it degenerates to a standard feature-learning model when using  group size $1$ \cite{korayCvpr09}.   For a summary of these various heuristics, see \cite{NaturalImageStatistics} (page 215).       Topographic grouping of frequency, orientation and position is also a well-known feature  of mammalian brains (eg.	   It can also be shown that even-symmetric functions, like the square, applied to  filter responses, show strong dependencies.  So they seem to capture a lot of what we are missing after performing a single  layer of feature learning \cite{NaturalImageStatistics}.  Another motivation is that Gabor features are local Fourier components, so computing  squares is like computing spectral energies.    Finally, it can be shown that in the presence of a upstream square-root  non-linearity, using squared features generalizes standard feature learning,  since it degenerates to a standard feature-learning model when using  group size $1$ \cite{korayCvpr09}.   For a summary of these various heuristics, see \cite{NaturalImageStatistics} (page 215).       Topographic grouping of frequency, orientation and position is also a well-known feature  of mammalian brains (eg.	score:327
Is computer vision advancing more rapidly than natural language processing (NLP) right now? If so, why?	 What is the best brand and what's reliable?","Weight and dimensions are important if you're planning to travel with the laptop. Get something with at least 512 mb of RAM. [..] is a good brand, and has an easy to use site where you can build a custom laptop."\\\hline   \end{tabular}     \caption{Examples of text samples and their labels.}       \label{figresults} \end{table*} The goal of natural language processing (NLP) is to process text with computers in order to analyze it, to extract information and eventually to represent the same information differently.	 By these means, the search space is heavily constrained and efficient solutions can be learned with gradient descent.  ConvNets are namely adapted for computer vision because of the compositional structure of an image. Texts have similar properties : characters combine to form n-grams, stems, words, phrase, sentences etc.                    We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences, jointly with the task.   In this paper, we propose to use deep architectures of many convolutional layers to approach this goal, using up to 29 layers.  The design of our architecture is inspired by recent progress in computer vision, in particular \cite{msr:2016:iclr:vgg,He:2015:resnet}.  This paper is structured as follows. There have been previous attempts to use ConvNets for text processing.	score:341
Is computer vision advancing more rapidly than natural language processing (NLP) right now? If so, why?	 By these means, the search space is heavily constrained and efficient solutions can be learned with gradient descent.  ConvNets are namely adapted for computer vision because of the compositional structure of an image. Texts have similar properties : characters combine to form n-grams, stems, words, phrase, sentences etc.                    We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences, jointly with the task.   In this paper, we propose to use deep architectures of many convolutional layers to approach this goal, using up to 29 layers.  The design of our architecture is inspired by recent progress in computer vision, in particular \cite{msr:2016:iclr:vgg,He:2015:resnet}.  This paper is structured as follows. There have been previous attempts to use ConvNets for text processing.	 By these means, the search space is heavily constrained and efficient solutions can be learned with gradient descent.  ConvNets are namely adapted for computer vision because of the compositional structure of an image. Texts have similar properties : characters combine to form n-grams, stems, words, phrase, sentences etc.                    We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences, jointly with the task.   In this paper, we propose to use deep architectures of many convolutional layers to approach this goal, using up to 29 layers.  The design of our architecture is inspired by recent progress in computer vision, in particular \cite{msr:2016:iclr:vgg,He:2015:resnet}.  This paper is structured as follows. There have been previous attempts to use ConvNets for text processing.	score:347
Is computer vision advancing more rapidly than natural language processing (NLP) right now? If so, why?	 Program synthesis from natural language (NL) is practical for humans and, once technically feasible, would significantly facilitate software development and revolutionize end-user programming. We present \mname, an end-to-end neural network capable of mapping relatively complex, multi-sentence NL specifications to snippets of executable code. The proposed architecture relies exclusively on neural components, and is trained on abstract syntax trees, combined with a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences.	 Program synthesis from natural language (NL) is practical for humans and, once technically feasible, would significantly facilitate software development and revolutionize end-user programming. We present \mname, an end-to-end neural network capable of mapping relatively complex, multi-sentence NL specifications to snippets of executable code. The proposed architecture relies exclusively on neural components, and is trained on abstract syntax trees, combined with a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences.	score:348
What machine learning algorithms are good choices to build a ranking engine?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:318
What machine learning algorithms are good choices to build a ranking engine?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:323
What machine learning algorithms are good choices to build a ranking engine?	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	score:341
What machine learning algorithms are good choices to build a ranking engine?	 This paper demonstrates how to apply machine learning algorithms to distinguish ``good" stocks from the ``bad" stocks. To this end, we construct 244 technical and fundamental features to characterize each stock, and label stocks according to their ranking with respect to the return-to-volatility ratio. Algorithms ranging from traditional statistical learning methods to recently popular deep learning method, e. g. Logistic Regression (LR), Random Forest (RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the classification task. Genetic Algorithm (GA) is also used to implement feature selection. The effectiveness of the stock selection strategy is validated in Chinese stock market in both statistical and practical aspects, showing that: 1) Stacking outperforms other models reaching an AUC score of 0.	 This paper demonstrates how to apply machine learning algorithms to distinguish ``good" stocks from the ``bad" stocks. To this end, we construct 244 technical and fundamental features to characterize each stock, and label stocks according to their ranking with respect to the return-to-volatility ratio. Algorithms ranging from traditional statistical learning methods to recently popular deep learning method, e. g. Logistic Regression (LR), Random Forest (RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the classification task. Genetic Algorithm (GA) is also used to implement feature selection. The effectiveness of the stock selection strategy is validated in Chinese stock market in both statistical and practical aspects, showing that: 1) Stacking outperforms other models reaching an AUC score of 0.	score:341
What machine learning algorithms are good choices to build a ranking engine?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:354
How can I learn machine learning well?	 It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, ``learning to learn'' as they do so.  In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal.   Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm.  Our aim is to learn new internal representations as the algorithm learns new target functions,  that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data.	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	score:408
How can I learn machine learning well?	 Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.   To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.	 Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.   To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.	score:408
How can I learn machine learning well?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:412
How can I learn machine learning well?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:413
How can I learn machine learning well?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:413
Is doing eco hons a good option from op jindal univ since it is the first batch?	 First, we know that in a general sequential decision process, obtaining the optimal strategy requires recursive solution of the Bellman equation by a backward induction. A positive answer to the question above implies that we can avoid the backward induction altogether, because the optimal strategy becomes time-horizon independent: we get the same, optimal strategy no matter how far to the future we look.  Thus, we only need to analyze the worst case regret with respect to the current outcome to be predicted.  Secondly, it has been shown \citep{KotlowskiG11,HedayatiB12AISTATS,HedayatiB12,Harremoes13} that when CNML and SNML coincide, then they become Bayesian strategies and the prior of the Bayesian strategy must be Jeffreys prior. In other words, if CNML is time-horizon independent, then the Bayesian strategy with Jeffreys prior is the (conditional) minimax strategy.	 First, we know that in a general sequential decision process, obtaining the optimal strategy requires recursive solution of the Bellman equation by a backward induction. A positive answer to the question above implies that we can avoid the backward induction altogether, because the optimal strategy becomes time-horizon independent: we get the same, optimal strategy no matter how far to the future we look.  Thus, we only need to analyze the worst case regret with respect to the current outcome to be predicted.  Secondly, it has been shown \citep{KotlowskiG11,HedayatiB12AISTATS,HedayatiB12,Harremoes13} that when CNML and SNML coincide, then they become Bayesian strategies and the prior of the Bayesian strategy must be Jeffreys prior. In other words, if CNML is time-horizon independent, then the Bayesian strategy with Jeffreys prior is the (conditional) minimax strategy.	score:376
Is doing eco hons a good option from op jindal univ since it is the first batch?	 A good option is to roll off Nesterov acceleration for later iterations. The common practice of training with nontrivial minibatches enhances the advantage of Nesterov acceleration. 	The scheme of Robbins and Monro (online stochastic gradient descent) has long been known to be optimal for stochastic approximation/optimization \dots\,provided that the starting point for the iterations is good.  Yet poor starting points are common in machine learning, so higher-order methods (such as Nesterov acceleration) may help during the early iterations (even though higher-order methods generally hurt during later iterations), as explained below. Below, we elaborate on Section~2 of~\citet{sutskever-martens-dahl-hinton}, giving some more rigorous mathematical details, somewhat similar to those given by~\citet{defossez-bach} and~\citet{flammarion-bach}.	 A good option is to roll off Nesterov acceleration for later iterations. The common practice of training with nontrivial minibatches enhances the advantage of Nesterov acceleration. 	The scheme of Robbins and Monro (online stochastic gradient descent) has long been known to be optimal for stochastic approximation/optimization \dots\,provided that the starting point for the iterations is good.  Yet poor starting points are common in machine learning, so higher-order methods (such as Nesterov acceleration) may help during the early iterations (even though higher-order methods generally hurt during later iterations), as explained below. Below, we elaborate on Section~2 of~\citet{sutskever-martens-dahl-hinton}, giving some more rigorous mathematical details, somewhat similar to those given by~\citet{defossez-bach} and~\citet{flammarion-bach}.	score:382
Is doing eco hons a good option from op jindal univ since it is the first batch?	e., when vehicles' queue lengths exceed a predefined threshold with non-negligible probability) using extreme value theory (EVT).  Leveraging principles from federated learning (FL), the distribution of these extreme events corresponding to the tail distribution of queues is estimated by VUEs in a decentralized manner. Finally, Lyapunov optimization is used to find the joint transmit power and resource allocation policies for each VUE in a distributed manner.   The proposed solution is validated via extensive simulations using a Manhattan mobility model. It is shown that FL enables the proposed distributed method to estimate the tail distribution of queues with an accuracy that is very close to a centralized solution with up to 79\% reductions in the amount of data that need to be exchanged. Furthermore, the proposed method yields up to 60\% reductions of VUEs with large queue lengths, without an additional power consumption, compared to an average queue-based baseline.	e., when vehicles' queue lengths exceed a predefined threshold with non-negligible probability) using extreme value theory (EVT).  Leveraging principles from federated learning (FL), the distribution of these extreme events corresponding to the tail distribution of queues is estimated by VUEs in a decentralized manner. Finally, Lyapunov optimization is used to find the joint transmit power and resource allocation policies for each VUE in a distributed manner.   The proposed solution is validated via extensive simulations using a Manhattan mobility model. It is shown that FL enables the proposed distributed method to estimate the tail distribution of queues with an accuracy that is very close to a centralized solution with up to 79\% reductions in the amount of data that need to be exchanged. Furthermore, the proposed method yields up to 60\% reductions of VUEs with large queue lengths, without an additional power consumption, compared to an average queue-based baseline.	score:384
Is doing eco hons a good option from op jindal univ since it is the first batch?	e., when vehicles' queue lengths exceed a predefined threshold with non-negligible probability) using extreme value theory (EVT).  Leveraging principles from federated learning (FL), the distribution of these extreme events corresponding to the tail distribution of queues is estimated by VUEs in a decentralized manner. Finally, Lyapunov optimization is used to find the joint transmit power and resource allocation policies for each VUE in a distributed manner.   The proposed solution is validated via extensive simulations using a Manhattan mobility model. It is shown that FL enables the proposed distributed method to estimate the tail distribution of queues with an accuracy that is very close to a centralized solution with up to 79\% reductions in the amount of data that need to be exchanged. Furthermore, the proposed method yields up to 60\% reductions of VUEs with large queue lengths, without an additional power consumption, compared to an average queue-based baseline.	e., when vehicles' queue lengths exceed a predefined threshold with non-negligible probability) using extreme value theory (EVT).  Leveraging principles from federated learning (FL), the distribution of these extreme events corresponding to the tail distribution of queues is estimated by VUEs in a decentralized manner. Finally, Lyapunov optimization is used to find the joint transmit power and resource allocation policies for each VUE in a distributed manner.   The proposed solution is validated via extensive simulations using a Manhattan mobility model. It is shown that FL enables the proposed distributed method to estimate the tail distribution of queues with an accuracy that is very close to a centralized solution with up to 79\% reductions in the amount of data that need to be exchanged. Furthermore, the proposed method yields up to 60\% reductions of VUEs with large queue lengths, without an additional power consumption, compared to an average queue-based baseline.	score:384
Is doing eco hons a good option from op jindal univ since it is the first batch?	 In this work, we propose an infinite restricted Boltzmann machine~(RBM),  whose maximum likelihood estimation~(MLE) corresponds to a constrained convex optimization.   We consider the Frank-Wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as  inserting a hidden unit at each iteration, so that the optimization process takes the form of a sequence  of finite models of increasing complexity.   As a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization.  The resulting model can also be used as  an initialization for typical state-of-the-art RBM training algorithms such as contrastive divergence, leading to models with consistently higher test likelihood than random initialization.	 In this work, we propose an infinite restricted Boltzmann machine~(RBM),  whose maximum likelihood estimation~(MLE) corresponds to a constrained convex optimization.   We consider the Frank-Wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as  inserting a hidden unit at each iteration, so that the optimization process takes the form of a sequence  of finite models of increasing complexity.   As a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization.  The resulting model can also be used as  an initialization for typical state-of-the-art RBM training algorithms such as contrastive divergence, leading to models with consistently higher test likelihood than random initialization.	score:394
What is the most important unresolved problem in machine learning?	    In addition to the capability of implementing training without locking, the proposed approach can be exploited for additional important applications.  First, we show that applying the proposed method automatically performs structural optimization of neural networks for a given problem, which has been a challenging issue in the machine learning field.   Second, a progressive inference algorithm using the network trained with the proposed method is presented, which can adaptively reduce the computational complexity during the inference process (i.e., test phase) depending on the given data.  Third, the network trained by the proposed method naturally enables ensemble inference that can improve the classification performance.	    In addition to the capability of implementing training without locking, the proposed approach can be exploited for additional important applications.  First, we show that applying the proposed method automatically performs structural optimization of neural networks for a given problem, which has been a challenging issue in the machine learning field.   Second, a progressive inference algorithm using the network trained with the proposed method is presented, which can adaptively reduce the computational complexity during the inference process (i.e., test phase) depending on the given data.  Third, the network trained by the proposed method naturally enables ensemble inference that can improve the classification performance.	score:329
What is the most important unresolved problem in machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:335
What is the most important unresolved problem in machine learning?	  	Since many problems in compressed sensing and machine learning can be formulated as a constrained linear regression problem, such as SVM, LASSO, signal recovery \cite{pilanci2015randomized}, large scale linear regression with constraints now becomes one of the most popular and basic models in Machine Learning and has received a great deal of attentions from both the Machine Learning and Theoretical Computer Science communities.  Formally, the problem can be defined as follows, \begin{equation*} \min_{x\in{\mathcal{W}}}f(x)={\|Ax-b\|_2^2}, \end{equation*}  where $A$ is a matrix  in  $\mathbb{R}^{n\times d}$ with $e^d>n>d$ and $\mathcal{W}$ is a closed convex set. The goal is to find an $x\in{\mathcal{W}}$ such that $f(x)\leq (1+\epsilon)\min_{x\in{\mathcal{W}}}f(x)$ or $f(x)- \min_{x\in{\mathcal{W}}}f(x)\leq \epsilon$.	  	Since many problems in compressed sensing and machine learning can be formulated as a constrained linear regression problem, such as SVM, LASSO, signal recovery \cite{pilanci2015randomized}, large scale linear regression with constraints now becomes one of the most popular and basic models in Machine Learning and has received a great deal of attentions from both the Machine Learning and Theoretical Computer Science communities.  Formally, the problem can be defined as follows, \begin{equation*} \min_{x\in{\mathcal{W}}}f(x)={\|Ax-b\|_2^2}, \end{equation*}  where $A$ is a matrix  in  $\mathbb{R}^{n\times d}$ with $e^d>n>d$ and $\mathcal{W}$ is a closed convex set. The goal is to find an $x\in{\mathcal{W}}$ such that $f(x)\leq (1+\epsilon)\min_{x\in{\mathcal{W}}}f(x)$ or $f(x)- \min_{x\in{\mathcal{W}}}f(x)\leq \epsilon$.	score:342
What is the most important unresolved problem in machine learning?	 While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings.  We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.     	Deep Reinforcement Learning has produced impressive results in the recent past, allowing machines to master Go \citep{go}, beat Ms.	 While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings.  We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.     	Deep Reinforcement Learning has produced impressive results in the recent past, allowing machines to master Go \citep{go}, beat Ms.	score:343
What is the most important unresolved problem in machine learning?	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	score:351
What are the best Scala libraries for machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:351
What are the best Scala libraries for machine learning?	  	With the rapid growth of the training data and the resulting machine learning model complexity, distributed optimization has become a popular solution for scaling up the machine learning problems. Parameters sever (PS)~\cite{li2014scaling} is one of the most popularly adopted distributed computing framework tailored for large scale machine learning.  PS splits the computers in a cluster into worker nodes and server nodes. The data and workload are distributed to worker nodes and the globally shared model parameters are maintained by the server nodes. During the training of machine learning models, the worker nodes process data and calculate the gradients while server nodes synchronize parameters and perform global updates.	  	With the rapid growth of the training data and the resulting machine learning model complexity, distributed optimization has become a popular solution for scaling up the machine learning problems. Parameters sever (PS)~\cite{li2014scaling} is one of the most popularly adopted distributed computing framework tailored for large scale machine learning.  PS splits the computers in a cluster into worker nodes and server nodes. The data and workload are distributed to worker nodes and the globally shared model parameters are maintained by the server nodes. During the training of machine learning models, the worker nodes process data and calculate the gradients while server nodes synchronize parameters and perform global updates.	score:354
What are the best Scala libraries for machine learning?	  	Since many problems in compressed sensing and machine learning can be formulated as a constrained linear regression problem, such as SVM, LASSO, signal recovery \cite{pilanci2015randomized}, large scale linear regression with constraints now becomes one of the most popular and basic models in Machine Learning and has received a great deal of attentions from both the Machine Learning and Theoretical Computer Science communities.  Formally, the problem can be defined as follows, \begin{equation*} \min_{x\in{\mathcal{W}}}f(x)={\|Ax-b\|_2^2}, \end{equation*}  where $A$ is a matrix  in  $\mathbb{R}^{n\times d}$ with $e^d>n>d$ and $\mathcal{W}$ is a closed convex set. The goal is to find an $x\in{\mathcal{W}}$ such that $f(x)\leq (1+\epsilon)\min_{x\in{\mathcal{W}}}f(x)$ or $f(x)- \min_{x\in{\mathcal{W}}}f(x)\leq \epsilon$.	  	Since many problems in compressed sensing and machine learning can be formulated as a constrained linear regression problem, such as SVM, LASSO, signal recovery \cite{pilanci2015randomized}, large scale linear regression with constraints now becomes one of the most popular and basic models in Machine Learning and has received a great deal of attentions from both the Machine Learning and Theoretical Computer Science communities.  Formally, the problem can be defined as follows, \begin{equation*} \min_{x\in{\mathcal{W}}}f(x)={\|Ax-b\|_2^2}, \end{equation*}  where $A$ is a matrix  in  $\mathbb{R}^{n\times d}$ with $e^d>n>d$ and $\mathcal{W}$ is a closed convex set. The goal is to find an $x\in{\mathcal{W}}$ such that $f(x)\leq (1+\epsilon)\min_{x\in{\mathcal{W}}}f(x)$ or $f(x)- \min_{x\in{\mathcal{W}}}f(x)\leq \epsilon$.	score:360
What are the best Scala libraries for machine learning?	   In many such applications, machine learning has shown strong potential to compete with or even outperform conventional \textit{ab-initio} computations\cite{FreeSolv, GDB7_dataset_arxiv}. It follows that introduction of novel machine learning methods has the potential to reshape research on properties of molecules. However, this potential has been limited by the lack of a standard evaluation platform for proposed machine learning algorithms.  Algorithmic papers often benchmark proposed methods on disjoint dataset collections, making it a challenge to gauge whether a proposed technique does in fact improve performance.  Data for molecule-based machine learning tasks are highly heterogeneous and expensive to gather. Obtaining precise and accurate results for chemical properties typically requires specialized instruments as well as expert supervision (contrast with computer speech and vision, where lightly trained workers can annotate data suitable for machine learning systems).	   In many such applications, machine learning has shown strong potential to compete with or even outperform conventional \textit{ab-initio} computations\cite{FreeSolv, GDB7_dataset_arxiv}. It follows that introduction of novel machine learning methods has the potential to reshape research on properties of molecules. However, this potential has been limited by the lack of a standard evaluation platform for proposed machine learning algorithms.  Algorithmic papers often benchmark proposed methods on disjoint dataset collections, making it a challenge to gauge whether a proposed technique does in fact improve performance.  Data for molecule-based machine learning tasks are highly heterogeneous and expensive to gather. Obtaining precise and accurate results for chemical properties typically requires specialized instruments as well as expert supervision (contrast with computer speech and vision, where lightly trained workers can annotate data suitable for machine learning systems).	score:364
What are the best Scala libraries for machine learning?	 Machine learning models are vulnerable to adversarial examples. An adversary  modifies the input data such that humans still assign the same label, however,   machine learning models misclassify it. Previous approaches in the literature demonstrated that adversarial examples can even be generated for the remotely hosted model.  In this paper, we propose a Siamese network based approach to generate adversarial examples for a multiclass target CNN.  We assume that the adversary do not possess any knowledge of the target data distribution, and we use an unlabeled mismatched dataset to query the target, e.g., for the ResNet-50 target, we use the Food-101 dataset as the query. Initially, the target model assigns labels to the query dataset, and a Siamese network is trained on the image pairs derived from these multiclass labels.	 Machine learning models are vulnerable to adversarial examples. An adversary  modifies the input data such that humans still assign the same label, however,   machine learning models misclassify it. Previous approaches in the literature demonstrated that adversarial examples can even be generated for the remotely hosted model.  In this paper, we propose a Siamese network based approach to generate adversarial examples for a multiclass target CNN.  We assume that the adversary do not possess any knowledge of the target data distribution, and we use an unlabeled mismatched dataset to query the target, e.g., for the ResNet-50 target, we use the Food-101 dataset as the query. Initially, the target model assigns labels to the query dataset, and a Siamese network is trained on the image pairs derived from these multiclass labels.	score:365
When will be the next batch of Infosys after October?	  In order to solve the first question, we will use computational machine learning techniques to estimate the fill level of each container. Supported by historical data (past data), the system generates predictions (future data) to decide when a container should be collected. This prediction could be performed long term or short term, considering seasonality, weekends, and holidays.  We want to highlight that, as far as we know, the fill-level prediction of all containers individually (fine grain) is a feature, which until now, has not been implemented in any tool for waste collection management. Additionally, our software system is able to interact with an Internet of Things (IoT) system to obtain the actual filling of the containers equipped with volumetric sensors.	  In order to solve the first question, we will use computational machine learning techniques to estimate the fill level of each container. Supported by historical data (past data), the system generates predictions (future data) to decide when a container should be collected. This prediction could be performed long term or short term, considering seasonality, weekends, and holidays.  We want to highlight that, as far as we know, the fill-level prediction of all containers individually (fine grain) is a feature, which until now, has not been implemented in any tool for waste collection management. Additionally, our software system is able to interact with an Internet of Things (IoT) system to obtain the actual filling of the containers equipped with volumetric sensors.	score:485
When will be the next batch of Infosys after October?	 In their experiments, TS is also more robust to delayed or batched feedback (delayed feedback means that the result of a play of an arm may become available only after some time delay, but we are required to make immediate decisions for which arm to play next) than the other methods.   A possible explanation may be that TS is a randomized algorithm and so it is unlikely to  get trapped in an early bad decision during the delay.   Microsoft's adPredictor (\cite{GraepelCBH10}) for CTR prediction of search ads on Bing uses the idea of Thompson Sampling.      It has been suggested (\cite{CLi}) that despite being easy to implement and being competitive to the state of the art methods, the reason TS is not very popular in literature could be its lack of strong theoretical analysis.	 In their experiments, TS is also more robust to delayed or batched feedback (delayed feedback means that the result of a play of an arm may become available only after some time delay, but we are required to make immediate decisions for which arm to play next) than the other methods.   A possible explanation may be that TS is a randomized algorithm and so it is unlikely to  get trapped in an early bad decision during the delay.   Microsoft's adPredictor (\cite{GraepelCBH10}) for CTR prediction of search ads on Bing uses the idea of Thompson Sampling.      It has been suggested (\cite{CLi}) that despite being easy to implement and being competitive to the state of the art methods, the reason TS is not very popular in literature could be its lack of strong theoretical analysis.	score:488
When will be the next batch of Infosys after October?	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.	score:490
When will be the next batch of Infosys after October?	   For example, the time lapse between successive elements in patient records can vary from days to months, which may lead to suboptimal performance for the traditional LSTM models. In addition, it is also difficult to interpret their impressive performance, especially when the data is high-dimensional, which in turn   limits the ability to design better architectures.   To address these challenges, three research questions have been raised as follows:   \begin{itemize}  \item (1) RQ1: ``Can we predict diseases status based on individual records?"  \item (2) RQ2: ``How to build an appropriate learning model that can deal with the irregular data collection times, and learn hidden patterns from time series features?	   For example, the time lapse between successive elements in patient records can vary from days to months, which may lead to suboptimal performance for the traditional LSTM models. In addition, it is also difficult to interpret their impressive performance, especially when the data is high-dimensional, which in turn   limits the ability to design better architectures.   To address these challenges, three research questions have been raised as follows:   \begin{itemize}  \item (1) RQ1: ``Can we predict diseases status based on individual records?"  \item (2) RQ2: ``How to build an appropriate learning model that can deal with the irregular data collection times, and learn hidden patterns from time series features?	score:499
When will be the next batch of Infosys after October?	 After several months, the aggressive behavior in both groups is measured.  If a significant correlation between group and outcome is observed (or  equivalently, the outcome is significantly different between the two groups), it can then be concluded that playing violent computer games indeed causes aggressive behavior.  Given the ethical and practical problems that such an experiment would involve,  one might wonder whether there are alternative ways to answer this question.  One such alternative is to combine data from different contexts. For example, in some countries the government may have decided to forbid certain ultra-violent games from being sold. In addition, some schools may have introduced certain measures to discourage aggressive behavior. By combining the data from these different contexts in an appropriate way, one may be able to identify the presence or absence of a causal effect of playing violent computer games on aggressive behavior.	 After several months, the aggressive behavior in both groups is measured.  If a significant correlation between group and outcome is observed (or  equivalently, the outcome is significantly different between the two groups), it can then be concluded that playing violent computer games indeed causes aggressive behavior.  Given the ethical and practical problems that such an experiment would involve,  one might wonder whether there are alternative ways to answer this question.  One such alternative is to combine data from different contexts. For example, in some countries the government may have decided to forbid certain ultra-violent games from being sold. In addition, some schools may have introduced certain measures to discourage aggressive behavior. By combining the data from these different contexts in an appropriate way, one may be able to identify the presence or absence of a causal effect of playing violent computer games on aggressive behavior.	score:503
When would be the next batch of infosys after October 17th?	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.	score:451
When would be the next batch of infosys after October 17th?	 The spectro-temporal information is considered imperative for discriminating between the component sources, while spatial information can be harnessed to achieve further separation \cite{Yu:16:lbssssptfmdnn,Zermini:17:blpsdnnsns}. The spectro-temporal information is typically extracted using the short-time Fourier transform (STFT), where there is a trade-off between frequency and time resolutions \cite{Griffin:84:semstft}.  Computing the STFT to obtain features with high resolution in frequency leads to features with low resolution in time, and vise versa \cite{Griffin:84:semstft}. Most audio processing approaches prefer an  auditory motivated frequency scale such as Mel, Bark, or Log scaling rather than a linear frequency scale \cite{gao:13:uscsnsgfinmt,Choi:17:tdlmir}.	 The spectro-temporal information is considered imperative for discriminating between the component sources, while spatial information can be harnessed to achieve further separation \cite{Yu:16:lbssssptfmdnn,Zermini:17:blpsdnnsns}. The spectro-temporal information is typically extracted using the short-time Fourier transform (STFT), where there is a trade-off between frequency and time resolutions \cite{Griffin:84:semstft}.  Computing the STFT to obtain features with high resolution in frequency leads to features with low resolution in time, and vise versa \cite{Griffin:84:semstft}. Most audio processing approaches prefer an  auditory motivated frequency scale such as Mel, Bark, or Log scaling rather than a linear frequency scale \cite{gao:13:uscsnsgfinmt,Choi:17:tdlmir}.	score:455
When would be the next batch of infosys after October 17th?	pdf}     \label{fig:performance-budget} } \caption{Illustration of different budget pacing schemes with respect   to the portion of the budget spent every time interval. Integrating   these plots gives the portion of the budget spend from the beginning   of the day.} \label{fig:BudgetPacing} \end{figure*}  It is challenging to perform real time bid optimization in a RTB environment for many reasons, including the following.  Firstly, the decision of placing a bid and evaluation of the bid price needs to be performed per ad request in few milliseconds. In addition, top DSPs typically receive as many as a million ad requests per second while hundreds of millions of users simultaneously explore the web around the globe. The short latency and high throughput requirements introduce extreme time sensitivity on the process.  Secondly, lots of information is missing in the real time evaluation of the individual ad requests, e.g., the feedback on previous decisions has normally a long delay in practice. More specifically, the collection of click information is delayed because of the duplication removal during the logging process. On the other hand, most of the view-through actions often require up to seven days to be converted and attributed to the corresponding impressions.  Finally, click and conversion events are usually very rare for non-search advertisement and therefore the variance will be large while estimating the past performance metrics.  In this paper, we present an online approach to optimize the performance  metrics while satisfying the smooth delivery constraint for each campaign. Our approach first applies a control feedback loop to iteratively estimate the future spending rate in order to impose smooth delivery constraints.	pdf}     \label{fig:performance-budget} } \caption{Illustration of different budget pacing schemes with respect   to the portion of the budget spent every time interval. Integrating   these plots gives the portion of the budget spend from the beginning   of the day.} \label{fig:BudgetPacing} \end{figure*}  It is challenging to perform real time bid optimization in a RTB environment for many reasons, including the following.  Firstly, the decision of placing a bid and evaluation of the bid price needs to be performed per ad request in few milliseconds. In addition, top DSPs typically receive as many as a million ad requests per second while hundreds of millions of users simultaneously explore the web around the globe. The short latency and high throughput requirements introduce extreme time sensitivity on the process.  Secondly, lots of information is missing in the real time evaluation of the individual ad requests, e.g., the feedback on previous decisions has normally a long delay in practice. More specifically, the collection of click information is delayed because of the duplication removal during the logging process. On the other hand, most of the view-through actions often require up to seven days to be converted and attributed to the corresponding impressions.  Finally, click and conversion events are usually very rare for non-search advertisement and therefore the variance will be large while estimating the past performance metrics.  In this paper, we present an online approach to optimize the performance  metrics while satisfying the smooth delivery constraint for each campaign. Our approach first applies a control feedback loop to iteratively estimate the future spending rate in order to impose smooth delivery constraints.	score:461
When would be the next batch of infosys after October 17th?	1553488, Whose_vote}. } That is, a large budget of $5-10$ times the number of actual annotated pairs required is allocated to obtain multiple annotations for each pair. These annotations are then averaged over so as to eliminate label noise. However, the effectiveness of the majority voting strategy is often limited by the sparsity problem -- it is typically infeasible to have many annotators for each pair.  Furthermore, there is no guarantee that outliers, particularly those caused by unintentional human errors can be dealt with effectively. This is because majority voting is a local consistency detection based strategy -- when there are contradictory/inconsistent pairwise rankings for a given pair, the pairwise rankings receiving minority votes are eliminated as outliers.	1553488, Whose_vote}. } That is, a large budget of $5-10$ times the number of actual annotated pairs required is allocated to obtain multiple annotations for each pair. These annotations are then averaged over so as to eliminate label noise. However, the effectiveness of the majority voting strategy is often limited by the sparsity problem -- it is typically infeasible to have many annotators for each pair.  Furthermore, there is no guarantee that outliers, particularly those caused by unintentional human errors can be dealt with effectively. This is because majority voting is a local consistency detection based strategy -- when there are contradictory/inconsistent pairwise rankings for a given pair, the pairwise rankings receiving minority votes are eliminated as outliers.	score:461
When would be the next batch of infosys after October 17th?	 We also prove an upper-bound for the regret of CLUCB, which can be decomposed into two terms. The first term is an upper-bound on the regret of LUCB that grows at the rate $\sqrt{T}\log (T)$. The second term is constant (does not grow with the horizon $T$) and accounts for the loss of being conservative in order to satisfy the safety constraint. This improves over the regret bound derived in~\cite{wu2016conservative} for the MAB setting, where the regret of being conservative grows with time. In Section~\ref{sec:CLUCB2}, we show how CLUCB can be extended to the case that the reward of the baseline policy is unknown without a change in its rate of regret. Finally in Section~\ref{sec:simulations}, we report experimental results that show CLUCB behaves as expected in practice and validate our theoretical analysis.	 We also prove an upper-bound for the regret of CLUCB, which can be decomposed into two terms. The first term is an upper-bound on the regret of LUCB that grows at the rate $\sqrt{T}\log (T)$. The second term is constant (does not grow with the horizon $T$) and accounts for the loss of being conservative in order to satisfy the safety constraint. This improves over the regret bound derived in~\cite{wu2016conservative} for the MAB setting, where the regret of being conservative grows with time. In Section~\ref{sec:CLUCB2}, we show how CLUCB can be extended to the case that the reward of the baseline policy is unknown without a change in its rate of regret. Finally in Section~\ref{sec:simulations}, we report experimental results that show CLUCB behaves as expected in practice and validate our theoretical analysis.	score:466
If there is a salary hike for the 2016 batch in TCS, then will this be effective for a 2015 recruited batch?	     A fundamental difficulty is that, in high-dimensional datasets, some features can be noisy, redundant or generally uninformative for clustering, and these can push clustering algorithms toward inappropriate or uninteresting results. If these uninformative or noise data points could be eliminated then, we argue, the results should be much more satisfying.  This is precisely our goal: to find an {\em informative set} of data points and to use these to drive the clustering.    We illustrate our goal with a motivating example from neurosciences (Figure \ref{fig:neuron}). Some neurons in mouse visual cortex respond well to certain grating orientations, while others do not respond systematically to gratings.	     A fundamental difficulty is that, in high-dimensional datasets, some features can be noisy, redundant or generally uninformative for clustering, and these can push clustering algorithms toward inappropriate or uninteresting results. If these uninformative or noise data points could be eliminated then, we argue, the results should be much more satisfying.  This is precisely our goal: to find an {\em informative set} of data points and to use these to drive the clustering.    We illustrate our goal with a motivating example from neurosciences (Figure \ref{fig:neuron}). Some neurons in mouse visual cortex respond well to certain grating orientations, while others do not respond systematically to gratings.	score:315
If there is a salary hike for the 2016 batch in TCS, then will this be effective for a 2015 recruited batch?	 In particular, to generate skills representing the expertise of the candidates, our approach estimates expertise scores of each candidate on the skills he or she might have and then aggregates the scores across the candidates to determine the most representative ones. To generate company attribute, our approach exploits co-viewing relationships amongst companies to discover companies similar to input candidates' companies.  To rank the final results, we propose a ranking function taking both query and input candidates into account. As the query increasingly deviates from the input candidates, the ranking function gradually focuses more on the impact of the query than it of the input candidates.  The new talent search paradigm is expected to be the next generation of LinkedIn Talent Solution. As of this writing, the product is being launched to a few pilot customers. We plan to ramp it up to other external customers in early 2016.	 To rank the final results, we propose a ranking function taking both query and input candidates into account. As the query increasingly deviates from the input candidates, the ranking function gradually focuses more on the impact of the query than it of the input candidates.  The new talent search paradigm is expected to be the next generation of LinkedIn Talent Solution. As of this writing, the product is being launched to a few pilot customers. We plan to ramp it up to other external customers in early 2016.	score:317
If there is a salary hike for the 2016 batch in TCS, then will this be effective for a 2015 recruited batch?	 This is a difficult decision since if done too early, it could result in the need for re-intubation, a technically challenging intervention that has been associated with increased morbidities and mortality \cite{manley2016extubating,chawla2017markers}. On the other hand, delayed extubation, and hence prolonged mechanical ventilation, could increase the risk of complications.  The most common complication is broncho-pulmonary dysplasia (BPD), a form of chronic lung disease associated with serious long-term sequelae \cite{walsh2005extremely}.    This project is part of an ongoing multicenter, multidisciplinary collaborative study aiming to develop an automated prediction tool of extubation readiness using analysis of cardiorespiratory signals in extremely preterm infants (Clinicaltrials.	 This is a difficult decision since if done too early, it could result in the need for re-intubation, a technically challenging intervention that has been associated with increased morbidities and mortality \cite{manley2016extubating,chawla2017markers}. On the other hand, delayed extubation, and hence prolonged mechanical ventilation, could increase the risk of complications.  The most common complication is broncho-pulmonary dysplasia (BPD), a form of chronic lung disease associated with serious long-term sequelae \cite{walsh2005extremely}.    This project is part of an ongoing multicenter, multidisciplinary collaborative study aiming to develop an automated prediction tool of extubation readiness using analysis of cardiorespiratory signals in extremely preterm infants (Clinicaltrials.	score:319
If there is a salary hike for the 2016 batch in TCS, then will this be effective for a 2015 recruited batch?	 These characteristics can be used for targeted energy feedback~\cite{kleiminger2013occupancy,beckel2012towards}, control of the home's thermostat for improved energy efficiency and user comfort~\cite{pisharoty05thermocoach,lu2010smart}, or targeted energy consulting~\cite{beckel2013automatic}. In this paper, we explore whether disaggregated energy values can either improve the accuracy of this inference or eliminate the need for training data and, if so, how good energy disaggregation must be to do so.   First, we present novel techniques that use unsupervised energy disaggregation to predict household occupancy. Our key intuition is that occupants interact with appliances causing events in the power stream while background loads such as refrigerator operate independently of occupants. Thus, we disaggregate the periodic background loads and predict occupancy based on the remaining aggregate traces, without any further disaggregation.	 These characteristics can be used for targeted energy feedback~\cite{kleiminger2013occupancy,beckel2012towards}, control of the home's thermostat for improved energy efficiency and user comfort~\cite{pisharoty05thermocoach,lu2010smart}, or targeted energy consulting~\cite{beckel2013automatic}. In this paper, we explore whether disaggregated energy values can either improve the accuracy of this inference or eliminate the need for training data and, if so, how good energy disaggregation must be to do so.   First, we present novel techniques that use unsupervised energy disaggregation to predict household occupancy. Our key intuition is that occupants interact with appliances causing events in the power stream while background loads such as refrigerator operate independently of occupants. Thus, we disaggregate the periodic background loads and predict occupancy based on the remaining aggregate traces, without any further disaggregation.	score:324
If there is a salary hike for the 2016 batch in TCS, then will this be effective for a 2015 recruited batch?	 To predict the effectiveness of drugs for a disease, longitudinal studies of patients would be needed to collect ground truth labels. For rare diseases, just acquiring a database of patients may require its own research project. Although high-throughput screening technologies have been developed and the effects of molecules can be evaluated \textit{in vitro}, there still remains a huge gap between experiments \textit{in vitro} and actual effects on a human body, as we can see from the fact that less than 10 percent of drugs passed from the Phase I of clinical trials to approval between 2006 and 2015 \cite{mullard2016parsing}.	 To predict the effectiveness of drugs for a disease, longitudinal studies of patients would be needed to collect ground truth labels. For rare diseases, just acquiring a database of patients may require its own research project. Although high-throughput screening technologies have been developed and the effects of molecules can be evaluated \textit{in vitro}, there still remains a huge gap between experiments \textit{in vitro} and actual effects on a human body, as we can see from the fact that less than 10 percent of drugs passed from the Phase I of clinical trials to approval between 2006 and 2015 \cite{mullard2016parsing}.	score:325
When does the second batch of training for freshers in CSC start? Is it when the first batch ends or is it in between?	  In the light of these empirical results, many efforts have been made to explain why the training of deep networks works so well (see the literature review in the next section). The authors believe, that equally important as it is to try to understand how and why the training of neural networks behaves well, it is also to understand what can go wrong.   There are empirical examples of things not always working well. Learning is susceptible to adversarial examples \citep{NguyenYC15,GoodfellowSS14,Fawzi16}. Neural networks identifying road stop signs as an interior of a refrigerator, invisible to human eyes perturbations make a perfectly good model suddenly misclassify, are some of the well known examples.	  In the light of these empirical results, many efforts have been made to explain why the training of deep networks works so well (see the literature review in the next section). The authors believe, that equally important as it is to try to understand how and why the training of neural networks behaves well, it is also to understand what can go wrong.   There are empirical examples of things not always working well. Learning is susceptible to adversarial examples \citep{NguyenYC15,GoodfellowSS14,Fawzi16}. Neural networks identifying road stop signs as an interior of a refrigerator, invisible to human eyes perturbations make a perfectly good model suddenly misclassify, are some of the well known examples.	score:386
When does the second batch of training for freshers in CSC start? Is it when the first batch ends or is it in between?	  We start with a static scenario and then extend to two \emph{mobile} scenarios. In the first mobile scenario, we keep the distance between the jammer and the receiver fixed and let the jammer move on a circle around the receiver. In the second mobile scenario, we keep the distance between the background transmitter and the jammer fixed. We show that the jammer is more effective as it gets closer to the receiver (so the received power level at the receiver is higher) or to the background traffic source (so it can infer better when the transmitter will succeed next).	  We start with a static scenario and then extend to two \emph{mobile} scenarios. In the first mobile scenario, we keep the distance between the jammer and the receiver fixed and let the jammer move on a circle around the receiver. In the second mobile scenario, we keep the distance between the background transmitter and the jammer fixed. We show that the jammer is more effective as it gets closer to the receiver (so the received power level at the receiver is higher) or to the background traffic source (so it can infer better when the transmitter will succeed next).	score:389
When does the second batch of training for freshers in CSC start? Is it when the first batch ends or is it in between?	 The networks are tested in both the RGB and multispectral cases when possible. They are extensions of the ones presented in \cite{daudt2018urban}, which was the first CD method to be trained end-to-end, to a fully convolutional paradigm. This improves both the accuracy and the inference speed of the networks without increasing the training times. We present fully convolutional encoder-decoder networks that use the skip connections concept introduced in \cite{ronneberger2015u}.	 The networks are tested in both the RGB and multispectral cases when possible. They are extensions of the ones presented in \cite{daudt2018urban}, which was the first CD method to be trained end-to-end, to a fully convolutional paradigm. This improves both the accuracy and the inference speed of the networks without increasing the training times. We present fully convolutional encoder-decoder networks that use the skip connections concept introduced in \cite{ronneberger2015u}.	score:391
When does the second batch of training for freshers in CSC start? Is it when the first batch ends or is it in between?	} \label{fig:teaser} \end{figure}  We propose a novel training method that follows an intuitive approach: learning by association (\autoref{fig:teaser}). We feed a batch of labeled and a batch of unlabeled data through a network, producing embeddings for both batches. Then, an imaginary walker is sent from samples in the labeled batch to samples in the unlabeled batch.  The transition follows a probability distribution obtained from the similarity of the respective embeddings which we refer to as an \emph{association}. In order to evaluate whether the association makes sense, a second step is taken back to the labeled batch - again guided by the similarity between the embeddings. It is now easy to check if the cycle ended at the same class from which it was started.	 The optimization schedule encourages correct association cycles that end up at the same class from which the association was started and penalizes wrong associations ending at a different class. The implementation is easy to use and can be added to any existing end-to-end training setup. We demonstrate the capabilities of learning by association on several data sets and show that it can improve performance on classification tasks tremendously by making use of additionally available unlabeled data.  In particular, for cases with few labeled data, our training scheme outperforms the current state of the art on SVHN.  	A child is able to learn new concepts quickly and without the need for millions examples that are pointed out individually. Once a child has seen one dog, she or he will be able to recognize other dogs and becomes better at recognition with subsequent exposure to more variety.	score:396
When does the second batch of training for freshers in CSC start? Is it when the first batch ends or is it in between?	 Since the labeling process is time-consuming and labor-intensive, acquiring labeled training data is, however, a cumbersome process. To speed this process up, labeling is often outsourced to less experienced annotators or crowd workers, for instance via Amazon's Mechanical Turk~\cite{khetan2017learning,rashtchian2010collecting}. In the context of deep learning, crowd workers are human labor, getting paid for labeling large data sets.  Sometimes, even automatic label assignment tools are used~\cite{usami2011automatic}. Unfortunately, such a label acquisition process usually leads to noisy labels, i.e., a training data set which contains many wrongly assigned labels. This can compromise training results~\cite{zhang2016understanding}. Thus, to be able to benefit from these approaches for training data acquisition, dedicated quality control mechanisms must be in place.	 Since the labeling process is time-consuming and labor-intensive, acquiring labeled training data is, however, a cumbersome process. To speed this process up, labeling is often outsourced to less experienced annotators or crowd workers, for instance via Amazon's Mechanical Turk~\cite{khetan2017learning,rashtchian2010collecting}. In the context of deep learning, crowd workers are human labor, getting paid for labeling large data sets.  Sometimes, even automatic label assignment tools are used~\cite{usami2011automatic}. Unfortunately, such a label acquisition process usually leads to noisy labels, i.e., a training data set which contains many wrongly assigned labels. This can compromise training results~\cite{zhang2016understanding}. Thus, to be able to benefit from these approaches for training data acquisition, dedicated quality control mechanisms must be in place.	score:402
Which batch now is under training in cts?	  Deep learning for clinical applications is subject to stringent performance requirements, which raises a need for large labeled datasets. However, the enormous cost of labeling medical data makes this challenging. In this paper, we build a cost-sensitive active learning system for the problem of intracranial hemorrhage detection and segmentation on head computed tomography (CT).   We show that our ensemble method compares favorably with the state-of-the-art, while running faster and using less memory. Moreover, our experiments are done using a substantially larger dataset than earlier papers on this topic. Since the labeling time could vary tremendously across examples, we model the labeling time and optimize the return on investment.	  Deep learning for clinical applications is subject to stringent performance requirements, which raises a need for large labeled datasets. However, the enormous cost of labeling medical data makes this challenging. In this paper, we build a cost-sensitive active learning system for the problem of intracranial hemorrhage detection and segmentation on head computed tomography (CT).   We show that our ensemble method compares favorably with the state-of-the-art, while running faster and using less memory. Moreover, our experiments are done using a substantially larger dataset than earlier papers on this topic. Since the labeling time could vary tremendously across examples, we model the labeling time and optimize the return on investment.	score:418
Which batch now is under training in cts?	  Deep learning for clinical applications is subject to stringent performance requirements, which raises a need for large labeled datasets. However, the enormous cost of labeling medical data makes this challenging. In this paper, we build a cost-sensitive active learning system for the problem of intracranial hemorrhage detection and segmentation on head computed tomography (CT).   We show that our ensemble method compares favorably with the state-of-the-art, while running faster and using less memory. Moreover, our experiments are done using a substantially larger dataset than earlier papers on this topic. Since the labeling time could vary tremendously across examples, we model the labeling time and optimize the return on investment.	  Deep learning for clinical applications is subject to stringent performance requirements, which raises a need for large labeled datasets. However, the enormous cost of labeling medical data makes this challenging. In this paper, we build a cost-sensitive active learning system for the problem of intracranial hemorrhage detection and segmentation on head computed tomography (CT).   We show that our ensemble method compares favorably with the state-of-the-art, while running faster and using less memory. Moreover, our experiments are done using a substantially larger dataset than earlier papers on this topic. Since the labeling time could vary tremendously across examples, we model the labeling time and optimize the return on investment.	score:418
Which batch now is under training in cts?	 \noindent {\bf Purpose:} Deep-neural-network-based image reconstruction has demonstrated promising performance in medical imaging for under-sampled and low-dose scenarios. However, it requires large amount of memory and extensive time for the training. It is especially challenging to train the reconstruction networks for three-dimensional computed tomography (CT) because of the high resolution of CT images.  The purpose of this work is to reduce the memory and time consumption of the training of the reconstruction networks for CT to make it practical for current hardware, while maintaining the quality of the reconstructed images. \\  {\bf Methods:} We unrolled the proximal gradient descent algorithm for iterative image reconstruction to finite iterations and replaced the terms related to the penalty function with trainable convolutional neural networks (CNN).	 \noindent {\bf Purpose:} Deep-neural-network-based image reconstruction has demonstrated promising performance in medical imaging for under-sampled and low-dose scenarios. However, it requires large amount of memory and extensive time for the training. It is especially challenging to train the reconstruction networks for three-dimensional computed tomography (CT) because of the high resolution of CT images.  The purpose of this work is to reduce the memory and time consumption of the training of the reconstruction networks for CT to make it practical for current hardware, while maintaining the quality of the reconstructed images. \\  {\bf Methods:} We unrolled the proximal gradient descent algorithm for iterative image reconstruction to finite iterations and replaced the terms related to the penalty function with trainable convolutional neural networks (CNN).	score:440
Which batch now is under training in cts?	 Then, the method trains policies by reinforcement learning which has the pre-trained network. The expected advantage is decreasing the number of training trials of the reinforcement learning phase. It is true that the proposed method additionally requires the training data for pre-training. However, the cost of obtaining this data is much lower than data for the reinforcement learning.  This is because rewards and optimal actions are not required for the pre-training data. In this paper, we evaluate in three different environments, including a physical interaction with a real robot. Note that we focus on discrete actions in this paper for a clear discussion about contributions by a simple architecture. We assume the proposed method will be easily applied to continuous controls with such reinforcement learning methods~\cite{ddpg, a3c}.	 Then, the method trains policies by reinforcement learning which has the pre-trained network. The expected advantage is decreasing the number of training trials of the reinforcement learning phase. It is true that the proposed method additionally requires the training data for pre-training. However, the cost of obtaining this data is much lower than data for the reinforcement learning.  This is because rewards and optimal actions are not required for the pre-training data. In this paper, we evaluate in three different environments, including a physical interaction with a real robot. Note that we focus on discrete actions in this paper for a clear discussion about contributions by a simple architecture. We assume the proposed method will be easily applied to continuous controls with such reinforcement learning methods~\cite{ddpg, a3c}.	score:447
Which batch now is under training in cts?	 Then, the method trains policies by reinforcement learning which has the pre-trained network. The expected advantage is decreasing the number of training trials of the reinforcement learning phase. It is true that the proposed method additionally requires the training data for pre-training. However, the cost of obtaining this data is much lower than data for the reinforcement learning.  This is because rewards and optimal actions are not required for the pre-training data. In this paper, we evaluate in three different environments, including a physical interaction with a real robot. Note that we focus on discrete actions in this paper for a clear discussion about contributions by a simple architecture. We assume the proposed method will be easily applied to continuous controls with such reinforcement learning methods~\cite{ddpg, a3c}.	 Then, the method trains policies by reinforcement learning which has the pre-trained network. The expected advantage is decreasing the number of training trials of the reinforcement learning phase. It is true that the proposed method additionally requires the training data for pre-training. However, the cost of obtaining this data is much lower than data for the reinforcement learning.  This is because rewards and optimal actions are not required for the pre-training data. In this paper, we evaluate in three different environments, including a physical interaction with a real robot. Note that we focus on discrete actions in this paper for a clear discussion about contributions by a simple architecture. We assume the proposed method will be easily applied to continuous controls with such reinforcement learning methods~\cite{ddpg, a3c}.	score:447
I heard some companies are developing tools that will do data analysis by generating algorithms, machine learning, etc..Does these tools cause impact to data scientist jobs?	 Experiments show that proposed schemes significantly improves algorithm performance and increase feature selection quality. 	Almost all business and scientific problems nowadays involve processing huge amounts of data with machine learning algorithms. Due to its universal applicability, machine learning became one of the most promising and researched scientific domains.  In particular, it has application in bioinformatics~\citep{bolon2014review,saeys2007review}, as giant amounts of data about gene expression of different organisms are obtained in this field. In order to filter data noise and reduce model complexity, it is necessary to select the most relevant features. Techniques and methods achieving this goal are called feature selection.	 Experiments show that proposed schemes significantly improves algorithm performance and increase feature selection quality. 	Almost all business and scientific problems nowadays involve processing huge amounts of data with machine learning algorithms. Due to its universal applicability, machine learning became one of the most promising and researched scientific domains.  In particular, it has application in bioinformatics~\citep{bolon2014review,saeys2007review}, as giant amounts of data about gene expression of different organisms are obtained in this field. In order to filter data noise and reduce model complexity, it is necessary to select the most relevant features. Techniques and methods achieving this goal are called feature selection.	score:211
I heard some companies are developing tools that will do data analysis by generating algorithms, machine learning, etc..Does these tools cause impact to data scientist jobs?	 Machine Learning algorithms are increasingly being used in recent years due to their flexibility in model fitting and increased predictive performance. However, the complexity of the models makes them hard for the data analyst to interpret the results and explain them without additional tools. This has led to much research in developing various approaches to understand the model behavior.  In this paper, we present the Explainable Neural Network (xNN), a structured neural network designed especially to learn interpretable features. Unlike fully connected neural networks, the features engineered by the xNN can be extracted from the network in a relatively straightforward manner and the results displayed. With appropriate regularization, the xNN provides a parsimonious explanation of the relationship between the features and the output.	 Machine Learning algorithms are increasingly being used in recent years due to their flexibility in model fitting and increased predictive performance. However, the complexity of the models makes them hard for the data analyst to interpret the results and explain them without additional tools. This has led to much research in developing various approaches to understand the model behavior.  In this paper, we present the Explainable Neural Network (xNN), a structured neural network designed especially to learn interpretable features. Unlike fully connected neural networks, the features engineered by the xNN can be extracted from the network in a relatively straightforward manner and the results displayed. With appropriate regularization, the xNN provides a parsimonious explanation of the relationship between the features and the output.	score:213
I heard some companies are developing tools that will do data analysis by generating algorithms, machine learning, etc..Does these tools cause impact to data scientist jobs?	 Machine learning algorithms are optimized to model statistical properties of the training data. If the input data reflects stereotypes and biases of the broader society, then the output of the learning algorithm also captures these stereotypes.  In this paper, we initiate the study of gender stereotypes in {\em word embedding}, a popular framework to represent text data.  As their use becomes increasingly common, applications can inadvertently amplify unwanted stereotypes. We show across multiple datasets that the embeddings contain significant gender stereotypes, especially with regard to professions. We created a novel gender analogy task and combined it with crowdsourcing to systematically quantify the gender bias in a given embedding.	 Machine learning algorithms are optimized to model statistical properties of the training data. If the input data reflects stereotypes and biases of the broader society, then the output of the learning algorithm also captures these stereotypes.  In this paper, we initiate the study of gender stereotypes in {\em word embedding}, a popular framework to represent text data.  As their use becomes increasingly common, applications can inadvertently amplify unwanted stereotypes. We show across multiple datasets that the embeddings contain significant gender stereotypes, especially with regard to professions. We created a novel gender analogy task and combined it with crowdsourcing to systematically quantify the gender bias in a given embedding.	score:215
I heard some companies are developing tools that will do data analysis by generating algorithms, machine learning, etc..Does these tools cause impact to data scientist jobs?	  Such feature engineering is important but labor-intensive and highlights the weakness of current learning algorithms: their inability to extract and organize the discriminative information from the data.  Feature engineering is a way to take advantage of human ingenuity and prior knowledge to compensate for that weakness. In order to expand the scope and ease of applicability of machine learning, it would be highly desirable to make learning algorithms less dependent on feature engineering, so that novel applications could be constructed faster, and more importantly, to make progress towards Artificial Intelligence (AI).	 \vsA  The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data.  Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors.  This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks.  This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.	score:222
I heard some companies are developing tools that will do data analysis by generating algorithms, machine learning, etc..Does these tools cause impact to data scientist jobs?	  It can automatic extract data features by unsupervised or semi-supervised feature learning algorithm and hierarchical feature extraction.  In contrast, traditional machine learning methods need to design features manually that seriously increases the burden on users. It can be said that deep learning is an representation learning algorithm based on large-scale data in machine learning.     \textit{Data dependence} is one of the most serious problem in deep learning. Deep learning has a very strong dependence on massive training data compared to traditional machine learning methods,  because it need a large amount of data to understand the latent patterns of data. An interesting phenomenon can be found that the scale of the model and the size of the required amount of data has a almost linear relationship.	  It can automatic extract data features by unsupervised or semi-supervised feature learning algorithm and hierarchical feature extraction.  In contrast, traditional machine learning methods need to design features manually that seriously increases the burden on users. It can be said that deep learning is an representation learning algorithm based on large-scale data in machine learning.     \textit{Data dependence} is one of the most serious problem in deep learning. Deep learning has a very strong dependence on massive training data compared to traditional machine learning methods,  because it need a large amount of data to understand the latent patterns of data. An interesting phenomenon can be found that the scale of the model and the size of the required amount of data has a almost linear relationship.	score:223
What are the fastest machine learning algorithms?	  For short-term solar forecasting, statistical methods are popularly used due to their learning power and high computational efficiency. Based on the algorithm complexity, statistical methods can be categorized into traditional time series methods (e.g., autoregressive integrated moving average (ARIMA) method~\cite{david2016probabilistic}), machine learning methods (e. g., support vector machine (SVM)~\cite{deo2016wavelet}), and deep learning models (e.g., long short term memory neural network (LSTM-NN)~\cite{gensler2016deep}). A more comprehensive review of statistical methods for solar forecasting can be found in recent review papers~\cite{voyant2017machine, antonanzas2016review, raza2016recent}.  To improve forecasting accuracy, different techniques have been proposed in the literature, such as dividing forecasting into different subtasks.	  For short-term solar forecasting, statistical methods are popularly used due to their learning power and high computational efficiency. Based on the algorithm complexity, statistical methods can be categorized into traditional time series methods (e.g., autoregressive integrated moving average (ARIMA) method~\cite{david2016probabilistic}), machine learning methods (e. g., support vector machine (SVM)~\cite{deo2016wavelet}), and deep learning models (e.g., long short term memory neural network (LSTM-NN)~\cite{gensler2016deep}). A more comprehensive review of statistical methods for solar forecasting can be found in recent review papers~\cite{voyant2017machine, antonanzas2016review, raza2016recent}.  To improve forecasting accuracy, different techniques have been proposed in the literature, such as dividing forecasting into different subtasks.	score:417
What are the fastest machine learning algorithms?	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	score:418
What are the fastest machine learning algorithms?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:419
What are the fastest machine learning algorithms?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:435
What are the fastest machine learning algorithms?	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	score:435
What is the best advice for a satisfying career in data science or machine learning?	 	Machine learning has achieved considerable successes in recent years and an ever-growing number of disciplines rely on it. Data is now ubiquitous, and there is great value from understanding the data, building probabilistic models and making predictions with them. However, in most cases, this success crucially relies on the data scientists to posit the right parametric form of the probabilistic model underlying the data, to select a good algorithm to fit to their data, and finally to perform inference on it.   These can be quite challenging even for experts and often go beyond non-experts' capabilities, specifically in hybrid domains, consisting of mixed---continuous, discrete and/or categorical---statistical types. Building a probabilistic model that is both expressive enough to capture complex dependencies among random variables of different types as well as allows for effective learning and efficient inference is still an open problem.	 	Machine learning has achieved considerable successes in recent years and an ever-growing number of disciplines rely on it. Data is now ubiquitous, and there is great value from understanding the data, building probabilistic models and making predictions with them. However, in most cases, this success crucially relies on the data scientists to posit the right parametric form of the probabilistic model underlying the data, to select a good algorithm to fit to their data, and finally to perform inference on it.   These can be quite challenging even for experts and often go beyond non-experts' capabilities, specifically in hybrid domains, consisting of mixed---continuous, discrete and/or categorical---statistical types. Building a probabilistic model that is both expressive enough to capture complex dependencies among random variables of different types as well as allows for effective learning and efficient inference is still an open problem.	score:335
What is the best advice for a satisfying career in data science or machine learning?	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	score:342
What is the best advice for a satisfying career in data science or machine learning?	 Moreover, it is unclear what data characteristics (e.g. number of covariates or samples)  need to be considered when selecting an algorithm and how this affects performance.  In part, this is due to a lack of common data sets and evaluation measures that are  used as agreed-upon benchmarks.   Many sub-fields of machine-learning have established benchmarking datasets that allow comparisons between methods.  Such datasets exist, for example, for handwriting recognition \cite{lecun1998gradient}, object detection \cite{deng2009imagenet}, and sentiment analysis in natural language processing \cite{go2009twitter}, among others. However, no such benchmarking dataset exists for causal inference analysis.  Here we present a comprehensive framework  for offline benchmarking of methods for causal effect inference, called the \emph{IBM Causal Inference Benchmarking Framework}, which is available online as open-source code.	 Moreover, it is unclear what data characteristics (e.g. number of covariates or samples)  need to be considered when selecting an algorithm and how this affects performance.  In part, this is due to a lack of common data sets and evaluation measures that are  used as agreed-upon benchmarks.   Many sub-fields of machine-learning have established benchmarking datasets that allow comparisons between methods.  Such datasets exist, for example, for handwriting recognition \cite{lecun1998gradient}, object detection \cite{deng2009imagenet}, and sentiment analysis in natural language processing \cite{go2009twitter}, among others. However, no such benchmarking dataset exists for causal inference analysis.  Here we present a comprehensive framework  for offline benchmarking of methods for causal effect inference, called the \emph{IBM Causal Inference Benchmarking Framework}, which is available online as open-source code.	score:344
What is the best advice for a satisfying career in data science or machine learning?	 However, while such applications hold great promise for the healthcare industry, the application of machine learning methodologies faces a significant set of obstacles intrinsic to the data being evaluated and the population from which the data is drawn.  Since its entrance into the digital era, the increasing scale and scope of data has placed great emphasis on the advent of \textit{Big Data} in healthcare and the challenges that come with it.  With an estimated 150 exabytes of data generated by 2011, early work addressed the challenges of processing data at such a scale~\cite{hughes2011big}. However, it is important to remember that Big Data is defined by more than just size, but rather by what are known as the four V's (The \textbf{V}olume, or quantity of data available. The \textbf{V}elocity, or speed at which the data is created.	 However, while such applications hold great promise for the healthcare industry, the application of machine learning methodologies faces a significant set of obstacles intrinsic to the data being evaluated and the population from which the data is drawn.  Since its entrance into the digital era, the increasing scale and scope of data has placed great emphasis on the advent of \textit{Big Data} in healthcare and the challenges that come with it.  With an estimated 150 exabytes of data generated by 2011, early work addressed the challenges of processing data at such a scale~\cite{hughes2011big}. However, it is important to remember that Big Data is defined by more than just size, but rather by what are known as the four V's (The \textbf{V}olume, or quantity of data available. The \textbf{V}elocity, or speed at which the data is created.	score:348
What is the best advice for a satisfying career in data science or machine learning?	 	Data science problems often require machine learning models to be trained on data in tables with one label and multiple feature columns. Data scientists must hand-craft additional features from the initial data. This process is known as \textit{feature engineering} and is one of the most tedious, but crucial, tasks in data science. Data scientists report that up to 95\% of the total project time must be allocated to carefully hand-craft new features to achieve the best models.	 	Data science problems often require machine learning models to be trained on data in tables with one label and multiple feature columns. Data scientists must hand-craft additional features from the initial data. This process is known as \textit{feature engineering} and is one of the most tedious, but crucial, tasks in data science. Data scientists report that up to 95\% of the total project time must be allocated to carefully hand-craft new features to achieve the best models.	score:353
Which is the best website/MOOC/Book to learn machine learning through R for data science?	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	score:321
Which is the best website/MOOC/Book to learn machine learning through R for data science?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:327
Which is the best website/MOOC/Book to learn machine learning through R for data science?	 Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.   To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.	 Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.   To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.	score:327
Which is the best website/MOOC/Book to learn machine learning through R for data science?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:330
Which is the best website/MOOC/Book to learn machine learning through R for data science?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:330
Why does a mini-batch SGD have a slower convergence rate than a standard one sample SGD?	e.,} Algorithm 2) by combining a new method called two step preconditioning with mini-batch SGD.    Mini-batch SGD is a popular way for improving the efficiency of SGD.  It uses several samples, instead of one, in each iteration and runs the method on all these samples (simultaneously).  Ideally, we would hope for a factor of $r$ speed-up on the convergence if using a batch of  size $r$.  However, this is not always possible for general case. Actually in some cases, there is even no speed-up at all when a large-size batch is used \cite{takac2013mini,byrd2012sample,dekel2012optimal}.  A unique feature of our method is its optimal speeding-up with respect to  the batch size, {\em i.e.} the iteration complexity will decrease by a factor of $b$ if we increase the batch size by a factor of  $b$.	e.,} Algorithm 2) by combining a new method called two step preconditioning with mini-batch SGD.    Mini-batch SGD is a popular way for improving the efficiency of SGD.  It uses several samples, instead of one, in each iteration and runs the method on all these samples (simultaneously).  Ideally, we would hope for a factor of $r$ speed-up on the convergence if using a batch of  size $r$.  However, this is not always possible for general case. Actually in some cases, there is even no speed-up at all when a large-size batch is used \cite{takac2013mini,byrd2012sample,dekel2012optimal}.  A unique feature of our method is its optimal speeding-up with respect to  the batch size, {\em i.e.} the iteration complexity will decrease by a factor of $b$ if we increase the batch size by a factor of  $b$.	score:277
Why does a mini-batch SGD have a slower convergence rate than a standard one sample SGD?	 This subset is often called a mini-batch. The use of mini-batches is common with SGD optimization, and it is beneficial when the processing time of a mini-batch of size $m$ is much smaller than $m$ times the processing time of one example (mini-batch of size $1$).  For example, in the practical training of neural networks with SGD, one is always advised to use mini-batches because it is more efficient to perform matrix-matrix multiplications over a mini-batch than an equivalent amount of matrix-vector multiplication operations (each over a single training example).	 This subset is often called a mini-batch. The use of mini-batches is common with SGD optimization, and it is beneficial when the processing time of a mini-batch of size $m$ is much smaller than $m$ times the processing time of one example (mini-batch of size $1$).  For example, in the practical training of neural networks with SGD, one is always advised to use mini-batches because it is more efficient to perform matrix-matrix multiplications over a mini-batch than an equivalent amount of matrix-vector multiplication operations (each over a single training example).	score:282
Why does a mini-batch SGD have a slower convergence rate than a standard one sample SGD?	 This slows down the performance of SGD. On balance, however, SGD is preferable to GD in applications where low accuracy solutions are sufficient. In such cases usually only a small number of passes through the data (i.e., work equivalent to a small number of full gradient evaluations) are needed to find an acceptable $x$. For this reason, SGD is extremely popular in fields such as machine learning.   \end{enumerate}   In order to improve upon GD, one needs to reduce the cost of computing a gradient. In order to improve upon SGD, one has to reduce the variance of the stochastic gradients.  In this paper we propose and analyze a \emph{Semi-Stochastic Gradient Descent} (S2GD) method. Our  method combines GD and SGD steps and reaps the benefits of both algorithms: it inherits the stability and speed of GD and at the same time retains the work-efficiency of SGD.	 This slows down the performance of SGD. On balance, however, SGD is preferable to GD in applications where low accuracy solutions are sufficient. In such cases usually only a small number of passes through the data (i.e., work equivalent to a small number of full gradient evaluations) are needed to find an acceptable $x$. For this reason, SGD is extremely popular in fields such as machine learning.   \end{enumerate}   In order to improve upon GD, one needs to reduce the cost of computing a gradient. In order to improve upon SGD, one has to reduce the variance of the stochastic gradients.  In this paper we propose and analyze a \emph{Semi-Stochastic Gradient Descent} (S2GD) method. Our  method combines GD and SGD steps and reaps the benefits of both algorithms: it inherits the stability and speed of GD and at the same time retains the work-efficiency of SGD.	score:303
Why does a mini-batch SGD have a slower convergence rate than a standard one sample SGD?	 This slows down the performance of SGD. On balance, however, SGD is preferable to GD in applications where low accuracy solutions are sufficient. In such cases usually only a small number of passes through the data (i.e., work equivalent to a small number of full gradient evaluations) are needed to find an acceptable $x$. For this reason, SGD is extremely popular in fields such as machine learning.   \end{enumerate}   In order to improve upon GD, one needs to reduce the cost of computing a gradient. In order to improve upon SGD, one has to reduce the variance of the stochastic gradients.  In this paper we propose and analyze a \emph{Semi-Stochastic Gradient Descent} (S2GD) method. Our  method combines GD and SGD steps and reaps the benefits of both algorithms: it inherits the stability and speed of GD and at the same time retains the work-efficiency of SGD.	 This slows down the performance of SGD. On balance, however, SGD is preferable to GD in applications where low accuracy solutions are sufficient. In such cases usually only a small number of passes through the data (i.e., work equivalent to a small number of full gradient evaluations) are needed to find an acceptable $x$. For this reason, SGD is extremely popular in fields such as machine learning.   \end{enumerate}   In order to improve upon GD, one needs to reduce the cost of computing a gradient. In order to improve upon SGD, one has to reduce the variance of the stochastic gradients.  In this paper we propose and analyze a \emph{Semi-Stochastic Gradient Descent} (S2GD) method. Our  method combines GD and SGD steps and reaps the benefits of both algorithms: it inherits the stability and speed of GD and at the same time retains the work-efficiency of SGD.	score:303
Why does a mini-batch SGD have a slower convergence rate than a standard one sample SGD?	 As a result, the faster convergence is observable on SGD compared to GD in practice. SGD hits a sweet spot between the good system utilization \cite{wang2016blasx} and the fast gradient updates. Therefore, it soon becomes a popular and effective method to train large scale neural networks.  The key operation in SGD is to draw a random batch from the dataset.  It is simple in math, while none-trivial to be implemented on a large-scale dataset such as ImageNet \cite{deng2009imagenet}. State of the art engineering approximation is the Fixed Cycle Pseudo Random (FCPR) sampling (defined in section \ref{FCPRS}), which retrieves batches from the pre-permuted dataset like a ring, e.g. $\mathbf{d_0} \rightarrow \mathbf{d_1} \rightarrow \mathbf{d_2} \rightarrow \mathbf{d_0} \rightarrow \mathbf{d_1} \rightarrow .	 As a result, the faster convergence is observable on SGD compared to GD in practice. SGD hits a sweet spot between the good system utilization \cite{wang2016blasx} and the fast gradient updates. Therefore, it soon becomes a popular and effective method to train large scale neural networks.  The key operation in SGD is to draw a random batch from the dataset.  It is simple in math, while none-trivial to be implemented on a large-scale dataset such as ImageNet \cite{deng2009imagenet}. State of the art engineering approximation is the Fixed Cycle Pseudo Random (FCPR) sampling (defined in section \ref{FCPRS}), which retrieves batches from the pre-permuted dataset like a ring, e.g. $\mathbf{d_0} \rightarrow \mathbf{d_1} \rightarrow \mathbf{d_2} \rightarrow \mathbf{d_0} \rightarrow \mathbf{d_1} \rightarrow .	score:304
What are the advantages of a dot matrix printer over a inkjet printer?	 In more details, we first propose to generalize the eigen-decomposition process applied on a single Laplacian matrix to the case of multiple graph Laplacian matrices. We design a joint matrix factorization framework in which each graph Laplacian is approximated by a set of joint eigenvectors shared by all the graph layers, as well as its specific eigenvalues from the eigen-decomposition.  These joint eigenvectors can then be used to form a joint low dimensional embedding of the vertices in the graph, based on which we perform clustering. In a second approach, we propose a graph regularization method that combines the spectra of two graph layers. Specifically, we treat the eigenvectors of the Laplacian matrix from one graph as functions on the other graph.	 In more details, we first propose to generalize the eigen-decomposition process applied on a single Laplacian matrix to the case of multiple graph Laplacian matrices. We design a joint matrix factorization framework in which each graph Laplacian is approximated by a set of joint eigenvectors shared by all the graph layers, as well as its specific eigenvalues from the eigen-decomposition.  These joint eigenvectors can then be used to form a joint low dimensional embedding of the vertices in the graph, based on which we perform clustering. In a second approach, we propose a graph regularization method that combines the spectra of two graph layers. Specifically, we treat the eigenvectors of the Laplacian matrix from one graph as functions on the other graph.	score:363
What are the advantages of a dot matrix printer over a inkjet printer?	  The more base images, the better the approximation, but the bigger the data set and the slower the retrieval.      A standard topic model can be thought of as a non-negative matrix factorization problem where both matrix factors are stochastic.  One matrix factor represents probability distributions over hidden topics conditional on the document and the other represents probability distributions over terms (words) conditional on the topic.   The matrix factor representing the distribution over terms, is likely to be relatively sparse while the matrix factor representing the distribution over topics may not be sparse.  \cite{Huang:2013} shows that if there exist ``anchor'' words and anchor documents, then the factorization is unique.  That is, for each topic there is a term that only occurs for that topic, similarly there are a number of documents that have only one topic.	  The more base images, the better the approximation, but the bigger the data set and the slower the retrieval.      A standard topic model can be thought of as a non-negative matrix factorization problem where both matrix factors are stochastic.  One matrix factor represents probability distributions over hidden topics conditional on the document and the other represents probability distributions over terms (words) conditional on the topic.   The matrix factor representing the distribution over terms, is likely to be relatively sparse while the matrix factor representing the distribution over topics may not be sparse.  \cite{Huang:2013} shows that if there exist ``anchor'' words and anchor documents, then the factorization is unique.  That is, for each topic there is a term that only occurs for that topic, similarly there are a number of documents that have only one topic.	score:374
What are the advantages of a dot matrix printer over a inkjet printer?	 Diagonal    and   full    covariance   matrices    were   investigated in~\cite{abe:training}   for  the   purpose   of  classification   and in~\cite{abe:regression} for the purpose  of regression. However, in a similar way, the  covariance matrix was computed for  all the training samples. Computing the covariance  matrix for the Mahalanobis distance with all the training samples is equivalent to project the data on all the principal components, scale the variance to one, and then applying the Euclidean distance.   By doing  so, classes could overlap more than in the original input space  and the discrimination between them would be decreased.  In this  work, the HDDA  model is used for  the definition of  a class specific covariance  matrix adapted for  HD data. The  specific signal and noise subspaces are estimated  for each considered class, ensuring a  parsimonious characterization  of the  classes.	 Diagonal    and   full    covariance   matrices    were   investigated in~\cite{abe:training}   for  the   purpose   of  classification   and in~\cite{abe:regression} for the purpose  of regression. However, in a similar way, the  covariance matrix was computed for  all the training samples. Computing the covariance  matrix for the Mahalanobis distance with all the training samples is equivalent to project the data on all the principal components, scale the variance to one, and then applying the Euclidean distance.   By doing  so, classes could overlap more than in the original input space  and the discrimination between them would be decreased.  In this  work, the HDDA  model is used for  the definition of  a class specific covariance  matrix adapted for  HD data. The  specific signal and noise subspaces are estimated  for each considered class, ensuring a  parsimonious characterization  of the  classes.	score:376
What are the advantages of a dot matrix printer over a inkjet printer?	 	Suppose we observe a subset of the entries of an unknown low-rank matrix~$M$, can we recover the matrix~$M$ knowing this subset alone? This problem, called Matrix Completion, is of fundamental interest in a number of fields including statistics, machine learning, signal processing and theoretical computer science. It is widely applicable to the design of recommender systems as popularized by the famous Netflix Prize.   We are interested in understanding the compuational complexity of Matrix Completion.  Much of the theory of Matrix Completion revolves around a beautiful line of positive results. These results show that under certain assumptions there is a natural semidefinite relaxation that solves the problem efficiently even if the number of visible entries is asymptotically much smaller than the total number of entries~\cite{CandesR09,CandesT10,Recht11}.	 	Suppose we observe a subset of the entries of an unknown low-rank matrix~$M$, can we recover the matrix~$M$ knowing this subset alone? This problem, called Matrix Completion, is of fundamental interest in a number of fields including statistics, machine learning, signal processing and theoretical computer science. It is widely applicable to the design of recommender systems as popularized by the famous Netflix Prize.   We are interested in understanding the compuational complexity of Matrix Completion.  Much of the theory of Matrix Completion revolves around a beautiful line of positive results. These results show that under certain assumptions there is a natural semidefinite relaxation that solves the problem efficiently even if the number of visible entries is asymptotically much smaller than the total number of entries~\cite{CandesR09,CandesT10,Recht11}.	score:376
What are the advantages of a dot matrix printer over a inkjet printer?	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.	score:376
What are the advantages of a dot matrix printer?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	 The method proposed in \cite{jain2013} is based on non-convex matrix factorization. In \cite{fithian2018}, the authors consider a general framework for reduced-rank modeling of matrix-valued data. They use a generalized weighted nuclear norm penalty where the matrix is multiplied by positive semidefinite matrices $P$ and $Q$ which depend on the matrix of features.  In \cite{agarwal2011}, the authors introduce a per-item user covariate logistic regression model augmenting  with user-specific random effects. Their approach is based on a multilevel hierarchical model.  In the case of the heterogeneous data coming from different sources, these approaches can be applied  for recovering each  source separately. In contrast, our approach aims at collecting all the available information in a single matrix which results in faster rates of convergence.	score:437
What are the advantages of a dot matrix printer?	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	score:451
What are the advantages of a dot matrix printer?	 	Suppose we observe a subset of the entries of an unknown low-rank matrix~$M$, can we recover the matrix~$M$ knowing this subset alone? This problem, called Matrix Completion, is of fundamental interest in a number of fields including statistics, machine learning, signal processing and theoretical computer science. It is widely applicable to the design of recommender systems as popularized by the famous Netflix Prize.   We are interested in understanding the compuational complexity of Matrix Completion.  Much of the theory of Matrix Completion revolves around a beautiful line of positive results. These results show that under certain assumptions there is a natural semidefinite relaxation that solves the problem efficiently even if the number of visible entries is asymptotically much smaller than the total number of entries~\cite{CandesR09,CandesT10,Recht11}.	 	Suppose we observe a subset of the entries of an unknown low-rank matrix~$M$, can we recover the matrix~$M$ knowing this subset alone? This problem, called Matrix Completion, is of fundamental interest in a number of fields including statistics, machine learning, signal processing and theoretical computer science. It is widely applicable to the design of recommender systems as popularized by the famous Netflix Prize.   We are interested in understanding the compuational complexity of Matrix Completion.  Much of the theory of Matrix Completion revolves around a beautiful line of positive results. These results show that under certain assumptions there is a natural semidefinite relaxation that solves the problem efficiently even if the number of visible entries is asymptotically much smaller than the total number of entries~\cite{CandesR09,CandesT10,Recht11}.	score:457
What are the advantages of a dot matrix printer?	 In more details, we first propose to generalize the eigen-decomposition process applied on a single Laplacian matrix to the case of multiple graph Laplacian matrices. We design a joint matrix factorization framework in which each graph Laplacian is approximated by a set of joint eigenvectors shared by all the graph layers, as well as its specific eigenvalues from the eigen-decomposition.  These joint eigenvectors can then be used to form a joint low dimensional embedding of the vertices in the graph, based on which we perform clustering. In a second approach, we propose a graph regularization method that combines the spectra of two graph layers. Specifically, we treat the eigenvectors of the Laplacian matrix from one graph as functions on the other graph.	 In more details, we first propose to generalize the eigen-decomposition process applied on a single Laplacian matrix to the case of multiple graph Laplacian matrices. We design a joint matrix factorization framework in which each graph Laplacian is approximated by a set of joint eigenvectors shared by all the graph layers, as well as its specific eigenvalues from the eigen-decomposition.  These joint eigenvectors can then be used to form a joint low dimensional embedding of the vertices in the graph, based on which we perform clustering. In a second approach, we propose a graph regularization method that combines the spectra of two graph layers. Specifically, we treat the eigenvectors of the Laplacian matrix from one graph as functions on the other graph.	score:457
What are the advantages of a dot matrix printer?	   One can attempt to solve  the robust tensor problem  in \eqref{eqn:robust-def}  using matrix methods. In other words, robust matrix PCA can be applied either to  each matrix slice of the tensor, or to the matrix obtained by flattening the tensor. However, such matrix methods ignore the  tensor algebraic constraints or the CP  rank constraints, which differ from the matrix rank constraints.  There are however a number of challenges to incorporating the tensor CP rank   constraints. Enforcing a given  tensor rank   is NP-hard~\cite{hillar2013most}, unlike the matrix case, where low rank projections can be computed efficiently. Moreover, finding the best convex relaxation of the tensor CP rank is also NP-hard~\cite{hillar2013most}, unlike   the matrix case, where the convex relaxation of the rank, \viz the nuclear norm,  can be computed efficiently.	   One can attempt to solve  the robust tensor problem  in \eqref{eqn:robust-def}  using matrix methods. In other words, robust matrix PCA can be applied either to  each matrix slice of the tensor, or to the matrix obtained by flattening the tensor. However, such matrix methods ignore the  tensor algebraic constraints or the CP  rank constraints, which differ from the matrix rank constraints.  There are however a number of challenges to incorporating the tensor CP rank   constraints. Enforcing a given  tensor rank   is NP-hard~\cite{hillar2013most}, unlike the matrix case, where low rank projections can be computed efficiently. Moreover, finding the best convex relaxation of the tensor CP rank is also NP-hard~\cite{hillar2013most}, unlike   the matrix case, where the convex relaxation of the rank, \viz the nuclear norm,  can be computed efficiently.	score:458
What is the best way to understand or think about convolutions in Differential Equations?	 However, the underlying ideas have a much longer history, stretching all the way back to the philosophers of ancient Greece. For a historical account of the philosophical ideas underlying factor analysis we would like to point the reader to the excellent work of \citet{Mulaik:1987un}.  A fundamental aspect of factor analysis is that the solution is unidentifiable.  This means that there are infinitely many solutions which cannot be differentiated by the model. Even though all solutions might be completely equivalent from a statistical view point, from an experimental view point it is often important to be able to select a particular one. This is because different solutions are often attributed with different ``meanings''.	 However, the underlying ideas have a much longer history, stretching all the way back to the philosophers of ancient Greece. For a historical account of the philosophical ideas underlying factor analysis we would like to point the reader to the excellent work of \citet{Mulaik:1987un}.  A fundamental aspect of factor analysis is that the solution is unidentifiable.  This means that there are infinitely many solutions which cannot be differentiated by the model. Even though all solutions might be completely equivalent from a statistical view point, from an experimental view point it is often important to be able to select a particular one. This is because different solutions are often attributed with different ``meanings''.	score:329
What is the best way to understand or think about convolutions in Differential Equations?	 Dilated convolutions, also known as atrous convolutions, have been widely explored in deep convolutional neural networks~(DCNNs) for various dense prediction tasks. However, dilated convolutions suffer from the gridding artifacts, which hampers the performance. In this work, we propose two simple yet effective degridding methods by studying a decomposition of dilated convolutions.  Unlike existing models, which explore solutions by focusing on a block of cascaded dilated convolutional layers, our methods address the gridding artifacts by smoothing the dilated convolution itself. In addition, we point out that the two degridding approaches are intrinsically related and define separable and shared~(SS) operations, which generalize the proposed methods.	 Dilated convolutions, also known as atrous convolutions, have been widely explored in deep convolutional neural networks~(DCNNs) for various dense prediction tasks. However, dilated convolutions suffer from the gridding artifacts, which hampers the performance. In this work, we propose two simple yet effective degridding methods by studying a decomposition of dilated convolutions.  Unlike existing models, which explore solutions by focusing on a block of cascaded dilated convolutional layers, our methods address the gridding artifacts by smoothing the dilated convolution itself. In addition, we point out that the two degridding approaches are intrinsically related and define separable and shared~(SS) operations, which generalize the proposed methods.	score:335
What is the best way to understand or think about convolutions in Differential Equations?	 One way to construct flexible variational distribution is to warp a simple density into a complex by normalizing flows, where the resulting density can be analytically evaluated. However, there is a trade-off between the flexibility of normalizing flow and computation cost for efficient transformation. In this paper, we propose a simple yet effective architecture of normalizing flows, \textit{ConvFlow}, based on convolution over the dimensions of random input vector.  Experiments on synthetic and real world posterior inference problems demonstrate the effectiveness and efficiency of the proposed method. 	Posterior inference is the key to Bayesian modeling, where we are interested to see how our belief over the variables of interest change after observing a set of data points. Predictions can also benefit from Bayesian modeling as every prediction will be equipped with confidence intervals representing how sure the prediction is.	 One way to construct flexible variational distribution is to warp a simple density into a complex by normalizing flows, where the resulting density can be analytically evaluated. However, there is a trade-off between the flexibility of normalizing flow and computation cost for efficient transformation. In this paper, we propose a simple yet effective architecture of normalizing flows, \textit{ConvFlow}, based on convolution over the dimensions of random input vector.  Experiments on synthetic and real world posterior inference problems demonstrate the effectiveness and efficiency of the proposed method. 	Posterior inference is the key to Bayesian modeling, where we are interested to see how our belief over the variables of interest change after observing a set of data points. Predictions can also benefit from Bayesian modeling as every prediction will be equipped with confidence intervals representing how sure the prediction is.	score:352
What is the best way to understand or think about convolutions in Differential Equations?	  Convolutional Neural Networks (CNNs) were recently shown to provide state-of-the-art results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key   factors that impact performance.  Followingly, we present a new joint training method with the detection task and demonstrate its benefit.         We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data.	  Convolutional Neural Networks (CNNs) were recently shown to provide state-of-the-art results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key   factors that impact performance.  Followingly, we present a new joint training method with the detection task and demonstrate its benefit.         We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data.	score:359
What is the best way to understand or think about convolutions in Differential Equations?	 The second solution is to increase the efficiency of existing infrastructure and the systems that govern them, such as traffic signal controllers (TSC). We advocate this second solution, by utilizing recent advancements from the domain of artificial intelligence \cite{mnih2015human} to develop a new traffic signal controller. \par We define the traffic signal control problem as follows; given the state of traffic at an intersection, what is the optimal traffic signal phase and sequence that should be enacted?  \iffalse Although traffic signal controllers have existed for decades, many currently in use are based on old technologies that can be improved with new ideas and perform better at solving the traffic signal control problem. The most common technology for traffic signal control are inductive-loop traffic detectors, or loop detectors, which are installed under the road to detect vehicles.	 The second solution is to increase the efficiency of existing infrastructure and the systems that govern them, such as traffic signal controllers (TSC). We advocate this second solution, by utilizing recent advancements from the domain of artificial intelligence \cite{mnih2015human} to develop a new traffic signal controller. \par We define the traffic signal control problem as follows; given the state of traffic at an intersection, what is the optimal traffic signal phase and sequence that should be enacted?  \iffalse Although traffic signal controllers have existed for decades, many currently in use are based on old technologies that can be improved with new ideas and perform better at solving the traffic signal control problem. The most common technology for traffic signal control are inductive-loop traffic detectors, or loop detectors, which are installed under the road to detect vehicles.	score:361
Do I need to be a PhD to get a good paying machine learning job?	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	score:443
Do I need to be a PhD to get a good paying machine learning job?	 Consider a researcher seeking a summer intern to work on a machine learning project on deep learning who searches for, say, ``linkedin graduate student machine learning neural networks.'' Now, a word embedding's semantic knowledge can improve relevance in the sense that a LinkedIn web page containing terms such as ``PhD student,'' ``embeddings,'' and ``deep learning,'' which are related to but different from the query terms, may be ranked highly in the results.  However, word embeddings also rank CS research related terms closer to male names than female names. The consequence would be, between two pages that differed in the names Mary and John but were otherwise identical, the search engine would rank John's higher than Mary. In this hypothetical example, the usage of word embedding makes it even harder for women to be recognized as computer scientists and would contribute to widening the existing gender gap in computer science.	 Consider a researcher seeking a summer intern to work on a machine learning project on deep learning who searches for, say, ``linkedin graduate student machine learning neural networks.'' Now, a word embedding's semantic knowledge can improve relevance in the sense that a LinkedIn web page containing terms such as ``PhD student,'' ``embeddings,'' and ``deep learning,'' which are related to but different from the query terms, may be ranked highly in the results.  However, word embeddings also rank CS research related terms closer to male names than female names. The consequence would be, between two pages that differed in the names Mary and John but were otherwise identical, the search engine would rank John's higher than Mary. In this hypothetical example, the usage of word embedding makes it even harder for women to be recognized as computer scientists and would contribute to widening the existing gender gap in computer science.	score:446
Do I need to be a PhD to get a good paying machine learning job?	 Therefore, while both rule-based models and physics-based simulations have met with some success in predicting IL properties, the use of DNN models will provide an added advantage.  \subsection{Machine \& Deep Learning in Chemistry} For the prediction of chemical properties that cannot be easily computed through physics-based or rule-based methods, machine learning (ML) methods have been used to correlate structural features with the activity or property of the chemical.  This approach is formally known as  Quantitative Structure-Activity or Structure-Property Relationship (QSAR/QSPR) modeling in the chemistry literature~\cite{cherkasov2014}. Molecular descriptors are engineered features developed based on first-principles knowledge, which typically are basic computable properties or descriptions of a chemical's structure, and they are used as input in QSAR/QSPR models.	 Therefore, while both rule-based models and physics-based simulations have met with some success in predicting IL properties, the use of DNN models will provide an added advantage.  \subsection{Machine \& Deep Learning in Chemistry} For the prediction of chemical properties that cannot be easily computed through physics-based or rule-based methods, machine learning (ML) methods have been used to correlate structural features with the activity or property of the chemical.  This approach is formally known as  Quantitative Structure-Activity or Structure-Property Relationship (QSAR/QSPR) modeling in the chemistry literature~\cite{cherkasov2014}. Molecular descriptors are engineered features developed based on first-principles knowledge, which typically are basic computable properties or descriptions of a chemical's structure, and they are used as input in QSAR/QSPR models.	score:451
Do I need to be a PhD to get a good paying machine learning job?	 Similar ideas have recently been explored in the context of convex optimization \cite{donti2017task}, but to our knowledge ours is the first attempt to train machine learning systems for performance on \emph{combinatorial} decision-making problems. Combinatorial settings raise new technical challenges because the optimization problem is discrete. However, machine learning systems (e. g., deep neural networks) are often trained via gradient descent.     Our first contribution is a general framework for training machine learning models via their performance on combinatorial problems. The starting point is to relax the combinatorial problem to a continuous one. Then, we analytically differentiate the optimal solution to the continuous problem as a function of the model's predictions.	 Similar ideas have recently been explored in the context of convex optimization \cite{donti2017task}, but to our knowledge ours is the first attempt to train machine learning systems for performance on \emph{combinatorial} decision-making problems. Combinatorial settings raise new technical challenges because the optimization problem is discrete. However, machine learning systems (e. g., deep neural networks) are often trained via gradient descent.     Our first contribution is a general framework for training machine learning models via their performance on combinatorial problems. The starting point is to relax the combinatorial problem to a continuous one. Then, we analytically differentiate the optimal solution to the continuous problem as a function of the model's predictions.	score:452
Do I need to be a PhD to get a good paying machine learning job?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:454
Who portrays Sherlock Holmes better: Benedict Cumberbatch or Robert Downey Jr?	 Each behavior is associated with a set of phase-portraits corresponding to the inputs the agent experiences while performing them. The sets of phase-portraits (and the attractors therein) could be overlapping (Fig.~\ref{fig:schematic}C,D) or could be unique to each behavior (Fig.~\ref{fig:schematic}B). The set of all attractors from all phase-portraits corresponding to a behavior are also referred to as the attractor set of the behavior in this paper.  The third level of reuse is that of ongoing transient dynamics as the agent is in continuous closed-loop interaction with the environment. When there is attractor reuse from the previous level, it is possible that multiple behaviors navigate different transients around those attractors (Fig.~\ref{fig:schematic}C) or, they might be reused too (Fig.~\ref{fig:schematic}D).	 Each behavior is associated with a set of phase-portraits corresponding to the inputs the agent experiences while performing them. The sets of phase-portraits (and the attractors therein) could be overlapping (Fig.~\ref{fig:schematic}C,D) or could be unique to each behavior (Fig.~\ref{fig:schematic}B). The set of all attractors from all phase-portraits corresponding to a behavior are also referred to as the attractor set of the behavior in this paper.  The third level of reuse is that of ongoing transient dynamics as the agent is in continuous closed-loop interaction with the environment. When there is attractor reuse from the previous level, it is possible that multiple behaviors navigate different transients around those attractors (Fig.~\ref{fig:schematic}C) or, they might be reused too (Fig.~\ref{fig:schematic}D).	score:391
Who portrays Sherlock Holmes better: Benedict Cumberbatch or Robert Downey Jr?	     The question was further developed and applied to portfolio management in Refs.~\cite{Howard,Barron,Cover}. Kelly's horse race appears in some aspects of evolutionary biology as well. There, the reward function is the population growth rate and information  refers to the state of the environment \cite{Bergstrom,Kussell,Donaldson-Matasci,Rivoire,Bialek}.     Neurobiology is the field where information theory is arguably the most popular in biological sciences. Barlow's efficient coding  \cite{Barlow61} postulated that  early neural sensory layers efficiently represent environmental information, i.e. their evolutionary fitness is proportional to  their efficiency in the transmission of information from the environment to higher parts of the brain.	     The question was further developed and applied to portfolio management in Refs.~\cite{Howard,Barron,Cover}. Kelly's horse race appears in some aspects of evolutionary biology as well. There, the reward function is the population growth rate and information  refers to the state of the environment \cite{Bergstrom,Kussell,Donaldson-Matasci,Rivoire,Bialek}.     Neurobiology is the field where information theory is arguably the most popular in biological sciences. Barlow's efficient coding  \cite{Barlow61} postulated that  early neural sensory layers efficiently represent environmental information, i.e. their evolutionary fitness is proportional to  their efficiency in the transmission of information from the environment to higher parts of the brain.	score:413
Who portrays Sherlock Holmes better: Benedict Cumberbatch or Robert Downey Jr?	 Each individual averages her Bayesian posterior belief with the opinion of her neighbors, and the beliefs tend to the truth under mild technical assumptions. Following up on the work of Duchi et al. \cite{duchi2012dual} on distributed dual averaging, an optimization-based algorithm is developed in \cite{shahrampour2013exponentially}. The authors demonstrate that the belief sequence generated according to their method is weakly consistent in undirected networks.  Lalitha et al. \cite{lalitha2014social} introduce another strategy which puts exponential weights on a linear combination of Bayesian log-posteriors. The convergence conditions of their method are similar to those of \cite{jadbabaie2012non}. On the other hand, Rahnama Rad et al. \cite{rahnama2010distributed} present a distributed algorithm for {\it continuous} state space, and prove its convergence.	 Each individual averages her Bayesian posterior belief with the opinion of her neighbors, and the beliefs tend to the truth under mild technical assumptions. Following up on the work of Duchi et al. \cite{duchi2012dual} on distributed dual averaging, an optimization-based algorithm is developed in \cite{shahrampour2013exponentially}. The authors demonstrate that the belief sequence generated according to their method is weakly consistent in undirected networks.  Lalitha et al. \cite{lalitha2014social} introduce another strategy which puts exponential weights on a linear combination of Bayesian log-posteriors. The convergence conditions of their method are similar to those of \cite{jadbabaie2012non}. On the other hand, Rahnama Rad et al. \cite{rahnama2010distributed} present a distributed algorithm for {\it continuous} state space, and prove its convergence.	score:421
Who portrays Sherlock Holmes better: Benedict Cumberbatch or Robert Downey Jr?	 The task of the clustering algorithm would be to group the data points in a manner such that the distances between data points within the same cluster is minimized, and the distances between data points belonging to different clusters are maximized. There are several existing popular methods for clustering e.g. Hierarchal Clustering, Bayesian Clustering, K-means, C-means, Spectral Clustering, Mean-shift and so on.  Some of these algorithms are iterative heuristics e.g. k-means, c-means. Mean Shift Clustering is based on feature space analysis to identify the minima of the density function \cite{fukunaga1975estimation, cheng1995mean}. Whereas, Spectral Clustering type algorithms \cite{ng2002spectral} try to analyze the algebraic properties of the distance matrix to cluster the data points.	 The task of the clustering algorithm would be to group the data points in a manner such that the distances between data points within the same cluster is minimized, and the distances between data points belonging to different clusters are maximized. There are several existing popular methods for clustering e.g. Hierarchal Clustering, Bayesian Clustering, K-means, C-means, Spectral Clustering, Mean-shift and so on.  Some of these algorithms are iterative heuristics e.g. k-means, c-means. Mean Shift Clustering is based on feature space analysis to identify the minima of the density function \cite{fukunaga1975estimation, cheng1995mean}. Whereas, Spectral Clustering type algorithms \cite{ng2002spectral} try to analyze the algebraic properties of the distance matrix to cluster the data points.	score:424
Who portrays Sherlock Holmes better: Benedict Cumberbatch or Robert Downey Jr?	 However, a drawback of such approaches consists in the fact that their postulated probability of random unit/connection omission is a constant that must be heuristically selected based on the obtained performance in some validation set. To alleviate this burden, in this paper we regard the DNN regularization problem from a Bayesian inference perspective: We impose a sparsity-inducing prior over the network synaptic weights, where the sparsity is induced by a set of Bernoulli-distributed binary variables with Beta (hyper-)priors over their prior parameters.	 However, a drawback of such approaches consists in the fact that their postulated probability of random unit/connection omission is a constant that must be heuristically selected based on the obtained performance in some validation set. To alleviate this burden, in this paper we regard the DNN regularization problem from a Bayesian inference perspective: We impose a sparsity-inducing prior over the network synaptic weights, where the sparsity is induced by a set of Bernoulli-distributed binary variables with Beta (hyper-)priors over their prior parameters.	score:427
Will Benedict Cumberbatch go out with me?	 This has two disadvantages. The prior on the flat space will put density outside of the embedding and traversals along the extra dimensions that are normal to the manifold will either leave the decoding invariant, or move out of the data manifold. This is because at each point there will be many more degrees of freedom than the dimensionality of the manifold.    In this paper we investigate this idea for the special case of Lie groups, which are symmetry groups that are simultaneously differentiable manifolds. Lie groups include rotations, translations, scaling, and other geometric transformations, which play an important role in many application domains such as robotics and computer vision. More specifically, we show how to construct\footnote{Our implementation is available at \url{https://github.	 This has two disadvantages. The prior on the flat space will put density outside of the embedding and traversals along the extra dimensions that are normal to the manifold will either leave the decoding invariant, or move out of the data manifold. This is because at each point there will be many more degrees of freedom than the dimensionality of the manifold.    In this paper we investigate this idea for the special case of Lie groups, which are symmetry groups that are simultaneously differentiable manifolds. Lie groups include rotations, translations, scaling, and other geometric transformations, which play an important role in many application domains such as robotics and computer vision. More specifically, we show how to construct\footnote{Our implementation is available at \url{https://github.	score:482
Will Benedict Cumberbatch go out with me?	 Since the current vector is outside the basis polytope it is easy to see that at least one of the endpoints of $\ell$ must also be outside. So we can move the current vector to this endpoint and if we choose these segments $\ell$ in an appropriate way we will quickly find a Boolean solution. The key is that a membership oracle for the basis polytope tells us which endpoint of $\ell$ we should move to.  Hence we obtain an algorithm that is not only an optimal tradeoff between efficiency and robustness, but is even deterministic:   \begin{theorem} If a set of $m$ points in $\R^n$ has strictly more than $\frac{d}{n} m$ inliers and meets Condition~\ref{cond:general}, then there is a deterministic polynomial time algorithm whose output is the set $L$ of inliers.	 Since the current vector is outside the basis polytope it is easy to see that at least one of the endpoints of $\ell$ must also be outside. So we can move the current vector to this endpoint and if we choose these segments $\ell$ in an appropriate way we will quickly find a Boolean solution. The key is that a membership oracle for the basis polytope tells us which endpoint of $\ell$ we should move to.  Hence we obtain an algorithm that is not only an optimal tradeoff between efficiency and robustness, but is even deterministic:   \begin{theorem} If a set of $m$ points in $\R^n$ has strictly more than $\frac{d}{n} m$ inliers and meets Condition~\ref{cond:general}, then there is a deterministic polynomial time algorithm whose output is the set $L$ of inliers.	score:483
Will Benedict Cumberbatch go out with me?	 To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.  	Over the past seven years, deep learning has transformed computer vision and has been implemented in scores of consumer-facing products. Many are excited that these approaches will continue to expand in scope and that new tools and products will be improved through the use of deep learning. One particularly exciting application area of deep learning has been in clinical medicine.	 To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.  	Over the past seven years, deep learning has transformed computer vision and has been implemented in scores of consumer-facing products. Many are excited that these approaches will continue to expand in scope and that new tools and products will be improved through the use of deep learning. One particularly exciting application area of deep learning has been in clinical medicine.	score:493
Will Benedict Cumberbatch go out with me?	 This context configuration turns out to be useful not only to address low-latency speech recognition, but also to boost the recognition performance under reverberant conditions.     This paper investigates on the mechanisms occurring inside DNNs, which lead to an effective application of asymmetric contexts.     In particular, we propose a novel method for automatic context window composition based on a gradient analysis.  The experiments, performed with different acoustic environments, features, DNN architectures, microphone settings, and recognition tasks show that our simple and efficient strategy leads to a less redundant frame configuration, which makes DNN training more effective in reverberant scenarios.        	Distant Speech Recognition (DSR) represents a fundamental technology towards flexible human-machine interfaces.	 This context configuration turns out to be useful not only to address low-latency speech recognition, but also to boost the recognition performance under reverberant conditions.     This paper investigates on the mechanisms occurring inside DNNs, which lead to an effective application of asymmetric contexts.     In particular, we propose a novel method for automatic context window composition based on a gradient analysis.  The experiments, performed with different acoustic environments, features, DNN architectures, microphone settings, and recognition tasks show that our simple and efficient strategy leads to a less redundant frame configuration, which makes DNN training more effective in reverberant scenarios.        	Distant Speech Recognition (DSR) represents a fundamental technology towards flexible human-machine interfaces.	score:493
Will Benedict Cumberbatch go out with me?	 Even if the base learners can provide there own measures of variable importance, there is no logical way to combine them to form an overall score for the super learner. However, since new predictions can be obtained from the super learner, our proposed variable importance measure is still applicable (examples are given in Sections \ref{sec:ensemble}--\ref{sec:automl}).  Thirdly, as shown in Section \ref{sec:interaction}, our proposed method can be modified to quantify the strength of potential interaction effects. Finally, since our approach is based on constructing PDPs for all the main effects, the analyst is forced to also look at the estimated functional relationship between each feature and the target---which should be done in tandem with studying the importance of each feature.	 Even if the base learners can provide there own measures of variable importance, there is no logical way to combine them to form an overall score for the super learner. However, since new predictions can be obtained from the super learner, our proposed variable importance measure is still applicable (examples are given in Sections \ref{sec:ensemble}--\ref{sec:automl}).  Thirdly, as shown in Section \ref{sec:interaction}, our proposed method can be modified to quantify the strength of potential interaction effects. Finally, since our approach is based on constructing PDPs for all the main effects, the analyst is forced to also look at the estimated functional relationship between each feature and the target---which should be done in tandem with studying the importance of each feature.	score:494
Is spiritual healing/pranic healing/healing through NLP true? If so, how does it work? What are the side effects?	 However, an implementation of exact Newton's method is often  computationally prohibitive.  Hence, approximations of the Hessian matrix are needed to address  the issue, such as a diagonal approximation structure \cite{batt:nc92},  and a block diagonal approximation structure \cite{wang:nn98}.  However, without a complete evaluation of the true Hessian, performance of these  heuristic approximations is hardly convincing.	 However, an implementation of exact Newton's method is often  computationally prohibitive.  Hence, approximations of the Hessian matrix are needed to address  the issue, such as a diagonal approximation structure \cite{batt:nc92},  and a block diagonal approximation structure \cite{wang:nn98}.  However, without a complete evaluation of the true Hessian, performance of these  heuristic approximations is hardly convincing.	score:352
Is spiritual healing/pranic healing/healing through NLP true? If so, how does it work? What are the side effects?	  The natural question is whether algorithms that can execute $\poly(n)$ function evaluations in each iteration can achieve faster convergence rates than those that make a single evaluation in every iteration.  \begin{center} \emph{ Can parallelization accelerate convex optimization?} \end{center}    \subsection{Main result}    Our main result is a spoiler.   We show that, in general, parallelization does not accelerate convex optimization.     In particular,  for the problem of minimizing a Lipschitz and strongly convex function $f:[0,1]^n\to \mathbb{R}$ \comment{over the unit Euclidean ball}, we give a tight lower bound that shows that even when $\poly(n)$ queries can be executed in parallel, there is no randomized algorithm that has convergence rate that is better than those achievable with a one-query-per-round algorithm~\cite{nesterov2013introductory,shamir2013stochastic}.	  The natural question is whether algorithms that can execute $\poly(n)$ function evaluations in each iteration can achieve faster convergence rates than those that make a single evaluation in every iteration.  \begin{center} \emph{ Can parallelization accelerate convex optimization?} \end{center}    \subsection{Main result}    Our main result is a spoiler.   We show that, in general, parallelization does not accelerate convex optimization.     In particular,  for the problem of minimizing a Lipschitz and strongly convex function $f:[0,1]^n\to \mathbb{R}$ \comment{over the unit Euclidean ball}, we give a tight lower bound that shows that even when $\poly(n)$ queries can be executed in parallel, there is no randomized algorithm that has convergence rate that is better than those achievable with a one-query-per-round algorithm~\cite{nesterov2013introductory,shamir2013stochastic}.	score:356
Is spiritual healing/pranic healing/healing through NLP true? If so, how does it work? What are the side effects?	 However, CTC simplifies the sequence-level error function by a product of the frame-level error functions (i.e., independence assumption), which means it essentially still does frame-level classification. It also requires the lengths of the input and output sequence to be the same, which is inappropriate for speech recognition. CTC deals with this problem by replicating the output labels so that a consecutive frames may correspond to the same output label or a {\it blank} token.     Attention-based RNNs have been demonstrated to be a powerful alternative sequence-to-sequence transducer, e.g., in machine translation ~\cite{bahdanau2014neural}, and speech recognition ~\cite{chorowski2015attention, lu2015study, chan2015listen}. A key difference of this model from HMMs and CTCs is that the attention-based approach does not apply the conditional independence assumption to the input sequence.	 However, CTC simplifies the sequence-level error function by a product of the frame-level error functions (i.e., independence assumption), which means it essentially still does frame-level classification. It also requires the lengths of the input and output sequence to be the same, which is inappropriate for speech recognition. CTC deals with this problem by replicating the output labels so that a consecutive frames may correspond to the same output label or a {\it blank} token.     Attention-based RNNs have been demonstrated to be a powerful alternative sequence-to-sequence transducer, e.g., in machine translation ~\cite{bahdanau2014neural}, and speech recognition ~\cite{chorowski2015attention, lu2015study, chan2015listen}. A key difference of this model from HMMs and CTCs is that the attention-based approach does not apply the conditional independence assumption to the input sequence.	score:356
Is spiritual healing/pranic healing/healing through NLP true? If so, how does it work? What are the side effects?	 This technique has also been implicitly used in the recent development of a concave-convex procedure (CCCP) for optimizing the weights of SPNs~\citep{zhao2016unified}. Essentially, by reducing the moment computation problem to a joint inference problem in SPNs, we are able to exploit the fact that the network polynomial of an SPN computes a multilinear function in the model parameters, so we can efficiently evaluate this polynomial by differentiation even if the polynomial may contain exponentially many monomials, provided that the polynomial admits a tractable circuit complexity.	 This technique has also been implicitly used in the recent development of a concave-convex procedure (CCCP) for optimizing the weights of SPNs~\citep{zhao2016unified}. Essentially, by reducing the moment computation problem to a joint inference problem in SPNs, we are able to exploit the fact that the network polynomial of an SPN computes a multilinear function in the model parameters, so we can efficiently evaluate this polynomial by differentiation even if the polynomial may contain exponentially many monomials, provided that the polynomial admits a tractable circuit complexity.	score:358
Is spiritual healing/pranic healing/healing through NLP true? If so, how does it work? What are the side effects?	 Is a rule that depends on a function of the output \cite{hyvarinen1998independent} Hebbian? Is a learning rule that depends only on the input Hebbian? and so forth. This lack of crispness is more than a simple semantic issue. While it may have helped the field in its early stages--in the same way that vague concepts like ``gene'' or ``consciousness'' may have helped molecular biology or neuroscience, it has also prevented clear thinking to address basic questions regarding, for instance, the behavior of linear networks under Hebbian learning, or the capabilities and limitations of Hebbian learning in both shallow and deep networks.	 Is a rule that depends on a function of the output \cite{hyvarinen1998independent} Hebbian? Is a learning rule that depends only on the input Hebbian? and so forth. This lack of crispness is more than a simple semantic issue. While it may have helped the field in its early stages--in the same way that vague concepts like ``gene'' or ``consciousness'' may have helped molecular biology or neuroscience, it has also prevented clear thinking to address basic questions regarding, for instance, the behavior of linear networks under Hebbian learning, or the capabilities and limitations of Hebbian learning in both shallow and deep networks.	score:358
Is there an R package that implements multinomial event model for text classification problems?	 The bag-of-words model is a standard representation of text for many linear classifier learners. In many problem domains, linear classifiers are preferred over more complex models due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions.  However in settings where there is a large vocabulary, large variance in the frequency of terms in the training corpus, many classes and very short text (e.g., single sentences or document titles) the bag-of-words representation becomes extremely sparse, and this can reduce the accuracy of classifiers. A particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class-conditional evidence.	 The bag-of-words model is a standard representation of text for many linear classifier learners. In many problem domains, linear classifiers are preferred over more complex models due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions.  However in settings where there is a large vocabulary, large variance in the frequency of terms in the training corpus, many classes and very short text (e.g., single sentences or document titles) the bag-of-words representation becomes extremely sparse, and this can reduce the accuracy of classifiers. A particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class-conditional evidence.	score:278
Is there an R package that implements multinomial event model for text classification problems?	  In our approach we devise a solution for extracting from the analysed texts some characteristics that can express the style of a specific author.  Obtaining this kind of information abstraction is crucial in order to create a precise and correct classification system.  On the other hand, while data abound in the context of text analysis, a robust classifier should rely on input sets that are compact enough to be apt to the training process.  Therefore, some data have to reflect averaged evaluations that concern some anthropological aspects such as the historical period, or the ethnicity, etc.  This work satisfies the above conditions of extracting compact data from texts since we use a preprocessing tool for word-grouping and time-period related analysis of the common lexicon.  Such a tool computes   a bias reference system for the recurrence frequency of the word used in the analysed texts.	  In our approach we devise a solution for extracting from the analysed texts some characteristics that can express the style of a specific author.  Obtaining this kind of information abstraction is crucial in order to create a precise and correct classification system.  On the other hand, while data abound in the context of text analysis, a robust classifier should rely on input sets that are compact enough to be apt to the training process.  Therefore, some data have to reflect averaged evaluations that concern some anthropological aspects such as the historical period, or the ethnicity, etc.  This work satisfies the above conditions of extracting compact data from texts since we use a preprocessing tool for word-grouping and time-period related analysis of the common lexicon.  Such a tool computes   a bias reference system for the recurrence frequency of the word used in the analysed texts.	score:281
Is there an R package that implements multinomial event model for text classification problems?	 This paper investigates, from information theoretic grounds, a learning problem based on the principle that any regularity in a given dataset can be exploited to extract compact features from data, i.e., using fewer bits than needed to fully describe the data itself, in order to build meaningful representations of a relevant content (multiple labels).  We begin by introducing the noisy lossy source coding paradigm with the log-loss fidelity criterion which provides the fundamental tradeoffs between the \emph{cross-entropy loss} (average risk) and the information rate of the features (model complexity). Our approach allows an information theoretic formulation of the \emph{multi-task learning} (MTL) problem which is a supervised learning framework in which the prediction models for several related tasks are learned jointly from common representations to achieve better generalization performance.	 This paper investigates, from information theoretic grounds, a learning problem based on the principle that any regularity in a given dataset can be exploited to extract compact features from data, i.e., using fewer bits than needed to fully describe the data itself, in order to build meaningful representations of a relevant content (multiple labels).  We begin by introducing the noisy lossy source coding paradigm with the log-loss fidelity criterion which provides the fundamental tradeoffs between the \emph{cross-entropy loss} (average risk) and the information rate of the features (model complexity). Our approach allows an information theoretic formulation of the \emph{multi-task learning} (MTL) problem which is a supervised learning framework in which the prediction models for several related tasks are learned jointly from common representations to achieve better generalization performance.	score:281
Is there an R package that implements multinomial event model for text classification problems?	 We dub this formulation {\em Multi-Sense LSTM} (MS-LSTM).      An important issue is the provision of a set of reliable anchors; that is, points in one-to-one correspondence between the two representations that would enforce some degree of structural similarity between pieces of text and KB entities and thus make the mapping more efficient. We deal with this problem by extending the original KB graph with nodes corresponding to {\em textual features}, i. e. to words strongly associated to a specific entity and collected from various resources. A novel sampling strategy is detailed for incorporating these nodes to random walks, which are then fed to the skipgram model for producing an entity space. The results indicate that the textual nodes, being words {\em and} KB entities at the same time, do an extremely effective job in transforming the geometry of the entity space to the benefit of mapping the textual modality.	 We dub this formulation {\em Multi-Sense LSTM} (MS-LSTM).      An important issue is the provision of a set of reliable anchors; that is, points in one-to-one correspondence between the two representations that would enforce some degree of structural similarity between pieces of text and KB entities and thus make the mapping more efficient. We deal with this problem by extending the original KB graph with nodes corresponding to {\em textual features}, i. e. to words strongly associated to a specific entity and collected from various resources. A novel sampling strategy is detailed for incorporating these nodes to random walks, which are then fed to the skipgram model for producing an entity space. The results indicate that the textual nodes, being words {\em and} KB entities at the same time, do an extremely effective job in transforming the geometry of the entity space to the benefit of mapping the textual modality.	score:284
Is there an R package that implements multinomial event model for text classification problems?	  The general problem we are solving is that of text classification. Given a body of text we have to derive its class from a known fixed set of classes. However, using text classification on data from customer service systems and task trackers has its caveats. For example, such data often includes unique fragments that are hard for automatic systems to reason about, such as stack traces or HTML snippets.  On the other hand, they often are well-structured and this structure can be leveraged by some of the novel methods, including the one we are proposing.  We are primarily interested in multi-class text classification, where number of classes we're predicting is larger than two. Historically, great results have been reached on binary classification tasks such as sentiment analysis (e.	  The general problem we are solving is that of text classification. Given a body of text we have to derive its class from a known fixed set of classes. However, using text classification on data from customer service systems and task trackers has its caveats. For example, such data often includes unique fragments that are hard for automatic systems to reason about, such as stack traces or HTML snippets.  On the other hand, they often are well-structured and this structure can be leveraged by some of the novel methods, including the one we are proposing.  We are primarily interested in multi-class text classification, where number of classes we're predicting is larger than two. Historically, great results have been reached on binary classification tasks such as sentiment analysis (e.	score:284
What is a good model for text classification?	 	Most text analysis, such as text categorization (TC), includes an essential step of feature extraction to find the best set of features that assimilate each text. Text categorization is one of the central problems in text mining and information retrieval, where it is the task of classifying documents by the words of which the documents include. Several machine learning algorithms have been developed for text classification, e. g.:  decision tree (J-48) \cite{BIB:R91}, k-nearest neighbor (KNN) \cite{BIB:R93}, support vector machine(SVM) \cite{BIB:R94} and random forests (RF) \cite{BIB:R95}. Thus, these text classifiers give acceptable accuracy with high dimensional data such as text.\\\\  There are many applications of text categorization such as topic detection \cite{BIB:R1}, phishing – email detection\cite{BIB:R2},  author identification \cite{BIB:R3}and etc.	 	Most text analysis, such as text categorization (TC), includes an essential step of feature extraction to find the best set of features that assimilate each text. Text categorization is one of the central problems in text mining and information retrieval, where it is the task of classifying documents by the words of which the documents include. Several machine learning algorithms have been developed for text classification, e. g.:  decision tree (J-48) \cite{BIB:R91}, k-nearest neighbor (KNN) \cite{BIB:R93}, support vector machine(SVM) \cite{BIB:R94} and random forests (RF) \cite{BIB:R95}. Thus, these text classifiers give acceptable accuracy with high dimensional data such as text.\\\\  There are many applications of text categorization such as topic detection \cite{BIB:R1}, phishing – email detection\cite{BIB:R2},  author identification \cite{BIB:R3}and etc.	score:335
What is a good model for text classification?	  The general problem we are solving is that of text classification. Given a body of text we have to derive its class from a known fixed set of classes. However, using text classification on data from customer service systems and task trackers has its caveats. For example, such data often includes unique fragments that are hard for automatic systems to reason about, such as stack traces or HTML snippets.  On the other hand, they often are well-structured and this structure can be leveraged by some of the novel methods, including the one we are proposing.  We are primarily interested in multi-class text classification, where number of classes we're predicting is larger than two. Historically, great results have been reached on binary classification tasks such as sentiment analysis (e.	  The general problem we are solving is that of text classification. Given a body of text we have to derive its class from a known fixed set of classes. However, using text classification on data from customer service systems and task trackers has its caveats. For example, such data often includes unique fragments that are hard for automatic systems to reason about, such as stack traces or HTML snippets.  On the other hand, they often are well-structured and this structure can be leveraged by some of the novel methods, including the one we are proposing.  We are primarily interested in multi-class text classification, where number of classes we're predicting is larger than two. Historically, great results have been reached on binary classification tasks such as sentiment analysis (e.	score:348
What is a good model for text classification?	 Through this concept, it is possible to model text from word vectors while holding semantic information. To incorporate the word frequency directly in the subspace model, we further extend the word subspace to the term-frequency (TF) weighted word subspace. Based on these new concepts, text classification can be performed under the mutual subspace method (MSM) framework.  The validity of our modeling is shown through experiments on the Reuters text database, comparing the results to various state-of-art algorithms.  	Text classification has become an indispensable task due to the rapid growth in the number of texts in digital form available online. It aims to classify different texts, also called documents, into a fixed number of predefined categories, helping to organize data, and making easier for users to find the desired information.  Over the past three decades, many methods based on machine learning and statistical models have been applied to perform this task, such as latent semantic analysis (LSA), support vector machines (SVM), and multinomial naive Bayes (MNB).  The first step in utilizing such methods to categorize textual data is to convert the texts into a vector representation.	 Through this concept, it is possible to model text from word vectors while holding semantic information. To incorporate the word frequency directly in the subspace model, we further extend the word subspace to the term-frequency (TF) weighted word subspace. Based on these new concepts, text classification can be performed under the mutual subspace method (MSM) framework.  The validity of our modeling is shown through experiments on the Reuters text database, comparing the results to various state-of-art algorithms.  	Text classification has become an indispensable task due to the rapid growth in the number of texts in digital form available online. It aims to classify different texts, also called documents, into a fixed number of predefined categories, helping to organize data, and making easier for users to find the desired information.  Over the past three decades, many methods based on machine learning and statistical models have been applied to perform this task, such as latent semantic analysis (LSA), support vector machines (SVM), and multinomial naive Bayes (MNB).  The first step in utilizing such methods to categorize textual data is to convert the texts into a vector representation.	score:354
What is a good model for text classification?	 The bag-of-words model is a standard representation of text for many linear classifier learners. In many problem domains, linear classifiers are preferred over more complex models due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions.  However in settings where there is a large vocabulary, large variance in the frequency of terms in the training corpus, many classes and very short text (e.g., single sentences or document titles) the bag-of-words representation becomes extremely sparse, and this can reduce the accuracy of classifiers. A particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class-conditional evidence.	 The bag-of-words model is a standard representation of text for many linear classifier learners. In many problem domains, linear classifiers are preferred over more complex models due to their efficiency, robustness and interpretability, and the bag-of-words text representation can capture sufficient information for linear classifiers to make highly accurate predictions.  However in settings where there is a large vocabulary, large variance in the frequency of terms in the training corpus, many classes and very short text (e.g., single sentences or document titles) the bag-of-words representation becomes extremely sparse, and this can reduce the accuracy of classifiers. A particular issue in such settings is that short texts tend to contain infrequently occurring or rare terms which lack class-conditional evidence.	score:355
What is a good model for text classification?	 	Modeling the structure of coherent texts is an important problem in NLP.  A well-written text has a particular high-level logical and topical structure.    The actual word and sentence choices and their transitions come together to convey the purpose of the text. Our primary goal is to build models that can learn such structure by arranging a given set of sentences to make coherent text.           Multi-document Summarization (MDS) and retrieval-based Question Answering (QA) involve extracting information from multiple documents and organizing it into a coherent summary. Since the relative ordering of sentences from different sources can be unclear, being able to automatically evaluate a particular order is essential. \citet{barzilay2002inferring} discuss the importance of an ordering component in MDS and show that finding acceptable orderings can enhance user comprehension.	 	Modeling the structure of coherent texts is an important problem in NLP.  A well-written text has a particular high-level logical and topical structure.    The actual word and sentence choices and their transitions come together to convey the purpose of the text. Our primary goal is to build models that can learn such structure by arranging a given set of sentences to make coherent text.           Multi-document Summarization (MDS) and retrieval-based Question Answering (QA) involve extracting information from multiple documents and organizing it into a coherent summary. Since the relative ordering of sentences from different sources can be unclear, being able to automatically evaluate a particular order is essential. \citet{barzilay2002inferring} discuss the importance of an ordering component in MDS and show that finding acceptable orderings can enhance user comprehension.	score:356
What is the salary range for classification level d at Stanford University?	 \textit{Is there a current DNN approach that reaches state-of-the-art performance for TSC and is less complex than HIVE-COTE}?  \textit{What type of DNN architectures works best for the TSC task}?  \textit{How does the random initialization affect the performance of deep learning classifiers}? And finally: \textit{Could the black-box effect of DNNs be avoided to provide interpretability}?   Given that the latter questions have not been addressed by the TSC community, it is surprising how much recent papers have neglected the possibility that TSC problems could be solved using a pure feature learning algorithm~\citep{neamtu2018generalized,bagnall2017the,lines2016hive}.  In fact, a recent empirical study~\citep{bagnall2017the} evaluated 18 TSC algorithms on 85 time series datasets, none of which was a deep learning model.	 \textit{Is there a current DNN approach that reaches state-of-the-art performance for TSC and is less complex than HIVE-COTE}?  \textit{What type of DNN architectures works best for the TSC task}?  \textit{How does the random initialization affect the performance of deep learning classifiers}? And finally: \textit{Could the black-box effect of DNNs be avoided to provide interpretability}?   Given that the latter questions have not been addressed by the TSC community, it is surprising how much recent papers have neglected the possibility that TSC problems could be solved using a pure feature learning algorithm~\citep{neamtu2018generalized,bagnall2017the,lines2016hive}.  In fact, a recent empirical study~\citep{bagnall2017the} evaluated 18 TSC algorithms on 85 time series datasets, none of which was a deep learning model.	score:520
What is the salary range for classification level d at Stanford University?	   There has been attempts in applying the second order method Limited memory BFGS (LBFGS) to optimizing DNNs in the famous deep learning framework PyTorch. However, when optimizing DNNs, the Hessian of LBFGS may become negative semi-definite and thus the objective loss will increase to a very large value. Moreover, the optimization process of LBFGS is highly sensitive to the learning rate.  If the learning rate is too large, the training process is unstable. Otherwise, the objective function does not decrease.  Recently, Wang et al \cite{wang2017stochastic} propose a second order algorithm, Stochastic damped LBFGS (SdLBFGS), to optimize non-convex functions. The Hessian in each step of SdLBFGS is guaranteed to be PSD. However, the convergence is not fully proved and we observed non-convergence and crashes in experiments.	   There has been attempts in applying the second order method Limited memory BFGS (LBFGS) to optimizing DNNs in the famous deep learning framework PyTorch. However, when optimizing DNNs, the Hessian of LBFGS may become negative semi-definite and thus the objective loss will increase to a very large value. Moreover, the optimization process of LBFGS is highly sensitive to the learning rate.  If the learning rate is too large, the training process is unstable. Otherwise, the objective function does not decrease.  Recently, Wang et al \cite{wang2017stochastic} propose a second order algorithm, Stochastic damped LBFGS (SdLBFGS), to optimize non-convex functions. The Hessian in each step of SdLBFGS is guaranteed to be PSD. However, the convergence is not fully proved and we observed non-convergence and crashes in experiments.	score:522
What is the salary range for classification level d at Stanford University?	 This is important to the behavior scientists for the state (feature) design. (b) From the perspective of optimization, the actor-critic algorithm has great properties of quick convergence with low variance\ \cite{Grondman_2012_IEEEts_surveyOfActorCriticRL}.  However, Lei's method assumes that the states at different decision points are i.i.d. and the current action only influences the immediate reward\ \cite{huitian_2016_PhdThesis_actCriticAlgorithm}.  This assumption is infeasible in real situations. Taking the delayed effect in the SDM or mHealth for example, the current action influences not only the immediate reward but also the next state and through that, all the subsequent rewards\ \cite{Sutton_2012_MitPress_RLintroduction}. Accordingly, Lei proposed a new method\ \cite{huitian_2016_PhdThesis_actCriticAlgorithm} by emphasizing on explorations and reducing exploitations.	 This is important to the behavior scientists for the state (feature) design. (b) From the perspective of optimization, the actor-critic algorithm has great properties of quick convergence with low variance\ \cite{Grondman_2012_IEEEts_surveyOfActorCriticRL}.  However, Lei's method assumes that the states at different decision points are i.i.d. and the current action only influences the immediate reward\ \cite{huitian_2016_PhdThesis_actCriticAlgorithm}.  This assumption is infeasible in real situations. Taking the delayed effect in the SDM or mHealth for example, the current action influences not only the immediate reward but also the next state and through that, all the subsequent rewards\ \cite{Sutton_2012_MitPress_RLintroduction}. Accordingly, Lei proposed a new method\ \cite{huitian_2016_PhdThesis_actCriticAlgorithm} by emphasizing on explorations and reducing exploitations.	score:524
What is the salary range for classification level d at Stanford University?	  We investigate properties of a  classifier applied   to the measurements of the CP state of the Higgs boson in $H\rightarrow\tau\tau$ decays.         The problem is framed as binary classifier applied to individual instances. Then the         prior knowledge that the instances belong to the same class is used to define the         multi-instance classifier.  Its final score is calculated as multiplication of single         instance scores for a given series of instances. In the paper we discuss properties of such classifier, notably its dependence on the number of instances in the series. This classifier exhibits very strong random dependence on the number of epochs used for training and requires   careful tuning of the classification threshold.	  We investigate properties of a  classifier applied   to the measurements of the CP state of the Higgs boson in $H\rightarrow\tau\tau$ decays.         The problem is framed as binary classifier applied to individual instances. Then the         prior knowledge that the instances belong to the same class is used to define the         multi-instance classifier.  Its final score is calculated as multiplication of single         instance scores for a given series of instances. In the paper we discuss properties of such classifier, notably its dependence on the number of instances in the series. This classifier exhibits very strong random dependence on the number of epochs used for training and requires   careful tuning of the classification threshold.	score:527
What is the salary range for classification level d at Stanford University?	 However, there are fundamental differences. The first difference lies in what is being modeled and the semantics of the latent variables. The LDA-based methods model the process by which documents are generated. The latent variables in the models are constructs in the hypothetical generation process, including a list of topics (usually denoted as $\beta$), a topic distribution vector for each document (usually denoted as $\theta_d$), and a topic assignment for each token in each document (usually denoted as $Z_{d, n}$).  In contrast,  HLTA models a collection of documents without referring to a document generation process. The latent variables in the model are considered unobserved attributes of the documents. If we compare whether words occur in particular documents to whether students do well in various subjects, then the latent variables correspond to latent traits such as analytical skill, literacy skill and general intelligence.	 However, there are fundamental differences. The first difference lies in what is being modeled and the semantics of the latent variables. The LDA-based methods model the process by which documents are generated. The latent variables in the models are constructs in the hypothetical generation process, including a list of topics (usually denoted as $\beta$), a topic distribution vector for each document (usually denoted as $\theta_d$), and a topic assignment for each token in each document (usually denoted as $Z_{d, n}$).  In contrast,  HLTA models a collection of documents without referring to a document generation process. The latent variables in the model are considered unobserved attributes of the documents. If we compare whether words occur in particular documents to whether students do well in various subjects, then the latent variables correspond to latent traits such as analytical skill, literacy skill and general intelligence.	score:531
What is the salary range for classification levels A through P at Stanford University?	  What is common to all these applications is uncertainty and then that they have to deal with decision and statistical inference.  Ranking is perhaps the most crucial task performed by the data management systems which have to deal with uncertainty. In many applications, ranking aims at deciding or inferring, for example, the class assigned to a unit or the order by relevance, usefulness, or utility of the units delivered to another application or to an end user.	  What is common to all these applications is uncertainty and then that they have to deal with decision and statistical inference.  Ranking is perhaps the most crucial task performed by the data management systems which have to deal with uncertainty. In many applications, ranking aims at deciding or inferring, for example, the class assigned to a unit or the order by relevance, usefulness, or utility of the units delivered to another application or to an end user.	score:461
What is the salary range for classification levels A through P at Stanford University?	  In practice, the latent representation learned by auto-encoders has typically been used to solve a secondary problem, often classification. The most common setup is to train a single auto-encoder on data from all classes and then a classifier is tasked to discriminate among classes.  However, this contrasts with the way probabilistic models have typically been used in the past: in that literature, it is more common to train one model per class and use Bayes' rule for classification.  There are two challenges to classifying using per-class auto-encoders. First, up until very recently, it was not known how to obtain the score of data under an auto-encoder, meaning how much the model ``likes'' an input. Second, auto-encoders are non-probabilistic, so even if they can be scored, the scores do not integrate to 1 and therefore the per-class models need to be calibrated.	  In practice, the latent representation learned by auto-encoders has typically been used to solve a secondary problem, often classification. The most common setup is to train a single auto-encoder on data from all classes and then a classifier is tasked to discriminate among classes.  However, this contrasts with the way probabilistic models have typically been used in the past: in that literature, it is more common to train one model per class and use Bayes' rule for classification.  There are two challenges to classifying using per-class auto-encoders. First, up until very recently, it was not known how to obtain the score of data under an auto-encoder, meaning how much the model ``likes'' an input. Second, auto-encoders are non-probabilistic, so even if they can be scored, the scores do not integrate to 1 and therefore the per-class models need to be calibrated.	score:473
What is the salary range for classification levels A through P at Stanford University?	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	score:484
What is the salary range for classification levels A through P at Stanford University?	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	score:484
What is the salary range for classification levels A through P at Stanford University?	 There are tremendous amount of research on improving the classification performance in such cases. One highly investigated field for this problem is ensemble learning \cite{kuncheva}, where multiple prediction are fused the produce a more efficient classification approach. One fundamental requirement for the creation of classifier ensembles is diversity among them \cite{Brown2005}, that is, the classifiers included in the ensemble need to complement each other to provide more generalization capabilities than a single learner.	 There are tremendous amount of research on improving the classification performance in such cases. One highly investigated field for this problem is ensemble learning \cite{kuncheva}, where multiple prediction are fused the produce a more efficient classification approach. One fundamental requirement for the creation of classifier ensembles is diversity among them \cite{Brown2005}, that is, the classifiers included in the ensemble need to complement each other to provide more generalization capabilities than a single learner.	score:484
What are some good machine learning books written in C#?	  However, all current deep learning frameworks still require detailed knowledge of the underlying machine learning algorithms and libraries as well as programming experience or tedious editing of text configuration files to set a vast amount of parameters.  Although there exist graphical interfaces such as NVIDIA's DIGITS\footnote{\url{https://github. com/NVIDIA/DIGITS}}, Intel's Deep Learning SDK\footnote{\url{https://software.intel.com/en-us/deep-learning-sdk}}, Caffe Gui Tool\footnote{\url{https://github.com/Chasvortex/caffe-gui-tool}}, or Expresso~\citep{Dholakiya2015}, none of the products currently available offer a graphical interface for the complete pipeline of deep learning in various applications.	  However, all current deep learning frameworks still require detailed knowledge of the underlying machine learning algorithms and libraries as well as programming experience or tedious editing of text configuration files to set a vast amount of parameters.  Although there exist graphical interfaces such as NVIDIA's DIGITS\footnote{\url{https://github. com/NVIDIA/DIGITS}}, Intel's Deep Learning SDK\footnote{\url{https://software.intel.com/en-us/deep-learning-sdk}}, Caffe Gui Tool\footnote{\url{https://github.com/Chasvortex/caffe-gui-tool}}, or Expresso~\citep{Dholakiya2015}, none of the products currently available offer a graphical interface for the complete pipeline of deep learning in various applications.	score:401
What are some good machine learning books written in C#?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:425
What are some good machine learning books written in C#?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:431
What are some good machine learning books written in C#?	  To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.  The machine learns from the inputs (examples) and makes predictions based on the examples provided.} \item{\textit{Unsupervised Learning:} In unsupervised learning, the machine is left to learn from the data provided to it in order to discover patterns that can be used to make predictions eventually.} \item{\textit{Reinforcement Learning:} Reinforcement learning, for example, a computer learning to play a game, is the process whereby the computer learns in a dynamic environment to perform a certain task without explicitly being told if it is close to achieving the goal.	  To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.  The machine learns from the inputs (examples) and makes predictions based on the examples provided.} \item{\textit{Unsupervised Learning:} In unsupervised learning, the machine is left to learn from the data provided to it in order to discover patterns that can be used to make predictions eventually.} \item{\textit{Reinforcement Learning:} Reinforcement learning, for example, a computer learning to play a game, is the process whereby the computer learns in a dynamic environment to perform a certain task without explicitly being told if it is close to achieving the goal.	score:432
What are some good machine learning books written in C#?	 However, in practice learning machines are not given access to this distribution, $Pr(C|\mathbf{X})$. Therefore, given a feature vector or variables $\mathbf{X}\in R^N$, the aim of most machine learning algorithms is to approximate this underlying distribution or estimate some of its characteristics. Unfortunately, in most practically relevant data mining applications, the dimensionality of the feature vector is quite high making it prohibitive to learn the underlying distribution.  For instance, gene expression data or images may easily have more than tens of thousands of features. While, at least in theory, having more features should result in a more discriminative classifier, it is not the case in practice because of the computational burden and curse of  dimensionality.   High-dimensional data poses different challenges on induction and prediction algorithms.	 However, in practice learning machines are not given access to this distribution, $Pr(C|\mathbf{X})$. Therefore, given a feature vector or variables $\mathbf{X}\in R^N$, the aim of most machine learning algorithms is to approximate this underlying distribution or estimate some of its characteristics. Unfortunately, in most practically relevant data mining applications, the dimensionality of the feature vector is quite high making it prohibitive to learn the underlying distribution.  For instance, gene expression data or images may easily have more than tens of thousands of features. While, at least in theory, having more features should result in a more discriminative classifier, it is not the case in practice because of the computational burden and curse of  dimensionality.   High-dimensional data poses different challenges on induction and prediction algorithms.	score:436
Is there any book that treats machine learning in C#?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:448
Is there any book that treats machine learning in C#?	 Many classical problems in machine learning and statistics can be analysed in this framework. On the machine learning side, multiple instance learning \citep{dietterich97solving,ray01multiple,dooly02multiple} can be thought of in this way,  where each instance in a labeled bag is an i.i.d.\ (independent identically distributed) sample from a distribution.  On the statistical side, tasks might include point estimation of statistics on a  distribution without closed form analytical expressions (e.g., its entropy or a hyperparameter).  \tb{Intuitive description of our goal:} Let us start with a somewhat informal definition of the distribution regression problem and an intuitive phrasing of  our goals. Suppose that our data consist of $\b{z}=\{(x_i,y_i)\}_{i=1}^l$, where $x_i$ is a probability distribution, $y_i$ is its label (in the simplest case $y_i\in \R$ or $y_i\in\R^d$) and each $(x_i,y_i)$  pair is i.	 Many classical problems in machine learning and statistics can be analysed in this framework. On the machine learning side, multiple instance learning \citep{dietterich97solving,ray01multiple,dooly02multiple} can be thought of in this way,  where each instance in a labeled bag is an i.i.d.\ (independent identically distributed) sample from a distribution.  On the statistical side, tasks might include point estimation of statistics on a  distribution without closed form analytical expressions (e.g., its entropy or a hyperparameter).  \tb{Intuitive description of our goal:} Let us start with a somewhat informal definition of the distribution regression problem and an intuitive phrasing of  our goals. Suppose that our data consist of $\b{z}=\{(x_i,y_i)\}_{i=1}^l$, where $x_i$ is a probability distribution, $y_i$ is its label (in the simplest case $y_i\in \R$ or $y_i\in\R^d$) and each $(x_i,y_i)$  pair is i.	score:455
Is there any book that treats machine learning in C#?	 According to \shortciteA{pan2010survey}, this approach would be an example of \textit{inductive transfer learning}, since labeled data is available for both $\mathcal{T_T} $ and $ \mathcal{T_S}$. An alternative use of transfer learning is to use a neural network trained for $\mathcal{T_S}$ as a feature extractor for $\mathcal{T_T}$, in order to use other machine learning methods on top of the resulting representations.  By doing so, one is representing the $\mathcal{D_T}$ data in a language learnt for the $\mathcal{T_S}$ task, enabling the use of pre-trained deep network representations (not deep network models by themselves) on datasets which lack the size required to train these methods \shortcite{azizpour2016factors,sharif2014cnn}. According to \shortciteA{pan2010survey} this case would be an example of \textit{feature representation transfer} (the analogous term \textit{transfer learning for feature extraction} is also widely used).	 According to \shortciteA{pan2010survey}, this approach would be an example of \textit{inductive transfer learning}, since labeled data is available for both $\mathcal{T_T} $ and $ \mathcal{T_S}$. An alternative use of transfer learning is to use a neural network trained for $\mathcal{T_S}$ as a feature extractor for $\mathcal{T_T}$, in order to use other machine learning methods on top of the resulting representations.  By doing so, one is representing the $\mathcal{D_T}$ data in a language learnt for the $\mathcal{T_S}$ task, enabling the use of pre-trained deep network representations (not deep network models by themselves) on datasets which lack the size required to train these methods \shortcite{azizpour2016factors,sharif2014cnn}. According to \shortciteA{pan2010survey} this case would be an example of \textit{feature representation transfer} (the analogous term \textit{transfer learning for feature extraction} is also widely used).	score:455
Is there any book that treats machine learning in C#?	   In recent years it has become popular to study machine learning   problems       in a setting of    ordinal distance information rather than numerical   distance measurements. By ordinal distance information we refer to   binary answers to distance comparisons such as    $d(A,B)<d(C,D)$.     For many problems in machine learning and statistics it is unclear how   to solve them in such a scenario.  Up to now, the main approach is to   explicitly construct an ordinal embedding of the data points in the   Euclidean space, an approach that has a number of drawbacks.  In   this paper, we propose algorithms for the problems of medoid   estimation, outlier identification, classification, and clustering   when given only ordinal    data.      They are based on   estimating the lens depth function and the $k$-relative neighborhood   graph on a data set.	   In recent years it has become popular to study machine learning   problems       in a setting of    ordinal distance information rather than numerical   distance measurements. By ordinal distance information we refer to   binary answers to distance comparisons such as    $d(A,B)<d(C,D)$.     For many problems in machine learning and statistics it is unclear how   to solve them in such a scenario.  Up to now, the main approach is to   explicitly construct an ordinal embedding of the data points in the   Euclidean space, an approach that has a number of drawbacks.  In   this paper, we propose algorithms for the problems of medoid   estimation, outlier identification, classification, and clustering   when given only ordinal    data.      They are based on   estimating the lens depth function and the $k$-relative neighborhood   graph on a data set.	score:462
Is there any book that treats machine learning in C#?	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	score:462
Who are the toppers of the 2015 CSE batch at IIT Kharagpur?	 In many of the situations, we want to know how an alternate system of making predictions would have performed which brings us to the realm of counterfactual inference. For example, in personalization, an alternate system could be a different method of picking appropriate contents from the content pool based on customer features.  The problem of counterfactual inference has a rich literature with some the earlier works dating back to the 1970s and some of the latest appearing in the last few years; see for example \cite{lewis:1973, rubin:1974, rosenbaum:rubin:1983, rubin:2005, bang:robins:2005, laan:petersen:2007, hill:2011, dudik:langford:li:2011, austin:2011, chernozhukov:frenandezval:melly:2013, bottou:others:2013, swaminathan:joachims:2015, johansson:shalit:sontag:2016}.	 In many of the situations, we want to know how an alternate system of making predictions would have performed which brings us to the realm of counterfactual inference. For example, in personalization, an alternate system could be a different method of picking appropriate contents from the content pool based on customer features.  The problem of counterfactual inference has a rich literature with some the earlier works dating back to the 1970s and some of the latest appearing in the last few years; see for example \cite{lewis:1973, rubin:1974, rosenbaum:rubin:1983, rubin:2005, bang:robins:2005, laan:petersen:2007, hill:2011, dudik:langford:li:2011, austin:2011, chernozhukov:frenandezval:melly:2013, bottou:others:2013, swaminathan:joachims:2015, johansson:shalit:sontag:2016}.	score:428
Who are the toppers of the 2015 CSE batch at IIT Kharagpur?	  There are a number of papers~\cite{hunter2004mm,   negahban_iterative_2012, hajek2014minimax, shah_estimation_2015,   shah_simple_2015} devoted to settings in which pairs to be compared are chosen a priori, whereas here we assume that the pairs may be chosen in an active manner.  Moreover, several works impose restrictions on the pairwise comparison probabilities, e. g., by assuming the Bradley-Terry-Luce (BTL) parametric model (discussed below) \cite{szorenyi_online_2015, hunter2004mm,   negahban_iterative_2012, hajek2014minimax, shah_estimation_2015}. \citet{eriksson_learning_2013} considers the problem of finding the very top items using graph-based techniques, whereas \citet{busa-fekete_top-k_2013} consider the problem of finding the top-k items.	  There are a number of papers~\cite{hunter2004mm,   negahban_iterative_2012, hajek2014minimax, shah_estimation_2015,   shah_simple_2015} devoted to settings in which pairs to be compared are chosen a priori, whereas here we assume that the pairs may be chosen in an active manner.  Moreover, several works impose restrictions on the pairwise comparison probabilities, e. g., by assuming the Bradley-Terry-Luce (BTL) parametric model (discussed below) \cite{szorenyi_online_2015, hunter2004mm,   negahban_iterative_2012, hajek2014minimax, shah_estimation_2015}. \citet{eriksson_learning_2013} considers the problem of finding the very top items using graph-based techniques, whereas \citet{busa-fekete_top-k_2013} consider the problem of finding the top-k items.	score:439
Who are the toppers of the 2015 CSE batch at IIT Kharagpur?	 Recent thriving development of battery techniques has greatly accelerated this tendency \cite{Tuttle2012Evolution}. While batteries of EVs have been advanced in term of both capacity and power density, this in turn indicates that EVs will consume more energy with larger power. For example, according to the U.S. Energy Information Administration \cite{howmuchelectricity}, in 2013, a U. S. residential utility customer consumes an average of $909$kWh of electricity per month, while an EV with a $24$kWh battery pack consumes around $720$kWh if charged once daily. Meanwhile, the charging power can be as high as $19.2$kW when charged by level 2 charging standard ($240$V and $80$A) \cite{Yilmaz2013Review}. Obviously, electricity load from EV charging would pose significant impacts on the smart grid if not handled properly \cite{GreenII2011544,Clement2010Impact}.	 Recent thriving development of battery techniques has greatly accelerated this tendency \cite{Tuttle2012Evolution}. While batteries of EVs have been advanced in term of both capacity and power density, this in turn indicates that EVs will consume more energy with larger power. For example, according to the U.S. Energy Information Administration \cite{howmuchelectricity}, in 2013, a U. S. residential utility customer consumes an average of $909$kWh of electricity per month, while an EV with a $24$kWh battery pack consumes around $720$kWh if charged once daily. Meanwhile, the charging power can be as high as $19.2$kW when charged by level 2 charging standard ($240$V and $80$A) \cite{Yilmaz2013Review}. Obviously, electricity load from EV charging would pose significant impacts on the smart grid if not handled properly \cite{GreenII2011544,Clement2010Impact}.	score:439
Who are the toppers of the 2015 CSE batch at IIT Kharagpur?	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	score:446
Who are the toppers of the 2015 CSE batch at IIT Kharagpur?	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	score:446
What is the training programme syllabus for the October 26th, 2016 batch of Infosys?	  We build on the representation in \cite{A.Stuhlmueller:2010:6d11a} and explore a family of algorithms for learning probabilistic programs from data.  \begin{figure}[b] \begin{center} \includegraphics[scale=.26]{./figures/1-tree-1.pdf} \includegraphics[scale=.26]{./figures/1-tree-3.pdf} \includegraphics[scale=.26]{./figures/1-tree-4.pdf} \includegraphics[scale=. 26]{./figures/1-tree-6.pdf} \end{center} \caption{Tree-like objects.} \label{fig:plants} \end{figure}  Generative models play a prominent role in modern machine learning (e.g., Hidden Markov models and probabilistic context-free grammars) and have led to a wide variety of applications.  There is a trade-off between the variety of patterns a model class is able to capture and the feasibility of learning models in that class \cite{Russell2003}.   Much of machine learning has focused on studying classes of models with limited expressiveness in order to develop tractable algorithms for modeling large data sets.  Our investigation takes a different approach and explores how learning might proceed in an expressive class of models with a focus on identifying abstract patterns from small amounts of data.	26]{./figures/1-tree-6.pdf} \end{center} \caption{Tree-like objects.} \label{fig:plants} \end{figure}  Generative models play a prominent role in modern machine learning (e.g., Hidden Markov models and probabilistic context-free grammars) and have led to a wide variety of applications.  There is a trade-off between the variety of patterns a model class is able to capture and the feasibility of learning models in that class \cite{Russell2003}.	score:403
What is the training programme syllabus for the October 26th, 2016 batch of Infosys?	 Experimental results using the NIST SRE 2010 dataset show that both methods provide significant improvement and result in a 24.51\% relative improvement in Equal Error Rates (EERs) from a baseline system. In order to learn a better joint representation, we further investigate the effect of a deep encoder with residual blocks, and the results indicate that the residual network can further improve the EERs of a baseline system by up to 26. 47\%. Moreover, in order to improve the short i-vector mapping to its long version, an additional vector, which represents the average value of phoneme posteriors across frames, is also added to the input, and results in a 28.43\% improvement. When further testing the best-validated models of SRE10 on the Speaker In The Wild (SITW) dataset, the methods result in a 23.	 Experimental results using the NIST SRE 2010 dataset show that both methods provide significant improvement and result in a 24.51\% relative improvement in Equal Error Rates (EERs) from a baseline system. In order to learn a better joint representation, we further investigate the effect of a deep encoder with residual blocks, and the results indicate that the residual network can further improve the EERs of a baseline system by up to 26. 47\%. Moreover, in order to improve the short i-vector mapping to its long version, an additional vector, which represents the average value of phoneme posteriors across frames, is also added to the input, and results in a 28.43\% improvement. When further testing the best-validated models of SRE10 on the Speaker In The Wild (SITW) dataset, the methods result in a 23.	score:442
What is the training programme syllabus for the October 26th, 2016 batch of Infosys?	 It is easy-to-implement and can handle very large embedding and softmax matrices.   Our method achieves good performance on compressing a range of benchmark models for language modeling and neural machine translation tasks, and outperforms previous methods. For example, on DE-EN NMT task, Our method achieves 10 times compression rate on the embedding and softmax matrices without much degradation of performance.  Results can be further improved to 24 times compression rate when combined with quantization scheme. On One Billion Word dataset, our method achieves 6.6 times compression rate on the embedding and softmax matrices that are originally more than 6GB. When combined with quantization scheme, our method achieves more than 26 times compression rate while maintaining similar perplexity.        \vspace{-10pt}	 Results can be further improved to 24 times compression rate when combined with quantization scheme. On One Billion Word dataset, our method achieves 6.6 times compression rate on the embedding and softmax matrices that are originally more than 6GB. When combined with quantization scheme, our method achieves more than 26 times compression rate while maintaining similar perplexity.        \vspace{-10pt}	score:448
What is the training programme syllabus for the October 26th, 2016 batch of Infosys?	  \thanks{http://github.com/umassbionlp/crowd-ranking.git}\\ 	The number of biomedical research papers published has increased dramatically in recent years. As of October, 2016, PubMed houses over 26 million citations, with almost 1 million from the first 3 quarters of 2016 alone \footnote{Accessed on October 6, 2016.}. It has become impossible for any one person to actually read all of the work being published.  We require tools to help us determine which research articles would be most informative and related to a particular question or document. For example, a common task when reading articles is to find articles that are most related to another. Major research search engines offer such a ``related articles'' feature. However, we propose that instead of measuring relatedness by text-similarity measures, we build a model that is able to infer relatedness from the authors' judgments.	  \thanks{http://github.com/umassbionlp/crowd-ranking.git}\\ 	The number of biomedical research papers published has increased dramatically in recent years. As of October, 2016, PubMed houses over 26 million citations, with almost 1 million from the first 3 quarters of 2016 alone \footnote{Accessed on October 6, 2016.}. It has become impossible for any one person to actually read all of the work being published.  We require tools to help us determine which research articles would be most informative and related to a particular question or document. For example, a common task when reading articles is to find articles that are most related to another. Major research search engines offer such a ``related articles'' feature. However, we propose that instead of measuring relatedness by text-similarity measures, we build a model that is able to infer relatedness from the authors' judgments.	score:459
What is the training programme syllabus for the October 26th, 2016 batch of Infosys?	 The covariance matrix \cite{27},which is considered as the representation of the second order data statistics, captures the structural information of the feature space. The histogram feature and the first-order feature further enhance the feature representation ability. Inspired by the Multi-kernel Metric Learning method \cite{25,26}, we propose a novel learning framework, termed Multi-view Feature Discrete Hashing (MFDH), which exploits the complementary information provided by multiple views information and combines the classifier learning to learn the discriminative unified hashing code.	 The covariance matrix \cite{27},which is considered as the representation of the second order data statistics, captures the structural information of the feature space. The histogram feature and the first-order feature further enhance the feature representation ability. Inspired by the Multi-kernel Metric Learning method \cite{25,26}, we propose a novel learning framework, termed Multi-view Feature Discrete Hashing (MFDH), which exploits the complementary information provided by multiple views information and combines the classifier learning to learn the discriminative unified hashing code.	score:473
What is the training programme syllabus for the October 26th, 2015 batch of Infosys?	  We build on the representation in \cite{A.Stuhlmueller:2010:6d11a} and explore a family of algorithms for learning probabilistic programs from data.  \begin{figure}[b] \begin{center} \includegraphics[scale=.26]{./figures/1-tree-1.pdf} \includegraphics[scale=.26]{./figures/1-tree-3.pdf} \includegraphics[scale=.26]{./figures/1-tree-4.pdf} \includegraphics[scale=. 26]{./figures/1-tree-6.pdf} \end{center} \caption{Tree-like objects.} \label{fig:plants} \end{figure}  Generative models play a prominent role in modern machine learning (e.g., Hidden Markov models and probabilistic context-free grammars) and have led to a wide variety of applications.  There is a trade-off between the variety of patterns a model class is able to capture and the feasibility of learning models in that class \cite{Russell2003}.   Much of machine learning has focused on studying classes of models with limited expressiveness in order to develop tractable algorithms for modeling large data sets.  Our investigation takes a different approach and explores how learning might proceed in an expressive class of models with a focus on identifying abstract patterns from small amounts of data.	26]{./figures/1-tree-6.pdf} \end{center} \caption{Tree-like objects.} \label{fig:plants} \end{figure}  Generative models play a prominent role in modern machine learning (e.g., Hidden Markov models and probabilistic context-free grammars) and have led to a wide variety of applications.  There is a trade-off between the variety of patterns a model class is able to capture and the feasibility of learning models in that class \cite{Russell2003}.	score:413
What is the training programme syllabus for the October 26th, 2015 batch of Infosys?	 Experimental results using the NIST SRE 2010 dataset show that both methods provide significant improvement and result in a 24.51\% relative improvement in Equal Error Rates (EERs) from a baseline system. In order to learn a better joint representation, we further investigate the effect of a deep encoder with residual blocks, and the results indicate that the residual network can further improve the EERs of a baseline system by up to 26. 47\%. Moreover, in order to improve the short i-vector mapping to its long version, an additional vector, which represents the average value of phoneme posteriors across frames, is also added to the input, and results in a 28.43\% improvement. When further testing the best-validated models of SRE10 on the Speaker In The Wild (SITW) dataset, the methods result in a 23.	   We further discuss several key factors of the proposed DNN mapping models in detail, including pre-training iteration, regularization weights and encoder depth. The best model provides more than 26.47\% relative improvement. We also show that by adding additional phoneme information as input, we can achieve further mapping improvements (28.43\%).  We apply the proposed mapping methods to different durations of evaluation utterances to represent real-life situations, and the results show their effectiveness across all conditions. The mapping results for both I-vector\_GMM and I-vector\_DNN systems are compared, and show significant improvement for both systems. In the end, in order to show the generalization of the proposed methods, we apply the best-validated models of SRE10 \citep{Martin:10} dataset to the Speaker In The Wild (SITW) dataset \citep{McLaren:15}, which also show considerable improvement (23.	score:442
What is the training programme syllabus for the October 26th, 2015 batch of Infosys?	 The covariance matrix \cite{27},which is considered as the representation of the second order data statistics, captures the structural information of the feature space. The histogram feature and the first-order feature further enhance the feature representation ability. Inspired by the Multi-kernel Metric Learning method \cite{25,26}, we propose a novel learning framework, termed Multi-view Feature Discrete Hashing (MFDH), which exploits the complementary information provided by multiple views information and combines the classifier learning to learn the discriminative unified hashing code.	 The covariance matrix \cite{27},which is considered as the representation of the second order data statistics, captures the structural information of the feature space. The histogram feature and the first-order feature further enhance the feature representation ability. Inspired by the Multi-kernel Metric Learning method \cite{25,26}, we propose a novel learning framework, termed Multi-view Feature Discrete Hashing (MFDH), which exploits the complementary information provided by multiple views information and combines the classifier learning to learn the discriminative unified hashing code.	score:459
What is the training programme syllabus for the October 26th, 2015 batch of Infosys?	  \thanks{http://github.com/umassbionlp/crowd-ranking.git}\\ 	The number of biomedical research papers published has increased dramatically in recent years. As of October, 2016, PubMed houses over 26 million citations, with almost 1 million from the first 3 quarters of 2016 alone \footnote{Accessed on October 6, 2016.}. It has become impossible for any one person to actually read all of the work being published.  We require tools to help us determine which research articles would be most informative and related to a particular question or document. For example, a common task when reading articles is to find articles that are most related to another. Major research search engines offer such a ``related articles'' feature. However, we propose that instead of measuring relatedness by text-similarity measures, we build a model that is able to infer relatedness from the authors' judgments.	  \thanks{http://github.com/umassbionlp/crowd-ranking.git}\\ 	The number of biomedical research papers published has increased dramatically in recent years. As of October, 2016, PubMed houses over 26 million citations, with almost 1 million from the first 3 quarters of 2016 alone \footnote{Accessed on October 6, 2016.}. It has become impossible for any one person to actually read all of the work being published.  We require tools to help us determine which research articles would be most informative and related to a particular question or document. For example, a common task when reading articles is to find articles that are most related to another. Major research search engines offer such a ``related articles'' feature. However, we propose that instead of measuring relatedness by text-similarity measures, we build a model that is able to infer relatedness from the authors' judgments.	score:463
What is the training programme syllabus for the October 26th, 2015 batch of Infosys?	 In this paper, we propose a new process for training and evaluation in the GZSL setting; this process addresses the gap in performance between samples from unseen and seen classes by penalizing the latter, and enables to select hyper-parameters well-suited to the GZSL task. It can be applied to any existing ZSL approach and leads to a significant performance boost: the experimental evaluation shows that GZSL performance, averaged over eight state-of-the-art methods, is improved from $28.	 In this paper, we propose a new process for training and evaluation in the GZSL setting; this process addresses the gap in performance between samples from unseen and seen classes by penalizing the latter, and enables to select hyper-parameters well-suited to the GZSL task. It can be applied to any existing ZSL approach and leads to a significant performance boost: the experimental evaluation shows that GZSL performance, averaged over eight state-of-the-art methods, is improved from $28.	score:472
How to solve the problem that the objective value rises up while I was training CNN?	 However, while CNNs capture key properties of the average responses of cortical neurons, they fail to explain other properties of these neurons. For one, CNNs typically require large quantities of labeled input data for training. Our own brains, in contrast, rarely have access to this kind of supervision, so to the extent that representations are similar between CNNs and brains, this similarity must arise via different training paths.   In addition, neurons in visual cortex produce complex time-varying responses even to static inputs, and they dynamically tune themselves to temporal regularities in the visual environment. We argue that these differences are clues to fundamental differences between the computations performed in the brain and in deep networks. To begin to close the gap, here we study the emergent properties of a previously-described recurrent generative network that is trained to predict future video frames in a self-supervised manner.	 However, while CNNs capture key properties of the average responses of cortical neurons, they fail to explain other properties of these neurons. For one, CNNs typically require large quantities of labeled input data for training. Our own brains, in contrast, rarely have access to this kind of supervision, so to the extent that representations are similar between CNNs and brains, this similarity must arise via different training paths.   In addition, neurons in visual cortex produce complex time-varying responses even to static inputs, and they dynamically tune themselves to temporal regularities in the visual environment. We argue that these differences are clues to fundamental differences between the computations performed in the brain and in deep networks. To begin to close the gap, here we study the emergent properties of a previously-described recurrent generative network that is trained to predict future video frames in a self-supervised manner.	score:349
How to solve the problem that the objective value rises up while I was training CNN?	  Deep convolutional neural network (CNN) inference requires significant amount of memory and computation, which limits its deployment on embedded devices.  To alleviate these problems to some extent, prior research utilize low precision  fixed-point numbers to represent the CNN weights  and activations. However, the minimum required data precision of fixed-point weights  varies across different networks and also across different layers of the same  network.  In this work, we propose using floating-point numbers for representing the weights and fixed-point numbers for representing the activations. We  show that using floating-point representation for weights is more efficient than  fixed-point representation for the same bit-width and demonstrate it on popular  large-scale CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16.	  Deep convolutional neural network (CNN) inference requires significant amount of memory and computation, which limits its deployment on embedded devices.  To alleviate these problems to some extent, prior research utilize low precision  fixed-point numbers to represent the CNN weights  and activations. However, the minimum required data precision of fixed-point weights  varies across different networks and also across different layers of the same  network.  In this work, we propose using floating-point numbers for representing the weights and fixed-point numbers for representing the activations. We  show that using floating-point representation for weights is more efficient than  fixed-point representation for the same bit-width and demonstrate it on popular  large-scale CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16.	score:366
How to solve the problem that the objective value rises up while I was training CNN?	 The CNN is then trained with supervision in identifying different EoSs. The performance is surprisingly robust against other simulation parameters such as the initial conditions, equilibrium time $\tau_0$, transport coefficients and freeze out temperature. The supervised learning with deep CNN identifies the hydrodynamic response which is much more tolerant to uncertainties in the initial conditions.	 The CNN is then trained with supervision in identifying different EoSs. The performance is surprisingly robust against other simulation parameters such as the initial conditions, equilibrium time $\tau_0$, transport coefficients and freeze out temperature. The supervised learning with deep CNN identifies the hydrodynamic response which is much more tolerant to uncertainties in the initial conditions.	score:368
How to solve the problem that the objective value rises up while I was training CNN?	 Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN).  We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images.  To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images.	 Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN).  We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images.  To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images.	score:381
How to solve the problem that the objective value rises up while I was training CNN?	 This poses a problem when a neural network outputs a diagnosis, different from the diagnosis made by the medical expert, as there is no human interpretable reasoning behind the neural networks' diagnosis. In such a case, visualizations of the network could serve as a reasoning tool to the expert.  In this paper, we train a CNN for binary classification on a skin lesion dataset, and inspect the features learned by the network, by visualizing its feature maps. In the next section, we first give an overview of the different visualization strategies for inspecting CNNs. Section 3 describes our CNN architecture and training procedure. In Section 4 we present and discuss the learned CNN features and we conclude the paper in Section 5.	 This poses a problem when a neural network outputs a diagnosis, different from the diagnosis made by the medical expert, as there is no human interpretable reasoning behind the neural networks' diagnosis. In such a case, visualizations of the network could serve as a reasoning tool to the expert.  In this paper, we train a CNN for binary classification on a skin lesion dataset, and inspect the features learned by the network, by visualizing its feature maps. In the next section, we first give an overview of the different visualization strategies for inspecting CNNs. Section 3 describes our CNN architecture and training procedure. In Section 4 we present and discuss the learned CNN features and we conclude the paper in Section 5.	score:384
What is classification?	  \begin{keywords} Feature selection, Classification, Nonlinear identification, Model selection, Randomized methods \end{keywords} 	In the supervised learning framework, classification is the task of predicting the class labels of unseen observations (each consisting of a set of measured attributes or features), based on the experience gathered through a learning process on a previously available set of observations whose classes are known (training set).	  \begin{keywords} Feature selection, Classification, Nonlinear identification, Model selection, Randomized methods \end{keywords} 	In the supervised learning framework, classification is the task of predicting the class labels of unseen observations (each consisting of a set of measured attributes or features), based on the experience gathered through a learning process on a previously available set of observations whose classes are known (training set).	score:448
What is classification?	 In a typical scenario, a classifier is trained on a set of labeled `source' samples, and applied to a new set of `target' samples. This is challenging in the presence of `data-set shifts'\cite{moreno2012unifying} where the underlying probability function of the source data differs from that of the target data. Contrary to classification, which is studied both in the presence and absence of domain shift, quantification is only of interest under data-set shift, since otherwise, the class-distribution can be estimated directly from the labeled source data.	 In a typical scenario, a classifier is trained on a set of labeled `source' samples, and applied to a new set of `target' samples. This is challenging in the presence of `data-set shifts'\cite{moreno2012unifying} where the underlying probability function of the source data differs from that of the target data. Contrary to classification, which is studied both in the presence and absence of domain shift, quantification is only of interest under data-set shift, since otherwise, the class-distribution can be estimated directly from the labeled source data.	score:485
What is classification?	 	The complexity of classification models is usually highly related, and typically linear w.r.t the number of possible categories denoted $C$. When facing problems with a very large number of classes, like text classification in large ontologies, object recognition or word prediction in deep learning language models, this becomes a critical point making classification methods inefficient in term of inference complexity.	 	The complexity of classification models is usually highly related, and typically linear w.r.t the number of possible categories denoted $C$. When facing problems with a very large number of classes, like text classification in large ontologies, object recognition or word prediction in deep learning language models, this becomes a critical point making classification methods inefficient in term of inference complexity.	score:490
What is classification?	 Classification is a fundamental task in machine learning and data mining. Existing classification methods are designed to classify unknown instances within a set of previously known training classes. Such a classification takes the form of a prediction within a closed-set of classes. However, a more realistic scenario that fits real-world applications is to consider the possibility of encountering instances that do not belong to any of the training classes, $i. e.$, an open-set classification. In such situation, existing closed-set classifiers will assign a training label to these instances resulting in a misclassification. In this paper, we introduce \textit{Galaxy-X}, a novel multi-class classification approach for open-set recognition problems. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances.	 Classification is a fundamental task in machine learning and data mining. Existing classification methods are designed to classify unknown instances within a set of previously known training classes. Such a classification takes the form of a prediction within a closed-set of classes. However, a more realistic scenario that fits real-world applications is to consider the possibility of encountering instances that do not belong to any of the training classes, $i. e.$, an open-set classification. In such situation, existing closed-set classifiers will assign a training label to these instances resulting in a misclassification. In this paper, we introduce \textit{Galaxy-X}, a novel multi-class classification approach for open-set recognition problems. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances.	score:492
What is classification?	g. classification vs. regression. First, this work broadly examines types of feature selection and defines RBAs within that context. Next, we introduce the original Relief algorithm and associated concepts, emphasizing the intuition behind how it works, how feature weights generated by the algorithm can be interpreted, and why it is sensitive to feature interactions without evaluating combinations of features.  Lastly, we include an expansive review of RBA methodological research beyond Relief and its popular descendant, ReliefF. In particular, we characterize branches of RBA research, and provide comparative summaries of RBA algorithms including contributions, strategies, functionality, time complexity, adaptation to key data characteristics, and software availability.	g. classification vs. regression. First, this work broadly examines types of feature selection and defines RBAs within that context. Next, we introduce the original Relief algorithm and associated concepts, emphasizing the intuition behind how it works, how feature weights generated by the algorithm can be interpreted, and why it is sensitive to feature interactions without evaluating combinations of features.  Lastly, we include an expansive review of RBA methodological research beyond Relief and its popular descendant, ReliefF. In particular, we characterize branches of RBA research, and provide comparative summaries of RBA algorithms including contributions, strategies, functionality, time complexity, adaptation to key data characteristics, and software availability.	score:495
What are the classifications of stars?	 However, over the past two decades, discoveries of exoplanets have poured in by the hundreds and the rate at which exoplanets are being discovered is increasing. The inference from this is that planets around stars are a rule rather than an exception with the actual number of planets exceeding the number of stars in our galaxy by orders of magnitude.  In order to find interesting samples from the massive ongoing growth in the data, a sophisticated pipeline may be developed which can quickly and efficiently classify exoplanets based on habitability classes.  The process of discovery of exoplanets is rather complex, \citep{Bains2016}, as the size of exoplanets is small compared to other types of stellar objects such as stars, galaxies, quasars, etc.	 However, over the past two decades, discoveries of exoplanets have poured in by the hundreds and the rate at which exoplanets are being discovered is increasing. The inference from this is that planets around stars are a rule rather than an exception with the actual number of planets exceeding the number of stars in our galaxy by orders of magnitude.  In order to find interesting samples from the massive ongoing growth in the data, a sophisticated pipeline may be developed which can quickly and efficiently classify exoplanets based on habitability classes.  The process of discovery of exoplanets is rather complex, \citep{Bains2016}, as the size of exoplanets is small compared to other types of stellar objects such as stars, galaxies, quasars, etc.	score:483
What are the classifications of stars?	 However, over the past two decades, discoveries of exoplanets have poured in by the hundreds and the rate at which exoplanets are being discovered is increasing. The inference from this is that planets around stars are a rule rather than an exception with the actual number of planets exceeding the number of stars in our galaxy by orders of magnitude.  In order to find interesting samples from the massive ongoing growth in the data, a sophisticated pipeline may be developed which can quickly and efficiently classify exoplanets based on habitability classes.  The process of discovery of exoplanets is rather complex, \citep{Bains2016}, as the size of exoplanets is small compared to other types of stellar objects such as stars, galaxies, quasars, etc.	 However, over the past two decades, discoveries of exoplanets have poured in by the hundreds and the rate at which exoplanets are being discovered is increasing. The inference from this is that planets around stars are a rule rather than an exception with the actual number of planets exceeding the number of stars in our galaxy by orders of magnitude.  In order to find interesting samples from the massive ongoing growth in the data, a sophisticated pipeline may be developed which can quickly and efficiently classify exoplanets based on habitability classes.  The process of discovery of exoplanets is rather complex, \citep{Bains2016}, as the size of exoplanets is small compared to other types of stellar objects such as stars, galaxies, quasars, etc.	score:483
What are the classifications of stars?	                          In this paper, we introduce \textit{Galaxy-X}, an open-set multi-class classification approach. For each class, Galaxy-X creates a minimum bounding hyper-sphere that encloses all of its instances. In such manner, it is able to distinguish between novel instances that fit the distribution of a known class from those that diverge from it.  Galaxy-X introduces a softening parameter for the adjustment of the minimum bounding hyper-spheres to add more generalization or specialization to the classification models. To properly evaluate open-set classification, we also propose a novel evaluation technique, namely \textit{Leave-P-Class-Out-Cross-Validation}. Experimental evaluations on benchmark datasets show the efficiency of Galaxy-X in open-set multi-class classification.	e.$, an open-set classification. In such situation, existing closed-set classifiers will assign a training label to these instances resulting in a misclassification. In this paper, we introduce \textit{Galaxy-X}, a novel multi-class classification approach for open-set recognition problems. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances.  In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unknown ones. To adequately evaluate open-set classification, we introduce a novel evaluation procedure. Experimental results on benchmark datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes.	score:516
What are the classifications of stars?	 Experimental results obtained using artificial datasets show that the classification performance of the FSG depends on how the individual classifiers share feature vectors of samples. Rather than the power of the individual base layer-classifiers, diversity and cooperation of the classifiers become an important issue to improve the overall performance of the proposed FSG.  A weak base-layer classifier may boost the overall performance more than a strong classifier, if it is capable of recognizing the samples, which are not recognized by the rest of the classifiers, in its own feature space. The experiments explore the type of the collaboration among the individual classifiers required for an improved performance of the suggested architecture.	   \item We make a thorough empirical analysis of the \textit{black art} problem of the suggested FSG. The empirical results show the effect of the samples which cannot be correctly classified by any of the base-layer classifiers on the classification performance of the FSG. It is observed that if the base-layer classifiers share all the samples in the training set to correctly classify them, then the performance of the overall FSG becomes higher than that of the individual base-layer classifiers.  On the other hand, if a sample is misclassified by all of the base-layer classifiers, then this sample causes the performance decrease of the overall FSG.    \end{enumerate}    The suggested Fuzzy Stacked Generalization algorithm is tested on artificial and real datasets by the comparisons with the state of the art ensemble learning algorithms such as Adaboost \cite{adaboost}, Random Subspace \cite{random_subspace} and Rotation Forest \cite{rf}.	score:532
What are the classifications of stars?	e.$, an open-set classification. In such situation, existing closed-set classifiers will assign a training label to these instances resulting in a misclassification. In this paper, we introduce \textit{Galaxy-X}, a novel multi-class classification approach for open-set recognition problems. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances.  In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unknown ones. To adequately evaluate open-set classification, we introduce a novel evaluation procedure. Experimental results on benchmark datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes.	e.$, an open-set classification. In such situation, existing closed-set classifiers will assign a training label to these instances resulting in a misclassification. In this paper, we introduce \textit{Galaxy-X}, a novel multi-class classification approach for open-set recognition problems. For each class of the training set, Galaxy-X creates a minimum bounding hyper-sphere that encompasses the distribution of the class by enclosing all of its instances.  In such manner, our method is able to distinguish instances resembling previously seen classes from those that are of unknown ones. To adequately evaluate open-set classification, we introduce a novel evaluation procedure. Experimental results on benchmark datasets show the efficiency of our approach in classifying novel instances from known as well as unknown classes.	score:534
How do I start learning machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:489
How do I start learning machine learning?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:513
How do I start learning machine learning?	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:518
How do I start learning machine learning?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:520
How do I start learning machine learning?	  Many machine learning algorithms have been analyzed by smoothed analysis, such as k-means clustering \citep{arthur2009k} and Support Vector Machines \citep{Blum:2002:SAP:545381.545499,spielman2009smoothed}, however, to the best of our knowledge, there is no previous research on using machine learning algorithms to predict the SC of sorting algorithms.   Due to recent modular smoothed analysis results, we can compute the SC of Quicksort exactly, for medium size inputs (e.g., $N \leq 3000$). For other sorting algorithms, currently the SC can only be computed using its original definition, and thus, due to computational requirements, only for small input sizes (e.g., $N \leq 100$), limiting our understanding of the general SC behaviour.	  Many machine learning algorithms have been analyzed by smoothed analysis, such as k-means clustering \citep{arthur2009k} and Support Vector Machines \citep{Blum:2002:SAP:545381.545499,spielman2009smoothed}, however, to the best of our knowledge, there is no previous research on using machine learning algorithms to predict the SC of sorting algorithms.   Due to recent modular smoothed analysis results, we can compute the SC of Quicksort exactly, for medium size inputs (e.g., $N \leq 3000$). For other sorting algorithms, currently the SC can only be computed using its original definition, and thus, due to computational requirements, only for small input sizes (e.g., $N \leq 100$), limiting our understanding of the general SC behaviour.	score:523
How do I learn machine learning and from where?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:400
How do I learn machine learning and from where?	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	score:415
How do I learn machine learning and from where?	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:415
How do I learn machine learning and from where?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:418
How do I learn machine learning and from where?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:420
Where can you download Tom Mitchell's video lectures from his course on machine learning?	 In this paper we describe the environment with ToriLLE's capabilities and limitations, and experimentally show its applicability as a learning environment with baseline and human experiments. The source code of the environment and conducted experiments can be found at \url{https://github.com/Miffyli/ToriLLE}. 	Video games provide a rich and complex environments for training machine learning agents, without limiting them to real-time progression like with robotics.  Popularity of such learning environments can be seen from the number of different video games used for such purpose, such as Atari games \cite{gym}, Doom \cite{vizdoom}, Quake \cite{dmlab} and Starcraft \cite{torchcraft,sc2}. These environments provide challenges for agents to complete in single-agent scenarios, which allows comparing performance of different learning methods, for example.	 In this paper we describe the environment with ToriLLE's capabilities and limitations, and experimentally show its applicability as a learning environment with baseline and human experiments. The source code of the environment and conducted experiments can be found at \url{https://github.com/Miffyli/ToriLLE}. 	Video games provide a rich and complex environments for training machine learning agents, without limiting them to real-time progression like with robotics.  Popularity of such learning environments can be seen from the number of different video games used for such purpose, such as Atari games \cite{gym}, Doom \cite{vizdoom}, Quake \cite{dmlab} and Starcraft \cite{torchcraft,sc2}. These environments provide challenges for agents to complete in single-agent scenarios, which allows comparing performance of different learning methods, for example.	score:415
Where can you download Tom Mitchell's video lectures from his course on machine learning?	 A typical active learning system consists of a learning engine, which does the annotation/retrieval and a sample selection engine, responsible for determining the labeling order of the unlabeled samples.  In this work, we use NormCRM as the learning engine and propose a novel sample selection algorithm. We call this integrated system CRMActive and apply it for video annotation and video retrieval tasks.  The algorithm uses a measure of  \emph{informativeness} for ranking unlabeled samples during active learning. This informativeness combines a new measure of sample uncertainty with a novel cluster-refinement based approach for determining sample density and diversity. Our experiments show that CRMActive outperforms a state-of-the-art approach and a random baseline.	 A typical active learning system consists of a learning engine, which does the annotation/retrieval and a sample selection engine, responsible for determining the labeling order of the unlabeled samples.  In this work, we use NormCRM as the learning engine and propose a novel sample selection algorithm. We call this integrated system CRMActive and apply it for video annotation and video retrieval tasks.  The algorithm uses a measure of  \emph{informativeness} for ranking unlabeled samples during active learning. This informativeness combines a new measure of sample uncertainty with a novel cluster-refinement based approach for determining sample density and diversity. Our experiments show that CRMActive outperforms a state-of-the-art approach and a random baseline.	score:424
Where can you download Tom Mitchell's video lectures from his course on machine learning?	), have conspired to spark much interest in developing distributed algorithms for machine learning (ML) methods. While it is easy to see how parallelism can be obtained for most of the computational problems in ML, the question arises whether online learning, which appears at first glance to be inherently serial, can be fruitfully parallelized to any significant degree.  While several recent papers have proposed distributed schemes, the question of whether significant speedups over the default serial scheme can be achieved has remained fairly open. Theory establishing or disallowing such a possibility is particularly to be desired. To the best of our knowledge, this paper is the first work that answers these questions for the general online learning setting.	), have conspired to spark much interest in developing distributed algorithms for machine learning (ML) methods. While it is easy to see how parallelism can be obtained for most of the computational problems in ML, the question arises whether online learning, which appears at first glance to be inherently serial, can be fruitfully parallelized to any significant degree.  While several recent papers have proposed distributed schemes, the question of whether significant speedups over the default serial scheme can be achieved has remained fairly open. Theory establishing or disallowing such a possibility is particularly to be desired. To the best of our knowledge, this paper is the first work that answers these questions for the general online learning setting.	score:431
Where can you download Tom Mitchell's video lectures from his course on machine learning?	 Even with parallel processing, recovery times make video CS prohibitive for modern commercial camera architectures.  In this work, we address this problem by employing deep learning and show that video frames can be recovered in a few seconds at significantly improved reconstruction quality compared to existing approaches.  Our contributions are summarized as follows:  \begin{enumerate} \item We present the first deep learning architecture for temporal video CS reconstruction approach, based on fully-connected neural networks, which learns to map directly temporal CS measurements to video frames.	 Even with parallel processing, recovery times make video CS prohibitive for modern commercial camera architectures.  In this work, we address this problem by employing deep learning and show that video frames can be recovered in a few seconds at significantly improved reconstruction quality compared to existing approaches.  Our contributions are summarized as follows:  \begin{enumerate} \item We present the first deep learning architecture for temporal video CS reconstruction approach, based on fully-connected neural networks, which learns to map directly temporal CS measurements to video frames.	score:432
Where can you download Tom Mitchell's video lectures from his course on machine learning?	   Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. However most current DRL video-game agents learn end-to-end from the video-output of the game, which is superfluous for many applications and creates a number of additional problems. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.  In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously.	   Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. However most current DRL video-game agents learn end-to-end from the video-output of the game, which is superfluous for many applications and creates a number of additional problems. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.  In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously.	score:432
What is an incidence list/matrix in graph representation?	  An example of such a \textit{jointly exchangeable matrix} \mbox{\citep{orbanz2015bayesian}},  modelling the interaction of the nodes in a graph, is the adjacency matrix deployed by graph convolution. Our approach reduces to graph convolution in the special case of 2D arrays with this additional parameter-sharing constraint, but also applies to arbitrary matrices and higher dimensional arrays.          Finally, we explain connections to some of our own past work. First, we introduced a similar parameter-sharing scheme in the context of behavioral game theory \citep{hartford2016deep}: rows and columns represented players' actions and the exchangeable matrix encoded payoffs. The current work provides a theoretical foundation for these ideas and shows how a similar architecture can be applied much more generally.	  An example of such a \textit{jointly exchangeable matrix} \mbox{\citep{orbanz2015bayesian}},  modelling the interaction of the nodes in a graph, is the adjacency matrix deployed by graph convolution. Our approach reduces to graph convolution in the special case of 2D arrays with this additional parameter-sharing constraint, but also applies to arbitrary matrices and higher dimensional arrays.          Finally, we explain connections to some of our own past work. First, we introduced a similar parameter-sharing scheme in the context of behavioral game theory \citep{hartford2016deep}: rows and columns represented players' actions and the exchangeable matrix encoded payoffs. The current work provides a theoretical foundation for these ideas and shows how a similar architecture can be applied much more generally.	score:322
What is an incidence list/matrix in graph representation?	  The GraphBLAS.org math library standard was developed to provide high performance manipulation of sparse weight matrices and input/output vectors. For sufficiently sparse matrices, a sparse matrix library requires significantly less memory than the corresponding dense matrix implementation.  This paper provides a brief description of the mathematics underlying the GraphBLAS.   In addition, the equations of a typical DNN are rewritten in a form designed to use the GraphBLAS.   An implementation of the DNN is given using a preliminary GraphBLAS C library.  The  performance of the GraphBLAS implementation is measured relative to a standard dense linear algebra library implementation. For various sizes of DNN weight matrices, it is shown that the GraphBLAS sparse implementation outperforms a BLAS dense implementation as the weight matrix becomes sparser.	  The GraphBLAS.org math library standard was developed to provide high performance manipulation of sparse weight matrices and input/output vectors. For sufficiently sparse matrices, a sparse matrix library requires significantly less memory than the corresponding dense matrix implementation.  This paper provides a brief description of the mathematics underlying the GraphBLAS.   In addition, the equations of a typical DNN are rewritten in a form designed to use the GraphBLAS.   An implementation of the DNN is given using a preliminary GraphBLAS C library.  The  performance of the GraphBLAS implementation is measured relative to a standard dense linear algebra library implementation. For various sizes of DNN weight matrices, it is shown that the GraphBLAS sparse implementation outperforms a BLAS dense implementation as the weight matrix becomes sparser.	score:339
What is an incidence list/matrix in graph representation?	 For these reasons, an increasing amount of work has recently been focusing on inferring (generalised) graph Laplacians. Among the first researchers to focus on graph Laplacian inference, Dong et al. \cite{Dong14} adopt a graph signal processing perspective to enforce data smoothness on the inferred graph. The proposed model results in assumptions similar to those in GMRFs, but with added constraints that ensure that the graph is given by a valid Laplacian matrix.  Kalofolias \cite{kalofolias2016learn} uses a similar framework and proposes a computationally more efficient solution by inferring a weight matrix, which can eventually be easily transformed into a Laplacian. An even more efficient, approximate solution has been proposed for large scale graph learning \cite{kalofolias2017large}, scaling to graphs with millions of nodes.  Other recent works in this vein include inference of graph shift operators, with the assumption that data is the result of a graph diffusion process. A popular approach to this problem consists in exploiting the fact that the eigenvectors of the graph will be shared with those of any graph filter. Therefore, they can be estimated from the data sample covariance matrix, and the optimisation can be done only over the eigenvalues \cite{segarra2017network} \cite{pasdeloup2017characterization}.     Dictionary based methods try to model signal heterogeneity by taking into account sparse combinations or dictionary atoms. In \cite{thanou2017learning}, signals are represented as linear combinations of atoms from a heat kernel dictionary. As those are still bound to be smooth, \cite{maretic2017graph} model signals as sparse linear combinations of atoms in a predefined polynomial graph dictionary.	 Other recent works in this vein include inference of graph shift operators, with the assumption that data is the result of a graph diffusion process. A popular approach to this problem consists in exploiting the fact that the eigenvectors of the graph will be shared with those of any graph filter. Therefore, they can be estimated from the data sample covariance matrix, and the optimisation can be done only over the eigenvalues \cite{segarra2017network} \cite{pasdeloup2017characterization}.	score:340
What is an incidence list/matrix in graph representation?	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	score:344
What is an incidence list/matrix in graph representation?	 Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates.  However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure.   The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning.  Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity. 	Supervised learning has emerged as a serious contender in the field of image segmentation, ever since the creation of training sets of images with {}``ground truth'' segmentations provided by humans, such as the Berkeley Segmentation Dataset \cite{Martin:2001}.	 However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure.	score:348
What is an adjacency matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	  In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	score:411
What is an adjacency matrix?	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	score:417
What is an adjacency matrix?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:433
What is an adjacency matrix?	 In this task we are usually given a set of objects, as well as an adjacency matrix capturing the pairwise relationships between these objects. This adjacency matrix is either represented by an unweighted graph, where the weight of edges is always equal to one, or a weighted graph, where the weight of edges can take any real positive values. The goal is to find an assignment of the objects into several subsets, such that the ones in the same subset are similar in some sense.  Due to the wide range of applications for this problem, numerous approaches have been proposed in literature, and we point the readers to \cite{Schaeffer07} for an extensive survey on this topic.   In contrast to the traditional problem, recent applications such as mobile and online social network analysis bring interesting new challenges. In these scenarios, it is common that observational data contains multiple modalities of information reflecting different aspects of human interactions.	 In this task we are usually given a set of objects, as well as an adjacency matrix capturing the pairwise relationships between these objects. This adjacency matrix is either represented by an unweighted graph, where the weight of edges is always equal to one, or a weighted graph, where the weight of edges can take any real positive values. The goal is to find an assignment of the objects into several subsets, such that the ones in the same subset are similar in some sense.  Due to the wide range of applications for this problem, numerous approaches have been proposed in literature, and we point the readers to \cite{Schaeffer07} for an extensive survey on this topic.   In contrast to the traditional problem, recent applications such as mobile and online social network analysis bring interesting new challenges. In these scenarios, it is common that observational data contains multiple modalities of information reflecting different aspects of human interactions.	score:444
What is an adjacency matrix?	  After describing and analyzing the general algorithm, we show how to apply it  to the problems of matrix completion and robust low-rank matrix approximation. As a side benefit, our general analysis yields a new sample complexity bound for matrix completion. We demonstrate the efficacy of our algorithm by conducting experiments on large-scale movie recommendation data sets.   \subsection{Related work} \label{sec:related}  As mentioned earlier, the problem defined in \eqref{eqn:rankMin} has many applications, and therefore it was studied in various contexts. A popular approach is to use the trace norm as a surrogate for the rank (e.g. \cite{FazelHiBo02}). This approach is closely related to the idea of using the $\ell_1$ norm as a surrogate for sparsity, because low rank corresponds to sparsity of the vector of singular values and the trace norm is the $\ell_1$ norm of the vector of singular values.	  After describing and analyzing the general algorithm, we show how to apply it  to the problems of matrix completion and robust low-rank matrix approximation. As a side benefit, our general analysis yields a new sample complexity bound for matrix completion. We demonstrate the efficacy of our algorithm by conducting experiments on large-scale movie recommendation data sets.   \subsection{Related work} \label{sec:related}  As mentioned earlier, the problem defined in \eqref{eqn:rankMin} has many applications, and therefore it was studied in various contexts. A popular approach is to use the trace norm as a surrogate for the rank (e.g. \cite{FazelHiBo02}). This approach is closely related to the idea of using the $\ell_1$ norm as a surrogate for sparsity, because low rank corresponds to sparsity of the vector of singular values and the trace norm is the $\ell_1$ norm of the vector of singular values.	score:444
Which statistical test should I do to validate that gender plays a role in job classification (job types) and salary distribution?	 The communication may also mimic human conversation, e.g., in settings where agents engage in natural language dialogue based on a shared visual modality \citep{das2017learning,strub2017end}.   In contrast, the goal of our work is to \textit{learn} the communication protocol, and aligns more closely with another line of research, which focuses on investigating and analyzing the emergence of communication in (cooperative) multi-agent referential games \citep{lewis2008convention,skyrms2010signals,steels2012grounded}, where one agent (the sender) must communicate what it sees using some discrete emergent communication protocol, while the other agent (the receiver) is tasked with figuring out what the first agent saw.	 The communication may also mimic human conversation, e.g., in settings where agents engage in natural language dialogue based on a shared visual modality \citep{das2017learning,strub2017end}.   In contrast, the goal of our work is to \textit{learn} the communication protocol, and aligns more closely with another line of research, which focuses on investigating and analyzing the emergence of communication in (cooperative) multi-agent referential games \citep{lewis2008convention,skyrms2010signals,steels2012grounded}, where one agent (the sender) must communicate what it sees using some discrete emergent communication protocol, while the other agent (the receiver) is tasked with figuring out what the first agent saw.	score:307
Which statistical test should I do to validate that gender plays a role in job classification (job types) and salary distribution?	 The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested examples and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures available?  This review attempts to answer this question through evaluating the performance (measured by accuracy, precision and recall) of the KNN using a large number of distance measures, tested on a number of real-world datasets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, and the results showed large gaps between the performances of different distances.	 The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested examples and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures available?  This review attempts to answer this question through evaluating the performance (measured by accuracy, precision and recall) of the KNN using a large number of distance measures, tested on a number of real-world datasets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, and the results showed large gaps between the performances of different distances.	score:309
Which statistical test should I do to validate that gender plays a role in job classification (job types) and salary distribution?	 We tested the proposed XHMM approaches on two activity recognition datasets and show high detection rates for falls in the absence of fall-specific training data. We show that the traditional method of choosing threshold based on maximum of negative of log-likelihood to identify unseen falls is ill-posed for this problem.  We also show that supervised classification methods perform poorly when very limited fall data is available during the training phase.  	Identification of normal Activities of Daily Living (ADL), for e.g., walking, hand washing, making breakfast, etc., is important to understand a person's behaviour, goals and actions \cite{Acampora2013}. However, in certain situations, a more challenging, useful and interesting research problem is to identify cases when an abnormal activity occurs, as it can have direct implications on the health and safety of an individual.	 We tested the proposed XHMM approaches on two activity recognition datasets and show high detection rates for falls in the absence of fall-specific training data. We show that the traditional method of choosing threshold based on maximum of negative of log-likelihood to identify unseen falls is ill-posed for this problem.  We also show that supervised classification methods perform poorly when very limited fall data is available during the training phase.  	Identification of normal Activities of Daily Living (ADL), for e.g., walking, hand washing, making breakfast, etc., is important to understand a person's behaviour, goals and actions \cite{Acampora2013}. However, in certain situations, a more challenging, useful and interesting research problem is to identify cases when an abnormal activity occurs, as it can have direct implications on the health and safety of an individual.	score:320
Which statistical test should I do to validate that gender plays a role in job classification (job types) and salary distribution?	 The generative adversarial network \citep{gan,Gutmann2014} is a popular method for training generative models, where a rival discriminator attempts to distinguish model samples from reference data. Training of the generator and discriminator is interleaved, such that a saddle point is eventually reached in the joint loss.  A useful insight into the behavior of \acro{GAN}s is to note that when the discriminator is properly trained, the generator is tasked with minimizing the Jensen-Shannon divergence measure between the model and data distributions.  When the model is insufficiently powerful to perfectly simulate the test data, as in most nontrivial settings, the choice of divergence measure is especially crucial: it determines which compromises will be made. A range of adversarial divergences were proposed by \cite{huszar16}, using a weight to interpolate between \acro{KL}, inverse \acro{KL}, and Jensen-Shannon.	 The generative adversarial network \citep{gan,Gutmann2014} is a popular method for training generative models, where a rival discriminator attempts to distinguish model samples from reference data. Training of the generator and discriminator is interleaved, such that a saddle point is eventually reached in the joint loss.  A useful insight into the behavior of \acro{GAN}s is to note that when the discriminator is properly trained, the generator is tasked with minimizing the Jensen-Shannon divergence measure between the model and data distributions.  When the model is insufficiently powerful to perfectly simulate the test data, as in most nontrivial settings, the choice of divergence measure is especially crucial: it determines which compromises will be made. A range of adversarial divergences were proposed by \cite{huszar16}, using a weight to interpolate between \acro{KL}, inverse \acro{KL}, and Jensen-Shannon.	score:321
Which statistical test should I do to validate that gender plays a role in job classification (job types) and salary distribution?	g. $a$ is drawn from the same distribution as  the distribution of the $x_i$'s).        \subsection{Relation to existing literature}  Confidence intervals  and hypothesis testing play a fundamental role in statistical theory and applications. However, compared to the point estimation  there is still much work to be done   for  statistical inference of high-dimensional models.  Existing work on the inference problems  predominantly focuses on  individual coordinates of \(\beta_*\).  Early work typically imposes conditions that guarantee consistent variable selection (see \cite{fan2001variable,zou2006adaptive,Zhao:2006}) or develops methods that lead  to conservative inferential guarantees (e.g. \cite{Buhlmann2013}). However, recent work focusses on asymptotically accurate inference  without  relying on  the variable selection consistency.	g. $a$ is drawn from the same distribution as  the distribution of the $x_i$'s).        \subsection{Relation to existing literature}  Confidence intervals  and hypothesis testing play a fundamental role in statistical theory and applications. However, compared to the point estimation  there is still much work to be done   for  statistical inference of high-dimensional models.  Existing work on the inference problems  predominantly focuses on  individual coordinates of \(\beta_*\).  Early work typically imposes conditions that guarantee consistent variable selection (see \cite{fan2001variable,zou2006adaptive,Zhao:2006}) or develops methods that lead  to conservative inferential guarantees (e.g. \cite{Buhlmann2013}). However, recent work focusses on asymptotically accurate inference  without  relying on  the variable selection consistency.	score:322
What are the best ten universities in the UK for a PhD in artificial intelligence or machine learning?	  An intuitive description of the paper's results is this: From prior and corresponding choice of learning machinery (Section \ref{sec:class-learn-probl}), we construct statements about the \emph{dynamics of the learning process} (Section \ref{sec:optim-contr-learn}). The learning machine itself provides a probabilistic description of the dynamics of the physical system.  Combining both dynamics yields a \emph{joint} system, which we aim to control optimally. Doing so amounts to simultaneously controlling exploration (controlling the learning system) and exploitation (controlling the physical system).  Because large parts of the analysis rely on concepts from optimal control theory, this paper will use notation from that field.	  An intuitive description of the paper's results is this: From prior and corresponding choice of learning machinery (Section \ref{sec:class-learn-probl}), we construct statements about the \emph{dynamics of the learning process} (Section \ref{sec:optim-contr-learn}). The learning machine itself provides a probabilistic description of the dynamics of the physical system.  Combining both dynamics yields a \emph{joint} system, which we aim to control optimally. Doing so amounts to simultaneously controlling exploration (controlling the learning system) and exploitation (controlling the physical system).  Because large parts of the analysis rely on concepts from optimal control theory, this paper will use notation from that field.	score:436
What are the best ten universities in the UK for a PhD in artificial intelligence or machine learning?	 Procedural Level Generation via Machine Learning (PLGML), the study of generating game levels with machine learning, has received a large amount of recent academic attention. For certain measures these approaches have shown success at replicating the quality of existing game levels. However, it is unclear the extent to which they might benefit human designers.  In this paper we present a framework for co-creative level design with a PLGML agent. In support of this framework we present results from a user study and results from a comparative study of PLGML approaches.   	Procedural content generation via Machine Learning (PCGML) has drawn increasing academic interest in recent years \cite{summerville2017procedural}.	 Procedural Level Generation via Machine Learning (PLGML), the study of generating game levels with machine learning, has received a large amount of recent academic attention. For certain measures these approaches have shown success at replicating the quality of existing game levels. However, it is unclear the extent to which they might benefit human designers.  In this paper we present a framework for co-creative level design with a PLGML agent. In support of this framework we present results from a user study and results from a comparative study of PLGML approaches.   	Procedural content generation via Machine Learning (PCGML) has drawn increasing academic interest in recent years \cite{summerville2017procedural}.	score:451
What are the best ten universities in the UK for a PhD in artificial intelligence or machine learning?	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:454
What are the best ten universities in the UK for a PhD in artificial intelligence or machine learning?	 However, in practice learning machines are not given access to this distribution, $Pr(C|\mathbf{X})$. Therefore, given a feature vector or variables $\mathbf{X}\in R^N$, the aim of most machine learning algorithms is to approximate this underlying distribution or estimate some of its characteristics. Unfortunately, in most practically relevant data mining applications, the dimensionality of the feature vector is quite high making it prohibitive to learn the underlying distribution.  For instance, gene expression data or images may easily have more than tens of thousands of features. While, at least in theory, having more features should result in a more discriminative classifier, it is not the case in practice because of the computational burden and curse of  dimensionality.   High-dimensional data poses different challenges on induction and prediction algorithms.	 However, in practice learning machines are not given access to this distribution, $Pr(C|\mathbf{X})$. Therefore, given a feature vector or variables $\mathbf{X}\in R^N$, the aim of most machine learning algorithms is to approximate this underlying distribution or estimate some of its characteristics. Unfortunately, in most practically relevant data mining applications, the dimensionality of the feature vector is quite high making it prohibitive to learn the underlying distribution.  For instance, gene expression data or images may easily have more than tens of thousands of features. While, at least in theory, having more features should result in a more discriminative classifier, it is not the case in practice because of the computational burden and curse of  dimensionality.   High-dimensional data poses different challenges on induction and prediction algorithms.	score:458
What are the best ten universities in the UK for a PhD in artificial intelligence or machine learning?	    Our thesis is that the best heuristic to use is dependent upon the problem considered.  However, the relationship between the problems and heuristics is far from obvious   and so we investigate whether machine learning can help with these choices.  Machine learning is a branch of artificial intelligence. It uses statistical methods to infer information from supplied data which is then used to make predictions for previously unseen data \cite{alpaydin2004introduction}.   We have applied machine learning (specifically a support vector machine) to the problem of selecting a variable ordering for both CAD itself and quantifier elimination by CAD, using the nlsat dataset \cite{nlsat} of fully existentially quantified problems.  Our results show that the choices made by machine learning are on average superior to both any individual heuristic and to picking a heuristic at random.	    Our thesis is that the best heuristic to use is dependent upon the problem considered.  However, the relationship between the problems and heuristics is far from obvious   and so we investigate whether machine learning can help with these choices.  Machine learning is a branch of artificial intelligence. It uses statistical methods to infer information from supplied data which is then used to make predictions for previously unseen data \cite{alpaydin2004introduction}.   We have applied machine learning (specifically a support vector machine) to the problem of selecting a variable ordering for both CAD itself and quantifier elimination by CAD, using the nlsat dataset \cite{nlsat} of fully existentially quantified problems.  Our results show that the choices made by machine learning are on average superior to both any individual heuristic and to picking a heuristic at random.	score:459
Is machine learning a part of artificial intelligence?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:362
Is machine learning a part of artificial intelligence?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:404
Is machine learning a part of artificial intelligence?	g., sirens, gun fire, etc.).    Machine learning is one promising approach to developing intelligent behavior in robotic materials.  Broadly speaking, machine learning approaches attempt to learn a particular general task, such as identifying a pattern or learning a function, from a set of provided examples.  Typically, the physical response of a material to some stimulus, such as the vibrations generated from an impact, is only well defined for very simple structures (e. g., rectangular planar composite panels).  More complex geometries, such as those found in real world structures, require computationally intensive numerical modeling approaches, such as finite element methods (FEM) \cite{zienkiewicz2005finite}.  Machine learning approaches, in contrast, utilize a set of examples to \emph{learn} how a structure responds to an external stimulus from several sample stimuli.	g., sirens, gun fire, etc.).    Machine learning is one promising approach to developing intelligent behavior in robotic materials.  Broadly speaking, machine learning approaches attempt to learn a particular general task, such as identifying a pattern or learning a function, from a set of provided examples.  Typically, the physical response of a material to some stimulus, such as the vibrations generated from an impact, is only well defined for very simple structures (e. g., rectangular planar composite panels).  More complex geometries, such as those found in real world structures, require computationally intensive numerical modeling approaches, such as finite element methods (FEM) \cite{zienkiewicz2005finite}.  Machine learning approaches, in contrast, utilize a set of examples to \emph{learn} how a structure responds to an external stimulus from several sample stimuli.	score:411
Is machine learning a part of artificial intelligence?	 Machine learning is a thriving part of computer science. There are many efficient approaches to machine learning that do not provide strong theoretical guarantees, and a beautiful general learning theory. Unfortunately, machine learning approaches that give strong theoretical guarantees have not been efficient enough to be applicable.  In this paper we introduce a logical approach to machine learning.  Models are represented by tuples of logical formulas and inputs and outputs are logical structures. We present our framework together with several applications where we evaluate it using SAT and SMT solvers. We argue that this approach to machine learning is particularly suited to bridge the gap between efficiency and theoretical soundness.  We exploit results from descriptive complexity theory to prove strong theoretical guarantees for our approach.	 Machine learning is a thriving part of computer science. There are many efficient approaches to machine learning that do not provide strong theoretical guarantees, and a beautiful general learning theory. Unfortunately, machine learning approaches that give strong theoretical guarantees have not been efficient enough to be applicable.  In this paper we introduce a logical approach to machine learning.  Models are represented by tuples of logical formulas and inputs and outputs are logical structures. We present our framework together with several applications where we evaluate it using SAT and SMT solvers. We argue that this approach to machine learning is particularly suited to bridge the gap between efficiency and theoretical soundness.  We exploit results from descriptive complexity theory to prove strong theoretical guarantees for our approach.	score:420
Is machine learning a part of artificial intelligence?	  We observe that learning with neural networks, either artificial or biological, is almost always done using a bounded-memory algorithm.  In the setting of machine learning, artificial neural networks most often use the Stochastic Gradient Descent (SGD) algorithm, one example of a bounded-memory algorithm.  In neuroscience, biological neural networks, i. e., the nervous system, inherently perform a bounded-memory computation under the accepted assumption that learning should be \emph{biologically plausible}. This places the problem of learning with neural networks in the framework of bounded-memory learning.   In recent years, several works have shown that under memory constraints numerous examples are needed in order to learn certain hypothesis classes \cite{shamir14,raz16,kol16,moshkovitz17,raz17}.	  We observe that learning with neural networks, either artificial or biological, is almost always done using a bounded-memory algorithm.  In the setting of machine learning, artificial neural networks most often use the Stochastic Gradient Descent (SGD) algorithm, one example of a bounded-memory algorithm.  In neuroscience, biological neural networks, i. e., the nervous system, inherently perform a bounded-memory computation under the accepted assumption that learning should be \emph{biologically plausible}. This places the problem of learning with neural networks in the framework of bounded-memory learning.   In recent years, several works have shown that under memory constraints numerous examples are needed in order to learn certain hypothesis classes \cite{shamir14,raz16,kol16,moshkovitz17,raz17}.	score:423
Using NetBeans using SVM classification how to measure training dataset accuracy and testing dataset accuracy?	 For example, we may use area under receiver operating characteristic curve (AUC) as a multivariate performance measure to evaluate the classification performance of SVM. Because SVM class label predictor is trained by minimizing the loss functions over training data points, it cannot be guaranteed to minimize the loss function corresponding to AUC.  Many other multivariate performance measures are also defined to compare a true class label tuple of a data point tuple against its predicted class label tuple, and they can also be used for different machine learning applications. Some examples of the multivariate performance measures are as F-score \cite{Zemmoudj2014371,Gao2014}, precision-recall curve eleven point (PRBEP) \cite{Boyd2013451,Lopes2014322}, and Matthews correlation coefficient  (MCC) \cite{Kumari2015175,Shepperd2015106}.	 For example, we may use area under receiver operating characteristic curve (AUC) as a multivariate performance measure to evaluate the classification performance of SVM. Because SVM class label predictor is trained by minimizing the loss functions over training data points, it cannot be guaranteed to minimize the loss function corresponding to AUC.  Many other multivariate performance measures are also defined to compare a true class label tuple of a data point tuple against its predicted class label tuple, and they can also be used for different machine learning applications. Some examples of the multivariate performance measures are as F-score \cite{Zemmoudj2014371,Gao2014}, precision-recall curve eleven point (PRBEP) \cite{Boyd2013451,Lopes2014322}, and Matthews correlation coefficient  (MCC) \cite{Kumari2015175,Shepperd2015106}.	score:275
Using NetBeans using SVM classification how to measure training dataset accuracy and testing dataset accuracy?	 We prove that for certain distributions, AM regularization can improve both accuracy and adversarial robustness of a classifier. Third, we use synthetic and real data to empirically show that AM regularization can generate support vector machine (SVM) classifiers that strictly dominate (in terms of accuracy and robustness) classifiers computed with or without adversarial training.  Taken together, these results suggest that the phenomenon of adversarial fragility is an issue of overfitting rather than a fundamental issue unique to adversarial attacks.        \subsection{Robust Linear SVM} \label{subsec:litreviewrobustness}  Linear SVM relies upon on maximizing training margin by minimizing the \textit{hinge-loss}, and several methods have been proposed to improve its robustness.	 We conclude by using both synthetic and real data to empirically show that AM regularization can strictly improve both accuracy and robustness for support vector machine's (SVM's), relative to unregularized classifiers and adversarially trained classifiers. 	There is renewed interest in robust learning due to the observed fragility of deep classifiers to imperceptible adversarial corruptions \cite{szegedy2013intriguing,fawzi2015fundamental,dalvi2004adversarial,biggio2017wild}.  Such classifiers are often key elements in a variety of cyber-physical systems (CPS) like self-driving cars, smart homes, or smart grids. Adversarial perturbations on sensors within CPS can have disastrous consequences, and this has been exhibited by several major attacks on mission-critical CPS \cite{abrams2008malicious,langner2011stuxnet,cardenas2008research,ferrag2018security}.	score:287
Using NetBeans using SVM classification how to measure training dataset accuracy and testing dataset accuracy?	 Recently, structured SVM has been used to perform initial phoneme recognition by learning the relationships between the acoustic vector sequence and the phoneme label sequence of the whole utterance jointly rather than on the frame level or from different sets of knowledge sources\cite{tang2010initial}, utilizing the nice properties of SVM\cite{boser1992training} to classify the structured patterns of utterances with maximized margin.	 Recently, structured SVM has been used to perform initial phoneme recognition by learning the relationships between the acoustic vector sequence and the phoneme label sequence of the whole utterance jointly rather than on the frame level or from different sets of knowledge sources\cite{tang2010initial}, utilizing the nice properties of SVM\cite{boser1992training} to classify the structured patterns of utterances with maximized margin.	score:289
Using NetBeans using SVM classification how to measure training dataset accuracy and testing dataset accuracy?	  In this paper, we propose a maximum margin classifier that deals with uncertainty in data input. More specifically, we reformulate the SVM framework such that each training example can be modeled by a multi-dimensional Gaussian distribution described by its mean vector and its covariance matrix -- the latter modeling the uncertainty. We address the classification problem and define a cost function that is the expected value of the classical SVM cost when data samples are drawn from the multi-dimensional Gaussian distributions that form the set of the training examples.	  In this paper, we propose a maximum margin classifier that deals with uncertainty in data input. More specifically, we reformulate the SVM framework such that each training example can be modeled by a multi-dimensional Gaussian distribution described by its mean vector and its covariance matrix -- the latter modeling the uncertainty. We address the classification problem and define a cost function that is the expected value of the classical SVM cost when data samples are drawn from the multi-dimensional Gaussian distributions that form the set of the training examples.	score:298
Using NetBeans using SVM classification how to measure training dataset accuracy and testing dataset accuracy?	 Support Vector Machines (SVMs) were primarily designed for 2-class classification. But they have been extended for N-class classification also based on the requirement of multiclasses in the practical applications. Although  N-class classification using SVM has considerable research attention, getting minimum number of classifiers at the time of training and testing is still a continuing research.  We propose a new algorithm CBTS-SVM (Centroid based Binary Tree Structured SVM) which addresses this issue. In this we build a binary tree of SVM models based on the similarity of the class labels by finding their distance from the corresponding centroids at the root level. The experimental results demonstrates the comparable accuracy for CBTS  with OVO with reasonable gamma and cost values.	 Support Vector Machines (SVMs) were primarily designed for 2-class classification. But they have been extended for N-class classification also based on the requirement of multiclasses in the practical applications. Although  N-class classification using SVM has considerable research attention, getting minimum number of classifiers at the time of training and testing is still a continuing research.  We propose a new algorithm CBTS-SVM (Centroid based Binary Tree Structured SVM) which addresses this issue. In this we build a binary tree of SVM models based on the similarity of the class labels by finding their distance from the corresponding centroids at the root level. The experimental results demonstrates the comparable accuracy for CBTS  with OVO with reasonable gamma and cost values.	score:298
What are the placement statistics of IIIT H 2015 passing-out batch?	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	score:427
What are the placement statistics of IIIT H 2015 passing-out batch?	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	score:427
What are the placement statistics of IIIT H 2015 passing-out batch?	 However, once initiated, dendrite growth is extremely hard to mitigate as pointed out by several studies\cite{diggle1969zinc,monroe2003-dendrite,Monroe2004Effect}. Therefore, it is best to prevent dendrites from initiating to ensure smooth electrodeposition throughout cycling of the battery.  In recent years, high-throughput computational materials design has emerged as a major driver of discovery of novel materials for various applications \cite{curtarolo2013highthroughput,saal2013oqmd}.	 However, once initiated, dendrite growth is extremely hard to mitigate as pointed out by several studies\cite{diggle1969zinc,monroe2003-dendrite,Monroe2004Effect}. Therefore, it is best to prevent dendrites from initiating to ensure smooth electrodeposition throughout cycling of the battery.  In recent years, high-throughput computational materials design has emerged as a major driver of discovery of novel materials for various applications \cite{curtarolo2013highthroughput,saal2013oqmd}.	score:437
What are the placement statistics of IIIT H 2015 passing-out batch?	 However, such environments present a major challenge for current RL algorithms. Environments which require a lot of diversity can be expressed in the framework of RL as having   a ``wide'' stochastic initial state distribution for the underlying Markov decision process. Highly stochastic initial state distributions lead to high-variance policy gradient estimates, which in turn hamper effective learning.  Similarly, diversity and variability can also be incorporated by picking a wide distribution over goals.    In this paper, we explore RL algorithms that are especially well-suited for tasks with a high degree of variability in both initial and goal states. We argue that a large class of practically interesting real-world problems fall into this category, but current RL algorithms are poorly equipped to handle them, as illustrated in our experimental evaluation.	 However, such environments present a major challenge for current RL algorithms. Environments which require a lot of diversity can be expressed in the framework of RL as having   a ``wide'' stochastic initial state distribution for the underlying Markov decision process. Highly stochastic initial state distributions lead to high-variance policy gradient estimates, which in turn hamper effective learning.  Similarly, diversity and variability can also be incorporated by picking a wide distribution over goals.    In this paper, we explore RL algorithms that are especially well-suited for tasks with a high degree of variability in both initial and goal states. We argue that a large class of practically interesting real-world problems fall into this category, but current RL algorithms are poorly equipped to handle them, as illustrated in our experimental evaluation.	score:438
What are the placement statistics of IIIT H 2015 passing-out batch?	 While using variegate types of data and addressing diverse problems, HLR is able to outperform state-of-the-art regression algorithms. \end{enumerate}  The paper is outlined as follows. In Sections \ref{sez:sssmvr} and \ref{sez:sol}, we present our exact optimization for the Huber loss after defining the general semi-supervised setting we consider.  Section \ref{sez:alg} broadly discusses the HLR algorithm. As to prove its versatility, once benchmarked with a state-of-the-art convex solver in Section \ref{sez:synt}, we registered a strong performance when applying HLR to curve fitting and learning with noisy labels (Section \ref{ssez:active}), classical regression problems (Section \ref{sez:altri}) and crowd counting application (Section \ref{sez:CC}). Finally, Section \ref{sez:end} draws the conclusions.	 While using variegate types of data and addressing diverse problems, HLR is able to outperform state-of-the-art regression algorithms. \end{enumerate}  The paper is outlined as follows. In Sections \ref{sez:sssmvr} and \ref{sez:sol}, we present our exact optimization for the Huber loss after defining the general semi-supervised setting we consider.  Section \ref{sez:alg} broadly discusses the HLR algorithm. As to prove its versatility, once benchmarked with a state-of-the-art convex solver in Section \ref{sez:synt}, we registered a strong performance when applying HLR to curve fitting and learning with noisy labels (Section \ref{ssez:active}), classical regression problems (Section \ref{sez:altri}) and crowd counting application (Section \ref{sez:CC}). Finally, Section \ref{sez:end} draws the conclusions.	score:444
What are the placement statistics of NSIT 2015 passing-out batch?	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	score:411
What are the placement statistics of NSIT 2015 passing-out batch?	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	  	Understanding what drives the travel behavior of people is a key research topic for developing effective and efficient intelligent transportation systems that adapt to the travel demand. However, typical approaches focus only on capturing recurrent mobility trends that relate to habitual/routine behaviour \cite{krygsman2004}, and on exploiting short-term correlations with recent observation patterns \cite{moreira2013predicting,Van2015}.  While this type of approaches can be successful for long-term planning applications or for modeling demand in non-eventful areas such as residential neighborhoods, in lively and highly dynamic areas that are prone to the occurrence of multiple special events, such as music concerts, sports games, festivals, parades and protests, these approaches fail to accurately model mobility demand \cite{pereira2015using}.	score:411
What are the placement statistics of NSIT 2015 passing-out batch?	  	With the immense success of deep learning in machine learning and artificial intelligence, there has been significant interest in determining the extent to which it explains learning in biological neural networks. For example, Bengio, et al. (2015) have pointed out several challenges in finding biologically plausible implementations of backpropagation, such as the need for precisely-clocked linear feedback paths that have exact knowledge of the weights and the derivatives involved in the feedforward paths.  Moreover, backpropagation requires output targets, and passes real-valued signals rather than binary-valued spikes between neurons \cite{bengio2015towards,rumelhart1988learning}. Contrastive divergence, on the other hand, trains a binary generative model, but it involves Gibbs sampling to generate data confabulations for the computation of a negative gradient \cite{hinton2002training}.	  	With the immense success of deep learning in machine learning and artificial intelligence, there has been significant interest in determining the extent to which it explains learning in biological neural networks. For example, Bengio, et al. (2015) have pointed out several challenges in finding biologically plausible implementations of backpropagation, such as the need for precisely-clocked linear feedback paths that have exact knowledge of the weights and the derivatives involved in the feedforward paths.  Moreover, backpropagation requires output targets, and passes real-valued signals rather than binary-valued spikes between neurons \cite{bengio2015towards,rumelhart1988learning}. Contrastive divergence, on the other hand, trains a binary generative model, but it involves Gibbs sampling to generate data confabulations for the computation of a negative gradient \cite{hinton2002training}.	score:426
What are the placement statistics of NSIT 2015 passing-out batch?	 Early symmetry breaking is the most conspicuous example of non-convexity.   One important question is what happens after SGD leaves this well-behaved linear subspace. The main text of this article is restricted to experiments that were peer-reviewed prior to ICLR 2014, but the appendix presents additional experiments added after the review process ended.  These experiments show that in some cases SGD does encounter obstacles, such as a ravine that shapes its path, but we never found evidence that local minima or saddle points slowed the SGD trajectory. This suggests that less exotic problems such as poor conditioning and variance in the gradient estimate are the primary difficulties in training neural networks.	 Early symmetry breaking is the most conspicuous example of non-convexity.   One important question is what happens after SGD leaves this well-behaved linear subspace. The main text of this article is restricted to experiments that were peer-reviewed prior to ICLR 2014, but the appendix presents additional experiments added after the review process ended.  These experiments show that in some cases SGD does encounter obstacles, such as a ravine that shapes its path, but we never found evidence that local minima or saddle points slowed the SGD trajectory. This suggests that less exotic problems such as poor conditioning and variance in the gradient estimate are the primary difficulties in training neural networks.	score:427
What are the placement statistics of NSIT 2015 passing-out batch?	 However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware \cite{Liao2012}, and the availability of high-performance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity \cite{Lotte2015,Makeig2012,Wang2015,Jayaram2016}.  This paper focuses on the last challenge, more specifically, how to generalize a BCI algorithm to a new subject, with zero or very little subject-specific calibration data.  Transfer learning (TL) \cite{Pan2010}, which improves learning in a new task by leveraging data or knowledge from other relevant tasks, represents a promising solution to the above challenge.	 However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware \cite{Liao2012}, and the availability of high-performance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity \cite{Lotte2015,Makeig2012,Wang2015,Jayaram2016}.  This paper focuses on the last challenge, more specifically, how to generalize a BCI algorithm to a new subject, with zero or very little subject-specific calibration data.  Transfer learning (TL) \cite{Pan2010}, which improves learning in a new task by leveraging data or knowledge from other relevant tasks, represents a promising solution to the above challenge.	score:430
What are the best algorithms for audio classification?	 We provide experiments on real data in the audio to audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better performance for the alignment.   	The problem of aligning temporal sequences is ubiquitous in applications ranging from bioinformatics \citep{cuturi2007kernel,aach2001aligning,thompson1999balibase} to audio processing \citep{cont2007evaluation,dixon2005match}. The idea is to align two similar time series that have the same global structure but local temporal differences. Most alignments algorithms rely on similarity measures, and having a good metric is crucial, especially in the high-dimensional setting where some features of the signals can be irrelevant to the alignment task.  The goal of this paper is to show how to learn this similarity measure from annotated examples in order to improve the precision of the alignments.    For example, in the context of music information retrieval, alignment is used in two different cases: (1) audio-to-audio alignment and (2) audio-to-score alignment. In the first case, the goal is to match two audio interpretations of the same piece that are potentially different in rythm, whereas audio-to-score alignment focuses on matching an audio signal to a symbolic representation of the score.  In the second case, there are some attempts to learn from annotated data a measure for performing the alignment. \citet{joder2013learning} propose to fit a generative model in that context, and \citet{keshet2007large} learn this measure in a discriminative setting.       Similarly to \citet{keshet2007large}, we use a discriminative loss to learn the measure, but our work focuses on audio-to-audio alignment.	 The goal of this paper is to show how to learn this similarity measure from annotated examples in order to improve the precision of the alignments.    For example, in the context of music information retrieval, alignment is used in two different cases: (1) audio-to-audio alignment and (2) audio-to-score alignment. In the first case, the goal is to match two audio interpretations of the same piece that are potentially different in rythm, whereas audio-to-score alignment focuses on matching an audio signal to a symbolic representation of the score.	score:355
What are the best algorithms for audio classification?	  	Computer audition is the general study of the systems and methods necessary for audio understanding by a machine. In a sense, computer audition concerns itself with the study of designing computers that can ``hear'' as humans do. The goal is a machine that can ``organize what they hear; learn names for recognizable objects, actions, events, places, musical styles, instruments, and speakers; and retrieve sounds by reference to those names. " \cite{lyon}  In this paper, we focus on the first two tasks. Given a musical recording, how can we train a system to identify the instruments that are present? We present an application of deep learning for the task of automatic musical instrument identification in polyphonic music. We show that an end-to-end system using convolutional neural networks trained on raw audio can surpasses traditional MIR models trained using hand-crafted features.	  	Computer audition is the general study of the systems and methods necessary for audio understanding by a machine. In a sense, computer audition concerns itself with the study of designing computers that can ``hear'' as humans do. The goal is a machine that can ``organize what they hear; learn names for recognizable objects, actions, events, places, musical styles, instruments, and speakers; and retrieve sounds by reference to those names. " \cite{lyon}  In this paper, we focus on the first two tasks. Given a musical recording, how can we train a system to identify the instruments that are present? We present an application of deep learning for the task of automatic musical instrument identification in polyphonic music. We show that an end-to-end system using convolutional neural networks trained on raw audio can surpasses traditional MIR models trained using hand-crafted features.	score:356
What are the best algorithms for audio classification?	 Audio Event Detection (AED) aims to recognize sounds within audio and video recordings. AED employs machine learning algorithms commonly trained and tested on annotated datasets. However, available datasets are limited in number of samples and hence it is difficult to model acoustic diversity. Therefore, we propose combining labeled audio from a dataset and unlabeled audio from the web to improve the sound models.  The audio event detectors are trained on the labeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever the detectors recognized any of the known sounds with high confidence, the unlabeled audio was use to re-train the detectors. The performance of the re-trained detectors is compared to the one from the original detectors using the annotated test set.	 Audio Event Detection (AED) aims to recognize sounds within audio and video recordings. AED employs machine learning algorithms commonly trained and tested on annotated datasets. However, available datasets are limited in number of samples and hence it is difficult to model acoustic diversity. Therefore, we propose combining labeled audio from a dataset and unlabeled audio from the web to improve the sound models.  The audio event detectors are trained on the labeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever the detectors recognized any of the known sounds with high confidence, the unlabeled audio was use to re-train the detectors. The performance of the re-trained detectors is compared to the one from the original detectors using the annotated test set.	score:367
What are the best algorithms for audio classification?	 In the context of the Internet of Things (IoT), sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition (AESR) algorithms are most often developed with limited consideration for computational cost, this article seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance {\em as a function of its computational cost}.	 In the context of the Internet of Things (IoT), sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition (AESR) algorithms are most often developed with limited consideration for computational cost, this article seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance {\em as a function of its computational cost}.	score:371
What are the best algorithms for audio classification?	g. human-robot interaction. We propose the EmoRL model that triggers an emotion classification as soon as it gains enough confidence while listening to a person speaking. As a result, we minimize the need for segmenting the audio signal for classification and achieve lower latency as the audio signal is processed incrementally. The method is competitive with the accuracy of a strong baseline model, while allowing much earlier prediction.    	\par Emotions are essential for natural communication between humans and have recently received growing interest in the research community. Dialog agents in human-robot interaction could be improved significantly if they were given the ability to evaluate an emotional state of a person and its dynamics. For instance, if a robot could detect that a person is speaking in an angry way which could be a sign that the robot should adjust its behavior.	g. human-robot interaction. We propose the EmoRL model that triggers an emotion classification as soon as it gains enough confidence while listening to a person speaking. As a result, we minimize the need for segmenting the audio signal for classification and achieve lower latency as the audio signal is processed incrementally. The method is competitive with the accuracy of a strong baseline model, while allowing much earlier prediction.    	\par Emotions are essential for natural communication between humans and have recently received growing interest in the research community. Dialog agents in human-robot interaction could be improved significantly if they were given the ability to evaluate an emotional state of a person and its dynamics. For instance, if a robot could detect that a person is speaking in an angry way which could be a sign that the robot should adjust its behavior.	score:373
Will machine learning help me in quantitative finance?	  Continuing the above example, suppose that in a particular protected community $S$, on average, individuals are financially disadvantaged and are unlikely to repay a loan.  A machine-learning algorithm that aims to optimize the institution's returns might devote resources to learning outside of $S$ -- where there is more opportunity for gains in utility -- and assign a fixed, low probability to all $i \in S$.   Such an algorithm would discriminate against the {\em qualified} members of $S$. If $S$ is an underrepresented subpopulation, this form of discrimination has the potential to amplify $S$'s underrepresentation by refusing to approve members that are capable of repaying the loan.    \subsection{Overview of our contributions} Focusing on such concerns, we investigate a notion we call \emph{multicalibration}.	  Continuing the above example, suppose that in a particular protected community $S$, on average, individuals are financially disadvantaged and are unlikely to repay a loan.  A machine-learning algorithm that aims to optimize the institution's returns might devote resources to learning outside of $S$ -- where there is more opportunity for gains in utility -- and assign a fixed, low probability to all $i \in S$.   Such an algorithm would discriminate against the {\em qualified} members of $S$. If $S$ is an underrepresented subpopulation, this form of discrimination has the potential to amplify $S$'s underrepresentation by refusing to approve members that are capable of repaying the loan.    \subsection{Overview of our contributions} Focusing on such concerns, we investigate a notion we call \emph{multicalibration}.	score:397
Will machine learning help me in quantitative finance?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	 Section~\ref{sec:better} surveys results in quantum learning theory that justify why we expect quantum computation to help in selected learning problems. We proceed in Section~\ref{sec:data} by discussing how to access data with a quantum computer and how these models compare with parallel architectures. We continue by presenting different computational and mathematical techniques, that find widespread application in machine learning, and can be accelerated with a quantum computer.  More specifically, we survey quantum subroutines to speedup linear algebra (Section~\ref{sec:linear}), sampling (Section~\ref{sec:sampling}), and optimisation problems (Section~\ref{sec:opt}). For each section, we discuss the asymptotic scaling of the classical and quantum subroutine and present some learning applications. The following section is dedicated to quantum neural networks (Section~\ref{sec:QNN}).	score:415
Will machine learning help me in quantitative finance?	   It is a fundamental, but still elusive question whether the schemes based on quantum mechanics, in particular on quantum entanglement, can be used for classical information processing and machine learning. Even partial answer to this question would bring important insights to both fields of machine learning and quantum mechanics. In this work, we implement simple numerical experiments, related to pattern/images classification, in which we represent the classifiers by many-qubit quantum states written in the matrix product states (MPS).  Classical machine learning algorithm is applied to these quantum states to learn the classical data. We explicitly show how quantum entanglement (i.e., single-site and bipartite entanglement) can emerge in such represented images. Entanglement characterizes here the importance of data, and such information are practically used to guide the architecture of MPS, and improve the efficiency.	   It is a fundamental, but still elusive question whether the schemes based on quantum mechanics, in particular on quantum entanglement, can be used for classical information processing and machine learning. Even partial answer to this question would bring important insights to both fields of machine learning and quantum mechanics. In this work, we implement simple numerical experiments, related to pattern/images classification, in which we represent the classifiers by many-qubit quantum states written in the matrix product states (MPS).  Classical machine learning algorithm is applied to these quantum states to learn the classical data. We explicitly show how quantum entanglement (i.e., single-site and bipartite entanglement) can emerge in such represented images. Entanglement characterizes here the importance of data, and such information are practically used to guide the architecture of MPS, and improve the efficiency.	score:425
Will machine learning help me in quantitative finance?	 Section~\ref{sec:better} surveys results in quantum learning theory that justify why we expect quantum computation to help in selected learning problems. We proceed in Section~\ref{sec:data} by discussing how to access data with a quantum computer and how these models compare with parallel architectures. We continue by presenting different computational and mathematical techniques, that find widespread application in machine learning, and can be accelerated with a quantum computer.  More specifically, we survey quantum subroutines to speedup linear algebra (Section~\ref{sec:linear}), sampling (Section~\ref{sec:sampling}), and optimisation problems (Section~\ref{sec:opt}). For each section, we discuss the asymptotic scaling of the classical and quantum subroutine and present some learning applications. The following section is dedicated to quantum neural networks (Section~\ref{sec:QNN}).	 Section~\ref{sec:better} surveys results in quantum learning theory that justify why we expect quantum computation to help in selected learning problems. We proceed in Section~\ref{sec:data} by discussing how to access data with a quantum computer and how these models compare with parallel architectures. We continue by presenting different computational and mathematical techniques, that find widespread application in machine learning, and can be accelerated with a quantum computer.  More specifically, we survey quantum subroutines to speedup linear algebra (Section~\ref{sec:linear}), sampling (Section~\ref{sec:sampling}), and optimisation problems (Section~\ref{sec:opt}). For each section, we discuss the asymptotic scaling of the classical and quantum subroutine and present some learning applications. The following section is dedicated to quantum neural networks (Section~\ref{sec:QNN}).	score:425
Will machine learning help me in quantitative finance?	  A number of heuristics have been developed to help with such choices, but the complicated nature of the geometric relationships involved means these are imperfect and can sometimes make poor choices.   We investigate the use of machine learning (specifically support vector machines) to make such choices instead.    Machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data.  In this paper we apply it in two case studies: the first to select between heuristics for choosing a CAD variable ordering; the second to identify when a CAD problem instance would benefit from Gr\"obner Basis preconditioning.  These appear to be the first such applications of machine learning to Symbolic Computation.  We demonstrate in both cases that the machine learned choice outperforms human developed heuristics.	  Each decision can have a large effect on the time and memory requirements of the CAD construction.  We demonstrate that in each case the decision can be made using machine learning, more specifically a Support Vector Machine (SVM) with Radial Basis Function.  Machine learning, an over arching term for tools that allow computers to make decisions that are not explicitly programmed, usually involves the statistical analysis of large quantities of data.   On the surface this sounds at odds with the field of Symbolic Computation which prizes exact correctness rather than inexact numerics or probabilistic tools.  We are able to combine these fields because the choices we use machine learning to make are between different methods that would all produce correct and exact answers, but potentially with very different computational costs.	score:426
Which areas of machine learning are relevant towards quantitative finance?	  Continuing the above example, suppose that in a particular protected community $S$, on average, individuals are financially disadvantaged and are unlikely to repay a loan.  A machine-learning algorithm that aims to optimize the institution's returns might devote resources to learning outside of $S$ -- where there is more opportunity for gains in utility -- and assign a fixed, low probability to all $i \in S$.   Such an algorithm would discriminate against the {\em qualified} members of $S$. If $S$ is an underrepresented subpopulation, this form of discrimination has the potential to amplify $S$'s underrepresentation by refusing to approve members that are capable of repaying the loan.    \subsection{Overview of our contributions} Focusing on such concerns, we investigate a notion we call \emph{multicalibration}.	  Continuing the above example, suppose that in a particular protected community $S$, on average, individuals are financially disadvantaged and are unlikely to repay a loan.  A machine-learning algorithm that aims to optimize the institution's returns might devote resources to learning outside of $S$ -- where there is more opportunity for gains in utility -- and assign a fixed, low probability to all $i \in S$.   Such an algorithm would discriminate against the {\em qualified} members of $S$. If $S$ is an underrepresented subpopulation, this form of discrimination has the potential to amplify $S$'s underrepresentation by refusing to approve members that are capable of repaying the loan.    \subsection{Overview of our contributions} Focusing on such concerns, we investigate a notion we call \emph{multicalibration}.	score:297
Which areas of machine learning are relevant towards quantitative finance?	 Quantum machine learning is expected to be one of the first potential general-purpose applications of near-term quantum devices. A major recent breakthrough in classical machine learning is the notion of generative adversarial training, where the gradients of a discriminator model are used to train a separate generative model. In this work and a companion paper, we extend adversarial training to the quantum domain and show how to construct generative adversarial networks using quantum circuits.  Furthermore, we also show how to compute gradients -- a key element in generative adversarial network training -- using another quantum circuit. We give an example of a simple practical circuit ansatz to parametrize quantum machine learning models and perform a simple numerical experiment to demonstrate that quantum generative adversarial networks can be trained successfully.	 Quantum machine learning is expected to be one of the first potential general-purpose applications of near-term quantum devices. A major recent breakthrough in classical machine learning is the notion of generative adversarial training, where the gradients of a discriminator model are used to train a separate generative model. In this work and a companion paper, we extend adversarial training to the quantum domain and show how to construct generative adversarial networks using quantum circuits.  Furthermore, we also show how to compute gradients -- a key element in generative adversarial network training -- using another quantum circuit. We give an example of a simple practical circuit ansatz to parametrize quantum machine learning models and perform a simple numerical experiment to demonstrate that quantum generative adversarial networks can be trained successfully.	score:329
Which areas of machine learning are relevant towards quantitative finance?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:330
Which areas of machine learning are relevant towards quantitative finance?	 The experimentation shows that the resulting pipeline is effective in making a profitable trade. 	Machine learning application in finance is a challenging problem owing to low signal to noise ratio. Moreover, domain expertise is essential for engineering features which assist in solving an appropriate classification or regression problem. Most prior work in this area concentrates on using popular ML techniques like SVM \cite{huang2005forecasting}, \cite{tay2001application}, \cite{kim2003financial} and neural networks \cite{kaastra1996designing}, \cite{gately1995neural}, \cite{trippi1992neural}  coupled with rigorously designed features, and the general area of focus is financial time series forecasting.	 The experimentation shows that the resulting pipeline is effective in making a profitable trade. 	Machine learning application in finance is a challenging problem owing to low signal to noise ratio. Moreover, domain expertise is essential for engineering features which assist in solving an appropriate classification or regression problem. Most prior work in this area concentrates on using popular ML techniques like SVM \cite{huang2005forecasting}, \cite{tay2001application}, \cite{kim2003financial} and neural networks \cite{kaastra1996designing}, \cite{gately1995neural}, \cite{trippi1992neural}  coupled with rigorously designed features, and the general area of focus is financial time series forecasting.	score:345
Which areas of machine learning are relevant towards quantitative finance?	  Quantum machine learning has received significant attention in recent years, and promising progress has been made in the development of quantum algorithms to speed up traditional machine learning tasks. In this work, however, we focus on investigating the information-theoretic upper bounds of sample complexity---how many training  samples are sufficient to predict the future behaviour of an unknown target function.  This kind of problem is, arguably, one of the most fundamental problems in statistical learning theory and the bounds for practical settings can be completely characterised by a simple measure of complexity.  Our main result in the paper is that, for learning an unknown quantum measurement, the upper bound, given by the fat-shattering dimension, is linearly proportional to the dimension of the underlying Hilbert space.	 However, it requires certain generalisation of current theory of machine learning to accommodate the operator-valued inputs and/or outputs.  We term  this line of research \emph{Quantum Statistical Learning}\footnote{Our catalogue of quantum ML is different from the learning class $\mathcal{L}^\mathit{context}_\mathit{goal}$, where the subscript ``\emph{goal}'' refers to the learning goal and the superscript ``\emph{context}'' refers to the training data set and/or the learner's abilities, introduced by A\"{i}meur, Brassard, and Gambs \cite{ABG06}.  According to the authors, $\mathcal{L}^\mathit{c}_\mathit{c}$ corresponds to pure classical ML tasks. When the learner can access to a quantum computer to accelerate the classical ML problems, it belongs to  the learning class $\mathcal{L}^\mathit{q}_\mathit{c}$.} \cite{ABG06, Aar07, Gam08, GK10, BCD+10, BDP+11, GLF+10, FGL+12, SCM+12, SGA14, LB14, BS13, AKK14, SSP14a, SSP14d, MBW10, BBA14, CHB+14, MW14}.	score:348
How much statistics and probability should I know to dive into machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:302
How much statistics and probability should I know to dive into machine learning?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:304
How much statistics and probability should I know to dive into machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:310
How much statistics and probability should I know to dive into machine learning?	 This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.  We discuss the use of the scikit-learn toolkit as it is a reference machine learning tool and has and a variety of algorithms that is matched by few packages, but also because it is implemented in Python, and thus dovetails nicely in the rich neuroimaging Python ecosystem.  This paper explores a few applications of statistical learning to resolve common neuroimaging needs, detailing the corresponding code, the choice of the methods, and the underlying assumptions.	 This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.  We discuss the use of the scikit-learn toolkit as it is a reference machine learning tool and has and a variety of algorithms that is matched by few packages, but also because it is implemented in Python, and thus dovetails nicely in the rich neuroimaging Python ecosystem.  This paper explores a few applications of statistical learning to resolve common neuroimaging needs, detailing the corresponding code, the choice of the methods, and the underlying assumptions.	score:315
How much statistics and probability should I know to dive into machine learning?	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	score:322
How do I learn statistics and probability for machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:342
How do I learn statistics and probability for machine learning?	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:348
How do I learn statistics and probability for machine learning?	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	score:350
How do I learn statistics and probability for machine learning?	  A number of heuristics have been developed to help with such choices, but the complicated nature of the geometric relationships involved means these are imperfect and can sometimes make poor choices.   We investigate the use of machine learning (specifically support vector machines) to make such choices instead.    Machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data.  In this paper we apply it in two case studies: the first to select between heuristics for choosing a CAD variable ordering; the second to identify when a CAD problem instance would benefit from Gr\"obner Basis preconditioning.  These appear to be the first such applications of machine learning to Symbolic Computation.  We demonstrate in both cases that the machine learned choice outperforms human developed heuristics.	  A number of heuristics have been developed to help with such choices, but the complicated nature of the geometric relationships involved means these are imperfect and can sometimes make poor choices.   We investigate the use of machine learning (specifically support vector machines) to make such choices instead.    Machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data.  In this paper we apply it in two case studies: the first to select between heuristics for choosing a CAD variable ordering; the second to identify when a CAD problem instance would benefit from Gr\"obner Basis preconditioning.  These appear to be the first such applications of machine learning to Symbolic Computation.  We demonstrate in both cases that the machine learned choice outperforms human developed heuristics.	score:353
How do I learn statistics and probability for machine learning?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:356
Given a matrix of numbers, how do I search if a grid of numbers with x rows and y columns are present within the matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	  In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	score:294
Given a matrix of numbers, how do I search if a grid of numbers with x rows and y columns are present within the matrix?	  In many applications, we have to recover a matrix from only a small number of observed entries, for example collaborative filtering for recommender systems. This problem is often called matrix completion, where missing entries or outliers are presented at arbitrary location in the measurement matrix. Matrix completion has been used in a wide range of problems such as collaborative filtering \cite{candes:emc, chen:lrmr}, structure-from-motion \cite{eriksson:lrma, zheng:rma}, click prediction \cite{yu:cp}, tag recommendation \cite{wang:at}, and face reconstruction \cite{meng:cwm}.	  In many applications, we have to recover a matrix from only a small number of observed entries, for example collaborative filtering for recommender systems. This problem is often called matrix completion, where missing entries or outliers are presented at arbitrary location in the measurement matrix. Matrix completion has been used in a wide range of problems such as collaborative filtering \cite{candes:emc, chen:lrmr}, structure-from-motion \cite{eriksson:lrma, zheng:rma}, click prediction \cite{yu:cp}, tag recommendation \cite{wang:at}, and face reconstruction \cite{meng:cwm}.	score:298
Given a matrix of numbers, how do I search if a grid of numbers with x rows and y columns are present within the matrix?	 Matrix $\mathbf{A}$ is often called sensitivity (or exposure) matrix and $\mathbf{B}$ is called factor matrix with the linear combinations $\mathbf{B}^{T}\mathbf{x}_{t}$ called latent factors. The ``low-rank structure'' formed by $\mathbf{A}\mathbf{B}^{T}$ essentially reduces the parameter dimension and improves explanatory ability of the model. The RRR model is widely used in situations when the response variables are believed to depend on a few linear combinations of the predictor variables, or when such linear combinations are of special interest.    The RRR model has been used in many signal processing problems, e.g., array signal processing \cite{VibergStoicaOttersten1997}, state space modeling \cite{StoicaJansson2000}, filter design \cite{MantonHua2001}, channel estimation and equalization for wireless communication \cite{LindskogTidestav1999,HuaNikpourStoica2001,NicoliSpagnolini2005}, etc.	 Matrix $\mathbf{A}$ is often called sensitivity (or exposure) matrix and $\mathbf{B}$ is called factor matrix with the linear combinations $\mathbf{B}^{T}\mathbf{x}_{t}$ called latent factors. The ``low-rank structure'' formed by $\mathbf{A}\mathbf{B}^{T}$ essentially reduces the parameter dimension and improves explanatory ability of the model. The RRR model is widely used in situations when the response variables are believed to depend on a few linear combinations of the predictor variables, or when such linear combinations are of special interest.    The RRR model has been used in many signal processing problems, e.g., array signal processing \cite{VibergStoicaOttersten1997}, state space modeling \cite{StoicaJansson2000}, filter design \cite{MantonHua2001}, channel estimation and equalization for wireless communication \cite{LindskogTidestav1999,HuaNikpourStoica2001,NicoliSpagnolini2005}, etc.	score:299
Given a matrix of numbers, how do I search if a grid of numbers with x rows and y columns are present within the matrix?	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.	score:301
Given a matrix of numbers, how do I search if a grid of numbers with x rows and y columns are present within the matrix?	 This paper describes a new approach, based on linear programming, for computing nonnegative matrix factorizations (NMFs).  The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix $\mtx{X}$, the algorithm identifies a matrix $\mtx{C}$ that satisfies $\mtx{X} \approx \mtx{CX}$ and some linear constraints.   The constraints are chosen to ensure that the matrix $\mtx{C}$ selects features; these features can then be used to find a low-rank NMF of $\mtx{X}$.  A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method extends to more general noise models and leads to efficient, scalable algorithms.	 This paper describes a new approach, based on linear programming, for computing nonnegative matrix factorizations (NMFs).  The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix $\mtx{X}$, the algorithm identifies a matrix $\mtx{C}$ that satisfies $\mtx{X} \approx \mtx{CX}$ and some linear constraints.   The constraints are chosen to ensure that the matrix $\mtx{C}$ selects features; these features can then be used to find a low-rank NMF of $\mtx{X}$.  A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method extends to more general noise models and leads to efficient, scalable algorithms.	score:304
What's the best algorithm for log event classification? (e.g. Bayesian network, Naive bayes, Neural network, ..)	  The effectiveness of the state-of-the-art classification algorithms relies on logical theory of sets, theory of probability and the algebra of vector spaces.  For example, the most straightforward technique is called Naive Bayes, which considers objects (e.g. documents) as elements of sets and applies basic probability measures to these sets for selecting classes.   Another effective classification technique called Support Vector Machines considers objects as points of a multi-dimensional space and aims to select subspaces as classes. However, an effective combination of techniques stemming from different theories is still missing, although it has been investigated in IR since the book on the Geometry of IR by \cite{vanRijsbergen79a}.	  The effectiveness of the state-of-the-art classification algorithms relies on logical theory of sets, theory of probability and the algebra of vector spaces.  For example, the most straightforward technique is called Naive Bayes, which considers objects (e.g. documents) as elements of sets and applies basic probability measures to these sets for selecting classes.   Another effective classification technique called Support Vector Machines considers objects as points of a multi-dimensional space and aims to select subspaces as classes. However, an effective combination of techniques stemming from different theories is still missing, although it has been investigated in IR since the book on the Geometry of IR by \cite{vanRijsbergen79a}.	score:290
What's the best algorithm for log event classification? (e.g. Bayesian network, Naive bayes, Neural network, ..)	 What's more, a Bayesian network provides a very flexible framework to fuse different types of data and prior knowledge together to derive a synthesized network.  To achieve more robust and proper results in Bayesian structure learning, it is preferable to integrate over all possible structure models by using Bayesian model averaging. However, with the number of network variables growing, the enumeration of all possible structures becomes intractable and impractical, for there are overall $O(n! 2^{\binom{n}{2}})$ possible structures given \emph{n} network variables\cite{Robinson1973}. In short, structure learning by using model averaging is NP-hard even when the maximum parents number of network variable is bound to certain constant value \emph{k}\cite{DBLP:journals/ml/HeckermanGC95}. However, in real applications, ranging from casuality network to Protein-Protein-Interaction network, the scales are much larger than traditional structure learning by using model averaging could support.	   The motivation for this paper is to apply Bayesian structure learning using Model Averaging in large-scale networks. Currently, Bayesian model averaging algorithm is applicable to networks with only tens of variables, restrained by its super-exponential complexity. We present a novel framework, called \emph{LSBN}(Large-Scale Bayesian Network), making it possible to handle networks with infinite size by following the principle of divide-and-conquer.   The method of \emph{LSBN} comprises three steps. In general, \emph{LSBN} first performs the partition by using a second-order partition strategy, which achieves more robust results. \emph{LSBN} conducts sampling and structure learning within each overlapping community after the community is isolated from other variables by Markov Blanket. Finally \emph{LSBN} employs an efficient algorithm, to merge structures of overlapping communities into a whole.	score:302
What's the best algorithm for log event classification? (e.g. Bayesian network, Naive bayes, Neural network, ..)	 We present a novel hybrid algorithm for Bayesian network structure learning, called H2PC. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. The algorithm is based on divide-and-conquer constraint-based subroutines to learn the local structure around a target variable.   We conduct two series of experimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning. First, we use eight well-known Bayesian network benchmarks with various data sizes to assess the quality of the learned structure returned by the algorithms.	 We present a novel hybrid algorithm for Bayesian network structure learning, called H2PC. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. The algorithm is based on divide-and-conquer constraint-based subroutines to learn the local structure around a target variable.   We conduct two series of experimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning. First, we use eight well-known Bayesian network benchmarks with various data sizes to assess the quality of the learned structure returned by the algorithms.	score:306
What's the best algorithm for log event classification? (e.g. Bayesian network, Naive bayes, Neural network, ..)	 In this paper, we present {\it regularized Bayesian inference} (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks whose Bayesian formulation results in hybrid chain graph models.	 When the regularization term is convex and induced from a linear operator (e.g., expectation) of the posterior distributions, RegBayes can be solved with convex analysis theory.  By allowing direct regularization over posterior distributions, RegBayes provides a significant source of extra flexibility for post-data posterior inference, which applies to both parametric and nonparametric Bayesian learning (see the remarks after the main Theorem~\ref{lemma:RegBayes}).	score:311
What's the best algorithm for log event classification? (e.g. Bayesian network, Naive bayes, Neural network, ..)	 We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization risk bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an \iid~distribution.   Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.   	Since its early beginning \citep{mcallester-99, shawetaylor-97}, the PAC-Bayesian theory claims to provide ``PAC guarantees to \emph{Bayesian} algorithms'' (\citet{mcallester-99}).  However, despite the amount of work dedicated to this statistical learning theory---many authors improved the initial results \citep{catoni-07,lever-13,mcallester-03a,seeger-thesis,tolstikhin-13}   and/or generalized them for various machine learning setups \citep{graal-aistats14,graal-icml16-dalc,grunwald&mehta-16,langford-02,pentina-14,seldin-10,seldin-11,seldin-12}  ---it is   mostly used as a \emph{frequentist} method.	 By doing so, we provide an alternative explanation for the Bayesian Occam's razor criteria  \citep{jeffreys-92,mackay-92} in the  context of model selection, expressed as the complexity-accuracy trade-off appearing in most PAC-Bayesian results. In Section~\ref{sec:more-bounds}, we extend PAC-Bayes theorems to regression problems with unbounded loss, adapted to the negative log-likelihood loss function. Finally, we study the Bayesian model selection from a PAC-Bayesian perspective (Section~\ref{sec:bayesian_model_comp}), and illustrate our finding on classical Bayesian regression tasks (Section~\ref{section:linreg}).	score:317
How many seats will JBIMS offer for 2017-2019 batch?	 For example, a particular customer may be contemplating switching to a competitor. What should the company do to retain this customer? There could be multiple correct actions. A sales person could write the customer and offer incentives for staying, or the CEO could call the customer to express how important he is to them. Of course, each action incurs a certain cost.  Having the CEO call a customer is more expensive than having a help-desk employee write the customer an e-mail, but both are viable solutions. However, if that customer happens to be the largest source of revenue for that company, then sending a generic e-mail may not be the best course of action to take. It may be the case where calling would only be correct \emph{if} the CEO made the phone call but writing an e-mail would be correct \emph{if} it came from a different person.	 For example, a particular customer may be contemplating switching to a competitor. What should the company do to retain this customer? There could be multiple correct actions. A sales person could write the customer and offer incentives for staying, or the CEO could call the customer to express how important he is to them. Of course, each action incurs a certain cost.  Having the CEO call a customer is more expensive than having a help-desk employee write the customer an e-mail, but both are viable solutions. However, if that customer happens to be the largest source of revenue for that company, then sending a generic e-mail may not be the best course of action to take. It may be the case where calling would only be correct \emph{if} the CEO made the phone call but writing an e-mail would be correct \emph{if} it came from a different person.	score:410
How many seats will JBIMS offer for 2017-2019 batch?	 Second, it is more \textit{well-supported theoretically}, as its one-step optimality justification contrasts with the heuristic justification offered in \citet{williams2000sequential,groot2010bayesian} and \citet{swersky2013multi}. (\citealt{Xie:2012} is one-step Bayes-optimal for the special case of \eqref{eq:goal2} that it considers.)  Also none of these previous methods come with a proof of consistency, and \citet{williams2000sequential} may fail to be consistent if a poor tie-breaking rule is chosen as we note below.  Third, it provides \textit{better empirical performance} in problems with noisy evaluations or when the integrand varies smoothly in the integrated variables, and performs comparably in other problems. We discuss this previous literature in more detail below.  This paper significantly extends the conference paper \cite{MCQMC}, where an early version of the BQO method was referred to as Stratified Bayesian Optimization (SBO).	 Second, it is more \textit{well-supported theoretically}, as its one-step optimality justification contrasts with the heuristic justification offered in \citet{williams2000sequential,groot2010bayesian} and \citet{swersky2013multi}. (\citealt{Xie:2012} is one-step Bayes-optimal for the special case of \eqref{eq:goal2} that it considers.)  Also none of these previous methods come with a proof of consistency, and \citet{williams2000sequential} may fail to be consistent if a poor tie-breaking rule is chosen as we note below.  Third, it provides \textit{better empirical performance} in problems with noisy evaluations or when the integrand varies smoothly in the integrated variables, and performs comparably in other problems. We discuss this previous literature in more detail below.  This paper significantly extends the conference paper \cite{MCQMC}, where an early version of the BQO method was referred to as Stratified Bayesian Optimization (SBO).	score:418
How many seats will JBIMS offer for 2017-2019 batch?	 However, in many circumstances, patients are (or can be) recruited in cohorts, the outcomes for patients in previous cohorts can be observed when recruiting a new cohort, and the population contains identifiable subgroups for which differences in effects might be expected.  (For example, different effects of treatment might be expected for patients with different genetic mutations \cite{moss2015efficacy}; see \cite{hebert1999multicenter} and Table \ref{table:rct_example} for additional examples. )  In such situations, the information learned from previous cohorts can be used in recruiting and allocating patients in the new cohort: the optimal policy should not necessarily recruit the same number of patients to each identifiable subgroup or allocate equal numbers of patients within a subgroup to the treatment  and the control.  We illustrate this point in the Experiments.	 However, in many circumstances, patients are (or can be) recruited in cohorts, the outcomes for patients in previous cohorts can be observed when recruiting a new cohort, and the population contains identifiable subgroups for which differences in effects might be expected.  (For example, different effects of treatment might be expected for patients with different genetic mutations \cite{moss2015efficacy}; see \cite{hebert1999multicenter} and Table \ref{table:rct_example} for additional examples. )  In such situations, the information learned from previous cohorts can be used in recruiting and allocating patients in the new cohort: the optimal policy should not necessarily recruit the same number of patients to each identifiable subgroup or allocate equal numbers of patients within a subgroup to the treatment  and the control.  We illustrate this point in the Experiments.	score:421
How many seats will JBIMS offer for 2017-2019 batch?	 An important application appears when we focus on medical scenarios, particularly in the field of psychiatry. CPD can be used to detect behavioral changes, which in patients with prevalent chronic disorders (e.\,g., schizophrenia or chronic depression) may be signals of future relapses \citep{barnett2018,berrouiguet2018}. To monitor psychiatric patients' symptoms there exist several alternatives, where the smartphone-based monitoring stands out \citep{Strickland14}.  Concretely, the ubiquitous presence of smartphones and the large amount of data gathered by these devices have opened new opportunities in this area. Among others, smartphones observations range from inertial sensors measurements or GPS information to timestamps and duration of calls or app usage log. However, all these monitoring traces pose some challenges that have not been considered in detail within the framework of change-point detection.	 An important application appears when we focus on medical scenarios, particularly in the field of psychiatry. CPD can be used to detect behavioral changes, which in patients with prevalent chronic disorders (e.\,g., schizophrenia or chronic depression) may be signals of future relapses \citep{barnett2018,berrouiguet2018}. To monitor psychiatric patients' symptoms there exist several alternatives, where the smartphone-based monitoring stands out \citep{Strickland14}.  Concretely, the ubiquitous presence of smartphones and the large amount of data gathered by these devices have opened new opportunities in this area. Among others, smartphones observations range from inertial sensors measurements or GPS information to timestamps and duration of calls or app usage log. However, all these monitoring traces pose some challenges that have not been considered in detail within the framework of change-point detection.	score:423
How many seats will JBIMS offer for 2017-2019 batch?	  This paper specifically analyzes how well such models can predict the next action given a context of previous actions the student has taken. The purpose of such analysis would be to eventually create a system whereby an automated recommender could query the model to provide meaningful guidance on what action the student can take next. The next action in many cases may be the next resource perscribed by the course but in other cases it may be a recommendation to consult a resource that is back in a previous lesson or enrichment material that is buried in a corner of the courseware unknown to the student.	  This paper specifically analyzes how well such models can predict the next action given a context of previous actions the student has taken. The purpose of such analysis would be to eventually create a system whereby an automated recommender could query the model to provide meaningful guidance on what action the student can take next. The next action in many cases may be the next resource perscribed by the course but in other cases it may be a recommendation to consult a resource that is back in a previous lesson or enrichment material that is buried in a corner of the courseware unknown to the student.	score:423
Does a transition matrix have to be square?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	}   In this paper, we extend the theory of low-rank matrix completion to a collection of multiple and heterogeneous matrices. We first consider general matrix completion setting where we assume that for each matrix its  entries are sampled from natural exponential distributions~\citep{lehmCase98}. In this setting, we may have Gaussian distribution for continuous data; Bernoulli for binary data; Poisson for count-data, etc.  In a second part, we relax the assumption of exponential family distribution for the noise and we do not assume any specific model for the observations.  This approach is more popular and widely used in machine learning.  The proposed estimation procedure is based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix.	score:396
Does a transition matrix have to be square?	    Orthogonal matrix has shown advantages in training Recurrent Neural Networks (RNNs), but such matrix is limited to be square for the hidden-to-hidden transformation in RNNs. In this paper, we generalize such square orthogonal matrix to  orthogonal rectangular matrix and formulating this problem in feed-forward Neural Networks (FNNs) as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM).      We show that the rectangular orthogonal matrix can stabilize the distribution of network activations and regularize FNNs.     We also propose a novel orthogonal weight normalization method to solve OMDSM. Particularly, it constructs orthogonal transformation over proxy parameters to ensure the weight matrix is orthogonal     and back-propagates gradient information through the transformation during training.	  Orthogonal matrix has been actively explored in Recurrent Neural Networks (RNNs)~\cite{2016_CoRR_Dorobantu,2016_ICML_Arjovsky,2016_NIPS_Wisdom,2017_ICML_Eugene,2017_ICML_Mhammedi}. It helps to avoiding gradient vanishing and explosion problem in RNNs due to its energy preservation property~\cite{2016_CoRR_Dorobantu}. However, the orthogonal matrix here is limited to be square for the hidden-to-hidden transformation in RNNs.  More general setting of learning \emph{orthogonal rectangular matrix} is barely studied in DNNs~\cite{2017_Corr_Harandi}, especially in deep Convolutional Neural Networks (CNNs)~\cite{2016_Corr_Ozay}. We formulate such a problem as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM), due to the fact that  the weight matrix with orthogonality constraint in each layer is an embedded Stiefel Manifold \cite{2008_Book_Absil} and the weight matrix in certain layer is affected by those in preceding layers in DNNs.	score:402
Does a transition matrix have to be square?	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	score:415
Does a transition matrix have to be square?	  Predicting unobserved entries of a partially observed matrix has found wide applicability in several areas, such as recommender systems, computational biology, and computer vision. Many scalable methods with rigorous theoretical guarantees have been developed for algorithms where the matrix is factored into low-rank components, and embeddings are learned for the row and column entities.  While there has been recent research on incorporating explicit side information in the low-rank matrix factorization setting, often implicit information can be gleaned from the data, via higher order interactions among entities. Such implicit information is especially useful in cases where the data is very sparse, as is often the case in real world datasets.	  Predicting unobserved entries of a partially observed matrix has found wide applicability in several areas, such as recommender systems, computational biology, and computer vision. Many scalable methods with rigorous theoretical guarantees have been developed for algorithms where the matrix is factored into low-rank components, and embeddings are learned for the row and column entities.  While there has been recent research on incorporating explicit side information in the low-rank matrix factorization setting, often implicit information can be gleaned from the data, via higher order interactions among entities. Such implicit information is especially useful in cases where the data is very sparse, as is often the case in real world datasets.	score:420
Does a transition matrix have to be square?	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	score:437
Does a transition matrix have to be square? Why?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	}   In this paper, we extend the theory of low-rank matrix completion to a collection of multiple and heterogeneous matrices. We first consider general matrix completion setting where we assume that for each matrix its  entries are sampled from natural exponential distributions~\citep{lehmCase98}. In this setting, we may have Gaussian distribution for continuous data; Bernoulli for binary data; Poisson for count-data, etc.  In a second part, we relax the assumption of exponential family distribution for the noise and we do not assume any specific model for the observations.  This approach is more popular and widely used in machine learning.  The proposed estimation procedure is based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix.	score:392
Does a transition matrix have to be square? Why?	    Orthogonal matrix has shown advantages in training Recurrent Neural Networks (RNNs), but such matrix is limited to be square for the hidden-to-hidden transformation in RNNs. In this paper, we generalize such square orthogonal matrix to  orthogonal rectangular matrix and formulating this problem in feed-forward Neural Networks (FNNs) as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM).      We show that the rectangular orthogonal matrix can stabilize the distribution of network activations and regularize FNNs.     We also propose a novel orthogonal weight normalization method to solve OMDSM. Particularly, it constructs orthogonal transformation over proxy parameters to ensure the weight matrix is orthogonal     and back-propagates gradient information through the transformation during training.	  Orthogonal matrix has been actively explored in Recurrent Neural Networks (RNNs)~\cite{2016_CoRR_Dorobantu,2016_ICML_Arjovsky,2016_NIPS_Wisdom,2017_ICML_Eugene,2017_ICML_Mhammedi}. It helps to avoiding gradient vanishing and explosion problem in RNNs due to its energy preservation property~\cite{2016_CoRR_Dorobantu}. However, the orthogonal matrix here is limited to be square for the hidden-to-hidden transformation in RNNs.  More general setting of learning \emph{orthogonal rectangular matrix} is barely studied in DNNs~\cite{2017_Corr_Harandi}, especially in deep Convolutional Neural Networks (CNNs)~\cite{2016_Corr_Ozay}. We formulate such a problem as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM), due to the fact that  the weight matrix with orthogonality constraint in each layer is an embedded Stiefel Manifold \cite{2008_Book_Absil} and the weight matrix in certain layer is affected by those in preceding layers in DNNs.	score:411
Does a transition matrix have to be square? Why?	 Intuitively, the tensor completion problem could be solved with matrix completion algorithms by downgrading the problem into a matrix level, typically by either slicing a tensor into multiple small matrices or unfolding it into one big matrix. However, several problems distinguish tensor completion from being treated as a straightforward extension of the matrix completion problem.  First, it has been shown that matrix completion algorithms may break the multi-way structure of a tensor and lose the redundancy among modes to improve the imputation accuracy~\cite{signoretto2011tensor}. While many tensor-based algorithms directly build upon matrix completion algorithms~\cite{mu2014square,xu2013parallel}, their key focus is out of the context of matrix level, i.	 Intuitively, the tensor completion problem could be solved with matrix completion algorithms by downgrading the problem into a matrix level, typically by either slicing a tensor into multiple small matrices or unfolding it into one big matrix. However, several problems distinguish tensor completion from being treated as a straightforward extension of the matrix completion problem.  First, it has been shown that matrix completion algorithms may break the multi-way structure of a tensor and lose the redundancy among modes to improve the imputation accuracy~\cite{signoretto2011tensor}. While many tensor-based algorithms directly build upon matrix completion algorithms~\cite{mu2014square,xu2013parallel}, their key focus is out of the context of matrix level, i.	score:417
Does a transition matrix have to be square? Why?	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	score:424
Does a transition matrix have to be square? Why?	  Predicting unobserved entries of a partially observed matrix has found wide applicability in several areas, such as recommender systems, computational biology, and computer vision. Many scalable methods with rigorous theoretical guarantees have been developed for algorithms where the matrix is factored into low-rank components, and embeddings are learned for the row and column entities.  While there has been recent research on incorporating explicit side information in the low-rank matrix factorization setting, often implicit information can be gleaned from the data, via higher order interactions among entities. Such implicit information is especially useful in cases where the data is very sparse, as is often the case in real world datasets.	  Predicting unobserved entries of a partially observed matrix has found wide applicability in several areas, such as recommender systems, computational biology, and computer vision. Many scalable methods with rigorous theoretical guarantees have been developed for algorithms where the matrix is factored into low-rank components, and embeddings are learned for the row and column entities.  While there has been recent research on incorporating explicit side information in the low-rank matrix factorization setting, often implicit information can be gleaned from the data, via higher order interactions among entities. Such implicit information is especially useful in cases where the data is very sparse, as is often the case in real world datasets.	score:431
What are some really interesting machine learning projects for beginners?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:266
What are some really interesting machine learning projects for beginners?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:314
What are some really interesting machine learning projects for beginners?	 With this understanding, we naturally simplify Ladder by eliminating its redundant computation and explain how to choose the parameter and interpret it. We also prove that the sample complexity is cubic to the desired precision of the leaderboard. 	Machine learning competitions have been a popular platform for young students to practice their knowledge, for scientists to apply their expertise, and for industries to solve their data mining problems.  For instance, the Internet streaming media company Netflix held the Netflix Prize competition in 2006, to find a better program to predict user preferences. Kaggle, an online platform, hosts regularly competitions since 2010.  These competitions are usually prediction problems. The participant is given the independent variable $X$, and then he is required to predict the dependent variable $Y$.	 With this understanding, we naturally simplify Ladder by eliminating its redundant computation and explain how to choose the parameter and interpret it. We also prove that the sample complexity is cubic to the desired precision of the leaderboard. 	Machine learning competitions have been a popular platform for young students to practice their knowledge, for scientists to apply their expertise, and for industries to solve their data mining problems.  For instance, the Internet streaming media company Netflix held the Netflix Prize competition in 2006, to find a better program to predict user preferences. Kaggle, an online platform, hosts regularly competitions since 2010.  These competitions are usually prediction problems. The participant is given the independent variable $X$, and then he is required to predict the dependent variable $Y$.	score:321
What are some really interesting machine learning projects for beginners?	  	As machine learning becomes a crucial component of an ever-growing number of user-facing applications, \emph{interpretable machine learning} has become an increasingly important area of research for a number of reasons.  First, as humans are the ones who train, deploy, and often use the predictions of machine learning models in the real world, it is of utmost importance for them to be able to trust the model.    Apart from indicators such as accuracy on sample instances, a user's trust is directly impacted by how much they can understand and predict the model's behavior, as opposed to treating it as a black box. Second, a system designer who understands why their model is making predictions is certainly better equipped to improve it by means of feature engineering, parameter tuning, or even by replacing the model with a different one.	  Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to \emph{trust} and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest.  In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency.  Even when they are not accurate, they may still be preferred when interpretability is of paramount importance.   However, restricting machine learning to interpretable models is often a severe limitation.  In this paper we argue for explaining machine learning predictions using \emph{model-agnostic} approaches.	score:339
What are some really interesting machine learning projects for beginners?	 Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.   To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.	 Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.   To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.	score:342
What are machine learning projects for beginners?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:348
What are machine learning projects for beginners?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:419
What are machine learning projects for beginners?	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	score:423
What are machine learning projects for beginners?	 Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.   To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.	 Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.   To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.	score:423
What are machine learning projects for beginners?	  Each decision can have a large effect on the time and memory requirements of the CAD construction.  We demonstrate that in each case the decision can be made using machine learning, more specifically a Support Vector Machine (SVM) with Radial Basis Function.  Machine learning, an over arching term for tools that allow computers to make decisions that are not explicitly programmed, usually involves the statistical analysis of large quantities of data.   On the surface this sounds at odds with the field of Symbolic Computation which prizes exact correctness rather than inexact numerics or probabilistic tools.  We are able to combine these fields because the choices we use machine learning to make are between different methods that would all produce correct and exact answers, but potentially with very different computational costs.	  Each decision can have a large effect on the time and memory requirements of the CAD construction.  We demonstrate that in each case the decision can be made using machine learning, more specifically a Support Vector Machine (SVM) with Radial Basis Function.  Machine learning, an over arching term for tools that allow computers to make decisions that are not explicitly programmed, usually involves the statistical analysis of large quantities of data.   On the surface this sounds at odds with the field of Symbolic Computation which prizes exact correctness rather than inexact numerics or probabilistic tools.  We are able to combine these fields because the choices we use machine learning to make are between different methods that would all produce correct and exact answers, but potentially with very different computational costs.	score:426
Can NLP algorithms help reduce duplicate questions on Quora?	 This raises the important question if there are fast algorithms for matrix completion that come with guarantees on the required sample size comparable to those achieved by nuclear norm minimization. In this work we make progress on this problem by proving strong sample complexity bounds for alternating minimization. Along the way our work helps to give a theoretical justification and understanding for why alternating minimization works.   \subsection{Our results}  We begin with our result on the \emph{exact} matrix completion problem where the goal is to recover an unknown rank~$k$ matrix~$M$ from a subsample~$\Omega$ of its entries where each entry is included independently with probability~$p.$ Here and in the following we will always assume that~$M=U\Lambda U^\trans$ is a  symmetric $n\times n$ matrix with singular values $\sigma_1\ge\dots\ge \sigma_k.	 This raises the important question if there are fast algorithms for matrix completion that come with guarantees on the required sample size comparable to those achieved by nuclear norm minimization. In this work we make progress on this problem by proving strong sample complexity bounds for alternating minimization. Along the way our work helps to give a theoretical justification and understanding for why alternating minimization works.   \subsection{Our results}  We begin with our result on the \emph{exact} matrix completion problem where the goal is to recover an unknown rank~$k$ matrix~$M$ from a subsample~$\Omega$ of its entries where each entry is included independently with probability~$p.$ Here and in the following we will always assume that~$M=U\Lambda U^\trans$ is a  symmetric $n\times n$ matrix with singular values $\sigma_1\ge\dots\ge \sigma_k.	score:407
Can NLP algorithms help reduce duplicate questions on Quora?	 Unsupervised learning algorithms are evaluated by considering how well a subsequent supervised classification algorithm performs on high-level features that are found by aggregating the learned low-level representations \cite{coates2011b}. We think that mingling these steps makes it difficult to assess the quality of the unsupervised algorithms. A more direct way is needed to evaluate these methods, preferably where a subsequent supervised learning step is completely optional.    We are not only at odds with the methodology of evaluating unsupervised learning algorithms. General object classification tasks are always based on orientation- and scale-rectified pictures with objects or themes firmly centered in the middle. We are looking for a dataset where it is possible to show that unsupervised feature learning is beneficial to the wide range of Computer Vision tasks beyond object classification, like tracking, stereo vision, panoramic stitching or structure from motion.	 Unsupervised learning algorithms are evaluated by considering how well a subsequent supervised classification algorithm performs on high-level features that are found by aggregating the learned low-level representations \cite{coates2011b}. We think that mingling these steps makes it difficult to assess the quality of the unsupervised algorithms. A more direct way is needed to evaluate these methods, preferably where a subsequent supervised learning step is completely optional.    We are not only at odds with the methodology of evaluating unsupervised learning algorithms. General object classification tasks are always based on orientation- and scale-rectified pictures with objects or themes firmly centered in the middle. We are looking for a dataset where it is possible to show that unsupervised feature learning is beneficial to the wide range of Computer Vision tasks beyond object classification, like tracking, stereo vision, panoramic stitching or structure from motion.	score:411
Can NLP algorithms help reduce duplicate questions on Quora?	 Unsupervised learning algorithms are evaluated by considering how well a subsequent supervised classification algorithm performs on high-level features that are found by aggregating the learned low-level representations \cite{coates2011b}. We think that mingling these steps makes it difficult to assess the quality of the unsupervised algorithms. A more direct way is needed to evaluate these methods, preferably where a subsequent supervised learning step is completely optional.    We are not only at odds with the methodology of evaluating unsupervised learning algorithms. General object classification tasks are always based on orientation- and scale-rectified pictures with objects or themes firmly centered in the middle. We are looking for a dataset where it is possible to show that unsupervised feature learning is beneficial to the wide range of Computer Vision tasks beyond object classification, like tracking, stereo vision, panoramic stitching or structure from motion.	 Unsupervised learning algorithms are evaluated by considering how well a subsequent supervised classification algorithm performs on high-level features that are found by aggregating the learned low-level representations \cite{coates2011b}. We think that mingling these steps makes it difficult to assess the quality of the unsupervised algorithms. A more direct way is needed to evaluate these methods, preferably where a subsequent supervised learning step is completely optional.    We are not only at odds with the methodology of evaluating unsupervised learning algorithms. General object classification tasks are always based on orientation- and scale-rectified pictures with objects or themes firmly centered in the middle. We are looking for a dataset where it is possible to show that unsupervised feature learning is beneficial to the wide range of Computer Vision tasks beyond object classification, like tracking, stereo vision, panoramic stitching or structure from motion.	score:411
Can NLP algorithms help reduce duplicate questions on Quora?	  So a natural question arises:  can we develop a computationally efficient robust sparse low rank matrix approximation procedure that is less sensitive to outliers and yet has sound statistical guarantees?    In this paper, we propose a novel method for fitting robust sparse reduced rank regression in the high-dimensional setting.  We propose to minimize the Huber loss function subject to both sparsity and rank constraints.  This leads to a non-convex optimization problem, and is thus computational intractable. To address this challenge, we consider a convex relaxation, which can be solved via an alternating direction method of multipliers algorithm.   Most of the existing theoretical analysis of reduced rank regression focuses on rank selection consistency and prediction consistency \citep{bunea2011optimal,mukherjee2011reduced,bunea2012joint,chen2013reduced}.	  So a natural question arises:  can we develop a computationally efficient robust sparse low rank matrix approximation procedure that is less sensitive to outliers and yet has sound statistical guarantees?    In this paper, we propose a novel method for fitting robust sparse reduced rank regression in the high-dimensional setting.  We propose to minimize the Huber loss function subject to both sparsity and rank constraints.  This leads to a non-convex optimization problem, and is thus computational intractable. To address this challenge, we consider a convex relaxation, which can be solved via an alternating direction method of multipliers algorithm.   Most of the existing theoretical analysis of reduced rank regression focuses on rank selection consistency and prediction consistency \citep{bunea2011optimal,mukherjee2011reduced,bunea2012joint,chen2013reduced}.	score:412
Can NLP algorithms help reduce duplicate questions on Quora?	 \end{equation} In the presence of a spectral gap the eigenmodes of $L(\Omega\setminus A\cup B,\Omega \setminus A\cup B)$ provides reduced coordinates for the reaction tube, thus solving \eqref{harmonic extension} for $q$ can be seen as expanding $q$ using the reduced coordinates. However, discretizing the generator using diffusion map may suffer from low order of convergence.  Therefore \cite{lai2017point} improves upon diffusion map by explicitly constructing the tangent plane of each point in the sampled point cloud and discretizing the generator in each of the tangent plane.     On the other hand, recent years have seen usage of machine learning techniques in solving high-dimensional partial differential equations. The success of \cite{carleo2017solving} where an NN parameterized spin wavefunction is used as an ansatz for solving the many-body Schr\"{o}dinger equation motivates us to consider solving \eqref{BK} using an NN as well.	 \end{equation} In the presence of a spectral gap the eigenmodes of $L(\Omega\setminus A\cup B,\Omega \setminus A\cup B)$ provides reduced coordinates for the reaction tube, thus solving \eqref{harmonic extension} for $q$ can be seen as expanding $q$ using the reduced coordinates. However, discretizing the generator using diffusion map may suffer from low order of convergence.  Therefore \cite{lai2017point} improves upon diffusion map by explicitly constructing the tangent plane of each point in the sampled point cloud and discretizing the generator in each of the tangent plane.     On the other hand, recent years have seen usage of machine learning techniques in solving high-dimensional partial differential equations. The success of \cite{carleo2017solving} where an NN parameterized spin wavefunction is used as an ansatz for solving the many-body Schr\"{o}dinger equation motivates us to consider solving \eqref{BK} using an NN as well.	score:412
How do I learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:448
How do I learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:448
How do I learn machine learning?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:452
How do I learn machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:455
How do I learn machine learning?	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.    \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:456
How can we use machine learning to predict exception in order flow?	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	score:312
How can we use machine learning to predict exception in order flow?	 Can one quantify how a constraint of getting only a few bits from each example affects our ability to learn? To the best of our knowledge, there are currently no generic tools which allow us to answer such questions, at least in the context of standard machine learning settings.  In this paper, we make a first step in developing such a framework. We consider a general class of learning processes, characterized only by information-theoretic constraints on how they may interact with the data (and independent of any specific problem semantics).	 Can one quantify how a constraint of getting only a few bits from each example affects our ability to learn? To the best of our knowledge, there are currently no generic tools which allow us to answer such questions, at least in the context of standard machine learning settings.  In this paper, we make a first step in developing such a framework. We consider a general class of learning processes, characterized only by information-theoretic constraints on how they may interact with the data (and independent of any specific problem semantics).	score:314
How can we use machine learning to predict exception in order flow?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:317
How can we use machine learning to predict exception in order flow?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:324
How can we use machine learning to predict exception in order flow?	   Due to the complexity of the power grid, it is difficult to come up with a verbatim standard depicting the potential stability after reconnection of a subnetwork. With advances in the artificial intelligence community, we can make use of machine learning algorithms in order to explore vast combinations of sensor inputs, states, and control actions.   This can be done in a similar fashion to successful techniques applied to other power system problems as seen in the research literature \cite{Paper0064,Paper0082,Paper0094,Paper0095}. In this paper we propose to use a machine learning algorithm, specifically a Support Vector Machine, to predict safe times to reconnect a portion of a grid. The Support Vector Machines allow one to build a classifier predicated upon training data by determining a linear separator in a specific feature dimension \cite{Paper0065}.	   Due to the complexity of the power grid, it is difficult to come up with a verbatim standard depicting the potential stability after reconnection of a subnetwork. With advances in the artificial intelligence community, we can make use of machine learning algorithms in order to explore vast combinations of sensor inputs, states, and control actions.   This can be done in a similar fashion to successful techniques applied to other power system problems as seen in the research literature \cite{Paper0064,Paper0082,Paper0094,Paper0095}. In this paper we propose to use a machine learning algorithm, specifically a Support Vector Machine, to predict safe times to reconnect a portion of a grid. The Support Vector Machines allow one to build a classifier predicated upon training data by determining a linear separator in a specific feature dimension \cite{Paper0065}.	score:327
Galgotias university morining evening batch criteria?and timings?	 Most of the propagation-based methods can be interpreted as variants of the backpropagation algorithm \cite{ancona2017unified} and the algorithms themselves are self-explanatory. Perturbation-based methods usually optimize or manually alter the input with respect to meaningful criteria \cite{fong2017interpretable}. However, noise-adding methods merely take the mean or the variance of saliency maps generated by adding Gaussian noise to the input.  Despite their apparent simplicity, the results are surprisingly effective. Ironically, the simplicity of their approach prevents us from understanding exactly how and why noise-adding methods work for the better model interpretation.  This situation poses a twofold problem. First, since the inner workings of the method are unclear, our understanding for the results produced by the noise-adding methods are also innately unclear.	 Most of the propagation-based methods can be interpreted as variants of the backpropagation algorithm \cite{ancona2017unified} and the algorithms themselves are self-explanatory. Perturbation-based methods usually optimize or manually alter the input with respect to meaningful criteria \cite{fong2017interpretable}. However, noise-adding methods merely take the mean or the variance of saliency maps generated by adding Gaussian noise to the input.  Despite their apparent simplicity, the results are surprisingly effective. Ironically, the simplicity of their approach prevents us from understanding exactly how and why noise-adding methods work for the better model interpretation.  This situation poses a twofold problem. First, since the inner workings of the method are unclear, our understanding for the results produced by the noise-adding methods are also innately unclear.	score:435
Galgotias university morining evening batch criteria?and timings?	  In this work, we focus on the most common estimation technique: Monte Carlo integration. Despite their generality and myriad other desirable properties, Monte Carlo approaches have consistently been regarded as non-differentiable and, therefore, inefficient in practice given the need to optimize~\eqref{eq:acquisition_integral}. However, it seems to have been overlooked that sample-based approaches can indeed be used to estimate gradients, well-known examples of which include stochastic backpropagation and the reparameterization trick \cite{JimenezRezende2014a,kingma2013}.	  In this work, we focus on the most common estimation technique: Monte Carlo integration. Despite their generality and myriad other desirable properties, Monte Carlo approaches have consistently been regarded as non-differentiable and, therefore, inefficient in practice given the need to optimize~\eqref{eq:acquisition_integral}. However, it seems to have been overlooked that sample-based approaches can indeed be used to estimate gradients, well-known examples of which include stochastic backpropagation and the reparameterization trick \cite{JimenezRezende2014a,kingma2013}.	score:440
Galgotias university morining evening batch criteria?and timings?	 For instance, in a typical Bayesian inference problem, the posterior distribution might be strongly non-Gaussian (perhaps multimodal) and high dimensional, and evaluations of its density might be computationally intensive.  There exist a wide range of algorithms for such problems, ranging from parametric variational inference \cite{blei2017variational} to Markov chain Monte Carlo (MCMC) techniques \cite{gilks1995markov}.   Each algorithm offers a different computational trade-off.  At one end of the spectrum, we find the parametric mean-field approximation---a cheap but potentially inaccurate variational approximation of the target density.    At the other end, we find MCMC---a nonparametric sampling technique yielding estimators that are consistent, but potentially slow to converge.	 For instance, in a typical Bayesian inference problem, the posterior distribution might be strongly non-Gaussian (perhaps multimodal) and high dimensional, and evaluations of its density might be computationally intensive.  There exist a wide range of algorithms for such problems, ranging from parametric variational inference \cite{blei2017variational} to Markov chain Monte Carlo (MCMC) techniques \cite{gilks1995markov}.   Each algorithm offers a different computational trade-off.  At one end of the spectrum, we find the parametric mean-field approximation---a cheap but potentially inaccurate variational approximation of the target density.    At the other end, we find MCMC---a nonparametric sampling technique yielding estimators that are consistent, but potentially slow to converge.	score:452
Galgotias university morining evening batch criteria?and timings?	 Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. The flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. This paper gives sufficient conditions to guarantee that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast mixing, in a precise sense.  Further, an algorithm is given to project onto this set of fast-mixing parameters in the Euclidean norm. Following recent work, we give an example use of this to project in various divergence measures, comparing univariate marginals obtained by sampling after projection to common variational methods and Gibbs sampling on the original parameters. 	Exact inference in Markov Random Fields (MRFs) is generally intractable, motivating approximate algorithms.	 Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. The flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. This paper gives sufficient conditions to guarantee that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast mixing, in a precise sense.  Further, an algorithm is given to project onto this set of fast-mixing parameters in the Euclidean norm. Following recent work, we give an example use of this to project in various divergence measures, comparing univariate marginals obtained by sampling after projection to common variational methods and Gibbs sampling on the original parameters. 	Exact inference in Markov Random Fields (MRFs) is generally intractable, motivating approximate algorithms.	score:457
Galgotias university morining evening batch criteria?and timings?	 This step can be computationally expensive especially for large-scale and high-dimensional data. In this work, we aim at accelerating these sparsity-constrained optimization algorithms by exploiting the key observation that, for these algorithms to work, one only needs the coordinate of the gradient's top entry. Hence, we introduce algorithms based on greedy methods and randomization approaches that aim at cheaply estimating the gradient and its top entry.  Another of our contribution is to cast the problem of finding the best gradient entry as a best arm identification in a multi-armed bandit problem. Owing to this novel insight, we are able to provide a bandit-based algorithm that directly estimates the top entry in a very efficient way. Theoretical observations stating that the resulting inexact Frank-Wolfe or Orthogonal  Matching Pursuit algorithms act, with high probability, similarly to their exact versions  are also given.	 This step can be computationally expensive especially for large-scale and high-dimensional data. In this work, we aim at accelerating these sparsity-constrained optimization algorithms by exploiting the key observation that, for these algorithms to work, one only needs the coordinate of the gradient's top entry. Hence, we introduce algorithms based on greedy methods and randomization approaches that aim at cheaply estimating the gradient and its top entry.  Another of our contribution is to cast the problem of finding the best gradient entry as a best arm identification in a multi-armed bandit problem. Owing to this novel insight, we are able to provide a bandit-based algorithm that directly estimates the top entry in a very efficient way. Theoretical observations stating that the resulting inexact Frank-Wolfe or Orthogonal  Matching Pursuit algorithms act, with high probability, similarly to their exact versions  are also given.	score:457
Which is best programming language for machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:389
Which is best programming language for machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:422
Which is best programming language for machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:424
Which is best programming language for machine learning?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:424
Which is best programming language for machine learning?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:434
Which programming language will be the best for machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:355
Which programming language will be the best for machine learning?	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	score:367
Which programming language will be the best for machine learning?	 Machine learning approaches learn the relation between a set of input and its related output through intensive observation from characteristics of the data, usually referred to as features. To capture several aspects that affect the cloud's performance variation, machine learning may provide a better solution by considering temporal changes and other various factors in task's performance as features.   In the case of predictions, the conventional machine learning approaches are based on a regression function that estimates the runtime of a task from a set of features. Evaluating these techniques to predict the task runtime in WaaS platforms is out of our scope. We are interested in exploring various ways of determining the features on which the regression functions depend on.	 Machine learning approaches learn the relation between a set of input and its related output through intensive observation from characteristics of the data, usually referred to as features. To capture several aspects that affect the cloud's performance variation, machine learning may provide a better solution by considering temporal changes and other various factors in task's performance as features.   In the case of predictions, the conventional machine learning approaches are based on a regression function that estimates the runtime of a task from a set of features. Evaluating these techniques to predict the task runtime in WaaS platforms is out of our scope. We are interested in exploring various ways of determining the features on which the regression functions depend on.	score:369
Which programming language will be the best for machine learning?	 In the learning process, inertial thinking schemes will be formed, including illusion inertial thinking, leading to low generalization ability of the machine learning methods. According to the law of Hoeffding \cite{Hoeffding1963}, the accuracy of machine learning methods depend heavily on a large number of training samples, and this is why most advanced machine learning methods require a large number of training samples \cite{Lake2015}, e. g., deep learning methods \cite{LeCun2015}. However, it is difficult to obtain a large number of training samples for most real applications, such as disease diagnosis, where the training samples are few, easily leading to formation of illusion inertial thinking in machine learning methods. One way for humans to solve these kinds of problem is the use of reverse thinking, which is an effective method of creative thinking \cite{Sawaguchi2015}.	 In the learning process, inertial thinking schemes will be formed, including illusion inertial thinking, leading to low generalization ability of the machine learning methods. According to the law of Hoeffding \cite{Hoeffding1963}, the accuracy of machine learning methods depend heavily on a large number of training samples, and this is why most advanced machine learning methods require a large number of training samples \cite{Lake2015}, e. g., deep learning methods \cite{LeCun2015}. However, it is difficult to obtain a large number of training samples for most real applications, such as disease diagnosis, where the training samples are few, easily leading to formation of illusion inertial thinking in machine learning methods. One way for humans to solve these kinds of problem is the use of reverse thinking, which is an effective method of creative thinking \cite{Sawaguchi2015}.	score:370
Which programming language will be the best for machine learning?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:374
What can convolutional neural network do?	 \item We introduce a normalized data representation, which can be used in Neural Network algorithms, or other methods.  \item We present a Deep Convolutional Network, which can be able to learn traffic flow of different traffic points. \item Then, we present a Recurrent Neural Network, which, apart from its structure, can do the same as Convolutional Network.  \item Both of these models are able to predict \textit{n}-level traffic prediction for different points of the traffic (e.g. Quiet, light traffic, heavy traffic, congested, etc.) \item They also put up the predicted average speeds on different points of the traffic network based on the speed limits in that point (e.g. 0.65 of speed limit). \item Then, we present a Recurrent Neural Network \end{enumerate}  In the rest of this paper, we start our work by some preliminaries in section \ref{prelim}.	 \item We introduce a normalized data representation, which can be used in Neural Network algorithms, or other methods.  \item We present a Deep Convolutional Network, which can be able to learn traffic flow of different traffic points. \item Then, we present a Recurrent Neural Network, which, apart from its structure, can do the same as Convolutional Network.  \item Both of these models are able to predict \textit{n}-level traffic prediction for different points of the traffic (e.g. Quiet, light traffic, heavy traffic, congested, etc.) \item They also put up the predicted average speeds on different points of the traffic network based on the speed limits in that point (e.g. 0.65 of speed limit). \item Then, we present a Recurrent Neural Network \end{enumerate}  In the rest of this paper, we start our work by some preliminaries in section \ref{prelim}.	score:397
What can convolutional neural network do?	    Convolutional neural networks have recently achieved significant breakthroughs in various image classification tasks. However, they are computationally expensive, which can make their feasible implementation on embedded and low-power devices difficult. In this paper convolutional neural network binarization is implemented on GPU-based platforms for real-time inference on resource constrained devices.  In binarized networks, all weights and intermediate computations between layers are quantized to +1 and -1, allowing multiplications and additions to be replaced with bit-wise operations between 32-bit words. This representation completely eliminates the need for floating point multiplications and additions and decreases both the computational load and the memory footprint compared to a full-precision network implemented in floating point, making it well-suited for resource-constrained environments.	    Convolutional neural networks have recently achieved significant breakthroughs in various image classification tasks. However, they are computationally expensive, which can make their feasible implementation on embedded and low-power devices difficult. In this paper convolutional neural network binarization is implemented on GPU-based platforms for real-time inference on resource constrained devices.  In binarized networks, all weights and intermediate computations between layers are quantized to +1 and -1, allowing multiplications and additions to be replaced with bit-wise operations between 32-bit words. This representation completely eliminates the need for floating point multiplications and additions and decreases both the computational load and the memory footprint compared to a full-precision network implemented in floating point, making it well-suited for resource-constrained environments.	score:432
What can convolutional neural network do?	 	Deep neural networks, especially convolutional neural networks (CNNs) \cite{lecun1995convolutional} and recurrent neural networks (RNNs) \cite{elman1991distributed}, have been widely used in a variety of subjects \cite{lecun2015deep}. However, traditional neural network blocks aim to learn the feature representations in a local sense. For example, both convolutional and recurrent operations process a local neighborhood (several nearest neighboring neurons) in either space or time.  Therefore, the long-range dependencies can only be captured when these operations are applied recursively, while those long-range dependencies are sometimes significant in practical learning problems, such as image or video classification, text summarization, and financial market analysis \cite{beran1995long,cont2005long,pipiras2017long,willinger2003long}.	 	Deep neural networks, especially convolutional neural networks (CNNs) \cite{lecun1995convolutional} and recurrent neural networks (RNNs) \cite{elman1991distributed}, have been widely used in a variety of subjects \cite{lecun2015deep}. However, traditional neural network blocks aim to learn the feature representations in a local sense. For example, both convolutional and recurrent operations process a local neighborhood (several nearest neighboring neurons) in either space or time.  Therefore, the long-range dependencies can only be captured when these operations are applied recursively, while those long-range dependencies are sometimes significant in practical learning problems, such as image or video classification, text summarization, and financial market analysis \cite{beran1995long,cont2005long,pipiras2017long,willinger2003long}.	score:433
What can convolutional neural network do?	 In each layer, they imposed equality constraint between the auxiliary variable and activation, and optimized the new problem using Alternating Direction Method which is easy to parallel. However, for the convolutional neural network, the performances of all above methods are much worse than backpropagation algorithm  when the network is deep.  In this paper, we focus on breaking the backward locking in  backpropagtion algorithm for training feedforward neural networks, such that we can update  models in parallel  without loss of accuracy.  The main contributions of our work are as follows:  \begin{itemize}  \item Firstly, we decouple the backpropagation using delayed gradients in Section \ref{sec_alg} such that all modules of the network can be updated in parallel without  backward locking.  \item Then,  we propose two stochastic algorithms using decoupled parallel backpropagation in Section \ref{sec_alg} for deep learning optimization.	 In each layer, they imposed equality constraint between the auxiliary variable and activation, and optimized the new problem using Alternating Direction Method which is easy to parallel. However, for the convolutional neural network, the performances of all above methods are much worse than backpropagation algorithm  when the network is deep.  In this paper, we focus on breaking the backward locking in  backpropagtion algorithm for training feedforward neural networks, such that we can update  models in parallel  without loss of accuracy.  The main contributions of our work are as follows:  \begin{itemize}  \item Firstly, we decouple the backpropagation using delayed gradients in Section \ref{sec_alg} such that all modules of the network can be updated in parallel without  backward locking.  \item Then,  we propose two stochastic algorithms using decoupled parallel backpropagation in Section \ref{sec_alg} for deep learning optimization.	score:443
What can convolutional neural network do?	 Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity.  The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90\% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN).	 Recently deep neural networks have received considerable attention due to their ability to extract and represent high-level abstractions in data sets. Deep neural networks such as fully-connected and convolutional neural networks have shown excellent performance on a wide range of recognition and classification tasks. However, their hardware implementations currently suffer from large silicon area and high power consumption due to the their high degree of complexity.  The power/energy consumption of neural networks is dominated by memory accesses, the majority of which occur in fully-connected networks. In fact, they contain most of the deep neural network parameters. In this paper, we propose sparsely-connected networks, by showing that the number of connections in fully-connected networks can be reduced by up to 90\% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10 and SVHN).	score:455
What is a convolutional neural network?	 \item We introduce a normalized data representation, which can be used in Neural Network algorithms, or other methods.  \item We present a Deep Convolutional Network, which can be able to learn traffic flow of different traffic points. \item Then, we present a Recurrent Neural Network, which, apart from its structure, can do the same as Convolutional Network.  \item Both of these models are able to predict \textit{n}-level traffic prediction for different points of the traffic (e.g. Quiet, light traffic, heavy traffic, congested, etc.) \item They also put up the predicted average speeds on different points of the traffic network based on the speed limits in that point (e.g. 0.65 of speed limit). \item Then, we present a Recurrent Neural Network \end{enumerate}  In the rest of this paper, we start our work by some preliminaries in section \ref{prelim}.	 \item We introduce a normalized data representation, which can be used in Neural Network algorithms, or other methods.  \item We present a Deep Convolutional Network, which can be able to learn traffic flow of different traffic points. \item Then, we present a Recurrent Neural Network, which, apart from its structure, can do the same as Convolutional Network.  \item Both of these models are able to predict \textit{n}-level traffic prediction for different points of the traffic (e.g. Quiet, light traffic, heavy traffic, congested, etc.) \item They also put up the predicted average speeds on different points of the traffic network based on the speed limits in that point (e.g. 0.65 of speed limit). \item Then, we present a Recurrent Neural Network \end{enumerate}  In the rest of this paper, we start our work by some preliminaries in section \ref{prelim}.	score:407
What is a convolutional neural network?	 	Deep neural networks, especially convolutional neural networks (CNNs) \cite{lecun1995convolutional} and recurrent neural networks (RNNs) \cite{elman1991distributed}, have been widely used in a variety of subjects \cite{lecun2015deep}. However, traditional neural network blocks aim to learn the feature representations in a local sense. For example, both convolutional and recurrent operations process a local neighborhood (several nearest neighboring neurons) in either space or time.  Therefore, the long-range dependencies can only be captured when these operations are applied recursively, while those long-range dependencies are sometimes significant in practical learning problems, such as image or video classification, text summarization, and financial market analysis \cite{beran1995long,cont2005long,pipiras2017long,willinger2003long}.   To address the above issue, a nonlocal neural network \cite{wang2017non} has been proposed recently, which is able to improve the performance on a couple of computer vision tasks. In contrast to convolutional or recurrent blocks, nonlocal operations \cite{wang2017non} capture long-range dependencies directly by computing interactions between each pair of positions in the feature space.  Generally speaking, nonlocality is ubiquitous in nature, and the nonlocal models and algorithms have been studied in various domains of physical, biological and social sciences \cite{ buades2005non, coifman2006diffusion, du2012analysis,silling2000reformulation,tadmor2015mathematical}.  In this work, we aim to study the nature of nonlocal networks, namely, what the nonlocal blocks have exactly learned through training on a real-world task.	  To address the above issue, a nonlocal neural network \cite{wang2017non} has been proposed recently, which is able to improve the performance on a couple of computer vision tasks. In contrast to convolutional or recurrent blocks, nonlocal operations \cite{wang2017non} capture long-range dependencies directly by computing interactions between each pair of positions in the feature space.	score:421
What is a convolutional neural network?	 The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision.  We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations.   We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks.  To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing. 	\begin{table*}[!	 The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision.  We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations.   We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks.  To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing. 	\begin{table*}[!	score:440
What is a convolutional neural network?	    Convolutional neural networks have recently achieved significant breakthroughs in various image classification tasks. However, they are computationally expensive, which can make their feasible implementation on embedded and low-power devices difficult. In this paper convolutional neural network binarization is implemented on GPU-based platforms for real-time inference on resource constrained devices.  In binarized networks, all weights and intermediate computations between layers are quantized to +1 and -1, allowing multiplications and additions to be replaced with bit-wise operations between 32-bit words. This representation completely eliminates the need for floating point multiplications and additions and decreases both the computational load and the memory footprint compared to a full-precision network implemented in floating point, making it well-suited for resource-constrained environments.	    Convolutional neural networks have recently achieved significant breakthroughs in various image classification tasks. However, they are computationally expensive, which can make their feasible implementation on embedded and low-power devices difficult. In this paper convolutional neural network binarization is implemented on GPU-based platforms for real-time inference on resource constrained devices.  In binarized networks, all weights and intermediate computations between layers are quantized to +1 and -1, allowing multiplications and additions to be replaced with bit-wise operations between 32-bit words. This representation completely eliminates the need for floating point multiplications and additions and decreases both the computational load and the memory footprint compared to a full-precision network implemented in floating point, making it well-suited for resource-constrained environments.	score:442
What is a convolutional neural network?	 Among these progresses, one challenging and interesting problem is to extract global topological features from local inputs, for instance, by supervised training a neural network, and to understand how the neural network works.  In Ref.~\cite{zhang2018machine}, a convolutional neural network is trained to predict the topological invariant for band insulators with high accuracy.  The highlights of that work are two-fold. First, only local Hamiltonians are used as the input and no human knowledge is used as a prior. Second, by analyzing the neural network after training, it is found the formula fitted by the neural network is precisely the same as the mathematical formula for the winding number. However, the limitations of Ref.	 Among these progresses, one challenging and interesting problem is to extract global topological features from local inputs, for instance, by supervised training a neural network, and to understand how the neural network works.  In Ref.~\cite{zhang2018machine}, a convolutional neural network is trained to predict the topological invariant for band insulators with high accuracy.  The highlights of that work are two-fold. First, only local Hamiltonians are used as the input and no human knowledge is used as a prior. Second, by analyzing the neural network after training, it is found the formula fitted by the neural network is precisely the same as the mathematical formula for the winding number. However, the limitations of Ref.	score:443
How important is it to have low level hardware knowledge if one works as a computer science researcher in fields like machine learning, theoretical computer science, intelligent systems, and robotics?	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:288
How important is it to have low level hardware knowledge if one works as a computer science researcher in fields like machine learning, theoretical computer science, intelligent systems, and robotics?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:288
How important is it to have low level hardware knowledge if one works as a computer science researcher in fields like machine learning, theoretical computer science, intelligent systems, and robotics?	 Reinforcement learning is a promising approach to developing hard-to-engineer adaptive solutions for complex and diverse robotic tasks. However, learning with real-world robots is often unreliable and difficult, which resulted in their low adoption in reinforcement learning research. This difficulty is worsened by the lack of guidelines for setting up learning tasks with robots.  In this work, we develop a learning task with a UR5 robotic arm to bring to light some key elements of a task setup and study their contributions to the challenges with robots. We find that learning performance can be highly sensitive to the setup, and thus oversights and omissions in setup details can make effective learning, reproducibility, and fair comparison hard.	 Reinforcement learning is a promising approach to developing hard-to-engineer adaptive solutions for complex and diverse robotic tasks. However, learning with real-world robots is often unreliable and difficult, which resulted in their low adoption in reinforcement learning research. This difficulty is worsened by the lack of guidelines for setting up learning tasks with robots.  In this work, we develop a learning task with a UR5 robotic arm to bring to light some key elements of a task setup and study their contributions to the challenges with robots. We find that learning performance can be highly sensitive to the setup, and thus oversights and omissions in setup details can make effective learning, reproducibility, and fair comparison hard.	score:301
How important is it to have low level hardware knowledge if one works as a computer science researcher in fields like machine learning, theoretical computer science, intelligent systems, and robotics?	 One would hope for much stronger theoretical guarantees of success, but no widely used machine learning methods provide them, so it is difficult to know when they will work.  One key question faced by any machine learning system is: what kind of models will it generate? In neural networks one asks whether the architecture is feed-forward or recurrent and how many layers it has.  In genetic programming one asks for the program representation and which functions are built-in. In general, each machine learning system must make this choice. If the class of generated models or programs is too broad, it might be impossible to learn them efficiently. If it is too narrow, it might not suffice for the task at hand. To solve a different task, one might need a different kind of model.	 One would hope for much stronger theoretical guarantees of success, but no widely used machine learning methods provide them, so it is difficult to know when they will work.  One key question faced by any machine learning system is: what kind of models will it generate? In neural networks one asks whether the architecture is feed-forward or recurrent and how many layers it has.  In genetic programming one asks for the program representation and which functions are built-in. In general, each machine learning system must make this choice. If the class of generated models or programs is too broad, it might be impossible to learn them efficiently. If it is too narrow, it might not suffice for the task at hand. To solve a different task, one might need a different kind of model.	score:308
How important is it to have low level hardware knowledge if one works as a computer science researcher in fields like machine learning, theoretical computer science, intelligent systems, and robotics?	  The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery.   This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google.  The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at \href{www.tensorflow.org}{www.tensorflow.org}. \end{small}  	The Google Brain project started in 2011 to explore the use of very-large-scale deep neural networks, both for research and for use in Google's products.	  The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery.   This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google.  The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at \href{www.tensorflow.org}{www.tensorflow.org}. \end{small}  	The Google Brain project started in 2011 to explore the use of very-large-scale deep neural networks, both for research and for use in Google's products.	score:312
What is the difference between the IES batch and the GATE batch at the ACE Academy Hyderabad?	 We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole.  To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start.	 We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole.  To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start.	score:389
What is the difference between the IES batch and the GATE batch at the ACE Academy Hyderabad?	 In \cite{Oh17Adversarial}, a game theoretical framework is used to study the relationship between attack and defense strategies in recognition systems in the context of adversarial attacks. In \cite{Transferable}, the authors examine the transferability of adversarial examples between different models and find that adversarial examples span a contiguous subspace of large dimensionality.  The authors also provide an insight into the decision boundaries of DNNs.  In \cite{Towards_DeepLearning_Resistance}, the authors claim that first order attacks are universal and suggest the Projected Gradient Descent (PGD) attack which relies on this notion. They also claim that networks require a significantly larger capacity in order to be more robust to adversarial attacks.	 In \cite{Oh17Adversarial}, a game theoretical framework is used to study the relationship between attack and defense strategies in recognition systems in the context of adversarial attacks. In \cite{Transferable}, the authors examine the transferability of adversarial examples between different models and find that adversarial examples span a contiguous subspace of large dimensionality.  The authors also provide an insight into the decision boundaries of DNNs.  In \cite{Towards_DeepLearning_Resistance}, the authors claim that first order attacks are universal and suggest the Projected Gradient Descent (PGD) attack which relies on this notion. They also claim that networks require a significantly larger capacity in order to be more robust to adversarial attacks.	score:415
What is the difference between the IES batch and the GATE batch at the ACE Academy Hyderabad?	html}}, caffe's\footnote{\url{http://caffe.berkeleyvision.org/tutorial/solver.html}}, and keras'\footnote{\url{http://keras.io/optimizers/}} documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.  This article aims at providing the reader with intuitions with regard to the behaviour of different algorithms for optimizing gradient descent that will help her put them to use.  In Section \ref{sec:gradient_descent_variants}, we are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training in Section \ref{sec:challenges}. Subsequently, in Section \ref{sec:algos}, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules.	html}}, caffe's\footnote{\url{http://caffe.berkeleyvision.org/tutorial/solver.html}}, and keras'\footnote{\url{http://keras.io/optimizers/}} documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.  This article aims at providing the reader with intuitions with regard to the behaviour of different algorithms for optimizing gradient descent that will help her put them to use.  In Section \ref{sec:gradient_descent_variants}, we are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training in Section \ref{sec:challenges}. Subsequently, in Section \ref{sec:algos}, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules.	score:416
What is the difference between the IES batch and the GATE batch at the ACE Academy Hyderabad?	 The impact of such an attack could be exacerbated if the attack took place during peak hours of electricity delivery and at a large scale. Cyber defenders currently lack the visibility to detect and quickly respond to such an attack.    To detect targeted cyber-attacks and achieve attack resiliency, there is a requirement for continuous monitoring of  DERs and their interactions with the electrical grid in real-time.  This work specifically focuses on the cyber security of distributed photovoltaics (PVs), where the objective of an adversary is to exploit the stochastic power generation/consumption characteristics and vulnerabilities in control system environment to create supply-demand mismatch and reverse power flow conditions that can lead to local disturbances or grid instabilities~\cite{Solar}.	 The impact of such an attack could be exacerbated if the attack took place during peak hours of electricity delivery and at a large scale. Cyber defenders currently lack the visibility to detect and quickly respond to such an attack.    To detect targeted cyber-attacks and achieve attack resiliency, there is a requirement for continuous monitoring of  DERs and their interactions with the electrical grid in real-time.  This work specifically focuses on the cyber security of distributed photovoltaics (PVs), where the objective of an adversary is to exploit the stochastic power generation/consumption characteristics and vulnerabilities in control system environment to create supply-demand mismatch and reverse power flow conditions that can lead to local disturbances or grid instabilities~\cite{Solar}.	score:420
What is the difference between the IES batch and the GATE batch at the ACE Academy Hyderabad?	 These tiny $4MB$ arrays of counts are sufficient for unsupervised anomaly detection. At the core of the ACE algorithm, there is a novel statistical estimator which is derived from the sampling view of Locality Sensitive Hashing(LSH). This view is significantly different and efficient than the widely popular view of LSH for near-neighbor search. We show the superiority of ACE algorithm over 11 popular baselines on 3 benchmark datasets, including the KDD-Cup99 data which is the largest available benchmark comprising of more than half a million entries with ground truth anomaly labels.  	The problem of Anomaly (or outlier) detections is the task of identifying instances (or patterns) in data that do not conform to the expected behavior~\cite{chandola2009anomaly}. These non-conforming examples are popularly referred to as anomalies, or outliers, sometimes interchangeably. Anomaly (or Outlier) detection algorithms are frequently required in large data processing applications.	 These tiny $4MB$ arrays of counts are sufficient for unsupervised anomaly detection. At the core of the ACE algorithm, there is a novel statistical estimator which is derived from the sampling view of Locality Sensitive Hashing(LSH). This view is significantly different and efficient than the widely popular view of LSH for near-neighbor search. We show the superiority of ACE algorithm over 11 popular baselines on 3 benchmark datasets, including the KDD-Cup99 data which is the largest available benchmark comprising of more than half a million entries with ground truth anomaly labels.  	The problem of Anomaly (or outlier) detections is the task of identifying instances (or patterns) in data that do not conform to the expected behavior~\cite{chandola2009anomaly}. These non-conforming examples are popularly referred to as anomalies, or outliers, sometimes interchangeably. Anomaly (or Outlier) detection algorithms are frequently required in large data processing applications.	score:421
What is the binomial classification (latin name) of the cookie monster?	 	Large hierarchical classification with more than 10000 categories is a challenging task (\cite{partalas2015lshtc}). Traditionally, hierarchical classification can be done by training a classifier on the flattened labels (\cite{babbar2013flat}) or by training a classifier at each hierarchical node (\cite{silla2011survey}), whereby each hierarchical node is a decision maker of which subsequent node to route to.	 	Large hierarchical classification with more than 10000 categories is a challenging task (\cite{partalas2015lshtc}). Traditionally, hierarchical classification can be done by training a classifier on the flattened labels (\cite{babbar2013flat}) or by training a classifier at each hierarchical node (\cite{silla2011survey}), whereby each hierarchical node is a decision maker of which subsequent node to route to.	score:355
What is the binomial classification (latin name) of the cookie monster?	  The performance of the methodology is evaluated in benchmark examples   and  theoretical aspects of subsample methods are discussed.         	In the context of pattern recognition, classification methods focus on learning  the relationship between a set of feature variables and a target ``class  variable'' of interest. From a set of training data points with known associated class labels, a classifier is adjusted, to be used on unlabeled test instances.  The diversity of problems that can be  addressed by classification algorithms is significant and covers many domains of application (see, for instance, \cite{Cart} or \cite{Hastie}). More  formally, in the setting of \emph{supervised learning}, examples are given  that consist of pairs, $(X_{i},y_{i})$, $i \leq n$, where $X_{i}$ is the  $d-$dimensional ``feature'' or ``covariate''  vector and $y_{i}$ is the corresponding  ``class''  or  ``category'', that lives in some finite set ${\cal C}$.	  The performance of the methodology is evaluated in benchmark examples   and  theoretical aspects of subsample methods are discussed.         	In the context of pattern recognition, classification methods focus on learning  the relationship between a set of feature variables and a target ``class  variable'' of interest. From a set of training data points with known associated class labels, a classifier is adjusted, to be used on unlabeled test instances.  The diversity of problems that can be  addressed by classification algorithms is significant and covers many domains of application (see, for instance, \cite{Cart} or \cite{Hastie}). More  formally, in the setting of \emph{supervised learning}, examples are given  that consist of pairs, $(X_{i},y_{i})$, $i \leq n$, where $X_{i}$ is the  $d-$dimensional ``feature'' or ``covariate''  vector and $y_{i}$ is the corresponding  ``class''  or  ``category'', that lives in some finite set ${\cal C}$.	score:371
What is the binomial classification (latin name) of the cookie monster?	 However, the loss function used in this method is a traditional logistic loss. In this paper, we ask the following question: How can we learn the sparse codes and its corresponding class prediction function to optimize a multivariate performance measure? To answer this question, we propose a novel multivariate performance optimization method. In this method, we try to learn sparse codes from the tuple of training data points, and apply a linear function to match the sparse code tuple against a candidate class label.	 However, the loss function used in this method is a traditional logistic loss. In this paper, we ask the following question: How can we learn the sparse codes and its corresponding class prediction function to optimize a multivariate performance measure? To answer this question, we propose a novel multivariate performance optimization method. In this method, we try to learn sparse codes from the tuple of training data points, and apply a linear function to match the sparse code tuple against a candidate class label.	score:375
What is the binomial classification (latin name) of the cookie monster?	 Clearly, without prior knowledge this is difficult. However, we do have the data from the seen training classes, which can tell us what kind of similarity/difference is expected for examples from the same class or from different classes. It is reasonable to assume that this knowledge can be transferred to the rejected examples and used to discover the hidden unseen classes in them.  This paper aims to solve this problem. It first proposes a joint open classification model with a sub-model for classifying whether a pair of examples belongs to the same or different classes. This sub-model can serve as a distance function for clustering to discover the hidden classes of the rejected examples. Experimental results show that the proposed model is highly promising.	 Clearly, without prior knowledge this is difficult. However, we do have the data from the seen training classes, which can tell us what kind of similarity/difference is expected for examples from the same class or from different classes. It is reasonable to assume that this knowledge can be transferred to the rejected examples and used to discover the hidden unseen classes in them.  This paper aims to solve this problem. It first proposes a joint open classification model with a sub-model for classifying whether a pair of examples belongs to the same or different classes. This sub-model can serve as a distance function for clustering to discover the hidden classes of the rejected examples. Experimental results show that the proposed model is highly promising.	score:376
What is the binomial classification (latin name) of the cookie monster?	 However, these methods require training data to be annotated with bounding boxes and segmentation ground truths which require expert domain knowledge and costly effort to obtain.   Due to the high cost of annotation, we intend to perform classification based on a raw, un-annotated whole mammogram. Each patch of a mammogram can be treated as an instance and a whole mammogram is treated as a bag of instances.  The whole mammogram classification problem can then be thought of as a standard multi-instance learning problem.  Thus, we propose three different schemes, i.e., max pooling, label assignment, and sparsity, to perform deep multi-instance learning for the whole mammogram classification task.  \begin{figure}[t]  \begin{center}      \begin{minipage}{\linewidth}    \centerline{\includegraphics[width=\textwidth]{framework3newnew.	 However, these methods require training data to be annotated with bounding boxes and segmentation ground truths which require expert domain knowledge and costly effort to obtain.   Due to the high cost of annotation, we intend to perform classification based on a raw, un-annotated whole mammogram. Each patch of a mammogram can be treated as an instance and a whole mammogram is treated as a bag of instances.  The whole mammogram classification problem can then be thought of as a standard multi-instance learning problem.  Thus, we propose three different schemes, i.e., max pooling, label assignment, and sparsity, to perform deep multi-instance learning for the whole mammogram classification task.  \begin{figure}[t]  \begin{center}      \begin{minipage}{\linewidth}    \centerline{\includegraphics[width=\textwidth]{framework3newnew.	score:377
What can one do to be certain that they are not living in the matrix?	  As robots are increasingly becoming part of our everyday lives, it is essential for them to be aware of the surrounding humans while performing their tasks. Navigation is a basic skill for autonomous robots, but many traditional algorithms, such as A* and D*, do not consider the fact that the obstacles in the environment may be humans. While maneuvers made by these algorithms may produce short paths and avoid direct collisions, they do not consider social norms, such as walking on the right side and passing on the left.  This can cause inconvenience for humans. We define human-aware navigation as the ability of the robot to navigate while complying with social norms and ensuring human safety.    While many existing systems allow robots to navigate safely within crowds \cite{ferrer2017robot,sisbot2007human}, they still rely heavily on manually crafted models of human motion.	  As robots are increasingly becoming part of our everyday lives, it is essential for them to be aware of the surrounding humans while performing their tasks. Navigation is a basic skill for autonomous robots, but many traditional algorithms, such as A* and D*, do not consider the fact that the obstacles in the environment may be humans. While maneuvers made by these algorithms may produce short paths and avoid direct collisions, they do not consider social norms, such as walking on the right side and passing on the left.  This can cause inconvenience for humans. We define human-aware navigation as the ability of the robot to navigate while complying with social norms and ensuring human safety.    While many existing systems allow robots to navigate safely within crowds \cite{ferrer2017robot,sisbot2007human}, they still rely heavily on manually crafted models of human motion.	score:362
What can one do to be certain that they are not living in the matrix?	 From our perceptual experience, we can say that space is \textit{content-independent}, which means that it does not depend on the nature of the objects it contains\footnote{Einstein would of course beg to differ as the theory of relativity describes how mass and energy distort space-time. However we are only interested here in the scale of interaction that humans and robots have with their environment, a context in which such distortion does not manifest directly. }. Space is also \textit{shared} by an agent and its environment, which means that we perceive ourselves as immersed in the same space as objects that surround us. Finally space has an isotropic structure, which means that we experience space the same way in different contexts (different environments, or different positions/orientations of the agent).	 From our perceptual experience, we can say that space is \textit{content-independent}, which means that it does not depend on the nature of the objects it contains\footnote{Einstein would of course beg to differ as the theory of relativity describes how mass and energy distort space-time. However we are only interested here in the scale of interaction that humans and robots have with their environment, a context in which such distortion does not manifest directly. }. Space is also \textit{shared} by an agent and its environment, which means that we perceive ourselves as immersed in the same space as objects that surround us. Finally space has an isotropic structure, which means that we experience space the same way in different contexts (different environments, or different positions/orientations of the agent).	score:383
What can one do to be certain that they are not living in the matrix?	  One of the major practical challenges in training machine learning models for group-based policy goals is that we often do not know which examples belong to which groups.  the protected group labels may not be available for privacy or expense reasons, or simply because one did not forsee the need for protected group labels when collecting the training data.	  One of the major practical challenges in training machine learning models for group-based policy goals is that we often do not know which examples belong to which groups.  the protected group labels may not be available for privacy or expense reasons, or simply because one did not forsee the need for protected group labels when collecting the training data.	score:391
What can one do to be certain that they are not living in the matrix?	 If a damaged robot embeds a simulation of itself, then behaviors that rely on damaged parts will not be transferable: they will perform very differently in the self-model and in reality. During the adaptation process, the robot will thus create an approximated transferability function that classifies behaviors as ``working as expected'' and ``not working as expected''.  Hence the robot will possess an ``intuition'' of the damages but it will not explicitly represent or identify them. By optimizing both the transferability and the performance, the algorithm will look for the most efficient behaviors among those that only use the reliable parts of the robots. The robot will thus be able to sustain a functioning behavior when damage occurs by learning to avoid behaviors that it is unable to achieve in the real world.	 If a damaged robot embeds a simulation of itself, then behaviors that rely on damaged parts will not be transferable: they will perform very differently in the self-model and in reality. During the adaptation process, the robot will thus create an approximated transferability function that classifies behaviors as ``working as expected'' and ``not working as expected''.  Hence the robot will possess an ``intuition'' of the damages but it will not explicitly represent or identify them. By optimizing both the transferability and the performance, the algorithm will look for the most efficient behaviors among those that only use the reliable parts of the robots. The robot will thus be able to sustain a functioning behavior when damage occurs by learning to avoid behaviors that it is unable to achieve in the real world.	score:398
What can one do to be certain that they are not living in the matrix?	 They may be applied not only to all of the above tasks, but to any where appearance priors or feature extraction are needed, and they are also of interest to understanding human vision. Generative models must capture most of the features of a texture that are significant to human perception in order to be successful, whereas texture features used for discrimination need not.    The most prevalent tool for image and texture modelling are Markovian undirected graphical models, a.k.a. Markov random fields (MRFs). An MRF together with an explicit Gibbs probability distribution (GPD) is called herein a Markov--Gibbs random field (MGRF). MGRFs are particularly popular for image analysis involving the determination of boundaries (as in segmentation) or enforcing smoothness (e.	 They may be applied not only to all of the above tasks, but to any where appearance priors or feature extraction are needed, and they are also of interest to understanding human vision. Generative models must capture most of the features of a texture that are significant to human perception in order to be successful, whereas texture features used for discrimination need not.    The most prevalent tool for image and texture modelling are Markovian undirected graphical models, a.k.a. Markov random fields (MRFs). An MRF together with an explicit Gibbs probability distribution (GPD) is called herein a Markov--Gibbs random field (MGRF). MGRFs are particularly popular for image analysis involving the determination of boundaries (as in segmentation) or enforcing smoothness (e.	score:398
How is life without seniors in MBBS college as ours is the first batch of this particular college?	 The theoretical analysis is validated with an empirical study of the sample complexity. In the below SNR regime, our analysis and simulations reveal a surprisingly fast super-linear error reduction phase comprised of multiple nodes label corrections following a query. The super-linear phase is followed by a linear reduction phase that lasts until full recovery and where MEMC has a clear advantage over Random. We conclude our work with numerical experiments with SBM networks which validated our analysis, as well as experiments with real social networks showing a clear advantage of MEMC over the baseline and other state-of-the-art active learning.	 The theoretical analysis is validated with an empirical study of the sample complexity. In the below SNR regime, our analysis and simulations reveal a surprisingly fast super-linear error reduction phase comprised of multiple nodes label corrections following a query. The super-linear phase is followed by a linear reduction phase that lasts until full recovery and where MEMC has a clear advantage over Random. We conclude our work with numerical experiments with SBM networks which validated our analysis, as well as experiments with real social networks showing a clear advantage of MEMC over the baseline and other state-of-the-art active learning.	score:381
How is life without seniors in MBBS college as ours is the first batch of this particular college?	 We are able to analyze follow-the-perturbed-leader (FTPL) as well as follow-the-regularized leader (FTRL) based algorithms resulting in both {\em zero-order} and {\em first-order regret bounds}; see Section~\ref{sec:prelim} for definitions of these bounds. However, our techniques are especially well-suited to proving first-order bounds for perturbation based methods.  Historically, the understanding of the regularization based algorithms has been more advanced thanks to connections with ideas from optimization such as duality and Bregman divergences. There has been recent work \citep{abernethy2014online} on developing a general framework to analyze FTPL algorithms, but it only yields zero-order bounds. A general framework that can yield first-order bounds for FTPL algorithms has been lacking so far, but we believe that the framework outlined in this paper may fill this gap in the literature.	 We are able to analyze follow-the-perturbed-leader (FTPL) as well as follow-the-regularized leader (FTRL) based algorithms resulting in both {\em zero-order} and {\em first-order regret bounds}; see Section~\ref{sec:prelim} for definitions of these bounds. However, our techniques are especially well-suited to proving first-order bounds for perturbation based methods.  Historically, the understanding of the regularization based algorithms has been more advanced thanks to connections with ideas from optimization such as duality and Bregman divergences. There has been recent work \citep{abernethy2014online} on developing a general framework to analyze FTPL algorithms, but it only yields zero-order bounds. A general framework that can yield first-order bounds for FTPL algorithms has been lacking so far, but we believe that the framework outlined in this paper may fill this gap in the literature.	score:385
How is life without seniors in MBBS college as ours is the first batch of this particular college?	 However, our procedure and the analysis are considerably simpler (thanks to the fact that we only use the so-called Restricted Isometry Property (RIP) for the $\ell_1$ norm instead of $\ell_p$ for $p > 1$). Moreover, our particular construction is immediately extendable for use in the Hadamard transform problem (due to the linearity properties).                The rest of the article is organized as follows.  Section~\ref{s:prelim} discusses notation and the straightforward observation that the sparse DHT problem reduces to compressed sensing with query access to the Discrete Hadamard Transform of the underlying sparse signal. Also the notion of Restricted Isometry Property, lossless condensers, and unbalanced expander graphs are introduced in this section.	 However, our procedure and the analysis are considerably simpler (thanks to the fact that we only use the so-called Restricted Isometry Property (RIP) for the $\ell_1$ norm instead of $\ell_p$ for $p > 1$). Moreover, our particular construction is immediately extendable for use in the Hadamard transform problem (due to the linearity properties).                The rest of the article is organized as follows.  Section~\ref{s:prelim} discusses notation and the straightforward observation that the sparse DHT problem reduces to compressed sensing with query access to the Discrete Hadamard Transform of the underlying sparse signal. Also the notion of Restricted Isometry Property, lossless condensers, and unbalanced expander graphs are introduced in this section.	score:385
How is life without seniors in MBBS college as ours is the first batch of this particular college?	 The first method incorporates the averaged prior beliefs among neighbors with the new private observation, while the second one takes into account the observations in the neighborhood as well. In both scenarios, we establish that the estimates are eventually unbiased, and we characterize an explicit expression for the mean-square deviation(MSD) of the beliefs from the truth, per individual.  Interestingly, this quantity relies on the whole spectrum of the communication matrix which exhibits the formidable role of the network structure in the asymptotic learning. We observe that the estimators outperform the upper bound provided for MSD in the previous work\cite{acemoglu2008convergence}. Furthermore, only one of the two proposed estimators can compete with the centralized optimal Kalman Filter\cite{kalman1960new} in certain circumstances.	 The first method incorporates the averaged prior beliefs among neighbors with the new private observation, while the second one takes into account the observations in the neighborhood as well. In both scenarios, we establish that the estimates are eventually unbiased, and we characterize an explicit expression for the mean-square deviation(MSD) of the beliefs from the truth, per individual.  Interestingly, this quantity relies on the whole spectrum of the communication matrix which exhibits the formidable role of the network structure in the asymptotic learning. We observe that the estimators outperform the upper bound provided for MSD in the previous work\cite{acemoglu2008convergence}. Furthermore, only one of the two proposed estimators can compete with the centralized optimal Kalman Filter\cite{kalman1960new} in certain circumstances.	score:387
How is life without seniors in MBBS college as ours is the first batch of this particular college?	 Then by introducing some intermediate events, we can decompose the expected pulling time of a suboptimal arm $i$ into several terms, each of which depends on only the reward or only the cost. After that, we can bound each term by the concentration inequalities and two gaps with careful derivations.   To our knowledge, it is the first time that Thompson sampling is applied to the budgeted MAB problem. We conduct a set of numerical simulations with different rewards/costs distributions and different number of arms. The simulation results demonstrate that our proposed algorithm is much better than several baseline algorithms.	 Then by introducing some intermediate events, we can decompose the expected pulling time of a suboptimal arm $i$ into several terms, each of which depends on only the reward or only the cost. After that, we can bound each term by the concentration inequalities and two gaps with careful derivations.   To our knowledge, it is the first time that Thompson sampling is applied to the budgeted MAB problem. We conduct a set of numerical simulations with different rewards/costs distributions and different number of arms. The simulation results demonstrate that our proposed algorithm is much better than several baseline algorithms.	score:387
Does Clinton have an influence on CNN (Clinton News Networks?)	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	score:391
Does Clinton have an influence on CNN (Clinton News Networks?)	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	score:391
Does Clinton have an influence on CNN (Clinton News Networks?)	   In this paper, to explore the information hidden in the outputs, we apply two unsupervised learning algorithms, Principle Component Analysis (PCA) and Independent Component Analysis (ICA)  to the outputs of a CNN trained on the ImageNet dataset \cite{deng2009imagenet} of 1000 object classes. Both PCA and ICA are special cases of the Factor Analysis model, with different assumptions on the latent variables.  Factor Analysis is a statistical model which can be used for revealing hidden factors that underlie a vector of random variables. In the case of CNN for image classification, the neurons or computational units in the output layer of a CNN, as random variables, represent object classes. A latent factor might represent a common visual attribute shared by several object classes. It is therefore desirable to visualize, interpret and make use of the Factor Analysis models learned on the outputs of a trained CNN.	 Factor Analysis is a statistical model which can be used for revealing hidden factors that underlie a vector of random variables. In the case of CNN for image classification, the neurons or computational units in the output layer of a CNN, as random variables, represent object classes. A latent factor might represent a common visual attribute shared by several object classes. It is therefore desirable to visualize, interpret and make use of the Factor Analysis models learned on the outputs of a trained CNN.	score:397
Does Clinton have an influence on CNN (Clinton News Networks?)	 However, we cannot simply apply CNN and LSTM on demand prediction problem. If treating the demand over an entire city as an image and applying CNN on this image, we fail to achieve the best result. We realize including regions with weak correlations to predict a target region actually hurts the performance. To address this issue, we propose a novel local CNN method which only considers spatially nearby regions.  This local CNN method is motivated by the First Law of Geography: ``near things are more related than distant things,''~\cite{tobler1970computer} and it is also supported by observations from real data that demand patterns are more correlated for spatially close regions.  While local CNN method filters weakly correlated remote regions, this fails to consider the case that two locations could be spatially distant but are similar in their demand patterns (i.	 However, we cannot simply apply CNN and LSTM on demand prediction problem. If treating the demand over an entire city as an image and applying CNN on this image, we fail to achieve the best result. We realize including regions with weak correlations to predict a target region actually hurts the performance. To address this issue, we propose a novel local CNN method which only considers spatially nearby regions.  This local CNN method is motivated by the First Law of Geography: ``near things are more related than distant things,''~\cite{tobler1970computer} and it is also supported by observations from real data that demand patterns are more correlated for spatially close regions.  While local CNN method filters weakly correlated remote regions, this fails to consider the case that two locations could be spatially distant but are similar in their demand patterns (i.	score:415
Does Clinton have an influence on CNN (Clinton News Networks?)	  To overcome the windowing issue, we propose to use a convolutional neural network (CNN) which can label the properties of each amino acid in the entire target sequence all at once. CNNs have been used successfully in computer vision \cite{pinheiro_recurrent_2013,szegedy_going_2014} and natural language processing \cite{kim_convolutional_2014,collobert_unified_2008}.  In addition to parameter sharing and pooling, which reduce computation, CNNs are also highly parallelizable.  Thus, CNNs can achieve a much greater speedup compared to a windowed MLP approach. The issue when trying to label each position in an input sequence with a CNN is that pooling leads to a decreased output resolution. To handle this issue, we propose a new multilayer shift-and-stitch method which allows us to efficiently label each target input at full resolution in a  computationally efficient manner.   We show that a MUltilayer Shift-and-sTitch CNN (MUST-CNN) trained end-to-end and per-position on protein property prediction beats the state-of-the-art without other machinery. To our knowledge, we are the first to train convolutional networks end-to-end for per-position protein property prediction. Both learning and inference are performed on entire arbitrarily sized sequences.	 In addition to parameter sharing and pooling, which reduce computation, CNNs are also highly parallelizable.  Thus, CNNs can achieve a much greater speedup compared to a windowed MLP approach. The issue when trying to label each position in an input sequence with a CNN is that pooling leads to a decreased output resolution. To handle this issue, we propose a new multilayer shift-and-stitch method which allows us to efficiently label each target input at full resolution in a  computationally efficient manner.	score:419
Did CNN and/or other major news organizations hold secret strategy meetings with the Clinton campaign?	 Most representative progress was made by \newcite{zeng14}, who proposed a CNN-based approach that can deliver quite competitive results without any extra knowledge resource and NLP modules. Following the success of CNN, there are some valuable models  such as multi-window CNN\cite{nguyenrelation}, CR-CNN\cite{dos2015classifying} and NS-depLCNN\cite{xu2015semantic} been proposed recently,  which are all based on CNN structure.  Though some models also based on other structures like MV-RNN\cite{SocherEtAl2012:MVRNN}, FCM\cite{yufactor14} and SDP-LSTM\cite{xu2015classifying}, CNN occupies a leading position.  Despite the success obtained so far, most of the current CNN-based learning approaches to relation learning and classification are weak in modeling temporal patterns. Though SDP-LSTM algorithm utilize recurrent structure on dependency parsing, no further analysis has been shown to compare RNN with CNN models.	 Most representative progress was made by \newcite{zeng14}, who proposed a CNN-based approach that can deliver quite competitive results without any extra knowledge resource and NLP modules. Following the success of CNN, there are some valuable models  such as multi-window CNN\cite{nguyenrelation}, CR-CNN\cite{dos2015classifying} and NS-depLCNN\cite{xu2015semantic} been proposed recently,  which are all based on CNN structure.  Though some models also based on other structures like MV-RNN\cite{SocherEtAl2012:MVRNN}, FCM\cite{yufactor14} and SDP-LSTM\cite{xu2015classifying}, CNN occupies a leading position.  Despite the success obtained so far, most of the current CNN-based learning approaches to relation learning and classification are weak in modeling temporal patterns. Though SDP-LSTM algorithm utilize recurrent structure on dependency parsing, no further analysis has been shown to compare RNN with CNN models.	score:407
Did CNN and/or other major news organizations hold secret strategy meetings with the Clinton campaign?	  On the other hand, Convolutional Neural Networks (CNN) typically consist of lesser number of parameters than FNNs and RNNs due to its weight sharing property. CNNs already proved its efficacy on extracting features in speech recognition \cite{abdel2014convolutional, amodei2015deep} or on eliminating noises in images \cite{mao2016image, he2015deep}.  But upon our knowledge, CNNs have not been tested in speech enhancement.  In this paper, we attempted to find a `memory efficient' denoising algorithm for babble noise that creates minimal artifacts and that can be implemented in an embedded device: the hearing aid. Through experiments, we demonstrated that CNN can perform better than Feedforward Neural Networks (FNN) or Recurrent Neural Networks (RNN) with much smaller network size.	  On the other hand, Convolutional Neural Networks (CNN) typically consist of lesser number of parameters than FNNs and RNNs due to its weight sharing property. CNNs already proved its efficacy on extracting features in speech recognition \cite{abdel2014convolutional, amodei2015deep} or on eliminating noises in images \cite{mao2016image, he2015deep}.  But upon our knowledge, CNNs have not been tested in speech enhancement.  In this paper, we attempted to find a `memory efficient' denoising algorithm for babble noise that creates minimal artifacts and that can be implemented in an embedded device: the hearing aid. Through experiments, we demonstrated that CNN can perform better than Feedforward Neural Networks (FNN) or Recurrent Neural Networks (RNN) with much smaller network size.	score:408
Did CNN and/or other major news organizations hold secret strategy meetings with the Clinton campaign?	 Person re-identification~(Re-ID) aims at matching images of the same person across disjoint camera views, which is a challenging problem in multimedia analysis, multimedia editing and content-based media retrieval communities. The major challenge lies in how to preserve similarity of the same person across video footages with large appearance variations, while discriminating different individuals.  To address this problem, conventional methods usually consider the pairwise similarity between persons by only measuring the point to point~(P2P) distance. In this paper, we propose to use deep learning technique to model a novel set to set~(S2S) distance, in which the underline objective focuses on preserving the compactness of intra-class samples for each camera view, while maximizing the margin between the intra-class set and inter-class set.	 Person re-identification~(Re-ID) aims at matching images of the same person across disjoint camera views, which is a challenging problem in multimedia analysis, multimedia editing and content-based media retrieval communities. The major challenge lies in how to preserve similarity of the same person across video footages with large appearance variations, while discriminating different individuals.  To address this problem, conventional methods usually consider the pairwise similarity between persons by only measuring the point to point~(P2P) distance. In this paper, we propose to use deep learning technique to model a novel set to set~(S2S) distance, in which the underline objective focuses on preserving the compactness of intra-class samples for each camera view, while maximizing the margin between the intra-class set and inter-class set.	score:409
Did CNN and/or other major news organizations hold secret strategy meetings with the Clinton campaign?	 Thanks to the condition, even with the bad control-knob that keeps oscillating the probabilistic accuracy up and down, we can preserve the privacy of CNNs in a certain level that the condition designates.  This paper is organized as follows: Section~\ref{sec:problem_description} describes the problem that controls the privacy loss of CNN with the IFMs of layers.  In Section~\ref{sec:proposed}, the degree of sanitization is introduced as the boundary condition that the method of decreasing the probabilistic accuracy should satisfy. Also, the IFM approximation scheme that reduces the accuracy and its network-wise control method are proposed. Section~\ref{sec:evaluation} evaluates the proposed scheme on the layers of AlexNet in Caffe~\cite{caffe} CNN framework. Finally, Section~\ref{sec:conclusion} concludes with the summary of our contribution.	  Thus, as presented in~\cite{sexual_orientation}, if CNN is used to leak a very private matter of individuals, it is very hard to prevent CNN from exposing the privacy because we do not know what noise can hamper the recognition of a CNN. Actually, some noise that does not affect humans' recognition does make a huge difference for CNN~\cite{intriguing}.  Also, some representation that does not make any sense for human may work for CNN~\cite{DNN_fool}. One thing obvious is that CNN learns all the things from the data fed into it. This implies what CNN recognizes as a noise comes from the data set. In this paper, we mainly focus on how to add the effective noise that disrupts CNN's recognition for the purpose of privacy protection.	score:410
Did CNN and/or other major news organizations hold secret strategy meetings with the Clinton campaign?	 Since their inception, CNNs have utilized some type of striding operator to reduce the overlap of receptive fields and spatial dimensions. Although having clear heuristic motivations (i.e. lowering the number of parameters to learn) the mathematical role of striding within CNN learning remains unclear. This paper offers a novel and mathematical rigorous perspective on the role of the striding operator within modern CNNs.  Specifically, we demonstrate theoretically that one can always represent a CNN that incorporates striding with an equivalent non-striding CNN which has more filters and smaller size. Through this equivalence we are then able to characterize striding as an additional mechanism for parameter sharing among channels, thus reducing training complexity. Finally, the framework presented in this paper offers a new mathematical perspective on the role of striding which we hope shall facilitate and simplify the future theoretical analysis of CNNs.	 Since their inception, CNNs have utilized some type of striding operator to reduce the overlap of receptive fields and spatial dimensions. Although having clear heuristic motivations (i.e. lowering the number of parameters to learn) the mathematical role of striding within CNN learning remains unclear. This paper offers a novel and mathematical rigorous perspective on the role of the striding operator within modern CNNs.  Specifically, we demonstrate theoretically that one can always represent a CNN that incorporates striding with an equivalent non-striding CNN which has more filters and smaller size. Through this equivalence we are then able to characterize striding as an additional mechanism for parameter sharing among channels, thus reducing training complexity. Finally, the framework presented in this paper offers a new mathematical perspective on the role of striding which we hope shall facilitate and simplify the future theoretical analysis of CNNs.	score:412
Why was the reclassification of Pluto as a dwarf planet met with such controversy?	  Consider, by way of example, the problem of detecting exoplanets via the so called transit method first proposed by \cite{struve1952proposal}. The luminosity of a star is measured at regular intervals, with the aim of detecting segments of reduced luminosity. These indicate the transit of a planet \citep{Transit} and can naturally be interpreted as collective anomalies.  The light curves are typically preprocessed \citep{Preprocessing} and both the raw and whitened light curves can be accessed online. We have included the whitened light curve of the star Kepler 1132 in Figure \ref{fig:Kepler1132First} to illustrate the nature of this type of data. We note the presence of a global anomaly on day 1550 and the noisy nature of the data.	  Consider, by way of example, the problem of detecting exoplanets via the so called transit method first proposed by \cite{struve1952proposal}. The luminosity of a star is measured at regular intervals, with the aim of detecting segments of reduced luminosity. These indicate the transit of a planet \citep{Transit} and can naturally be interpreted as collective anomalies.  The light curves are typically preprocessed \citep{Preprocessing} and both the raw and whitened light curves can be accessed online. We have included the whitened light curve of the star Kepler 1132 in Figure \ref{fig:Kepler1132First} to illustrate the nature of this type of data. We note the presence of a global anomaly on day 1550 and the noisy nature of the data.	score:298
Why was the reclassification of Pluto as a dwarf planet met with such controversy?	   What makes this penalty especially useful for transport problems is that the solution to~\eqref{eq:penalized} can be computed quickly by a simple iterative algorithm known as the \emph{Sinkhorn} or \emph{RAS} algorithm~\citep{Sin67}. This fact was popularized by~\citet{Cut13}, and his work led to widespread adoption of the entropic penalty for computing optimal transport.  The introduction of the entropic penalty makes an enormous difference in practice: since optimal transport can be formulated as a linear program, it can, of course, be solved in polynomial time, but the numerical experiments conducted by~\citet{Cut13} indicate that the same linear program with an entropic penalty can be solved up to 10,000 times faster, as long as $\eta$ is not too large.	   What makes this penalty especially useful for transport problems is that the solution to~\eqref{eq:penalized} can be computed quickly by a simple iterative algorithm known as the \emph{Sinkhorn} or \emph{RAS} algorithm~\citep{Sin67}. This fact was popularized by~\citet{Cut13}, and his work led to widespread adoption of the entropic penalty for computing optimal transport.  The introduction of the entropic penalty makes an enormous difference in practice: since optimal transport can be formulated as a linear program, it can, of course, be solved in polynomial time, but the numerical experiments conducted by~\citet{Cut13} indicate that the same linear program with an entropic penalty can be solved up to 10,000 times faster, as long as $\eta$ is not too large.	score:364
Why was the reclassification of Pluto as a dwarf planet met with such controversy?	 These types of algorithms also have the advantage that additional  information, such as galaxy profiles, morphology, concentration, or environmental properties, can be included in the \pz estimation process in addition to the standard galaxy magnitudes or colors. These methods, however, are only reliable within the limits of the training data, and sufficient caution must be exercised when extrapolating these algorithms beyond those limits.   All of the aforementioned techniques can be categorized as supervised learning algorithms, where the input attributes (\eg magnitudes or colors) are provided along with the desired outputs (\eg redshift), which are all employed during the learning process. In this sense, the redshift information from the training set \textit{supervises} the training phase.	 These types of algorithms also have the advantage that additional  information, such as galaxy profiles, morphology, concentration, or environmental properties, can be included in the \pz estimation process in addition to the standard galaxy magnitudes or colors. These methods, however, are only reliable within the limits of the training data, and sufficient caution must be exercised when extrapolating these algorithms beyond those limits.   All of the aforementioned techniques can be categorized as supervised learning algorithms, where the input attributes (\eg magnitudes or colors) are provided along with the desired outputs (\eg redshift), which are all employed during the learning process. In this sense, the redshift information from the training set \textit{supervises} the training phase.	score:367
Why was the reclassification of Pluto as a dwarf planet met with such controversy?	 And the bilingual evaluation understudy (BLEU) metric \cite{papineni2002bleu} is used to evaluate the similarity between the uploaded search tasks and the generated captions, so as to prioritize the images such that the ones with higher similarity scores will be transmitted first. Moreover, this system can also be deployed as a multi-instance content-based search engine on ground-based planetary data systems, such as NASA's Planetary Data System (PDS) Imaging Atlas\footnote{\url{https://pds-imaging.	 And the bilingual evaluation understudy (BLEU) metric \cite{papineni2002bleu} is used to evaluate the similarity between the uploaded search tasks and the generated captions, so as to prioritize the images such that the ones with higher similarity scores will be transmitted first. Moreover, this system can also be deployed as a multi-instance content-based search engine on ground-based planetary data systems, such as NASA's Planetary Data System (PDS) Imaging Atlas\footnote{\url{https://pds-imaging.	score:370
Why was the reclassification of Pluto as a dwarf planet met with such controversy?	    Our approach is inspired by recent progress regarding the convex relaxation of k-means~\cite{PengW07,AwasBCKVW15,IguchiMPV17,LLLSW17} and follows the ``Relax, no need to round''-paradigm.  Note however, while the convex relaxation of k-means in~\cite{AwasBCKVW15,IguchiMPV17,LLLSW17} can provide a theoretical analysis concerning a successful computation of the optimal solution to the k-means objective function, it cannot overcome the fundamental limitations of the k-means objective function itself vis-a-vis nonconvex and anisotropic clusters.  One attempt to address the latter shortcomings of k-means consists in replacing the Euclidean distance with a kernel function, leading to the aptly named kernel k-means algorithm~\cite{DhillonGK04,XingJ03}. One can interpret the SDP spectral clustering framework derived in the current paper as an extension of the theoretical analysis of the convex relaxation k-means approach in~\cite{AwasBCKVW15,LLLSW17} to kernel k-means.	    Our approach is inspired by recent progress regarding the convex relaxation of k-means~\cite{PengW07,AwasBCKVW15,IguchiMPV17,LLLSW17} and follows the ``Relax, no need to round''-paradigm.  Note however, while the convex relaxation of k-means in~\cite{AwasBCKVW15,IguchiMPV17,LLLSW17} can provide a theoretical analysis concerning a successful computation of the optimal solution to the k-means objective function, it cannot overcome the fundamental limitations of the k-means objective function itself vis-a-vis nonconvex and anisotropic clusters.  One attempt to address the latter shortcomings of k-means consists in replacing the Euclidean distance with a kernel function, leading to the aptly named kernel k-means algorithm~\cite{DhillonGK04,XingJ03}. One can interpret the SDP spectral clustering framework derived in the current paper as an extension of the theoretical analysis of the convex relaxation k-means approach in~\cite{AwasBCKVW15,LLLSW17} to kernel k-means.	score:373
I am a DOSA guy in BITS Pilani, Pilani campus 2014 batch. But I have good interest in being part of CRC or EC. What should I know?	g., to select and monitor functionally relevant features for brain-computer interface protocols in stroke rehabilitation. 	\IEEEPARstart{M}{any} cortical processes on microscopic and macroscopic levels can be described by oscillatory features~\cite{wang_neurophysiological_2010}. When recording rhythmic brain activity by non-invasive imaging techniques such as electroencephalography (EEG), these signals allow to extract task-related information but also enable to describe and characterize the underlying dynamics of cognitive or motor processes~\cite{lopesdasilva_eeg_2013}.  While many studies reported that oscillations are correlated with such processes~\cite{dijk:2008}, some studies have even proposed a causal relation~\cite{herrmann_eeg_2016}.     A state-of-the-art approach to characterize oscillatory activity from EEG recordings is based on verifying the presence of time-locked, frequency-specific envelope modulations.	g., to select and monitor functionally relevant features for brain-computer interface protocols in stroke rehabilitation. 	\IEEEPARstart{M}{any} cortical processes on microscopic and macroscopic levels can be described by oscillatory features~\cite{wang_neurophysiological_2010}. When recording rhythmic brain activity by non-invasive imaging techniques such as electroencephalography (EEG), these signals allow to extract task-related information but also enable to describe and characterize the underlying dynamics of cognitive or motor processes~\cite{lopesdasilva_eeg_2013}.  While many studies reported that oscillations are correlated with such processes~\cite{dijk:2008}, some studies have even proposed a causal relation~\cite{herrmann_eeg_2016}.     A state-of-the-art approach to characterize oscillatory activity from EEG recordings is based on verifying the presence of time-locked, frequency-specific envelope modulations.	score:400
I am a DOSA guy in BITS Pilani, Pilani campus 2014 batch. But I have good interest in being part of CRC or EC. What should I know?	  \item The proposed algorithm combines the recent advances of sparsity and robust PCA into a joint framework to leverage the mutual benefit. To the best of our knowledge, this is the first convex sparse and robust PCA algorithm, which ensures our algorithm always achieves the global optima.  \item Different from the existing robust PCA algorithms \cite{robustPCAMAYI} \cite{RPCAOP}, which can only deal with the in-sample data, our algorithm is capable of mapping the data which are unseen during the training phase.	  \item The proposed algorithm combines the recent advances of sparsity and robust PCA into a joint framework to leverage the mutual benefit. To the best of our knowledge, this is the first convex sparse and robust PCA algorithm, which ensures our algorithm always achieves the global optima.  \item Different from the existing robust PCA algorithms \cite{robustPCAMAYI} \cite{RPCAOP}, which can only deal with the in-sample data, our algorithm is capable of mapping the data which are unseen during the training phase.	score:401
I am a DOSA guy in BITS Pilani, Pilani campus 2014 batch. But I have good interest in being part of CRC or EC. What should I know?	) EI proposes maximizing the expected improvement over the current best known point: \begin{equation} EI(x) = \mu(x) - f^{*}\Phi(\gamma) + \sigma(x) \phi(\gamma) \label{eq:ei} \end{equation} where $\phi$ denotes the PDF of the standard normal distribution.  By maximizing the expectation in this way, EI is able to more efficiently weigh the risk-reward balance of acquiring a data point, as it considers not just the probability that a data point offers an improvement over the current best, but also how large that improvement will be.  Thus a larger, but more uncertain, reward can be preferred to a small but high-probability reward (which would have been selected using PI).   EI has been shown to have strong theoretical guarantees \cite{vazquez_convergence_2010}and empirical effectiveness \cite{snoek_practical_2012} and so we use it throughout this study as the baseline.	) EI proposes maximizing the expected improvement over the current best known point: \begin{equation} EI(x) = \mu(x) - f^{*}\Phi(\gamma) + \sigma(x) \phi(\gamma) \label{eq:ei} \end{equation} where $\phi$ denotes the PDF of the standard normal distribution.  By maximizing the expectation in this way, EI is able to more efficiently weigh the risk-reward balance of acquiring a data point, as it considers not just the probability that a data point offers an improvement over the current best, but also how large that improvement will be.  Thus a larger, but more uncertain, reward can be preferred to a small but high-probability reward (which would have been selected using PI).   EI has been shown to have strong theoretical guarantees \cite{vazquez_convergence_2010}and empirical effectiveness \cite{snoek_practical_2012} and so we use it throughout this study as the baseline.	score:403
I am a DOSA guy in BITS Pilani, Pilani campus 2014 batch. But I have good interest in being part of CRC or EC. What should I know?	 However, SCD requires the computation of the whole gradient vector in each iteration which is prohibitive (except for special applications, cf.~\citet{Dhillon:2011uh,Shrivastava:2014:ALS}).     In this paper we propose approximate steepest coordinate descent (ASCD), a novel scheme which combines the best parts of the aforementioned strategies: (i) ASCD maintains an approximation of the \emph{full} gradient in each iteration and selects the active coordinate   among the components of this vector that have large absolute values --- similar to SCD; and (ii) in many situations the gradient approximation can be updated cheaply at no extra cost --- similar to  UCD.	 However, SCD requires the computation of the whole gradient vector in each iteration which is prohibitive (except for special applications, cf.~\citet{Dhillon:2011uh,Shrivastava:2014:ALS}).     In this paper we propose approximate steepest coordinate descent (ASCD), a novel scheme which combines the best parts of the aforementioned strategies: (i) ASCD maintains an approximation of the \emph{full} gradient in each iteration and selects the active coordinate   among the components of this vector that have large absolute values --- similar to SCD; and (ii) in many situations the gradient approximation can be updated cheaply at no extra cost --- similar to  UCD.	score:406
I am a DOSA guy in BITS Pilani, Pilani campus 2014 batch. But I have good interest in being part of CRC or EC. What should I know?	 These observations motivate us to study whether strong theoretical guarantees under \DCSBM can be established for convex optimization-based methods.  Building on the work of~\cite{CSX2013} and~\cite{Cai2014robust}, we introduce in \prettyref{sec:methodology} a new community detection approach called Convexified Modularity Maximization (CMM). CMM is based on convexification of the elegant modularity maximization formulation, followed by a novel and computationally tractable weighted $ \ell_1 $-norm $ k $-median clustering procedure. As we show in~\prettyref{sec:theory} and~\prettyref{sec:simulation}, our approach has strong theoretical guarantees, applicable even in the sparse graph regime with bounded average degree, as well as state-of-the-art empirical performance. In both aspects our approach is comparable to or improves upon the best-known results in the literature.	 These observations motivate us to study whether strong theoretical guarantees under \DCSBM can be established for convex optimization-based methods.  Building on the work of~\cite{CSX2013} and~\cite{Cai2014robust}, we introduce in \prettyref{sec:methodology} a new community detection approach called Convexified Modularity Maximization (CMM). CMM is based on convexification of the elegant modularity maximization formulation, followed by a novel and computationally tractable weighted $ \ell_1 $-norm $ k $-median clustering procedure. As we show in~\prettyref{sec:theory} and~\prettyref{sec:simulation}, our approach has strong theoretical guarantees, applicable even in the sparse graph regime with bounded average degree, as well as state-of-the-art empirical performance. In both aspects our approach is comparable to or improves upon the best-known results in the literature.	score:406
What does 'Series' mean in classification of BMW cars?	 Also in CAR, it's important to note that the weights assigned to various $n$-step returns  change based on the different states from which bootstrapping is done. In this sense, CAR weights are dynamic and using them represents a significant level of sophistication as compared to the usage of $\lambda$-returns.   \\ \\ In summary, our contributions are: \begin{enumerate} \item To alleviate the need for some ad-hoc choice of weights as in the case of $\lambda$-returns, we propose a generalization called \textit{Autodidactic Returns} and further present a novel derivative of it called  \textit{Confidence-based Autodidactic Returns (CAR)} in the DRL setting.	 Also in CAR, it's important to note that the weights assigned to various $n$-step returns  change based on the different states from which bootstrapping is done. In this sense, CAR weights are dynamic and using them represents a significant level of sophistication as compared to the usage of $\lambda$-returns.   \\ \\ In summary, our contributions are: \begin{enumerate} \item To alleviate the need for some ad-hoc choice of weights as in the case of $\lambda$-returns, we propose a generalization called \textit{Autodidactic Returns} and further present a novel derivative of it called  \textit{Confidence-based Autodidactic Returns (CAR)} in the DRL setting.	score:467
What does 'Series' mean in classification of BMW cars?	 Among the main non- linear methods, one should mention Classification and Regression Trees (CART) and its general version,  Random Forest, Neural Networks, Nearest Neighbor Classifiers, Support Vector  Machines (SVM) and  probabilistic methods. For detailed accounts of these methodologies, the reader can consult \cite{Cart}, \cite{Breiman}, \cite{Breiman2}, \cite{Duda}, \cite{Devroye}, \cite{Hastie} and \cite{CrisShaw}.   Support Vector Machines is a widely used approach in classification, introduced  by Boser, Guyon and Vapnik \cite{Boser}, combining ideas from linear  classification methods, Optimization and Reproducing Kernel Hilbert Spaces.  (See also \cite{Cortes} and \cite{CrisShaw} for details). The  success of SVM in terms of classification error in a variety of  contexts, can be explained, in part, by their flexibility.	 Among the main non- linear methods, one should mention Classification and Regression Trees (CART) and its general version,  Random Forest, Neural Networks, Nearest Neighbor Classifiers, Support Vector  Machines (SVM) and  probabilistic methods. For detailed accounts of these methodologies, the reader can consult \cite{Cart}, \cite{Breiman}, \cite{Breiman2}, \cite{Duda}, \cite{Devroye}, \cite{Hastie} and \cite{CrisShaw}.   Support Vector Machines is a widely used approach in classification, introduced  by Boser, Guyon and Vapnik \cite{Boser}, combining ideas from linear  classification methods, Optimization and Reproducing Kernel Hilbert Spaces.  (See also \cite{Cortes} and \cite{CrisShaw} for details). The  success of SVM in terms of classification error in a variety of  contexts, can be explained, in part, by their flexibility.	score:474
What does 'Series' mean in classification of BMW cars?	\ref{fig:intro} [SVR]).  However, imagine knowing that there also exist many other motor vehicles  (trucks, mini-vans, {\em etc}). Even without having visually seen such objects, the very basic knowledge that they {\em exist} in the world and are closely related to a {\em car} should, in principal, alter the criterion for recognizing instance as a {\em car} (making the recognition criterion stricter in this case).	\ref{fig:intro} [SVR]).  However, imagine knowing that there also exist many other motor vehicles  (trucks, mini-vans, {\em etc}). Even without having visually seen such objects, the very basic knowledge that they {\em exist} in the world and are closely related to a {\em car} should, in principal, alter the criterion for recognizing instance as a {\em car} (making the recognition criterion stricter in this case).	score:476
What does 'Series' mean in classification of BMW cars?	   However, all of the aforementioned methods aim at improving the \emph{overall} robustness of the classifier. This means that the methods to improve robustness are designed to prevent seed examples in any class from being misclassified as any other class. Achieving such a goal (at least for some definitions of adversarial robustness) requires producing a perfect classifier, and has, unsurprisingly, remained elusive.  Indeed, \citet{mahloujifar2018curse} proved that if the metric probability space is concentrated, overall adversarial robustness is unattainable for any classifier with initial constant error.  We argue that overall robustness may not be the appropriate criteria for measuring system performance in security-sensitive applications, since only certain kinds of adversarial misclassifications pose meaningful threats that provide value for potential adversaries.	   However, all of the aforementioned methods aim at improving the \emph{overall} robustness of the classifier. This means that the methods to improve robustness are designed to prevent seed examples in any class from being misclassified as any other class. Achieving such a goal (at least for some definitions of adversarial robustness) requires producing a perfect classifier, and has, unsurprisingly, remained elusive.  Indeed, \citet{mahloujifar2018curse} proved that if the metric probability space is concentrated, overall adversarial robustness is unattainable for any classifier with initial constant error.  We argue that overall robustness may not be the appropriate criteria for measuring system performance in security-sensitive applications, since only certain kinds of adversarial misclassifications pose meaningful threats that provide value for potential adversaries.	score:481
What does 'Series' mean in classification of BMW cars?	g. medical diagnosis systems or self-driving cars and opens up possibilities to actively attack an ML system in an adversarial way \cite{PapEtAl2016a,LiuEtAl2016,KurGooBen2016a,LiuEtAl2016}. Moreover, this non-robustness has also implications on follow-up processes like interpretability. How should we be able to interpret classifier decisions if very small changes of the input lead to different decisions?    The finding of the non-robustness initiated a competition where on the one hand increasingly more sophisticated attack procedures were proposed  \cite{GooShlSze2015,HuaEtAl2016,MooFawFro2016,CarWag2016} and on the other hand research was focused to develop  stronger defenses  against these attacks \cite{GuRig2015,GooShlSze2015,ZheEtAl2016,PapEtAl2016a,HuaEtAl2016,BasEtAl2016,MadEtAl2018}.	g. medical diagnosis systems or self-driving cars and opens up possibilities to actively attack an ML system in an adversarial way \cite{PapEtAl2016a,LiuEtAl2016,KurGooBen2016a,LiuEtAl2016}. Moreover, this non-robustness has also implications on follow-up processes like interpretability. How should we be able to interpret classifier decisions if very small changes of the input lead to different decisions?    The finding of the non-robustness initiated a competition where on the one hand increasingly more sophisticated attack procedures were proposed  \cite{GooShlSze2015,HuaEtAl2016,MooFawFro2016,CarWag2016} and on the other hand research was focused to develop  stronger defenses  against these attacks \cite{GuRig2015,GooShlSze2015,ZheEtAl2016,PapEtAl2016a,HuaEtAl2016,BasEtAl2016,MadEtAl2018}.	score:481
What is epochs in machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:472
What is epochs in machine learning?	 \textbf{Introduction:} Machine learning provides fundamental tools both for scientific research and for the development of technologies with significant impact on society. It provides methods that facilitate the discovery of regularities in data and that give predictions without explicit knowledge of the rules governing a system. However, a price is paid for exploiting such flexibility: machine learning methods are typically black-boxes where it is difficult to fully understand what the machine is doing or how it is operating.  This poses constraints on the applicability and explainability of such methods. \textbf{Methods:} Our research aims to open the black-box of recurrent neural networks, an important family of neural networks used for processing sequential data. We propose a novel methodology that provides a mechanistic interpretation of behaviour when solving a computational task.	 \textbf{Introduction:} Machine learning provides fundamental tools both for scientific research and for the development of technologies with significant impact on society. It provides methods that facilitate the discovery of regularities in data and that give predictions without explicit knowledge of the rules governing a system. However, a price is paid for exploiting such flexibility: machine learning methods are typically black-boxes where it is difficult to fully understand what the machine is doing or how it is operating.  This poses constraints on the applicability and explainability of such methods. \textbf{Methods:} Our research aims to open the black-box of recurrent neural networks, an important family of neural networks used for processing sequential data. We propose a novel methodology that provides a mechanistic interpretation of behaviour when solving a computational task.	score:481
What is epochs in machine learning?	   Optimization of machine learning models is commonly performed through stochastic gradient updates on    randomly ordered training examples. This practice means that   sub-epochs comprise of independent   random samples of the training data that may not preserve informative   structure present in the full data.  We hypothesize that the   training can be more effective with {\em self-similar}  arrangements that potentially allow each epoch to   provide benefits of multiple ones.  We study this for  ``matrix factorization'' -- the common task of learning metric embeddings of entities such as queries, videos,  or words from  example pairwise associations. We construct arrangements that  preserve the weighted Jaccard similarities of  rows and columns and experimentally observe training acceleration of 3\%-37\% on synthetic and recommendation datasets.	   Optimization of machine learning models is commonly performed through stochastic gradient updates on    randomly ordered training examples. This practice means that   sub-epochs comprise of independent   random samples of the training data that may not preserve informative   structure present in the full data.  We hypothesize that the   training can be more effective with {\em self-similar}  arrangements that potentially allow each epoch to   provide benefits of multiple ones.  We study this for  ``matrix factorization'' -- the common task of learning metric embeddings of entities such as queries, videos,  or words from  example pairwise associations. We construct arrangements that  preserve the weighted Jaccard similarities of  rows and columns and experimentally observe training acceleration of 3\%-37\% on synthetic and recommendation datasets.	score:487
What is epochs in machine learning?	 Many machine learning algorithms are based on the assumption that training examples are drawn independently.  However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects,  and hence share the features of these shared objects.  We show that the classic approach of ignoring this problem potentially can have a harmful effect on the accuracy of statistics, and then consider alternatives.   One of these is to only use independent examples, discarding other information.  However, this is clearly suboptimal.  We analyze sample error bounds in this networked setting, providing significantly improved results.  An important component of our approach is formed by efficient sample weighting schemes, which leads to novel concentration inequalities.	 Many machine learning algorithms are based on the assumption that training examples are drawn independently.  However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects,  and hence share the features of these shared objects.  We show that the classic approach of ignoring this problem potentially can have a harmful effect on the accuracy of statistics, and then consider alternatives.   One of these is to only use independent examples, discarding other information.  However, this is clearly suboptimal.  We analyze sample error bounds in this networked setting, providing significantly improved results.  An important component of our approach is formed by efficient sample weighting schemes, which leads to novel concentration inequalities.	score:501
What is epochs in machine learning?	 An alternative approach is to first break the system into sub modules, where each individual module has a clear semantic meaning. Second, each individual module is constructed either by using a machine learning approach (e.g. the module is trained in an end-to-end manner) or by relying on manual engineering The choice of which option to use for each module is based on empirical success.  We call this approach Semantic Abstraction.  To further demonstrate the two approaches, consider a simplified autonomous driving system, where a car driving in a highway should keep its lane and adapt its speed according to other vehicles. The input to the system is the sensory input (e.g., a video stream from a camera and a radar signal). The output is a two dimensional vector consisting of a steering command and an acceleration/deceleration command.	 An alternative approach is to first break the system into sub modules, where each individual module has a clear semantic meaning. Second, each individual module is constructed either by using a machine learning approach (e.g. the module is trained in an end-to-end manner) or by relying on manual engineering The choice of which option to use for each module is based on empirical success.  We call this approach Semantic Abstraction.  To further demonstrate the two approaches, consider a simplified autonomous driving system, where a car driving in a highway should keep its lane and adapt its speed according to other vehicles. The input to the system is the sensory input (e.g., a video stream from a camera and a radar signal). The output is a two dimensional vector consisting of a steering command and an acceleration/deceleration command.	score:504
What are the uses of matrix in information technology?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	  In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	score:403
What are the uses of matrix in information technology?	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	score:427
What are the uses of matrix in information technology?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:432
What are the uses of matrix in information technology?	 	Suppose we observe a subset of the entries of an unknown low-rank matrix~$M$, can we recover the matrix~$M$ knowing this subset alone? This problem, called Matrix Completion, is of fundamental interest in a number of fields including statistics, machine learning, signal processing and theoretical computer science. It is widely applicable to the design of recommender systems as popularized by the famous Netflix Prize.   We are interested in understanding the compuational complexity of Matrix Completion.  Much of the theory of Matrix Completion revolves around a beautiful line of positive results. These results show that under certain assumptions there is a natural semidefinite relaxation that solves the problem efficiently even if the number of visible entries is asymptotically much smaller than the total number of entries~\cite{CandesR09,CandesT10,Recht11}.	 	Suppose we observe a subset of the entries of an unknown low-rank matrix~$M$, can we recover the matrix~$M$ knowing this subset alone? This problem, called Matrix Completion, is of fundamental interest in a number of fields including statistics, machine learning, signal processing and theoretical computer science. It is widely applicable to the design of recommender systems as popularized by the famous Netflix Prize.   We are interested in understanding the compuational complexity of Matrix Completion.  Much of the theory of Matrix Completion revolves around a beautiful line of positive results. These results show that under certain assumptions there is a natural semidefinite relaxation that solves the problem efficiently even if the number of visible entries is asymptotically much smaller than the total number of entries~\cite{CandesR09,CandesT10,Recht11}.	score:440
What are the uses of matrix in information technology?	 Organizations need to rapidly turn these terabytes of raw data into significant insights for their users to guide their research, marketing, investment, and/or management strategies.  Matrix factorization is a fundamental data analysis technique. Whereas its usefulness as a theoretical tool is beyond doubt now, its usage in practical situations has undergone a few challenges in recent years.  Among other factors contributing to this are new developments in computer hardware architecture and new applications in the information sciences.  Perhaps the key aspect is that the matrices to analyze are becoming astonishingly big. Classical algorithms are not designed to cope with the amount of information present in these large-scale problems. We may even hypothesize that, if proper tools for these problems were widely available for commercial computer power, such rich datasets would be created at an increasing speed.	 Organizations need to rapidly turn these terabytes of raw data into significant insights for their users to guide their research, marketing, investment, and/or management strategies.  Matrix factorization is a fundamental data analysis technique. Whereas its usefulness as a theoretical tool is beyond doubt now, its usage in practical situations has undergone a few challenges in recent years.  Among other factors contributing to this are new developments in computer hardware architecture and new applications in the information sciences.  Perhaps the key aspect is that the matrices to analyze are becoming astonishingly big. Classical algorithms are not designed to cope with the amount of information present in these large-scale problems. We may even hypothesize that, if proper tools for these problems were widely available for commercial computer power, such rich datasets would be created at an increasing speed.	score:445
How can I learn machine learning?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:446
How can I learn machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:452
How can I learn machine learning?	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.    \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	 However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	score:452
How can I learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:454
How can I learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:454
What kind of clustering algorithm and parameters are used in clustering friends into friends cluster by the Wolfram|Alpha Facebook report? Which Facebook SDK are they using?	  The analysis of  k-flats, however, requires  developing substantially new mathematical tools.                                          The rest of the paper is organized as follows.  In section~\ref{sec:algo}, we describe the formal  setting and the algorithms that we study. We begin our analysis by discussing the reconstruction properties of k-means in section~\ref{sec:disc}.    In section~\ref{sec:results}, we present and discuss our main results, whose proofs are postponed to the appendices.	  The analysis of  k-flats, however, requires  developing substantially new mathematical tools.                                          The rest of the paper is organized as follows.  In section~\ref{sec:algo}, we describe the formal  setting and the algorithms that we study. We begin our analysis by discussing the reconstruction properties of k-means in section~\ref{sec:disc}.    In section~\ref{sec:results}, we present and discuss our main results, whose proofs are postponed to the appendices.	score:359
What kind of clustering algorithm and parameters are used in clustering friends into friends cluster by the Wolfram|Alpha Facebook report? Which Facebook SDK are they using?	 The need for explanations encompasses various perspectives, including those of managers, customer-facing employees, and the technical team~\cite{EDC}.       An important aspect of fine-grained behavior data is its very high dimensionality and sparsity~\cite{junque2014}. Returning to our running example: the Facebook like data can be represented as a matrix where each row (data instance) corresponds to a person, and each column (feature) corresponds to a page on Facebook that one can like.  If someone likes a page, the entry is 1, and 0 otherwise. There exists a huge number of possible pages to like, and hence there a huge number of dimensions. On the other hand, a particular user will like only a very small proportion of these pages.   In ``traditional'' data mining (working with non-behavior data), feature selection and dimensionality reduction techniques are often employed to cope with high dimensionality.	 The need for explanations encompasses various perspectives, including those of managers, customer-facing employees, and the technical team~\cite{EDC}.       An important aspect of fine-grained behavior data is its very high dimensionality and sparsity~\cite{junque2014}. Returning to our running example: the Facebook like data can be represented as a matrix where each row (data instance) corresponds to a person, and each column (feature) corresponds to a page on Facebook that one can like.  If someone likes a page, the entry is 1, and 0 otherwise. There exists a huge number of possible pages to like, and hence there a huge number of dimensions. On the other hand, a particular user will like only a very small proportion of these pages.   In ``traditional'' data mining (working with non-behavior data), feature selection and dimensionality reduction techniques are often employed to cope with high dimensionality.	score:364
What kind of clustering algorithm and parameters are used in clustering friends into friends cluster by the Wolfram|Alpha Facebook report? Which Facebook SDK are they using?	 CF models, however, suffer from data sparsity and the imbalance of ratings. They perform poorly on cold users and cold items for which there are no or few data.   To overcome these weaknesses, additional sources of information are integrated into RSs. Social networking and knowledge sharing sites like Twitter.com, Facebook.com, Epinions.com, and Ciao. com are popular platforms for users to connect to each other, to participate in online activities, and to generate shared content or opinions~\cite{li2015recommending}. Social relations~\cite{SoRec,tranMF,DecTrust,LOCABAL,TrustSVD} and item reviews~\cite{jakob09review,ganu09,CTR,HFT,TopicMF} provide independent and diverse sources for recommendation beyond the explicit rating information source, which present both opportunities and challenges for traditional RSs.	 CF models, however, suffer from data sparsity and the imbalance of ratings. They perform poorly on cold users and cold items for which there are no or few data.   To overcome these weaknesses, additional sources of information are integrated into RSs. Social networking and knowledge sharing sites like Twitter.com, Facebook.com, Epinions.com, and Ciao. com are popular platforms for users to connect to each other, to participate in online activities, and to generate shared content or opinions~\cite{li2015recommending}. Social relations~\cite{SoRec,tranMF,DecTrust,LOCABAL,TrustSVD} and item reviews~\cite{jakob09review,ganu09,CTR,HFT,TopicMF} provide independent and diverse sources for recommendation beyond the explicit rating information source, which present both opportunities and challenges for traditional RSs.	score:373
What kind of clustering algorithm and parameters are used in clustering friends into friends cluster by the Wolfram|Alpha Facebook report? Which Facebook SDK are they using?	  Partially observed graphs appear in many applications. For example, in online social networks like Facebook, we observe an edge/no edge between two users when they accept each other as a friend or explicitly decline a friendship suggestion. For the other user pairs, however, we simply have no friendship information between them, which are thus unobserved.  More generally, we have partial observations whenever obtaining similarity data is difficult or expensive (e.g., because it requires human participation). In these applications, it is often the case that \emph{most} pairs are unobserved, which is the regime we are particularly interested in.  As with any clustering problem, we need a precise mathematical definition of the clustering criterion with potentially a guaranteed performance.	  Partially observed graphs appear in many applications. For example, in online social networks like Facebook, we observe an edge/no edge between two users when they accept each other as a friend or explicitly decline a friendship suggestion. For the other user pairs, however, we simply have no friendship information between them, which are thus unobserved.  More generally, we have partial observations whenever obtaining similarity data is difficult or expensive (e.g., because it requires human participation). In these applications, it is often the case that \emph{most} pairs are unobserved, which is the regime we are particularly interested in.  As with any clustering problem, we need a precise mathematical definition of the clustering criterion with potentially a guaranteed performance.	score:375
What kind of clustering algorithm and parameters are used in clustering friends into friends cluster by the Wolfram|Alpha Facebook report? Which Facebook SDK are they using?	 In a standard cluster analysis, such as k-means, in addition to clusters locations and distances between them, it's important to know if they are connected or well separated from each other. The main focus of this paper is discovering the relations between the resulting clusters. We propose a new method which is based on pairwise overlapping k-means clustering, that in addition to means of clusters provides the graph structure of their relations.	 In a standard cluster analysis, such as k-means, in addition to clusters locations and distances between them, it's important to know if they are connected or well separated from each other. The main focus of this paper is discovering the relations between the resulting clusters. We propose a new method which is based on pairwise overlapping k-means clustering, that in addition to means of clusters provides the graph structure of their relations.	score:378
Do I need a training data if I use clustering?	 To encode this requirement in the training process of a neural network, we employ so-called generative adversarial networks (GANs). These methods train a {\em generator}, as well as a second network, the {\em discriminator}  that learns to judge how closely the generated output matches the ground truth data. In this way, we train a specialized, data-driven loss function alongside the generative network, while making sure it is differentiable and compatible with the training process.  We not only employ this adversarial approach for the smoke density outputs, but we also train a specialized and novel adversarial loss function that learns to judge the temporal coherence of the outputs.   We additionally present best practices to set up a training pipeline for physics-based GANs. E.g., we found it particularly useful to have physics-aware data augmentation functionality in place.	 To encode this requirement in the training process of a neural network, we employ so-called generative adversarial networks (GANs). These methods train a {\em generator}, as well as a second network, the {\em discriminator}  that learns to judge how closely the generated output matches the ground truth data. In this way, we train a specialized, data-driven loss function alongside the generative network, while making sure it is differentiable and compatible with the training process.  We not only employ this adversarial approach for the smoke density outputs, but we also train a specialized and novel adversarial loss function that learns to judge the temporal coherence of the outputs.   We additionally present best practices to set up a training pipeline for physics-based GANs. E.g., we found it particularly useful to have physics-aware data augmentation functionality in place.	score:418
Do I need a training data if I use clustering?	 The computation can be implemented in a \emph{batch} setting, i.e., one-shot over the entire training data, or in a \emph{sequential} setting where the data is processed incrementally. In either setting, the scalability of the computation grows with the number of points in the training data. The accuracy of the predictions depends upon the values of a set of parameters that define the Gaussian process, termed as the \emph{hyperparameters}.  The hyperparameters are selected by optimizing an appropriate cost function that captures the prediction error over the dataset, and is typically a complex problem from a computational perspective. This paper addresses both of these scalability aspects of Gaussian processes -- a randomized matrix factorization approach to efficiently compute the predictions and an efficient approach for the hyperparameter selection problem presently applicable only to a class of kernel functions.	 The computation can be implemented in a \emph{batch} setting, i.e., one-shot over the entire training data, or in a \emph{sequential} setting where the data is processed incrementally. In either setting, the scalability of the computation grows with the number of points in the training data. The accuracy of the predictions depends upon the values of a set of parameters that define the Gaussian process, termed as the \emph{hyperparameters}.  The hyperparameters are selected by optimizing an appropriate cost function that captures the prediction error over the dataset, and is typically a complex problem from a computational perspective. This paper addresses both of these scalability aspects of Gaussian processes -- a randomized matrix factorization approach to efficiently compute the predictions and an efficient approach for the hyperparameter selection problem presently applicable only to a class of kernel functions.	score:427
Do I need a training data if I use clustering?	 The computation can be implemented in a \emph{batch} setting, i.e., one-shot over the entire training data, or in a \emph{sequential} setting where the data is processed incrementally. In either setting, the scalability of the computation grows with the number of points in the training data. The accuracy of the predictions depends upon the values of a set of parameters that define the Gaussian process, termed as the \emph{hyperparameters}.  The hyperparameters are selected by optimizing an appropriate cost function that captures the prediction error over the dataset, and is typically a complex problem from a computational perspective. This paper addresses both of these scalability aspects of Gaussian processes -- a randomized matrix factorization approach to efficiently compute the predictions and an efficient approach for the hyperparameter selection problem presently applicable only to a class of kernel functions.	 The computation can be implemented in a \emph{batch} setting, i.e., one-shot over the entire training data, or in a \emph{sequential} setting where the data is processed incrementally. In either setting, the scalability of the computation grows with the number of points in the training data. The accuracy of the predictions depends upon the values of a set of parameters that define the Gaussian process, termed as the \emph{hyperparameters}.  The hyperparameters are selected by optimizing an appropriate cost function that captures the prediction error over the dataset, and is typically a complex problem from a computational perspective. This paper addresses both of these scalability aspects of Gaussian processes -- a randomized matrix factorization approach to efficiently compute the predictions and an efficient approach for the hyperparameter selection problem presently applicable only to a class of kernel functions.	score:427
Do I need a training data if I use clustering?	 As a result, each hidden neuron becomes better at recognizing specific data patterns and the overall model can learn meaningful representations of the input data. After training the model, each hidden neuron is distinct from the others and no competition is needed in the testing/encoding phase. We conduct comprehensive experiments qualitatively and quantitatively to evaluate \alg and to demonstrate the effectiveness of our model.	 As a result, each hidden neuron becomes better at recognizing specific data patterns and the overall model can learn meaningful representations of the input data. After training the model, each hidden neuron is distinct from the others and no competition is needed in the testing/encoding phase. We conduct comprehensive experiments qualitatively and quantitatively to evaluate \alg and to demonstrate the effectiveness of our model.	score:428
Do I need a training data if I use clustering?	 In Section \ref{sec:nn_training}, we outline practical intricacies of our approach, ranging from considerations related to generating synthetic, tailored \emph{labeled data} for training, validation and testing to tricks of the trade when training neural networks and performing hyperparameter optimization. Finally, in Section \ref{sec:numerical_results}, we collect the results of our numerical experiments.	 In Section \ref{sec:nn_training}, we outline practical intricacies of our approach, ranging from considerations related to generating synthetic, tailored \emph{labeled data} for training, validation and testing to tricks of the trade when training neural networks and performing hyperparameter optimization. Finally, in Section \ref{sec:numerical_results}, we collect the results of our numerical experiments.	score:431
What is the new machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:428
What is the new machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:465
What is the new machine learning?	  Arguments in recent debates have claimed that it is only the accuracy of machine learning models that matters, not their interpretability.  However, taking this view ignores the fact that the overall system in high-stakes settings is a machine learning model communicating with a human who makes the final decision, and thus it is the accuracy of the overall system that is of relevance.   Interpretable machine learning models are an appropriate means for communication between AI and human \cite{DhurandharILS2017}; the contribution of this paper is to abstractly model the overall system and theoretically show the system performance advantage of interpretable machine learning models over black box machine learning models.  In this paper, we consider the population setting (the limit as the number of samples goes to infinity, allowing access to the probability distributions of the data) and appeal to the theory of distributed detection and data fusion \cite{Varshney1997}.	  Arguments in recent debates have claimed that it is only the accuracy of machine learning models that matters, not their interpretability.  However, taking this view ignores the fact that the overall system in high-stakes settings is a machine learning model communicating with a human who makes the final decision, and thus it is the accuracy of the overall system that is of relevance.   Interpretable machine learning models are an appropriate means for communication between AI and human \cite{DhurandharILS2017}; the contribution of this paper is to abstractly model the overall system and theoretically show the system performance advantage of interpretable machine learning models over black box machine learning models.  In this paper, we consider the population setting (the limit as the number of samples goes to infinity, allowing access to the probability distributions of the data) and appeal to the theory of distributed detection and data fusion \cite{Varshney1997}.	score:467
What is the new machine learning?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:472
What is the new machine learning?	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	score:476
Closed captions on CNN are badly garbled and unreadable and have been that way for hours. Why can't CNN fix that in a timely manner?	  We performed time domain analysis of the preprocessed data and extracted satisfactory features.  Considering the above problem, we choose CNN to achieve this work and BP as a comparison.  Finally, CNN detects the tool breakage in a minute with the 93\% accuracy, the performance of our model is not good as Corne,  but we found the performance of CNN is more robust and reliable than BP, and we believe the performance of CNN still has a lot of room for improvement.   The rest of this paper is organized as follows, In Section \ref{sec:bg}, we talk about the technical background, which contains the main techniques used in the study. In Section \ref{sec:approach}, we introduce the design of the experiment for detecting tool breakage.  In Section \ref{sec:results}, we show our analysis results such as time domain features and the results of our network. Section \ref{sec:conclusion} shows the conclusion part of this study.	  We performed time domain analysis of the preprocessed data and extracted satisfactory features.  Considering the above problem, we choose CNN to achieve this work and BP as a comparison.  Finally, CNN detects the tool breakage in a minute with the 93\% accuracy, the performance of our model is not good as Corne,  but we found the performance of CNN is more robust and reliable than BP, and we believe the performance of CNN still has a lot of room for improvement.   The rest of this paper is organized as follows, In Section \ref{sec:bg}, we talk about the technical background, which contains the main techniques used in the study. In Section \ref{sec:approach}, we introduce the design of the experiment for detecting tool breakage.  In Section \ref{sec:results}, we show our analysis results such as time domain features and the results of our network. Section \ref{sec:conclusion} shows the conclusion part of this study.	score:328
Closed captions on CNN are badly garbled and unreadable and have been that way for hours. Why can't CNN fix that in a timely manner?	   To the extent that representations in the brain are similar to those in CNNs trained on e.g. ImageNet, the brain must be arriving at these representations by different, largely unsupervised routes. Another key difference is that CNNs are fundamentally static and lack a notion of time, whereas neuronal systems are highly dynamic, producing responses that vary dramatically in time, even in response to static inputs.  Figure \ref{on_off}a shows a typical response profile of a visual cortical neuron to a static input \cite{Schmolesky_1998}. The neuron produces a brief transient response to the onset of the visual stimulus, followed by near total suppression of that response. When the stimulus is removed, the neuron responds again with a transient burst of activity (known as an ``off'' response).	   To the extent that representations in the brain are similar to those in CNNs trained on e.g. ImageNet, the brain must be arriving at these representations by different, largely unsupervised routes. Another key difference is that CNNs are fundamentally static and lack a notion of time, whereas neuronal systems are highly dynamic, producing responses that vary dramatically in time, even in response to static inputs.  Figure \ref{on_off}a shows a typical response profile of a visual cortical neuron to a static input \cite{Schmolesky_1998}. The neuron produces a brief transient response to the onset of the visual stimulus, followed by near total suppression of that response. When the stimulus is removed, the neuron responds again with a transient burst of activity (known as an ``off'' response).	score:333
Closed captions on CNN are badly garbled and unreadable and have been that way for hours. Why can't CNN fix that in a timely manner?	e. `hard') and whose size is comparable to the available positive examples, thus re-balancing the dataset. As an example training in Region-CNN uses the Hard Negative Mining procedure of~\cite{Sung1996} and it requires hours to complete.    We compose our pipeline by first adopting the Region Proposal Newtork (RPN) presented in Faster R-CNN~\cite{ren2015_faster} for learning the candidate regions: this is a double-layer Convolutional Neural Network (CNN)  that can be trained off-line in relatively short time and, since it is object-agnostic, can be used for multiple object detection tasks.  Using another deep CNN we encode each region into a set of feature vectors. These feature vectors are then fed to a kernel-based classifier which is trained on-line. In doing so, for the first off-line part, we basically adopt the architecture of Faster R-CNN~\cite{ren2015_faster}, except that the RPN and the feature extractor CNN are learned on one task, and then re-used on different tasks.	e. `hard') and whose size is comparable to the available positive examples, thus re-balancing the dataset. As an example training in Region-CNN uses the Hard Negative Mining procedure of~\cite{Sung1996} and it requires hours to complete.    We compose our pipeline by first adopting the Region Proposal Newtork (RPN) presented in Faster R-CNN~\cite{ren2015_faster} for learning the candidate regions: this is a double-layer Convolutional Neural Network (CNN)  that can be trained off-line in relatively short time and, since it is object-agnostic, can be used for multiple object detection tasks.  Using another deep CNN we encode each region into a set of feature vectors. These feature vectors are then fed to a kernel-based classifier which is trained on-line. In doing so, for the first off-line part, we basically adopt the architecture of Faster R-CNN~\cite{ren2015_faster}, except that the RPN and the feature extractor CNN are learned on one task, and then re-used on different tasks.	score:337
Closed captions on CNN are badly garbled and unreadable and have been that way for hours. Why can't CNN fix that in a timely manner?	   Deep learning has gained much success in sentence-level relation classification.   For example, convolutional neural networks (CNN) have delivered competitive performance   without much effort on feature engineering as the conventional pattern-based methods.   Thus a lot of works have been produced based on CNN structures. However, a key issue   that has not been well addressed by the CNN-based method is the lack of capability   to learn temporal features, especially long-distance dependency between nominal pairs.	   Deep learning has gained much success in sentence-level relation classification.   For example, convolutional neural networks (CNN) have delivered competitive performance   without much effort on feature engineering as the conventional pattern-based methods.   Thus a lot of works have been produced based on CNN structures. However, a key issue   that has not been well addressed by the CNN-based method is the lack of capability   to learn temporal features, especially long-distance dependency between nominal pairs.	score:345
Closed captions on CNN are badly garbled and unreadable and have been that way for hours. Why can't CNN fix that in a timely manner?	 CNN models are static models, and are potentially weak especially when learning long-distance relation patterns. For example, the CNN model can learn only local patterns, and so is hard to deal with patterns that is outside of the window of the convolutional filter. Dependency path can alleviate this problem by removing noise compared with natural sequence input (for example NS-depLCNN\cite{xu2015semantic}), but the computation cost and adding error caused by dependency parser is inevitable.  And the same limitation still exists when the dependency path is long.  \begin{figure*}[!htb]  \centering \includegraphics[height=7.4cm,width=11.6cm]{framework.pdf} \caption{The framework of the proposed model. } \label{fig:framework} \end{figure*}   In this paper, we propose a simple framework based on recurrent neural networks (RNN) to tackle the problem of long-distance pattern learning.	   Deep learning has gained much success in sentence-level relation classification.   For example, convolutional neural networks (CNN) have delivered competitive performance   without much effort on feature engineering as the conventional pattern-based methods.   Thus a lot of works have been produced based on CNN structures. However, a key issue   that has not been well addressed by the CNN-based method is the lack of capability   to learn temporal features, especially long-distance dependency between nominal pairs.	score:345
How many times of BBC and CNN talk about the WeiBo and WeChat in the reports from 2010 to 2015?	  	\vspace{-2pt} Nowadays, individuals or organizations can easily share or post information to the public on the social network. Take the popular Chinese microblogging website (Sina Weibo) as an example, the People's Daily, one of the media in China, posts more than tens of weibos (analogous to tweets) each day. Most of these weibos are well-written and highly informative because of the text length limitation (less than140 Chinese characters).  Such data is regarded as naturally annotated web resources~\cite{sun}. If we can mine these high-quality data from these naturally annotated web resources, it will be beneficial to the research that has been hampered by the lack of data.  \begin{figure}[!tb] \centering \includegraphics[width=\columnwidth]{figures/weibo.pdf} \vspace{-20pt} \caption{A Weibo Posted by People's Daily.	 For example, the popular document summarization dataset DUC\footnote{http://duc.nist.gov/data.html}, TAC\footnote{http://www.nist.gov/tac/2015/KBP/} and TREC\footnote{http://trec.nist.gov/} have only hundreds of human written English text summarizations. The problem is even worse for Chinese. In this paper, we take one step back and focus on constructing \textbf{LCSTS}, the \textbf{L}arge-scale \textbf{C}hinese \textbf{S}hort \textbf{T}ext \textbf{S}ummarization dataset by utilizing the naturally annotated web resources on Sina Weibo.  Figure~\ref{weibo-example} shows one weibo posted by the People's Daily. In order to convey the import information to the public quickly, it also writes a very informative and short summary (in the blue circle) of the news. Our goal is to mine a large scale, high-quality short text summarization dataset from these texts.  This paper makes the following contributions: (1) We introduce a large scale Chinese short text summarization dataset.  To our knowledge, it is the largest one to date; (2) We provide standard splits for the dataset into large scale training set and human labeled test set which will be easier for benchmarking the related methods; (3) We explore the properties of the dataset and sample 10,666 instances for manually checking and scoring the quality of the dataset; (4) We perform recurrent neural network based encoder-decoder method on the dataset to generate summary and get promising results, which can be used as one baseline of the task.	score:427
How many times of BBC and CNN talk about the WeiBo and WeChat in the reports from 2010 to 2015?	 Experimental result shows the high performance of our algorithm. Some possible improvements are discussed, which leads to further study. 	Online social networking services like Twitter have been tremendously popular, with a considerable speed of user growth. Thousands of new registrations are observed everyday in dominant platforms like Sina and Tencent Microblog since the introduction of microblog - Chinese twitter - in 2007.  Celebrities and organizations also register microblog, which leads to diversity of topics and helps attract more potential users. However, flooded information can puzzle the users and even result in the loss of them. So reducing the risk of puzzlement and recommending attractive items - specific users selected for recommendation - are crucial for user experience improvement and prosperity maintenance, which present opportunities for novel machine learning and data mining approaches.	 Experimental result shows the high performance of our algorithm. Some possible improvements are discussed, which leads to further study. 	Online social networking services like Twitter have been tremendously popular, with a considerable speed of user growth. Thousands of new registrations are observed everyday in dominant platforms like Sina and Tencent Microblog since the introduction of microblog - Chinese twitter - in 2007.  Celebrities and organizations also register microblog, which leads to diversity of topics and helps attract more potential users. However, flooded information can puzzle the users and even result in the loss of them. So reducing the risk of puzzlement and recommending attractive items - specific users selected for recommendation - are crucial for user experience improvement and prosperity maintenance, which present opportunities for novel machine learning and data mining approaches.	score:446
How many times of BBC and CNN talk about the WeiBo and WeChat in the reports from 2010 to 2015?	   \\*   The findings reported in this paper, other than being supported by a   thorough experimental methodology and interesting on their own, also   pave the way for further investigation on the novel issue of fake Twitter followers.    	Originally started as a personal microblogging site,  Twitter has been transformed by common use to an information publishing venue.   Statistics reported about a billion of Twitter subscribers, with 302 million monthly active users\footnote{C. Smith, By The Numbers: 150+ Amazing Twitter Statistics, http://goo.gl/o1lNi8 - June 2015. Last checked: July 23, 2015.}.  Twitter annual advertising revenue in 2014 has been estimated to around \$480 million\footnote{Statistic Brain, Twitter statistics, http://goo. gl/XEXB1 - March 2015. Last checked: July 23, 2015.}.  Popular public characters, such as actors and singers, as well as traditional mass media (radio, TV, and newspapers) use Twitter as a new media channel.  Such a versatility and spread of use have made Twitter the ideal arena for proliferation of  anomalous accounts, that behave in unconventional ways.  Academia has mostly focused its attention on \textit{spammers}, those accounts actively putting their efforts in spreading malware,  sending spam, and advertising activities of doubtful legality~\cite{chu2012detecting, Yang:2013, ahmed2013generic, miller2014twitter}. To enhance their effectiveness, these malicious accounts are often armed with automated twitting programs, as stealthy as to mimic real users, known as {\em bots}.	   \\*   The findings reported in this paper, other than being supported by a   thorough experimental methodology and interesting on their own, also   pave the way for further investigation on the novel issue of fake Twitter followers.    	Originally started as a personal microblogging site,  Twitter has been transformed by common use to an information publishing venue.   Statistics reported about a billion of Twitter subscribers, with 302 million monthly active users\footnote{C. Smith, By The Numbers: 150+ Amazing Twitter Statistics, http://goo.gl/o1lNi8 - June 2015. Last checked: July 23, 2015.}.  Twitter annual advertising revenue in 2014 has been estimated to around \$480 million\footnote{Statistic Brain, Twitter statistics, http://goo. gl/XEXB1 - March 2015. Last checked: July 23, 2015.}.  Popular public characters, such as actors and singers, as well as traditional mass media (radio, TV, and newspapers) use Twitter as a new media channel.  Such a versatility and spread of use have made Twitter the ideal arena for proliferation of  anomalous accounts, that behave in unconventional ways.  Academia has mostly focused its attention on \textit{spammers}, those accounts actively putting their efforts in spreading malware,  sending spam, and advertising activities of doubtful legality~\cite{chu2012detecting, Yang:2013, ahmed2013generic, miller2014twitter}. To enhance their effectiveness, these malicious accounts are often armed with automated twitting programs, as stealthy as to mimic real users, known as {\em bots}.	score:458
How many times of BBC and CNN talk about the WeiBo and WeChat in the reports from 2010 to 2015?	  We argue that while tweets are generally unstructured, Twitter is a useful source of reviews since  it provides a convenient platform for users to express their opinions.  Twitter is also integrated to a person's social life, making it easier for  users to express their opinions on products by tweeting instead of writing a review on review websites.      In this paper, we demonstrate the usefulness of Twitter as a source for aspect-based target-opinion mining.  We propose a novel LDA-based opinion model that is designed for tweets, which we name Twitter Opinion Topic Model (TOTM).  TOTM models the target-opinion interaction directly, which significantly improves opinion prediction, {\it e.g.}\ TOTM discovers {\it{`grilled'}} is positive for {\it{sausage}} but not other targets.	  We argue that while tweets are generally unstructured, Twitter is a useful source of reviews since  it provides a convenient platform for users to express their opinions.  Twitter is also integrated to a person's social life, making it easier for  users to express their opinions on products by tweeting instead of writing a review on review websites.      In this paper, we demonstrate the usefulness of Twitter as a source for aspect-based target-opinion mining.  We propose a novel LDA-based opinion model that is designed for tweets, which we name Twitter Opinion Topic Model (TOTM).  TOTM models the target-opinion interaction directly, which significantly improves opinion prediction, {\it e.g.}\ TOTM discovers {\it{`grilled'}} is positive for {\it{sausage}} but not other targets.	score:459
How many times of BBC and CNN talk about the WeiBo and WeChat in the reports from 2010 to 2015?	Tong.2011, Li.Tong.2016, tong2016np} in some common situations of data distortion, such as data obtained from censored Chinese social media.  Since 2009 when \textit{Sina Weibo} – the Chinese equivalent to Twitter – was launched, social media have created an unprecedented informational shock to the Chinese society. Notably, Sina Weibo enables millions of citizens to generate and communicate political information that is scarce in traditional media.  Government agents, media outlets, NGOs and firms, and researchers have invested heavily in machine learning techniques to mine the wealth of textual information circulated on Sina Weibo \citep{Economist.2013, ChinaInternet.2013, ChinaInternet.2014}. However, due to the potential effect of widespread political information on social unrest and regime stability, the Chinese government extensively censors social media \citep{chen2011internet, king2013censorship, king2014reverse}.  Such censorship gives rise to two major challenges faced by data analysts in their endeavor of text mining. First, although the Chinese government allows for relatively free information flow on social media for the purposes of surveillance and monitoring officials \citep{qin2017does}, censorship substantially reduces the amount of information circulating on social media that can practically be used to classify data and predict hidden social events.	 Government agents, media outlets, NGOs and firms, and researchers have invested heavily in machine learning techniques to mine the wealth of textual information circulated on Sina Weibo \citep{Economist.2013, ChinaInternet.2013, ChinaInternet.2014}. However, due to the potential effect of widespread political information on social unrest and regime stability, the Chinese government extensively censors social media \citep{chen2011internet, king2013censorship, king2014reverse}.	score:462
Where can I find computation power for students in machine learning free without credit card… Please no Google cloud, Azure or AWS link?	  	Machine learning serves as the backbone for a wide variety of cognitive tasks such as image classification, object recognition, and natural language processing. Today, applications can leverage state-of-the-art machine learning models by using cloud services that offer machine learning as a service~\cite{azure-ml, google-ai, aws-ml}. To handle large traffic, such service providers typically use a distributed setup with a large number of interconnected servers (compute nodes).   It is well-known that such a distributed compute infrastructure faces a number of unavailability events~\cite{jeff-dean-failures,rashmi2014hitchhiker,asterisxoring}. First, these clusters are typically built out of commodity components making failures the norm rather than the exception. Second, various factors including load imbalance and resource contention cause transient slowdowns.	  	Machine learning serves as the backbone for a wide variety of cognitive tasks such as image classification, object recognition, and natural language processing. Today, applications can leverage state-of-the-art machine learning models by using cloud services that offer machine learning as a service~\cite{azure-ml, google-ai, aws-ml}. To handle large traffic, such service providers typically use a distributed setup with a large number of interconnected servers (compute nodes).   It is well-known that such a distributed compute infrastructure faces a number of unavailability events~\cite{jeff-dean-failures,rashmi2014hitchhiker,asterisxoring}. First, these clusters are typically built out of commodity components making failures the norm rather than the exception. Second, various factors including load imbalance and resource contention cause transient slowdowns.	score:385
Where can I find computation power for students in machine learning free without credit card… Please no Google cloud, Azure or AWS link?	 Cloud based machine learning services (such as Amazon AWS Machine Learning\footnote{\url{https://aws.amazon.com/machine-learning/}} and Google Cloud Platform\footnote{\url{cloud.google.com/machine-learning}}), which provide APIs for accessing predictive analytics as a service, are also vulnerable to similar black box attacks \citep{tramer2016stealing}.    \begin{figure}[t] \centering \subfloat[An adversary making probes to the black box model \textit{C}, can learn it as \textit{C}', using active learning.]{\includegraphics[width=1\linewidth]{figs/adversarail_model.png}} \\  \subfloat[Example task of attacking behavioral CAPTCHA. Black box model $C$, based on Mouse Speed and Click Time features, is used to detect benign users from bots.	 Cloud based machine learning services (such as Amazon AWS Machine Learning\footnote{\url{https://aws.amazon.com/machine-learning/}} and Google Cloud Platform\footnote{\url{cloud.google.com/machine-learning}}), which provide APIs for accessing predictive analytics as a service, are also vulnerable to similar black box attacks \citep{tramer2016stealing}.    \begin{figure}[t] \centering \subfloat[An adversary making probes to the black box model \textit{C}, can learn it as \textit{C}', using active learning.]{\includegraphics[width=1\linewidth]{figs/adversarail_model.png}} \\  \subfloat[Example task of attacking behavioral CAPTCHA. Black box model $C$, based on Mouse Speed and Click Time features, is used to detect benign users from bots.	score:406
Where can I find computation power for students in machine learning free without credit card… Please no Google cloud, Azure or AWS link?	 Training deep neural networks requires access to large amounts of  data and computing powers. As a result, these neural networks are often trained by leveraging cheaper, yet more powerful cloud GPU clusters. Once trained, the inference phase can be completed in a reasonable amount of time, e.g., less than one second, using a single machine. Pre-trained models can be hosted for private use or offered as public cloud deep learning services~\cite{ws:googleCloudVision,ws:clarifai}.   To utilize cloud-based pre-trained models, mobile app developers use exposed cloud APIs to offload deep learning inference tasks, such as object recognition shown in Figure~\ref{fig:object_recognition}, to the hosting server. Mobile apps that execute inference tasks this way is referred to as \emph{cloud-based} deep inference.   Despite their increasing popularity, the use case scenarios of cloud-based deep inference  can be limited due to data privacy concern, unreliable network condition, and impact on battery life.   Alternatively, we can perform inference tasks locally using mobile CPU and GPU~\cite{cnndroid:2016}. We refer to this mobile deep learning approach as \emph{on-device} deep inference.  On-device deep inference can be a very attractive alternative to the cloud-based approach, e.g., by providing mobile applications the ability to function even without network access.	  To utilize cloud-based pre-trained models, mobile app developers use exposed cloud APIs to offload deep learning inference tasks, such as object recognition shown in Figure~\ref{fig:object_recognition}, to the hosting server. Mobile apps that execute inference tasks this way is referred to as \emph{cloud-based} deep inference.   Despite their increasing popularity, the use case scenarios of cloud-based deep inference  can be limited due to data privacy concern, unreliable network condition, and impact on battery life.	score:412
Where can I find computation power for students in machine learning free without credit card… Please no Google cloud, Azure or AWS link?	 	Recently, many efforts have been devoted to cloud machine learning (CML), where machine learning (ML) services are running on commercial providers' infrastructure. Examples include Microsoft Azure Machine Learning\footnote{http://azure.microsoft.com/en-us/services/machine-learning/}, Google Prediction API\footnote{https://developers.google.com/prediction/},  GraphLab\footnote{http://graphlab. com/} and Ersatz Labs\footnote{http://www.ersatzlabs.com/}, to name a few. CML allows training and deploying models on cloud servers. Once deployed users can use these models to make predictions without having to worry about maintaining the service and the models. Moreover, it allows the model owner to be paid for every prediction being made by the model.	 	Recently, many efforts have been devoted to cloud machine learning (CML), where machine learning (ML) services are running on commercial providers' infrastructure. Examples include Microsoft Azure Machine Learning\footnote{http://azure.microsoft.com/en-us/services/machine-learning/}, Google Prediction API\footnote{https://developers.google.com/prediction/},  GraphLab\footnote{http://graphlab. com/} and Ersatz Labs\footnote{http://www.ersatzlabs.com/}, to name a few. CML allows training and deploying models on cloud servers. Once deployed users can use these models to make predictions without having to worry about maintaining the service and the models. Moreover, it allows the model owner to be paid for every prediction being made by the model.	score:413
Where can I find computation power for students in machine learning free without credit card… Please no Google cloud, Azure or AWS link?	 Typical MLaaS platforms include Amazon Sagemaker \cite{amazonsagemaker}, Google Cloud ML Engine \cite{googleMLEngine}, Microsoft Azure ML Studio \cite{microsoftazure}. To use the MLaaS, customers provide training dataset and (or) machine learning algorithms to the cloud provider. The cloud provider sets up the machine learning environment, allocates certain amount of computation resources, and runs the model training task automatically.  The cloud provider can also offer the model serving services, in which a model trained in the cloud side or the customer side is stored in the cloud platform. The cloud provider releases query APIs to end users, enabling them to use the model for prediction or classification.   A training dataset is necessary to train and generate a machine learning model.	 Typical MLaaS platforms include Amazon Sagemaker \cite{amazonsagemaker}, Google Cloud ML Engine \cite{googleMLEngine}, Microsoft Azure ML Studio \cite{microsoftazure}. To use the MLaaS, customers provide training dataset and (or) machine learning algorithms to the cloud provider. The cloud provider sets up the machine learning environment, allocates certain amount of computation resources, and runs the model training task automatically.  The cloud provider can also offer the model serving services, in which a model trained in the cloud side or the customer side is stored in the cloud platform. The cloud provider releases query APIs to end users, enabling them to use the model for prediction or classification.   A training dataset is necessary to train and generate a machine learning model.	score:433
How do I write a batch program using spring batch?	 While dynamic declaration means that each minibatch can have its own computational architecture, the user is still responsible for batching operations themselves.} Our method relies on separating the graph construction from its execution, using operator overloading and lazy evaluation (\S\ref{sec:batching}). Once this separation is in place, we propose a fast batching heuristic that can be performed in real time, for each training instance (or minibatch), between the graph construction and its execution (\S\ref{sec:algorithm}).   We extend the DyNet toolkit \cite{dynet} with this capability. From the end-user's perspective, the result is a simple mechanism for exploiting efficient data-parallel algorithms in networks that would be cumbersome to batch by hand. The user simply defines the computation independently for each instance in the batch (using standard Python or C++ language constructs), and the framework takes care of the rest.	 This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations.  On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually. \footnote{The proposed algorithm is implemented in DyNet (\url{http://github.com/clab/dynet}), and can be activated by using the ``\texttt{-{}-dynet-autobatch 1}'' command line flag.	score:442
How do I write a batch program using spring batch?	 In addition, prior work has shown that decorrelated activations result in better features~\cite{1961_Barlow_possible,1992_NC_Schmidhuber,2009_NIPS_Bengio} and better generalization~\cite{2016_ICLR_Cogswell,2016_ICDM_Xiong}, suggesting room for further improving Batch Normalization.  In this paper, we propose Decorrelated Batch Normalization, in which we whiten the activations of each layer within a mini-batch.  Let $\mathbf{x}_i \in \mathbb{R}^d$ be the input to a layer for the $i$-th example in a mini-batch of size $m$.  The whitened input is given by {\setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt} \begin{equation} \hat{\mathbf{x}}_i = \Sigma^{-\frac{1}{2}}(\mathbf{x}_i - \mathbf{\mu}), \end{equation} } \hspace{-0.05in}where $\mathbf{\mu}=\frac{1}{m} \sum_{j=1}^{m} \mathbf{x}_j$ is the mini-batch mean and $\Sigma =\frac{1}{m} \sum_{j=1}^{m} (\mathbf{x}_j-\mathbf{\mu})(\mathbf{x}_j-\mathbf{\mu})^T$ is the mini-batch covariance matrix.	 In addition, prior work has shown that decorrelated activations result in better features~\cite{1961_Barlow_possible,1992_NC_Schmidhuber,2009_NIPS_Bengio} and better generalization~\cite{2016_ICLR_Cogswell,2016_ICDM_Xiong}, suggesting room for further improving Batch Normalization.  In this paper, we propose Decorrelated Batch Normalization, in which we whiten the activations of each layer within a mini-batch.  Let $\mathbf{x}_i \in \mathbb{R}^d$ be the input to a layer for the $i$-th example in a mini-batch of size $m$.  The whitened input is given by {\setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt} \begin{equation} \hat{\mathbf{x}}_i = \Sigma^{-\frac{1}{2}}(\mathbf{x}_i - \mathbf{\mu}), \end{equation} } \hspace{-0.05in}where $\mathbf{\mu}=\frac{1}{m} \sum_{j=1}^{m} \mathbf{x}_j$ is the mini-batch mean and $\Sigma =\frac{1}{m} \sum_{j=1}^{m} (\mathbf{x}_j-\mathbf{\mu})(\mathbf{x}_j-\mathbf{\mu})^T$ is the mini-batch covariance matrix.	score:451
How do I write a batch program using spring batch?	 If the loan-month is historical data, this value may be known, otherwise it it necessary to project it. The purpose of the example model is to predict next month's status for any loan-month in which we know this month's status. In this way, a Markov Chain can be built up, progressively projecting the status further and further into the future.   \paragraph{}   In each month, the borrower transitions from his current delinquency state to a new state depending on the number of payments made.  For a borrower who is Current (status "C"), he can make 1 payment (remaining "C"), 0 payments (to become "3"), or all the payments (to become "P"). Similarly, if a borrower is already in status "3", he could become more delinquent by missing additional payments and so on. A separate model can be fit for each loan status, modeling the transitions available to loan-months in that status, so for the purpose of this paper it is enough to consider only the status "C".	 If the loan-month is historical data, this value may be known, otherwise it it necessary to project it. The purpose of the example model is to predict next month's status for any loan-month in which we know this month's status. In this way, a Markov Chain can be built up, progressively projecting the status further and further into the future.   \paragraph{}   In each month, the borrower transitions from his current delinquency state to a new state depending on the number of payments made.  For a borrower who is Current (status "C"), he can make 1 payment (remaining "C"), 0 payments (to become "3"), or all the payments (to become "P"). Similarly, if a borrower is already in status "3", he could become more delinquent by missing additional payments and so on. A separate model can be fit for each loan status, modeling the transitions available to loan-months in that status, so for the purpose of this paper it is enough to consider only the status "C".	score:453
How do I write a batch program using spring batch?	g., using a different decoder for each task. The later is the one we investigate in this paper. However, our setup differs from previous works in that we focus an information-theoretic formulation of the MTL problem. We should also mention that we restrict our setup to MTL scenarios  where the inputs are common to all tasks. Although this can be mathematically equivalent to the problem of multi-label learning (MLL), there are some important differences (see~\cite{zhang_review_2014} for further details).    \subsection{Our contribution}  We first introduce an information-theoretic paradigm which provides the fundamental tradeoff between the \emph{log-loss} (average risk) and the information rate of the features (statistical model complexity). We derive an iterative Arimoto-Blahut like algorithm to address the non-convex optimization problem of the IB method in presence of side information available only at the decoder~\cite{wyner_ziv76, our_isit15}, as described in Fig.	g., using a different decoder for each task. The later is the one we investigate in this paper. However, our setup differs from previous works in that we focus an information-theoretic formulation of the MTL problem. We should also mention that we restrict our setup to MTL scenarios  where the inputs are common to all tasks. Although this can be mathematically equivalent to the problem of multi-label learning (MLL), there are some important differences (see~\cite{zhang_review_2014} for further details).    \subsection{Our contribution}  We first introduce an information-theoretic paradigm which provides the fundamental tradeoff between the \emph{log-loss} (average risk) and the information rate of the features (statistical model complexity). We derive an iterative Arimoto-Blahut like algorithm to address the non-convex optimization problem of the IB method in presence of side information available only at the decoder~\cite{wyner_ziv76, our_isit15}, as described in Fig.	score:459
How do I write a batch program using spring batch?	     Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. \par\medskip 	Will a computer program ever be  able to convert a piece of English text into a data structure  that unambiguously and completely describes the meaning of the  natural language text?  Among numerous problems, no consensus has emerged  about the form of such a data structure. Until such fundamental Artificial Intelligence problems are resolved,  computer scientists must settle for reduced objectives: extracting simpler representations describing  restricted aspects of the textual information.    These simpler representations are often motivated  by specific applications, for instance,  bag-of-words variants for information retrieval.	     Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements. \par\medskip 	Will a computer program ever be  able to convert a piece of English text into a data structure  that unambiguously and completely describes the meaning of the  natural language text?  Among numerous problems, no consensus has emerged  about the form of such a data structure. Until such fundamental Artificial Intelligence problems are resolved,  computer scientists must settle for reduced objectives: extracting simpler representations describing  restricted aspects of the textual information.    These simpler representations are often motivated  by specific applications, for instance,  bag-of-words variants for information retrieval.	score:465
Why should I learn the math behind machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:381
Why should I learn the math behind machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:381
Why should I learn the math behind machine learning?	 Our accelerated algorithm  performs much better than the original regularized sub-sampled Newton in experiments, which validates our theory empirically. Besides, the accelerated regularized sub-sampled Newton has good performance comparable to or even better than classical algorithms.  	Optimization has become an increasingly popular issue in machine learning.  Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Our accelerated algorithm  performs much better than the original regularized sub-sampled Newton in experiments, which validates our theory empirically. Besides, the accelerated regularized sub-sampled Newton has good performance comparable to or even better than classical algorithms.  	Optimization has become an increasingly popular issue in machine learning.  Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:382
Why should I learn the math behind machine learning?	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	score:382
Why should I learn the math behind machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:384
What's the math about machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:361
What's the math about machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:391
What's the math about machine learning?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:420
What's the math about machine learning?	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	score:425
What's the math about machine learning?	 Our empirical study verifies the efficiency of the proposed method for various non-smooth optimization problems that arise ubiquitously  in machine learning by comparing it to the state-of-the-art first order methods.      	Formulating machine learning tasks as a regularized empirical loss minimization problem  makes an intimate connection between machine learning and mathematical  optimization.  In regularized empirical loss minimization,  one tries to jointly minimize an empirical loss over training samples plus a regularization term of the model. This formulation includes support vector machine (SVM)~\citep{HastieEtAl2008}, support vector regression~\citep{Smola:2004:TSV:1011935.1011939}, Lasso~\citep{Zhu031-normsupport},  logistic regression,  and ridge regression~\citep{HastieEtAl2008} among many others.	 Our empirical study verifies the efficiency of the proposed method for various non-smooth optimization problems that arise ubiquitously  in machine learning by comparing it to the state-of-the-art first order methods.      	Formulating machine learning tasks as a regularized empirical loss minimization problem  makes an intimate connection between machine learning and mathematical  optimization.  In regularized empirical loss minimization,  one tries to jointly minimize an empirical loss over training samples plus a regularization term of the model. This formulation includes support vector machine (SVM)~\citep{HastieEtAl2008}, support vector regression~\citep{Smola:2004:TSV:1011935.1011939}, Lasso~\citep{Zhu031-normsupport},  logistic regression,  and ridge regression~\citep{HastieEtAl2008} among many others.	score:427
If I run "reg query the-key-name /s" in the cmd console it gives me the keys and subkeys back, but if I run it as a batch file not, how to fix?	g. the running score in an Atari game~\cite{mnih2015human}, or the distance between a robot arm and an object in a reaching task~\cite{lillicrap2015continuous}. However, in many real-world scenarios, rewards extrinsic to the agent are extremely sparse or missing altogether, and it is not possible to construct a shaped reward function. This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state.  Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments.  \begin{figure}     \centering     \begin{subfigure}[b]{0.48\linewidth}         \includegraphics[width=\linewidth]{images/mario_1-1.png}         \caption{learn to explore in Level-1}     \end{subfigure}     ~     \begin{subfigure}[b]{0.	g. the running score in an Atari game~\cite{mnih2015human}, or the distance between a robot arm and an object in a reaching task~\cite{lillicrap2015continuous}. However, in many real-world scenarios, rewards extrinsic to the agent are extremely sparse or missing altogether, and it is not possible to construct a shaped reward function. This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state.  Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments.  \begin{figure}     \centering     \begin{subfigure}[b]{0.48\linewidth}         \includegraphics[width=\linewidth]{images/mario_1-1.png}         \caption{learn to explore in Level-1}     \end{subfigure}     ~     \begin{subfigure}[b]{0.	score:332
If I run "reg query the-key-name /s" in the cmd console it gives me the keys and subkeys back, but if I run it as a batch file not, how to fix?	 As a consequence, it only makes a difference in the online optimization setup, where we now need to assume that the same function is observed twice in a row.  We introduce this  two-point setting as it allows us to consider the case where the constraint set is the whole space~$\rb^d$. Moreover, the algorithms  do not need to perform a projection at each step and rates of convergence are independent of the maximal value of the loss functions (which should not appear as the problem in translation invariant).  Note that (a) this unconstrained setting is common in smooth optimization, and (b) that our proof technique can extend to composite optimization where a non-smooth term is added with its proximal operator~\citep{xiao2010dual,hu2009accelerated}. On the other hand, when the constraint set is a compact convex subset, of diameter denoted by $R>0$, then we shall use a classical ``one-point'' algorithm that queries each $f_n$ only once.	  We emphasize these differences between set-ups as the complexity of  stochastic zero-th order optimization and the convex bandit problem have been widely studied recently \citep{recht2012query,shamir2013complexity}. It has been observed that minimax rates of convergence in bandit problems and stochastic optimization might differ, which is not the case in our setting for our upper-bounds (one can therefore conclude that the complexity of convex bandits is not hidden in the evolving sequence of loss functions, but more importantly on the constraint that the query point is where the loss is evaluated).	score:339
If I run "reg query the-key-name /s" in the cmd console it gives me the keys and subkeys back, but if I run it as a batch file not, how to fix?	 Our approach leads to the implementation of compression algorithms that perform out-of-core computations (i.e., loading information in main memory only as needed).  We propose to use structured random projections for NMF and show that, in practice, their use implies a substantial increase in speed. This performance boost does not come at the price of significant errors with respect to the uncompressed solutions.	 Our approach leads to the implementation of compression algorithms that perform out-of-core computations (i.e., loading information in main memory only as needed).  We propose to use structured random projections for NMF and show that, in practice, their use implies a substantial increase in speed. This performance boost does not come at the price of significant errors with respect to the uncompressed solutions.	score:341
If I run "reg query the-key-name /s" in the cmd console it gives me the keys and subkeys back, but if I run it as a batch file not, how to fix?	 These works have shown promising results, demonstrating the ability of using high-level parameters (such as a walking-path) to synthesize locomotion tasks such as jumping, running, walking, balancing, etc. These networks do not generate new variations of complex motion, however, being instead limited to specific use cases.   In contrast, our paper provides a robust framework that can synthesize highly complex human motion variations of arbitrary styles, such as dancing and martial arts, without querying a database.  We achieve this by using a novel deep auto-conditioned RNN (acRNN) network architecture.   Recurrent neural networks are autoregressive deep learning frameworks which seek to predict sequences of data similar to a training distribution. Such a framework is intuitive to apply to human motion, which can be naturally modeled as a time series of skeletal joint positions.	 These works have shown promising results, demonstrating the ability of using high-level parameters (such as a walking-path) to synthesize locomotion tasks such as jumping, running, walking, balancing, etc. These networks do not generate new variations of complex motion, however, being instead limited to specific use cases.   In contrast, our paper provides a robust framework that can synthesize highly complex human motion variations of arbitrary styles, such as dancing and martial arts, without querying a database.  We achieve this by using a novel deep auto-conditioned RNN (acRNN) network architecture.   Recurrent neural networks are autoregressive deep learning frameworks which seek to predict sequences of data similar to a training distribution. Such a framework is intuitive to apply to human motion, which can be naturally modeled as a time series of skeletal joint positions.	score:346
If I run "reg query the-key-name /s" in the cmd console it gives me the keys and subkeys back, but if I run it as a batch file not, how to fix?	 These works have shown promising results, demonstrating the ability of using high-level parameters (such as a walking-path) to synthesize locomotion tasks such as jumping, running, walking, balancing, etc. These networks do not generate new variations of complex motion, however, being instead limited to specific use cases.   In contrast, our paper provides a robust framework that can synthesize highly complex human motion variations of arbitrary styles, such as dancing and martial arts, without querying a database.  We achieve this by using a novel deep auto-conditioned RNN (acRNN) network architecture.   Recurrent neural networks are autoregressive deep learning frameworks which seek to predict sequences of data similar to a training distribution. Such a framework is intuitive to apply to human motion, which can be naturally modeled as a time series of skeletal joint positions.	 These works have shown promising results, demonstrating the ability of using high-level parameters (such as a walking-path) to synthesize locomotion tasks such as jumping, running, walking, balancing, etc. These networks do not generate new variations of complex motion, however, being instead limited to specific use cases.   In contrast, our paper provides a robust framework that can synthesize highly complex human motion variations of arbitrary styles, such as dancing and martial arts, without querying a database.  We achieve this by using a novel deep auto-conditioned RNN (acRNN) network architecture.   Recurrent neural networks are autoregressive deep learning frameworks which seek to predict sequences of data similar to a training distribution. Such a framework is intuitive to apply to human motion, which can be naturally modeled as a time series of skeletal joint positions.	score:346
How is real analysis used in machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:313
How is real analysis used in machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:360
How is real analysis used in machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:366
How is real analysis used in machine learning?	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	score:377
How is real analysis used in machine learning?	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	score:378
Why are matrices/vectors used in machine learning/data analysis?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:272
Why are matrices/vectors used in machine learning/data analysis?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:289
Why are matrices/vectors used in machine learning/data analysis?	   The mathematical properties of tensors have long been the subject of theoretical study. Previously, in machine learning,   data points were typically assumed to be vectors  and datasets to be matrices.  Hence, spectral methods,  such as matrix decompositions,  have been popular in machine learning.  Recently, tensor methods, which generalize these techniques to higher-order tensors, have gained prominence.  One class of broadly useful techniques  within tensor methods are tensor decompositions,  which have been studied for a variety of applications in signal processing and machine learning~\citep{tensor_decomposition_sidiropoulos,cichocki_2015},  data mining and fusion~\citep{tensor_mining_papalexakis},  blind source separation~\citep{nonnegative_cichocki_2009}, computer vision~\citep{tensor_faces} and learning latent variable models~\citep{anandkumar2014tensor}.	   The mathematical properties of tensors have long been the subject of theoretical study. Previously, in machine learning,   data points were typically assumed to be vectors  and datasets to be matrices.  Hence, spectral methods,  such as matrix decompositions,  have been popular in machine learning.  Recently, tensor methods, which generalize these techniques to higher-order tensors, have gained prominence.  One class of broadly useful techniques  within tensor methods are tensor decompositions,  which have been studied for a variety of applications in signal processing and machine learning~\citep{tensor_decomposition_sidiropoulos,cichocki_2015},  data mining and fusion~\citep{tensor_mining_papalexakis},  blind source separation~\citep{nonnegative_cichocki_2009}, computer vision~\citep{tensor_faces} and learning latent variable models~\citep{anandkumar2014tensor}.	score:290
Why are matrices/vectors used in machine learning/data analysis?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:296
Why are matrices/vectors used in machine learning/data analysis?	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	score:303
Does it necessary to have very strong mathematics base to learn machine learning?	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	score:320
Does it necessary to have very strong mathematics base to learn machine learning?	 At the same time, machine learning---one of the primary tools of the modern data scientist---has experienced a revitalization as academics, businesses, and governments alike discovered new applications for automated algorithms that can learn from data. As a consequence, there has been a growing demand for tools that make machine learning more accessible, scalable, and flexible.  Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.	 At the same time, machine learning---one of the primary tools of the modern data scientist---has experienced a revitalization as academics, businesses, and governments alike discovered new applications for automated algorithms that can learn from data. As a consequence, there has been a growing demand for tools that make machine learning more accessible, scalable, and flexible.  Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.	score:324
Does it necessary to have very strong mathematics base to learn machine learning?	  Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. 	Machine learning is very different from human learning. Humans are able to learn from very few labeled examples and apply the learned knowledge to new examples in novel conditions.  In contrast, supervised machine learning methods only perform well when the given extensive labeled data are from the same distribution as the test distribution. Both theoretical \cite{bendavid,Blitzer07Biographies} and practical results~\cite{saenko2010adapting,efros-cvpr11} have shown that the test error of supervised methods generally increases in proportion to the ``difference'' between the distributions of training and test examples.	  Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. 	Machine learning is very different from human learning. Humans are able to learn from very few labeled examples and apply the learned knowledge to new examples in novel conditions.  In contrast, supervised machine learning methods only perform well when the given extensive labeled data are from the same distribution as the test distribution. Both theoretical \cite{bendavid,Blitzer07Biographies} and practical results~\cite{saenko2010adapting,efros-cvpr11} have shown that the test error of supervised methods generally increases in proportion to the ``difference'' between the distributions of training and test examples.	score:328
Does it necessary to have very strong mathematics base to learn machine learning?	  The main adopted principles in this work are based on the belief that an intelligent machine should be able to learn simple patterns from a few number of examples, and to use the learned models in learning more complex ones. It should also be able to interact with human users, and thus we need a communication algorithm to make teaching machines easier and we also need a technique for tuning hyperparameters that can be easily done by non-experts.    A variety of recent works have tried to learn simple patterns (algorithms). For example, \citet{zaremba:2015} used an RNN-based controller, trained using Q-learning, that interacts with the environment through a set of interfaces selected manually for each task. However, they failed to find one controller suitable for all tasks, and the learned models overfit the length of input in some tasks.	  The main adopted principles in this work are based on the belief that an intelligent machine should be able to learn simple patterns from a few number of examples, and to use the learned models in learning more complex ones. It should also be able to interact with human users, and thus we need a communication algorithm to make teaching machines easier and we also need a technique for tuning hyperparameters that can be easily done by non-experts.    A variety of recent works have tried to learn simple patterns (algorithms). For example, \citet{zaremba:2015} used an RNN-based controller, trained using Q-learning, that interacts with the environment through a set of interfaces selected manually for each task. However, they failed to find one controller suitable for all tasks, and the learned models overfit the length of input in some tasks.	score:338
Does it necessary to have very strong mathematics base to learn machine learning?	 The latter have shown very promising performance in recent years, for complex tasks such as image classification. One key enabler of machine learning is the ability to learn (train) models using a very large amount of data. With the increasing amount of data being generated by new applications and with more applications becoming data-driven, one can foresee that machine learning tasks will become a dominant workload in distributed MEC systems in the future.  However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig.	 The latter have shown very promising performance in recent years, for complex tasks such as image classification. One key enabler of machine learning is the ability to learn (train) models using a very large amount of data. With the increasing amount of data being generated by new applications and with more applications becoming data-driven, one can foresee that machine learning tasks will become a dominant workload in distributed MEC systems in the future.  However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig.	score:339
What do you see as advantages or disadvantages to a universal procedure coding system similar to the ICD-10 system for disease classification?	 To achieve effective and efficient detection of Alzheimer's disease (AD), many machine learning methods have been introduced into this realm. However, the general case of limited training samples, as well as different feature representations typically makes this problem challenging. In this work, we propose a novel multiple kernel learning framework to combine multi-modal features for AD classification, which is scalable and easy to implement.  Contrary to the usual way of solving the problem in the dual space, we look at the optimization from a new perspective. By conducting Fourier transform on the Gaussian kernel, we explicitly compute the mapping function, which leads to a more straightforward solution of the problem in the primal space. Furthermore, we impose the mixed $L_{21}$ norm constraint on the kernel weights, known as the group lasso regularization, to enforce group sparsity among different feature modalities.  This actually acts as a role of feature modality selection, while at the same time exploiting complementary information among different kernels. Therefore it is able to extract the most discriminative features for classification. Experiments on the ADNI data set demonstrate the effectiveness of the proposed method. 	As the most common type of dementia among the elders, Alzheimer's disease (AD) is now affecting millions of people all over the world.  It is characterized by progressive brain disorder that damages brain cells, leading to memory loss, confusion and eventually to death. The huge price of caring AD patients has made it one of the most costly diseases in the developed countries, and also caused great physical, as well as psychological burdens on the caregivers. From this perspective, early diagnosis of AD can be of great significance.	 To achieve effective and efficient detection of Alzheimer's disease (AD), many machine learning methods have been introduced into this realm. However, the general case of limited training samples, as well as different feature representations typically makes this problem challenging. In this work, we propose a novel multiple kernel learning framework to combine multi-modal features for AD classification, which is scalable and easy to implement.  Contrary to the usual way of solving the problem in the dual space, we look at the optimization from a new perspective. By conducting Fourier transform on the Gaussian kernel, we explicitly compute the mapping function, which leads to a more straightforward solution of the problem in the primal space. Furthermore, we impose the mixed $L_{21}$ norm constraint on the kernel weights, known as the group lasso regularization, to enforce group sparsity among different feature modalities.  This actually acts as a role of feature modality selection, while at the same time exploiting complementary information among different kernels. Therefore it is able to extract the most discriminative features for classification. Experiments on the ADNI data set demonstrate the effectiveness of the proposed method. 	As the most common type of dementia among the elders, Alzheimer's disease (AD) is now affecting millions of people all over the world.  It is characterized by progressive brain disorder that damages brain cells, leading to memory loss, confusion and eventually to death. The huge price of caring AD patients has made it one of the most costly diseases in the developed countries, and also caused great physical, as well as psychological burdens on the caregivers. From this perspective, early diagnosis of AD can be of great significance.	score:335
What do you see as advantages or disadvantages to a universal procedure coding system similar to the ICD-10 system for disease classification?	 The performance of the ensemble classifier is evaluated on several multi-class datasets where it demonstrates a superior performance compared to other state-of-the-art classifiers. 	The performance of classifiers can be significantly improved by aggregating the decisions of several classifiers instead of using only a single classifier. This is generally known as ensemble of classifiers, or multiple classifier systems.  The ensemble is obtained by perturbing and combining several individual classifiers \cite{breiman1996bias}. Specifically, it is obtained by perturbing the training set or injecting some randomness in each classifier and aggregating the outputs of the these classifiers in a suitable way.  Decision trees (DT) and Neural networks (NN) are generally used for ensemble generation.	 The performance of the ensemble classifier is evaluated on several multi-class datasets where it demonstrates a superior performance compared to other state-of-the-art classifiers. 	The performance of classifiers can be significantly improved by aggregating the decisions of several classifiers instead of using only a single classifier. This is generally known as ensemble of classifiers, or multiple classifier systems.  The ensemble is obtained by perturbing and combining several individual classifiers \cite{breiman1996bias}. Specifically, it is obtained by perturbing the training set or injecting some randomness in each classifier and aggregating the outputs of the these classifiers in a suitable way.  Decision trees (DT) and Neural networks (NN) are generally used for ensemble generation.	score:346
What do you see as advantages or disadvantages to a universal procedure coding system similar to the ICD-10 system for disease classification?	 To predict the effectiveness of drugs for a disease, longitudinal studies of patients would be needed to collect ground truth labels. For rare diseases, just acquiring a database of patients may require its own research project. Although high-throughput screening technologies have been developed and the effects of molecules can be evaluated \textit{in vitro}, there still remains a huge gap between experiments \textit{in vitro} and actual effects on a human body, as we can see from the fact that less than 10 percent of drugs passed from the Phase I of clinical trials to approval between 2006 and 2015 \cite{mullard2016parsing}.	 To predict the effectiveness of drugs for a disease, longitudinal studies of patients would be needed to collect ground truth labels. For rare diseases, just acquiring a database of patients may require its own research project. Although high-throughput screening technologies have been developed and the effects of molecules can be evaluated \textit{in vitro}, there still remains a huge gap between experiments \textit{in vitro} and actual effects on a human body, as we can see from the fact that less than 10 percent of drugs passed from the Phase I of clinical trials to approval between 2006 and 2015 \cite{mullard2016parsing}.	score:348
What do you see as advantages or disadvantages to a universal procedure coding system similar to the ICD-10 system for disease classification?	  The multiple classifier system classifies each data-point corresponding to a drug-medical event pair as an ADR or non-ADRs based on a weighted combination of each individual classifiers confidence of the data-point belonging to the ADR class.  The weights are determined by using genetic algorithms to search for the values that optimise the SEC framework's ability to detect ADRs.      This paper continues are follows. The next section gives an overview of genetic algorithms, multiple classifier systems and pharmacovigilance, including the SEC framework.  In section \ref{mat}, we described the longitudinal medical database used in this research, known as The Health Improvement Network (THIN) database (www.thin-uk.com), and highlight current issues with the data.	  The multiple classifier system classifies each data-point corresponding to a drug-medical event pair as an ADR or non-ADRs based on a weighted combination of each individual classifiers confidence of the data-point belonging to the ADR class.  The weights are determined by using genetic algorithms to search for the values that optimise the SEC framework's ability to detect ADRs.      This paper continues are follows. The next section gives an overview of genetic algorithms, multiple classifier systems and pharmacovigilance, including the SEC framework.  In section \ref{mat}, we described the longitudinal medical database used in this research, known as The Health Improvement Network (THIN) database (www.thin-uk.com), and highlight current issues with the data.	score:351
What do you see as advantages or disadvantages to a universal procedure coding system similar to the ICD-10 system for disease classification?	  This gap is important when considering clinical-admission analysis, the motivating application of this paper.  Patient admissions in hospitals are recorded by the code of international classification of diseases (ICD).  For each admission, one may observe a sequence of ICD codes corresponding to certain kinds of diseases and procedures, and each code is treated as a ``word. ''  To reveal the characteristics of the admissions and relationships between different diseases/procedures, we seek to model the ``topics'' of admissions and also learn an embedding for each ICD code.  However, while we want embeddings of similar diseases/procedures to be nearby in the embedding space, learning the embedding vectors based on surrounding ICD codes for a given patient admission is less relevant, as there is often a diversity in the observed codes for a given admission, and the code order may hold less meaning.	  This gap is important when considering clinical-admission analysis, the motivating application of this paper.  Patient admissions in hospitals are recorded by the code of international classification of diseases (ICD).  For each admission, one may observe a sequence of ICD codes corresponding to certain kinds of diseases and procedures, and each code is treated as a ``word. ''  To reveal the characteristics of the admissions and relationships between different diseases/procedures, we seek to model the ``topics'' of admissions and also learn an embedding for each ICD code.  However, while we want embeddings of similar diseases/procedures to be nearby in the embedding space, learning the embedding vectors based on surrounding ICD codes for a given patient admission is less relevant, as there is often a diversity in the observed codes for a given admission, and the code order may hold less meaning.	score:351
What are the merits and demerits of attending Absentee batch in NDA SSB?	   Moreover,  the nuclear norm minimization required by LRR results in significant computational complexity.  The analytical conditions guaranteeing success of SSC, and RSSC for the noisy case, reported in \cite{soltanolkotabi_geometric_2011} and \cite{soltanolkotabi_robust_2013}, respectively, are very similar to those found for TSC in this paper. TSC is, however, computationally much less demanding than SSC/RSSC.   These complexity savings may come at the cost of clustering performance.  Experiments on real and synthetic data, many of which are reported in Section~\ref{sec:numres}, show that while there are situations where TSC outperforms SSC, SSC outperforming TSC is more common.  Dyer et al.~\cite{dyer_greedy_2013} propose to substitute the $\ell_1$-minimization step in SSC by an orthogonal matching pursuit (OMP) step, and derive    performance guarantees for the resulting SSC-OMP algorithm.     Lerman and Zhang \cite{lerman_robust_2011} consider the problem of recovering multiple subspaces from data drawn from a distribution on the union of these subspaces and pose recovery as a non-convex optimization problem. No computationally tractable algorithm to solve this recovery problem \cite{lerman_robust_2012}  seems to be available, though.	   Section~\ref{sec:detoutl} describes an outlier detection scheme and contains corresponding  performance results. In Section~\ref{sec:comp}, we compare our analytical performance results for TSC to analytical performance results for SSC/RSSC and further subspace clustering algorithms. Section~\ref{sec:numres} contains numerical results on synthetic and on real data,  including a comparison of TSC to SSC/RSSC.    We discuss the various settings (noiseless, noisy, incomplete observations, and outliers) in an isolated fashion to keep the exposition accessible. All proofs are relegated to appendices.    \paragraph*{Notation:} We use lowercase boldface letters to denote (column) vectors, e.g., $\vx$, and uppercase boldface letters to designate matrices, e.g., $\mA$.	score:334
What are the merits and demerits of attending Absentee batch in NDA SSB?	    However, the goals of classifier training and metric learning are different.  The former aims at finding the best decision function while the latter is to learn an embedding  such that embeddings of samples of the same category are compact while those of samples of different categories are ``spread-out''.    This motivates us to investigate the relation between metric learning and classifier training.    In this paper, we show that the temperature parameter in the softmax function, defined by~\citet{hinton_distilling_2015} for knowledge transfer, plays an important role in determining the distribution of the embeddings from the bottleneck layer.  Based on the observed relation, we propose to learn a classifier with an intermediate temperature at the beginning and increase the temperature during training.	    However, the goals of classifier training and metric learning are different.  The former aims at finding the best decision function while the latter is to learn an embedding  such that embeddings of samples of the same category are compact while those of samples of different categories are ``spread-out''.    This motivates us to investigate the relation between metric learning and classifier training.    In this paper, we show that the temperature parameter in the softmax function, defined by~\citet{hinton_distilling_2015} for knowledge transfer, plays an important role in determining the distribution of the embeddings from the bottleneck layer.  Based on the observed relation, we propose to learn a classifier with an intermediate temperature at the beginning and increase the temperature during training.	score:334
What are the merits and demerits of attending Absentee batch in NDA SSB?	 It uses, as a numerical calculation of the Bayesian posterior distribution, the Gibbs sampling method, which is a kind of Markov chain Monte Carlo method (MCMC). Bayesian~NMF is more robust than the usual recursive methods of NMF, because it numerically expressed the posterior distribution; the parameters are subject to a probability distribution, and that makes it possible to determine the degree of fluctuation of the learning/inference result.  As described later, the Bayesian method generally has higher estimation accuracy than maximum likelihood estimation and maximum posterior estimation if the model has a hierarchical structure or hidden variables, like in NMF.  On the other hand, a variational Bayesian algorithm (VB) for NMF has also been established \cite{Cemgil}, inspired by the mean-field approximation.	 It uses, as a numerical calculation of the Bayesian posterior distribution, the Gibbs sampling method, which is a kind of Markov chain Monte Carlo method (MCMC). Bayesian~NMF is more robust than the usual recursive methods of NMF, because it numerically expressed the posterior distribution; the parameters are subject to a probability distribution, and that makes it possible to determine the degree of fluctuation of the learning/inference result.  As described later, the Bayesian method generally has higher estimation accuracy than maximum likelihood estimation and maximum posterior estimation if the model has a hierarchical structure or hidden variables, like in NMF.  On the other hand, a variational Bayesian algorithm (VB) for NMF has also been established \cite{Cemgil}, inspired by the mean-field approximation.	score:335
What are the merits and demerits of attending Absentee batch in NDA SSB?	 Many feature importance estimators have interesting theoretical properties e.g.~preservation of relevance \cite{Bach2015} or implementation invariance \cite{Mukund2017}. However even these methods need to be configured correctly \cite{Montavon2017,Mukund2017} and it has been shown that using the wrong configuration can easily render them ineffective~\cite{kitty_paper2017}.  For this reason, it is important that we build a framework to empirically validate the relative merits and reliability of these methods.  A commonly used strategy is to remove the supposedly informative features from the input and look at how the classifier degrades \cite{samek2017}. This method is cheap to evaluate but comes at a significant drawback.	 Many feature importance estimators have interesting theoretical properties e.g.~preservation of relevance \cite{Bach2015} or implementation invariance \cite{Mukund2017}. However even these methods need to be configured correctly \cite{Montavon2017,Mukund2017} and it has been shown that using the wrong configuration can easily render them ineffective~\cite{kitty_paper2017}.  For this reason, it is important that we build a framework to empirically validate the relative merits and reliability of these methods.  A commonly used strategy is to remove the supposedly informative features from the input and look at how the classifier degrades \cite{samek2017}. This method is cheap to evaluate but comes at a significant drawback.	score:344
What are the merits and demerits of attending Absentee batch in NDA SSB?	   Section~\ref{sec:detoutl} describes an outlier detection scheme and contains corresponding  performance results. In Section~\ref{sec:comp}, we compare our analytical performance results for TSC to analytical performance results for SSC/RSSC and further subspace clustering algorithms. Section~\ref{sec:numres} contains numerical results on synthetic and on real data,  including a comparison of TSC to SSC/RSSC.    We discuss the various settings (noiseless, noisy, incomplete observations, and outliers) in an isolated fashion to keep the exposition accessible. All proofs are relegated to appendices.    \paragraph*{Notation:} We use lowercase boldface letters to denote (column) vectors, e.g., $\vx$, and uppercase boldface letters to designate matrices, e.g., $\mA$.	   Section~\ref{sec:detoutl} describes an outlier detection scheme and contains corresponding  performance results. In Section~\ref{sec:comp}, we compare our analytical performance results for TSC to analytical performance results for SSC/RSSC and further subspace clustering algorithms. Section~\ref{sec:numres} contains numerical results on synthetic and on real data,  including a comparison of TSC to SSC/RSSC.    We discuss the various settings (noiseless, noisy, incomplete observations, and outliers) in an isolated fashion to keep the exposition accessible. All proofs are relegated to appendices.    \paragraph*{Notation:} We use lowercase boldface letters to denote (column) vectors, e.g., $\vx$, and uppercase boldface letters to designate matrices, e.g., $\mA$.	score:344
What is the usual timing for absentee SSB batch in Bangalore?	   We train the system on Bach's (SATB) chorales (transposed to C), which have been an attractive corpus for analyzing knowledge of voice leading, counterpoint, and tonality due to their relative uniformity of rhythm \cite{taube1999automatic,rohrmeier2008statistical}. We demonstrate that MUS-ROVER is able to automatically recover compositional rules for these chorales that have been previously identified by music theorists. In addition, we present new, human-interpretable rules discovered by MUS-ROVER that are characteristic of Bach's chorales. Finally, we discuss how the extracted rules can be used in both machine and human composition.	   We train the system on Bach's (SATB) chorales (transposed to C), which have been an attractive corpus for analyzing knowledge of voice leading, counterpoint, and tonality due to their relative uniformity of rhythm \cite{taube1999automatic,rohrmeier2008statistical}. We demonstrate that MUS-ROVER is able to automatically recover compositional rules for these chorales that have been previously identified by music theorists. In addition, we present new, human-interpretable rules discovered by MUS-ROVER that are characteristic of Bach's chorales. Finally, we discuss how the extracted rules can be used in both machine and human composition.	score:411
What is the usual timing for absentee SSB batch in Bangalore?	 We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole.  To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start.	 We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole.  To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start.	score:417
What is the usual timing for absentee SSB batch in Bangalore?	   \subsection{Our Contributions}  Given that functions sampled from GPs usually have some degree of smoothness, in the so-called \textit{batch Bayesian optimization} (BBO) methods, it is desirable to choose batches which are diverse. Indeed, this is the motivation behind many popular BBO methods like the BUCB \cite{DesautelsBUCB}, UCB-PE \cite{ContalUCBPE} and Local Penalization \cite{BBOLP}.  Motivated by this long line of work in BBO, we propose a new approach that employs Determinantal Point Processes (DPPs) to select diverse batches of evaluations. DPPs are probability measures over subsets of a ground set that promote diversity, have applications in statistical physics and random matrix theory \cite{Shirai2003,Lyons2003}, and have efficient sampling algorithms \cite{Kulesza2011,Kulesza2012}.	   \subsection{Our Contributions}  Given that functions sampled from GPs usually have some degree of smoothness, in the so-called \textit{batch Bayesian optimization} (BBO) methods, it is desirable to choose batches which are diverse. Indeed, this is the motivation behind many popular BBO methods like the BUCB \cite{DesautelsBUCB}, UCB-PE \cite{ContalUCBPE} and Local Penalization \cite{BBOLP}.  Motivated by this long line of work in BBO, we propose a new approach that employs Determinantal Point Processes (DPPs) to select diverse batches of evaluations. DPPs are probability measures over subsets of a ground set that promote diversity, have applications in statistical physics and random matrix theory \cite{Shirai2003,Lyons2003}, and have efficient sampling algorithms \cite{Kulesza2011,Kulesza2012}.	score:418
What is the usual timing for absentee SSB batch in Bangalore?	html}}, caffe's\footnote{\url{http://caffe.berkeleyvision.org/tutorial/solver.html}}, and keras'\footnote{\url{http://keras.io/optimizers/}} documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.  This article aims at providing the reader with intuitions with regard to the behaviour of different algorithms for optimizing gradient descent that will help her put them to use.  In Section \ref{sec:gradient_descent_variants}, we are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training in Section \ref{sec:challenges}. Subsequently, in Section \ref{sec:algos}, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules.	html}}, caffe's\footnote{\url{http://caffe.berkeleyvision.org/tutorial/solver.html}}, and keras'\footnote{\url{http://keras.io/optimizers/}} documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.  This article aims at providing the reader with intuitions with regard to the behaviour of different algorithms for optimizing gradient descent that will help her put them to use.  In Section \ref{sec:gradient_descent_variants}, we are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training in Section \ref{sec:challenges}. Subsequently, in Section \ref{sec:algos}, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules.	score:419
What is the usual timing for absentee SSB batch in Bangalore?	 The former work establishes Bayesian regret bounds for Thompson sampling in the canonical parameterization setup (i.e., each state-action pair having independent transition/reward parameters) whereas the latter considers the same for parameterized MDPs as we do here. Our interest, however, is in the continuous (non-episodic) learning setting, and more importantly in the {\em frequentist} of regret performance, where the ``prior'' plays the role of merely a parameter used by the algorithm operating in an unknown, fixed environment.  We are also interested in problem (or ``gap'') dependent $O\left(\log T\right)$ regret bounds depending on the explicit structure of the MDP parameterization.         In this work, we overcome these hurdles to derive the first regret-type bounds for TSMDP at the level of a general parameter space and prior. First, we directly consider the posterior density in its general form of a normalized, exponentiated, empirical Kullback-Leibler divergence.	 The former work establishes Bayesian regret bounds for Thompson sampling in the canonical parameterization setup (i.e., each state-action pair having independent transition/reward parameters) whereas the latter considers the same for parameterized MDPs as we do here. Our interest, however, is in the continuous (non-episodic) learning setting, and more importantly in the {\em frequentist} of regret performance, where the ``prior'' plays the role of merely a parameter used by the algorithm operating in an unknown, fixed environment.  We are also interested in problem (or ``gap'') dependent $O\left(\log T\right)$ regret bounds depending on the explicit structure of the MDP parameterization.         In this work, we overcome these hurdles to derive the first regret-type bounds for TSMDP at the level of a general parameter space and prior. First, we directly consider the posterior density in its general form of a normalized, exponentiated, empirical Kullback-Leibler divergence.	score:430
What is the current placement scenario for August 2015 batch of DAC course at ACTS Pune?	 Learning hidden features in unlabeled training examples is called unsupervised learning. Understanding how the number of examples confines the learning process is of fundamental importance in both cognitive neuroscience and machine learning~\cite{Kersten-2004,Hinton-2007}. As already observed in training of deep neural networks, unsupervised pretraining can significantly enhance the final performance, because the unsupervised pretraining provides a good initial region  in parameter space from which the  final fine-tuning starts~\cite{Bengio-2013}.  However, there are few theoretical works addressing how unsupervised learning extracts hidden features.  One potential reason is that the unsupervised learning process in a deep neural network is typically very complicated.  Hence, understanding the mechanism of unsupervised learning in simple models is of significant importance.  This topic is recently studied based on the Bayesian inference framework~\cite{Huang-2016}.	 Learning hidden features in unlabeled training examples is called unsupervised learning. Understanding how the number of examples confines the learning process is of fundamental importance in both cognitive neuroscience and machine learning~\cite{Kersten-2004,Hinton-2007}. As already observed in training of deep neural networks, unsupervised pretraining can significantly enhance the final performance, because the unsupervised pretraining provides a good initial region  in parameter space from which the  final fine-tuning starts~\cite{Bengio-2013}.  However, there are few theoretical works addressing how unsupervised learning extracts hidden features.  One potential reason is that the unsupervised learning process in a deep neural network is typically very complicated.  Hence, understanding the mechanism of unsupervised learning in simple models is of significant importance.  This topic is recently studied based on the Bayesian inference framework~\cite{Huang-2016}.	score:382
What is the current placement scenario for August 2015 batch of DAC course at ACTS Pune?	      Recently, significant progress has been made to scale Bayesian neural networks to large tasks and to provide better approximations of the posterior distribution \citep{blundell2015weight,louizos2017multiplicative,krueger2017bayesian}. This, however, comes with an important question: ``What does the posterior distribution actually represent?''.    For neural networks, the prior is often chosen for convenience and the approximate posterior is often very limited \citep{blundell2015weight}. For sufficiently large datasets, the observations overcome the prior, and the posterior becomes a single mode around the true model\footnote{The true model must have positive probability under the prior. Also, when the true model can be parameterized differently, modeling one or multiple modes is equivalent.	      Recently, significant progress has been made to scale Bayesian neural networks to large tasks and to provide better approximations of the posterior distribution \citep{blundell2015weight,louizos2017multiplicative,krueger2017bayesian}. This, however, comes with an important question: ``What does the posterior distribution actually represent?''.    For neural networks, the prior is often chosen for convenience and the approximate posterior is often very limited \citep{blundell2015weight}. For sufficiently large datasets, the observations overcome the prior, and the posterior becomes a single mode around the true model\footnote{The true model must have positive probability under the prior. Also, when the true model can be parameterized differently, modeling one or multiple modes is equivalent.	score:385
What is the current placement scenario for August 2015 batch of DAC course at ACTS Pune?	 Methods that subtype raw observation trajectories are being developed in recent times \citep{Doshi-Velez2014,Schulam2015,Saria2010}. Existing methods, however, either treat the evolution of clinical markers as a homogeneous process or employ data available at regular intervals; in reality diseases may have transient underlying states and a state-dependent observation pattern.    In this paper, we present an approach to subtype raw unsummarized data from electronic medical records, while acknowledging the underlying progression of disease states. Our approach consists of two components: a probabilistic model to determine the likelihood of a patient's observation trajectory and a mixture model to measure similarity between asynchronous patient trajectories.	 Methods that subtype raw observation trajectories are being developed in recent times \citep{Doshi-Velez2014,Schulam2015,Saria2010}. Existing methods, however, either treat the evolution of clinical markers as a homogeneous process or employ data available at regular intervals; in reality diseases may have transient underlying states and a state-dependent observation pattern.    In this paper, we present an approach to subtype raw unsummarized data from electronic medical records, while acknowledging the underlying progression of disease states. Our approach consists of two components: a probabilistic model to determine the likelihood of a patient's observation trajectory and a mixture model to measure similarity between asynchronous patient trajectories.	score:391
What is the current placement scenario for August 2015 batch of DAC course at ACTS Pune?	  We show precise rates of convergence for the optimal version of AdaBoost (without any modifications) with respect to the maximum margin problem for any given step-size rule. Our results seem apparently contradictory to Rudin et al.~\cite{rudinDynamics2004}, who show that even in the optimal case considered herein (where the weak learner always returns the best feature) AdaBoost may fail to converge to a maximum margin solution.  However, in~\cite{rudinDynamics2004} their analysis is limited to the case where AdaBoost uses the originally prescribed step-size $\alpha_k := \frac{1}{2}\ln\left(\frac{1 + r_k}{1 - r_k}\right)$, where $r_k$ is the edge at iteration $k$, which can be interpreted as a line-search with respect to the exponential loss (not the margin) in the coordinate direction of the base feature chosen at iteration $k$ (see~\cite{boostingGradient} for a derivation of this).	  We show precise rates of convergence for the optimal version of AdaBoost (without any modifications) with respect to the maximum margin problem for any given step-size rule. Our results seem apparently contradictory to Rudin et al.~\cite{rudinDynamics2004}, who show that even in the optimal case considered herein (where the weak learner always returns the best feature) AdaBoost may fail to converge to a maximum margin solution.  However, in~\cite{rudinDynamics2004} their analysis is limited to the case where AdaBoost uses the originally prescribed step-size $\alpha_k := \frac{1}{2}\ln\left(\frac{1 + r_k}{1 - r_k}\right)$, where $r_k$ is the edge at iteration $k$, which can be interpreted as a line-search with respect to the exponential loss (not the margin) in the coordinate direction of the base feature chosen at iteration $k$ (see~\cite{boostingGradient} for a derivation of this).	score:393
What is the current placement scenario for August 2015 batch of DAC course at ACTS Pune?	 That is, in asynchronous optimization itself adds an additional implicit momentum. Once again, a parallel issue can easily be mapped to asynchronous Deep RL methods \citep{a3c} or distributed learning~\citep{heess2017emergence,barth2018distributed}. However, more importantly, it is easy to imagine how this might affect synchronous methods as well. In TD methods especially, there is a staleness to the gradients since the value function is bootstrapping off of its own predictions and updates may be biased toward previous policies and points in a trajectory.	 That is, in asynchronous optimization itself adds an additional implicit momentum. Once again, a parallel issue can easily be mapped to asynchronous Deep RL methods \citep{a3c} or distributed learning~\citep{heess2017emergence,barth2018distributed}. However, more importantly, it is easy to imagine how this might affect synchronous methods as well. In TD methods especially, there is a staleness to the gradients since the value function is bootstrapping off of its own predictions and updates may be biased toward previous policies and points in a trajectory.	score:393
What are some good statistics/data science/machine learning books to read for someone who has just taken AP Statistics?	 	Data science problems often require machine learning models to be trained on data in tables with one label and multiple feature columns. Data scientists must hand-craft additional features from the initial data. This process is known as \textit{feature engineering} and is one of the most tedious, but crucial, tasks in data science. Data scientists report that up to 95\% of the total project time must be allocated to carefully hand-craft new features to achieve the best models.	 	Data science problems often require machine learning models to be trained on data in tables with one label and multiple feature columns. Data scientists must hand-craft additional features from the initial data. This process is known as \textit{feature engineering} and is one of the most tedious, but crucial, tasks in data science. Data scientists report that up to 95\% of the total project time must be allocated to carefully hand-craft new features to achieve the best models.	score:368
What are some good statistics/data science/machine learning books to read for someone who has just taken AP Statistics?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:379
What are some good statistics/data science/machine learning books to read for someone who has just taken AP Statistics?	 Algebraic topology methods have recently played an important role for statistical analysis with complicated geometric structured data such as shapes, linked twist maps, and material data. Among them, \textit{persistent homology} is a well-known tool to extract robust topological features, and outputs as \textit{persistence diagrams} (PDs). However, PDs are point multi-sets which can not be used in machine learning algorithms for vector data.  To deal with it, an emerged approach is to use kernel methods, and an appropriate geometry for PDs is an important factor to measure the similarity of PDs. A popular geometry for PDs is the \textit{Wasserstein metric}. However, Wasserstein distance is not \textit{negative definite}. Thus, it is limited to build positive definite kernels upon the Wasserstein distance \textit{without approximation}.	 Algebraic topology methods have recently played an important role for statistical analysis with complicated geometric structured data such as shapes, linked twist maps, and material data. Among them, \textit{persistent homology} is a well-known tool to extract robust topological features, and outputs as \textit{persistence diagrams} (PDs). However, PDs are point multi-sets which can not be used in machine learning algorithms for vector data.  To deal with it, an emerged approach is to use kernel methods, and an appropriate geometry for PDs is an important factor to measure the similarity of PDs. A popular geometry for PDs is the \textit{Wasserstein metric}. However, Wasserstein distance is not \textit{negative definite}. Thus, it is limited to build positive definite kernels upon the Wasserstein distance \textit{without approximation}.	score:381
What are some good statistics/data science/machine learning books to read for someone who has just taken AP Statistics?	 Many machine learning algorithms are based on the assumption that training examples are drawn independently.  However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects,  and hence share the features of these shared objects.  We show that the classic approach of ignoring this problem potentially can have a harmful effect on the accuracy of statistics, and then consider alternatives.   One of these is to only use independent examples, discarding other information.  However, this is clearly suboptimal.  We analyze sample error bounds in this networked setting, providing significantly improved results.  An important component of our approach is formed by efficient sample weighting schemes, which leads to novel concentration inequalities.	 Many machine learning algorithms are based on the assumption that training examples are drawn independently.  However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects,  and hence share the features of these shared objects.  We show that the classic approach of ignoring this problem potentially can have a harmful effect on the accuracy of statistics, and then consider alternatives.   One of these is to only use independent examples, discarding other information.  However, this is clearly suboptimal.  We analyze sample error bounds in this networked setting, providing significantly improved results.  An important component of our approach is formed by efficient sample weighting schemes, which leads to novel concentration inequalities.	score:385
What are some good statistics/data science/machine learning books to read for someone who has just taken AP Statistics?	State-of-the-art machine learning algorithms are able to solve many problems sufficiently well.  However, both theoretical and experimental studies have shown that in order to achieve solutions of reasonable quality they need an access to extensive amounts of training data.  In contrast, humans are known to be able to learn concepts from just a few examples.   A possible explanation may lie in the fact that humans are able to reuse the knowledge they have gained from previously learned tasks for solving a new one, while traditional machine learning algorithms solve tasks in isolation.  This observation motivates an alternative, transfer learning approach.  It is based on idea of transferring information between related learning tasks in order to improve performance.	State-of-the-art machine learning algorithms are able to solve many problems sufficiently well.  However, both theoretical and experimental studies have shown that in order to achieve solutions of reasonable quality they need an access to extensive amounts of training data.  In contrast, humans are known to be able to learn concepts from just a few examples.   A possible explanation may lie in the fact that humans are able to reuse the knowledge they have gained from previously learned tasks for solving a new one, while traditional machine learning algorithms solve tasks in isolation.  This observation motivates an alternative, transfer learning approach.  It is based on idea of transferring information between related learning tasks in order to improve performance.	score:387
What are some best ways to represent word for morphology tasks using unsupervised machine learning techniques?	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	score:216
What are some best ways to represent word for morphology tasks using unsupervised machine learning techniques?	 The results show that the proposed method in this paper significantly outperforms it for web document retrieval task. 	\IEEEPARstart{L}{earning} a good representation (or features) of input data is an important task in machine learning. In text and language processing, one such problem is  learning of an embedding vector for a sentence; that is, to train a model that can automatically transform a sentence to a vector that encodes the semantic meaning of the sentence.   While word embedding is learned using a loss function defined on word pairs, sentence embedding is learned using a loss function defined on sentence pairs. In the sentence embedding usually the relationship among words in the sentence, i.e., the context information, is taken into consideration. Therefore, sentence embedding is more suitable for tasks that require computing semantic similarities between text strings.	 The results show that the proposed method in this paper significantly outperforms it for web document retrieval task. 	\IEEEPARstart{L}{earning} a good representation (or features) of input data is an important task in machine learning. In text and language processing, one such problem is  learning of an embedding vector for a sentence; that is, to train a model that can automatically transform a sentence to a vector that encodes the semantic meaning of the sentence.   While word embedding is learned using a loss function defined on word pairs, sentence embedding is learned using a loss function defined on sentence pairs. In the sentence embedding usually the relationship among words in the sentence, i.e., the context information, is taken into consideration. Therefore, sentence embedding is more suitable for tasks that require computing semantic similarities between text strings.	score:234
What are some best ways to represent word for morphology tasks using unsupervised machine learning techniques?	 Mainstream machine-learning techniques such as deep learning and probabilistic programming rely heavily on sampling from generally intractable probability distributions. There is increasing interest in the potential advantages of using quantum computing technologies as sampling engines to speed up these tasks or to make them more effective. However, some pressing challenges in state-of-the-art quantum annealers have to be overcome before we can assess their actual performance.  The sparse connectivity, resulting from the local interaction between quantum bits in physical hardware implementations, is considered the most severe limitation to the quality of constructing powerful generative unsupervised machine-learning models. Here we use embedding techniques to add redundancy to data sets, allowing us to increase the modeling capacity of quantum annealers.	 Mainstream machine-learning techniques such as deep learning and probabilistic programming rely heavily on sampling from generally intractable probability distributions. There is increasing interest in the potential advantages of using quantum computing technologies as sampling engines to speed up these tasks or to make them more effective. However, some pressing challenges in state-of-the-art quantum annealers have to be overcome before we can assess their actual performance.  The sparse connectivity, resulting from the local interaction between quantum bits in physical hardware implementations, is considered the most severe limitation to the quality of constructing powerful generative unsupervised machine-learning models. Here we use embedding techniques to add redundancy to data sets, allowing us to increase the modeling capacity of quantum annealers.	score:241
What are some best ways to represent word for morphology tasks using unsupervised machine learning techniques?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:244
What are some best ways to represent word for morphology tasks using unsupervised machine learning techniques?	 In this paper, we demonstrate how the state-of-the-art machine learning and text mining techniques can be used to build effective social media-based substance use detection systems.  Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different feature learning methods to take advantage of a large amount of unsupervised social media data.  We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as Facebook ``likes" and  ``status updates"  to enhance system performance.  Based on our evaluation, our best models achieved 86\% AUC for predicting tobacco use,  81\% for alcohol use and 84\% for drug use, all of which significantly outperformed existing methods.	 In this paper, we demonstrate how the state-of-the-art machine learning and text mining techniques can be used to build effective social media-based substance use detection systems.  Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different feature learning methods to take advantage of a large amount of unsupervised social media data.  We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as Facebook ``likes" and  ``status updates"  to enhance system performance.  Based on our evaluation, our best models achieved 86\% AUC for predicting tobacco use,  81\% for alcohol use and 84\% for drug use, all of which significantly outperformed existing methods.	score:247
Is it true that IIM-A would not consider application rating scores for giving a call to a CAT aspirant of batch 2015–2017?	  For instance, it is fairly common to observe sentences ``Look at the cat'', or ``Look at the dog'', but not ``Look at the mammal''.  Therefore, due to the way we typically express natural language, it is unlikely that the word ``mammal'' would be learned as a distribution that encompasses both ``cat'' and ``dog'', since ``mammal''  rarely occurs in similar contexts.     Rather than relying on the information from word occurrences, one can do \emph{supervised} training of density embeddings on hierarchical data.  In this paper, we propose new training methodology to enable effective supervised probabilistic density embeddings.  Despite providing rich and intuitive word representations, with a natural ability to represent order relationships, probabilistic embeddings have only been considered in a small number of pioneering works such as \citet{word2gauss}, and these works are almost exclusively focused on \emph{unsupervised embeddings}.	  For instance, it is fairly common to observe sentences ``Look at the cat'', or ``Look at the dog'', but not ``Look at the mammal''.  Therefore, due to the way we typically express natural language, it is unlikely that the word ``mammal'' would be learned as a distribution that encompasses both ``cat'' and ``dog'', since ``mammal''  rarely occurs in similar contexts.     Rather than relying on the information from word occurrences, one can do \emph{supervised} training of density embeddings on hierarchical data.  In this paper, we propose new training methodology to enable effective supervised probabilistic density embeddings.  Despite providing rich and intuitive word representations, with a natural ability to represent order relationships, probabilistic embeddings have only been considered in a small number of pioneering works such as \citet{word2gauss}, and these works are almost exclusively focused on \emph{unsupervised embeddings}.	score:299
Is it true that IIM-A would not consider application rating scores for giving a call to a CAT aspirant of batch 2015–2017?	e., to not ``forget" (from now on we will refer to those initial categories by calling them base categories).  Motivated by this observation, in this work we propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training data is assumed to exist for a set of base categories and, using these data as the sole input, we want to develop an object recognition learning system that, not only is able to recognize these base categories, but also learns to dynamically recognize novel categories from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (\emph{dynamic few-shot learning without forgetting}).	e., to not ``forget" (from now on we will refer to those initial categories by calling them base categories).  Motivated by this observation, in this work we propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training data is assumed to exist for a set of base categories and, using these data as the sole input, we want to develop an object recognition learning system that, not only is able to recognize these base categories, but also learns to dynamically recognize novel categories from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (\emph{dynamic few-shot learning without forgetting}).	score:300
Is it true that IIM-A would not consider application rating scores for giving a call to a CAT aspirant of batch 2015–2017?	g. the running score in an Atari game~\cite{mnih2015human}, or the distance between a robot arm and an object in a reaching task~\cite{lillicrap2015continuous}. However, in many real-world scenarios, rewards extrinsic to the agent are extremely sparse or missing altogether, and it is not possible to construct a shaped reward function. This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state.  Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments.  \begin{figure}     \centering     \begin{subfigure}[b]{0.48\linewidth}         \includegraphics[width=\linewidth]{images/mario_1-1.png}         \caption{learn to explore in Level-1}     \end{subfigure}     ~     \begin{subfigure}[b]{0.	g. the running score in an Atari game~\cite{mnih2015human}, or the distance between a robot arm and an object in a reaching task~\cite{lillicrap2015continuous}. However, in many real-world scenarios, rewards extrinsic to the agent are extremely sparse or missing altogether, and it is not possible to construct a shaped reward function. This is a problem as the agent receives reinforcement for updating its policy only if it succeeds in reaching a pre-specified goal state.  Hoping to stumble into a goal state by chance (i.e. random exploration) is likely to be futile for all but the simplest of environments.  \begin{figure}     \centering     \begin{subfigure}[b]{0.48\linewidth}         \includegraphics[width=\linewidth]{images/mario_1-1.png}         \caption{learn to explore in Level-1}     \end{subfigure}     ~     \begin{subfigure}[b]{0.	score:307
Is it true that IIM-A would not consider application rating scores for giving a call to a CAT aspirant of batch 2015–2017?	  We attribute this limit of adaptation to the fact that, in existing platforms such as Amazon's Mechanical Turk,  the workers are fleeting and the system does not allow for exploiting good workers.  Therefore, a positive message of this result  is that a good ‘rating system’ for workers is essential to truly benefit from crowdsourcing platforms using adaptivity.   Another novel contribution of our work is the analysis technique.  The iterative inference algorithm we introduce operates on real-valued messages  whose distribution is a priori difficult to analyze. To overcome this challenge, we develop a novel technique of establishing  that these messages are sub-Gaussian  and compute the parameters recursively in a closed form.  This allows us to prove the sharp result on the error rate.   This technique could be of independent interest in  analyzing a more general class of message-passing algorithms.	  We attribute this limit of adaptation to the fact that, in existing platforms such as Amazon's Mechanical Turk,  the workers are fleeting and the system does not allow for exploiting good workers.  Therefore, a positive message of this result  is that a good ‘rating system’ for workers is essential to truly benefit from crowdsourcing platforms using adaptivity.   Another novel contribution of our work is the analysis technique.  The iterative inference algorithm we introduce operates on real-valued messages  whose distribution is a priori difficult to analyze. To overcome this challenge, we develop a novel technique of establishing  that these messages are sub-Gaussian  and compute the parameters recursively in a closed form.  This allows us to prove the sharp result on the error rate.   This technique could be of independent interest in  analyzing a more general class of message-passing algorithms.	score:313
Is it true that IIM-A would not consider application rating scores for giving a call to a CAT aspirant of batch 2015–2017?	 The instantaneous deviation between the agent's current policy and such a reference policy directly results in an intrinsic penalty to be subtracted from the reward. Information-theoretic RL approaches \cite{Azar2012,Rawlik2012,Fox2016} have been designed for the tabular setting but do not readily apply to high-dimensional environments that require parametric function approximators.   Since we are interested in improving sample complexity of RL in high-dimensional state spaces, we contribute by adapting information-theoretic concepts to phrase a novel optimization objective for learning Q-values with deep parametric function approximators. The resultant algorithm encompasses a wide range of learning outcomes that can be demonstrated by tuning a Lagrange multiplier.	 The instantaneous deviation between the agent's current policy and such a reference policy directly results in an intrinsic penalty to be subtracted from the reward. Information-theoretic RL approaches \cite{Azar2012,Rawlik2012,Fox2016} have been designed for the tabular setting but do not readily apply to high-dimensional environments that require parametric function approximators.   Since we are interested in improving sample complexity of RL in high-dimensional state spaces, we contribute by adapting information-theoretic concepts to phrase a novel optimization objective for learning Q-values with deep parametric function approximators. The resultant algorithm encompasses a wide range of learning outcomes that can be demonstrated by tuning a Lagrange multiplier.	score:316
What are some unsolved problems in deep machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:334
What are some unsolved problems in deep machine learning?	  Recently, Deep Learning has been showing promising results in various Artificial Intelligence applications like image recognition, natural language processing, language modeling, neural machine translation, etc. Although, in general, it is computationally more expensive as compared to classical machine learning techniques, their results are found to be more effective in some cases.  Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 \& 7 layers architectures with the four different feature sets, and found that  irrespective of the features inputs, the classical RF accuracy outperforms the DNN.	  Recently, Deep Learning has been showing promising results in various Artificial Intelligence applications like image recognition, natural language processing, language modeling, neural machine translation, etc. Although, in general, it is computationally more expensive as compared to classical machine learning techniques, their results are found to be more effective in some cases.  Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 \& 7 layers architectures with the four different feature sets, and found that  irrespective of the features inputs, the classical RF accuracy outperforms the DNN.	score:344
What are some unsolved problems in deep machine learning?	 It turns out that the same problem is also at the heart of many other subjects such as machine learning and quantum many-body physics.  In recent years, deep learning has shown impressive results on a variety of hard problems in machine learning \citep{Lecun1998,Bengio2009,Krizhevsky2012,LeCun2015},  suggesting that deep neural networks might be an effective tool for dealing with the curse of dimensionality problem.  It should be emphasized that although there are partial analytical results, the reason why deep neural networks have performed so well still largely remains a mystery. Nevertheless, it motivates using the deep neural network approximation in other contexts where curse of dimensionality is the essential obstacle.  In this paper, we develop the deep neural network approximation  in the context of stochastic control problems.	 It turns out that the same problem is also at the heart of many other subjects such as machine learning and quantum many-body physics.  In recent years, deep learning has shown impressive results on a variety of hard problems in machine learning \citep{Lecun1998,Bengio2009,Krizhevsky2012,LeCun2015},  suggesting that deep neural networks might be an effective tool for dealing with the curse of dimensionality problem.  It should be emphasized that although there are partial analytical results, the reason why deep neural networks have performed so well still largely remains a mystery. Nevertheless, it motivates using the deep neural network approximation in other contexts where curse of dimensionality is the essential obstacle.  In this paper, we develop the deep neural network approximation  in the context of stochastic control problems.	score:346
What are some unsolved problems in deep machine learning?	 So input quality is an important practical challenge that is often overlooked in the design of machine learning systems. Deep learning has obtained state-of-the-art performance on many machine vision and audition tasks. However, the relationship between lossy compression and machine learning performance mostly remains unexplored.  In this article, we propose a fundamental answer to this question.  Our approach to the problem is a reinterpretation of the Helmholtz free energy formula from physics to explain the relationship between content and noise when using sensors to capture multimedia data. This allows the empirical estimation of the noise content in images, audio, and videos based on combining a classifier with perceptual compression, such as JPEG or MP3.	 So input quality is an important practical challenge that is often overlooked in the design of machine learning systems. Deep learning has obtained state-of-the-art performance on many machine vision and audition tasks. However, the relationship between lossy compression and machine learning performance mostly remains unexplored.  In this article, we propose a fundamental answer to this question.  Our approach to the problem is a reinterpretation of the Helmholtz free energy formula from physics to explain the relationship between content and noise when using sensors to capture multimedia data. This allows the empirical estimation of the noise content in images, audio, and videos based on combining a classifier with perceptual compression, such as JPEG or MP3.	score:350
What are some unsolved problems in deep machine learning?	 Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning \emph{can make mistakes} and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems.  We show that in limited contexts the required number of training samples can be low and self-improvement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications.	 Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning \emph{can make mistakes} and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems.  We show that in limited contexts the required number of training samples can be low and self-improvement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications.	score:359
Who played Sherlock Holmes better, Benedict Cumberbatch or Robert Downey Jr.?	 Each behavior is associated with a set of phase-portraits corresponding to the inputs the agent experiences while performing them. The sets of phase-portraits (and the attractors therein) could be overlapping (Fig.~\ref{fig:schematic}C,D) or could be unique to each behavior (Fig.~\ref{fig:schematic}B). The set of all attractors from all phase-portraits corresponding to a behavior are also referred to as the attractor set of the behavior in this paper.  The third level of reuse is that of ongoing transient dynamics as the agent is in continuous closed-loop interaction with the environment. When there is attractor reuse from the previous level, it is possible that multiple behaviors navigate different transients around those attractors (Fig.~\ref{fig:schematic}C) or, they might be reused too (Fig.~\ref{fig:schematic}D).	 Each behavior is associated with a set of phase-portraits corresponding to the inputs the agent experiences while performing them. The sets of phase-portraits (and the attractors therein) could be overlapping (Fig.~\ref{fig:schematic}C,D) or could be unique to each behavior (Fig.~\ref{fig:schematic}B). The set of all attractors from all phase-portraits corresponding to a behavior are also referred to as the attractor set of the behavior in this paper.  The third level of reuse is that of ongoing transient dynamics as the agent is in continuous closed-loop interaction with the environment. When there is attractor reuse from the previous level, it is possible that multiple behaviors navigate different transients around those attractors (Fig.~\ref{fig:schematic}C) or, they might be reused too (Fig.~\ref{fig:schematic}D).	score:389
Who played Sherlock Holmes better, Benedict Cumberbatch or Robert Downey Jr.?	    Very computationally efficient methods based on simple scoring methodologies that come with certain guaranties exist since the work of Huber in the 1960s \cite{HuberRowSum1963} (based on a simple row-sum procedure), and very recently Wauthier et al.  \cite{RankingWauthierJordan} in the context of ranking from a random sample of binary comparisons, who can also account for whether one seeks an  approximately uniform quality across the ranking, or more accuracy near the top of the ranking than the bottom.         The idea of angular embedding, which we exploit in this paper, is not new, and aside from recent work by Singer in the context of the angular synchronization problem \cite{sync}, has also been explored by Yu \cite{StellaAE}, who observes that embedding in the angular space is significantly more robust to outliers when compared to   embedding in the linear space.	    Very computationally efficient methods based on simple scoring methodologies that come with certain guaranties exist since the work of Huber in the 1960s \cite{HuberRowSum1963} (based on a simple row-sum procedure), and very recently Wauthier et al.  \cite{RankingWauthierJordan} in the context of ranking from a random sample of binary comparisons, who can also account for whether one seeks an  approximately uniform quality across the ranking, or more accuracy near the top of the ranking than the bottom.         The idea of angular embedding, which we exploit in this paper, is not new, and aside from recent work by Singer in the context of the angular synchronization problem \cite{sync}, has also been explored by Yu \cite{StellaAE}, who observes that embedding in the angular space is significantly more robust to outliers when compared to   embedding in the linear space.	score:422
Who played Sherlock Holmes better, Benedict Cumberbatch or Robert Downey Jr.?	     The question was further developed and applied to portfolio management in Refs.~\cite{Howard,Barron,Cover}. Kelly's horse race appears in some aspects of evolutionary biology as well. There, the reward function is the population growth rate and information  refers to the state of the environment \cite{Bergstrom,Kussell,Donaldson-Matasci,Rivoire,Bialek}.     Neurobiology is the field where information theory is arguably the most popular in biological sciences. Barlow's efficient coding  \cite{Barlow61} postulated that  early neural sensory layers efficiently represent environmental information, i.e. their evolutionary fitness is proportional to  their efficiency in the transmission of information from the environment to higher parts of the brain.	     The question was further developed and applied to portfolio management in Refs.~\cite{Howard,Barron,Cover}. Kelly's horse race appears in some aspects of evolutionary biology as well. There, the reward function is the population growth rate and information  refers to the state of the environment \cite{Bergstrom,Kussell,Donaldson-Matasci,Rivoire,Bialek}.     Neurobiology is the field where information theory is arguably the most popular in biological sciences. Barlow's efficient coding  \cite{Barlow61} postulated that  early neural sensory layers efficiently represent environmental information, i.e. their evolutionary fitness is proportional to  their efficiency in the transmission of information from the environment to higher parts of the brain.	score:423
Who played Sherlock Holmes better, Benedict Cumberbatch or Robert Downey Jr.?	 The task of the clustering algorithm would be to group the data points in a manner such that the distances between data points within the same cluster is minimized, and the distances between data points belonging to different clusters are maximized. There are several existing popular methods for clustering e.g. Hierarchal Clustering, Bayesian Clustering, K-means, C-means, Spectral Clustering, Mean-shift and so on.  Some of these algorithms are iterative heuristics e.g. k-means, c-means. Mean Shift Clustering is based on feature space analysis to identify the minima of the density function \cite{fukunaga1975estimation, cheng1995mean}. Whereas, Spectral Clustering type algorithms \cite{ng2002spectral} try to analyze the algebraic properties of the distance matrix to cluster the data points.	 The task of the clustering algorithm would be to group the data points in a manner such that the distances between data points within the same cluster is minimized, and the distances between data points belonging to different clusters are maximized. There are several existing popular methods for clustering e.g. Hierarchal Clustering, Bayesian Clustering, K-means, C-means, Spectral Clustering, Mean-shift and so on.  Some of these algorithms are iterative heuristics e.g. k-means, c-means. Mean Shift Clustering is based on feature space analysis to identify the minima of the density function \cite{fukunaga1975estimation, cheng1995mean}. Whereas, Spectral Clustering type algorithms \cite{ng2002spectral} try to analyze the algebraic properties of the distance matrix to cluster the data points.	score:426
Who played Sherlock Holmes better, Benedict Cumberbatch or Robert Downey Jr.?	 With the substantially faster SWAP, we can now also explore alternative (faster) strategies for choosing the initial medoids. We also show how the CLARA and CLARANS algorithms benefit from the proposed modifications.  While we do not further study the parallelization of our approach in this work, it can easily be combined with earlier approaches to use PAM and CLARA on big data (some of which use PAM as a subroutine, hence can immediately benefit from these improvements), where the performance with high $k$ becomes increasingly important.   In experiments on real data with $k=100$, we observed a $200\times$ speedup compared to the original PAM SWAP algorithm, making PAM applicable to larger data sets as long as we can afford to compute a distance matrix, and in particular to higher~$k$ (at $k=2$, the new SWAP was only 1.5 times faster, as the speedup is expected to increase with $k$).	 With the substantially faster SWAP, we can now also explore alternative (faster) strategies for choosing the initial medoids. We also show how the CLARA and CLARANS algorithms benefit from the proposed modifications.  While we do not further study the parallelization of our approach in this work, it can easily be combined with earlier approaches to use PAM and CLARA on big data (some of which use PAM as a subroutine, hence can immediately benefit from these improvements), where the performance with high $k$ becomes increasingly important.   In experiments on real data with $k=100$, we observed a $200\times$ speedup compared to the original PAM SWAP algorithm, making PAM applicable to larger data sets as long as we can afford to compute a distance matrix, and in particular to higher~$k$ (at $k=2$, the new SWAP was only 1.5 times faster, as the speedup is expected to increase with $k$).	score:428
Who played a better Sherlock: Benedict Cumberbatch or Downey Jr.?	 Each behavior is associated with a set of phase-portraits corresponding to the inputs the agent experiences while performing them. The sets of phase-portraits (and the attractors therein) could be overlapping (Fig.~\ref{fig:schematic}C,D) or could be unique to each behavior (Fig.~\ref{fig:schematic}B). The set of all attractors from all phase-portraits corresponding to a behavior are also referred to as the attractor set of the behavior in this paper.  The third level of reuse is that of ongoing transient dynamics as the agent is in continuous closed-loop interaction with the environment. When there is attractor reuse from the previous level, it is possible that multiple behaviors navigate different transients around those attractors (Fig.~\ref{fig:schematic}C) or, they might be reused too (Fig.~\ref{fig:schematic}D).	 Each behavior is associated with a set of phase-portraits corresponding to the inputs the agent experiences while performing them. The sets of phase-portraits (and the attractors therein) could be overlapping (Fig.~\ref{fig:schematic}C,D) or could be unique to each behavior (Fig.~\ref{fig:schematic}B). The set of all attractors from all phase-portraits corresponding to a behavior are also referred to as the attractor set of the behavior in this paper.  The third level of reuse is that of ongoing transient dynamics as the agent is in continuous closed-loop interaction with the environment. When there is attractor reuse from the previous level, it is possible that multiple behaviors navigate different transients around those attractors (Fig.~\ref{fig:schematic}C) or, they might be reused too (Fig.~\ref{fig:schematic}D).	score:372
Who played a better Sherlock: Benedict Cumberbatch or Downey Jr.?	 Comparison between IRL and MIRL is made in the context of an abstract soccer game, using both  a  game model in which the reward depends only on state and one in which it depends on both state and action.  Results suggest that the IRL approach performs much worse than the MIRL approach. We speculate that the underperformance of IRL is because it fails to capture equilibrium information in the manner possible in MIRL.   	The Multi-agent Reinforcement Learning (MRL) problem was first proposed by proposed by Littman \cite{Littman1994} to address the  limiting assumption in Reinforcement Learning (RL) that potentially responsive agents in a system area part of a passive environment. In a RL model, an agent can fully control the state transition process by taking actions on its own (though some stochastic variation is allowed); in a MRL problem, by contrast, the state transition process is determined by joint actions of all interacting rational agents.	 Comparison between IRL and MIRL is made in the context of an abstract soccer game, using both  a  game model in which the reward depends only on state and one in which it depends on both state and action.  Results suggest that the IRL approach performs much worse than the MIRL approach. We speculate that the underperformance of IRL is because it fails to capture equilibrium information in the manner possible in MIRL.   	The Multi-agent Reinforcement Learning (MRL) problem was first proposed by proposed by Littman \cite{Littman1994} to address the  limiting assumption in Reinforcement Learning (RL) that potentially responsive agents in a system area part of a passive environment. In a RL model, an agent can fully control the state transition process by taking actions on its own (though some stochastic variation is allowed); in a MRL problem, by contrast, the state transition process is determined by joint actions of all interacting rational agents.	score:382
Who played a better Sherlock: Benedict Cumberbatch or Downey Jr.?	 The evaluation can be done using Statistical Hypothesis Testing (SHT)~\cite{Frakt1998,lehmann2005testing, LockeWangPasTech, Paschalidis2009}. Deterministic methods, on the other hand, try to partition the feature space into ``normal'' and ``abnormal'' regions through a deterministic decision boundary. The boundary can be determined using methods like Support Vector Machine (SVM), particularly 1-class SVM~\cite{hastie2001elements, Perdisci2006, shon2007hybrid}, and clustering analysis~\cite{anderberg1973cluster, gu2008botminer}.   From the perspective of data, network anomaly methods can be either packet-based~\cite{hareesh2011anomaly,Mahoney2001}, flow-based~\cite{Androulidakis2008, Manikopoulo2006} or window-based~\cite{Lee2001}.  Packet-based methods evaluate the raw packets directly while both flow-based and window-based methods aggregate the packets first. Flow-based methods evalulate each flow individually, which is defined as a collection of packets with similar properties.	 The evaluation can be done using Statistical Hypothesis Testing (SHT)~\cite{Frakt1998,lehmann2005testing, LockeWangPasTech, Paschalidis2009}. Deterministic methods, on the other hand, try to partition the feature space into ``normal'' and ``abnormal'' regions through a deterministic decision boundary. The boundary can be determined using methods like Support Vector Machine (SVM), particularly 1-class SVM~\cite{hastie2001elements, Perdisci2006, shon2007hybrid}, and clustering analysis~\cite{anderberg1973cluster, gu2008botminer}.   From the perspective of data, network anomaly methods can be either packet-based~\cite{hareesh2011anomaly,Mahoney2001}, flow-based~\cite{Androulidakis2008, Manikopoulo2006} or window-based~\cite{Lee2001}.  Packet-based methods evaluate the raw packets directly while both flow-based and window-based methods aggregate the packets first. Flow-based methods evalulate each flow individually, which is defined as a collection of packets with similar properties.	score:395
Who played a better Sherlock: Benedict Cumberbatch or Downey Jr.?	   This paper addresses a subset of problems of multi-agent inverse reinforcement learning (MIRL) in a two-player general-sum stochastic game framework. Specifically, five variants of MIRL are considered: \emph{uCS-MIRL}, \emph{advE-MIRL}, \emph{cooE-MIRL}, \emph{uCE-MIRL}, and \emph{uNE-MIRL}, each distinguished by its solution concept. Problem uCS-MIRL is a cooperative game in which the agents employ cooperative strategies that aim to maximize the total game value.  In problem uCE-MIRL, agents are assumed to follow strategies that constitute a correlated equilibrium while maximizing total game value. Problem uNE-MIRL  is similar to uCE-MIRL in total game value maximization, but it is assumed that the agents are playing a Nash equilibrium. Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively.  We propose novel approaches to address these five problems under the assumption that the game observer either knows or is able to accurately estimate the policies and solution concepts for players. For uCS-MIRL, we first develop a characteristic set of solutions ensuring that the observed bi-policy is a uCS and then apply a Bayesian inverse learning method.	 In problem uCE-MIRL, agents are assumed to follow strategies that constitute a correlated equilibrium while maximizing total game value. Problem uNE-MIRL  is similar to uCE-MIRL in total game value maximization, but it is assumed that the agents are playing a Nash equilibrium. Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively.	score:397
Who played a better Sherlock: Benedict Cumberbatch or Downey Jr.?	 These biases prove to be a very useful constraint in the context of Atari, as humans can immediately identify e.g. that a skull represents danger, or that a key unlocks a door.  Among existing imitation learning methods, DQfD by Hester et al.~\cite{hester2017deep} has shown the best performance on Atari's hardest exploration games. Despite these impressive results, there are two limitations of DQfD~\cite{hester2017deep} and related methods.  First, they assume that there is no ``domain gap” between the agent’s and demonstrator’s observation space, e.g. variations in color or resolution, or the introduction of other visual artifacts. An example of domain gap in \textsc{Montezuma’s Revenge} is shown in Figure~\ref{fig:youtube_videos}, considering the first frame of (a) our environment compared to (b) YouTube gameplay footage.	 These biases prove to be a very useful constraint in the context of Atari, as humans can immediately identify e.g. that a skull represents danger, or that a key unlocks a door.  Among existing imitation learning methods, DQfD by Hester et al.~\cite{hester2017deep} has shown the best performance on Atari's hardest exploration games. Despite these impressive results, there are two limitations of DQfD~\cite{hester2017deep} and related methods.  First, they assume that there is no ``domain gap” between the agent’s and demonstrator’s observation space, e.g. variations in color or resolution, or the introduction of other visual artifacts. An example of domain gap in \textsc{Montezuma’s Revenge} is shown in Figure~\ref{fig:youtube_videos}, considering the first frame of (a) our environment compared to (b) YouTube gameplay footage.	score:398
Use of matrix in busines?	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	score:437
Use of matrix in busines?	  Specifically, motivated by the recent work~\cite{li2015} that explains the word embedding model of Skip-Gram Negative Sampling (SGNS) as a matrix factorization of the words' co-occurrence matrix, we build a co-occurrence matrix of structural proximities for a network based on a random walk sampling procedure. The process of SGNS can then be formulated as minimizing a matrix factorization loss, which can be naturally integrated with representation learning of node content.	  Specifically, motivated by the recent work~\cite{li2015} that explains the word embedding model of Skip-Gram Negative Sampling (SGNS) as a matrix factorization of the words' co-occurrence matrix, we build a co-occurrence matrix of structural proximities for a network based on a random walk sampling procedure. The process of SGNS can then be formulated as minimizing a matrix factorization loss, which can be naturally integrated with representation learning of node content.	score:442
Use of matrix in busines?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:443
Use of matrix in busines?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	  In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	score:445
Use of matrix in busines?	  Predicting unobserved entries of a partially observed matrix has found wide applicability in several areas, such as recommender systems, computational biology, and computer vision. Many scalable methods with rigorous theoretical guarantees have been developed for algorithms where the matrix is factored into low-rank components, and embeddings are learned for the row and column entities.  While there has been recent research on incorporating explicit side information in the low-rank matrix factorization setting, often implicit information can be gleaned from the data, via higher order interactions among entities. Such implicit information is especially useful in cases where the data is very sparse, as is often the case in real world datasets.	  Predicting unobserved entries of a partially observed matrix has found wide applicability in several areas, such as recommender systems, computational biology, and computer vision. Many scalable methods with rigorous theoretical guarantees have been developed for algorithms where the matrix is factored into low-rank components, and embeddings are learned for the row and column entities.  While there has been recent research on incorporating explicit side information in the low-rank matrix factorization setting, often implicit information can be gleaned from the data, via higher order interactions among entities. Such implicit information is especially useful in cases where the data is very sparse, as is often the case in real world datasets.	score:453
What is the difference between machine learning and data analysis?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:261
What is the difference between machine learning and data analysis?	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	score:279
What is the difference between machine learning and data analysis?	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	score:284
What is the difference between machine learning and data analysis?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:289
What is the difference between machine learning and data analysis?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	score:292
I am from non programming background, how can I learn machine learning by self?	 Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes, but it has not yet been successfully used for automotive applications. There has recently been a revival of interest in the topic, however, driven by the ability of deep learning algorithms to learn good representations of the environment.  Motivated by Google DeepMind's successful demonstrations of learning for games from Breakout to Go, we will propose different methods for autonomous driving using deep reinforcement learning. This is of particular interest as it is difficult to pose autonomous driving as a supervised learning problem as it has a strong interaction with the environment including other vehicles, pedestrians and roadworks.	 Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes, but it has not yet been successfully used for automotive applications. There has recently been a revival of interest in the topic, however, driven by the ability of deep learning algorithms to learn good representations of the environment.  Motivated by Google DeepMind's successful demonstrations of learning for games from Breakout to Go, we will propose different methods for autonomous driving using deep reinforcement learning. This is of particular interest as it is difficult to pose autonomous driving as a supervised learning problem as it has a strong interaction with the environment including other vehicles, pedestrians and roadworks.	score:389
I am from non programming background, how can I learn machine learning by self?	 	Machine learning problems with categorical data require modeling interactions between the features to solve them. As an example, consider a sentiment analysis problem -- detecting whether a review is positive or negative -- and the following dataset: `I liked it', `I did not like it', `I'm not sure'.  Judging by the presence of the word `like' or the word `not' alone, it is hard to understand the tone of the review.  But the presence of the \emph{pair} of words `not' and `like' strongly indicates a negative opinion.  If the dictionary has $d$ words, modeling pairwise interactions requires~$O(d^2)$ parameters and will probably overfit to the data. Taking into account all interactions (all pairs, triplets, etc. of words) requires impractical~$2^d$ parameters.  In this paper, we show a scalable way to account for all interactions.	 	Machine learning problems with categorical data require modeling interactions between the features to solve them. As an example, consider a sentiment analysis problem -- detecting whether a review is positive or negative -- and the following dataset: `I liked it', `I did not like it', `I'm not sure'.  Judging by the presence of the word `like' or the word `not' alone, it is hard to understand the tone of the review.  But the presence of the \emph{pair} of words `not' and `like' strongly indicates a negative opinion.  If the dictionary has $d$ words, modeling pairwise interactions requires~$O(d^2)$ parameters and will probably overfit to the data. Taking into account all interactions (all pairs, triplets, etc. of words) requires impractical~$2^d$ parameters.  In this paper, we show a scalable way to account for all interactions.	score:393
I am from non programming background, how can I learn machine learning by self?	 While classical machine learning with shallow neural networks requires that the relevant features of the raw data should be selected by using domain knowledge before the learning can start, deep learning algorithms appear to select the right features automatically. However, it is typically not clear how to interpret these features. Indeed, from a mathematical point of view, it is easy to show that a structure such as (\ref{compositionfigure}) is not unique, so that the hierarchical features cannot be defined uniquely, except perhaps in some very special examples.	 While classical machine learning with shallow neural networks requires that the relevant features of the raw data should be selected by using domain knowledge before the learning can start, deep learning algorithms appear to select the right features automatically. However, it is typically not clear how to interpret these features. Indeed, from a mathematical point of view, it is easy to show that a structure such as (\ref{compositionfigure}) is not unique, so that the hierarchical features cannot be defined uniquely, except perhaps in some very special examples.	score:395
I am from non programming background, how can I learn machine learning by self?	 To use world knowledge as supervision, we introduce the linking and inference techniques that can relate a domain problem to a general-purpose knowledge base. Then we introduce some new learning paradigms that can be enabled by world knowledge. Finally we discuss the future directions of the ideas of machine learning with world knowledge and conclude our paper. Note that, we will not focus on machine learning for world knowledge acquisition or organization. Instead, we assume that the world knowledge is already existing for machines to use.	 To use world knowledge as supervision, we introduce the linking and inference techniques that can relate a domain problem to a general-purpose knowledge base. Then we introduce some new learning paradigms that can be enabled by world knowledge. Finally we discuss the future directions of the ideas of machine learning with world knowledge and conclude our paper. Note that, we will not focus on machine learning for world knowledge acquisition or organization. Instead, we assume that the world knowledge is already existing for machines to use.	score:401
I am from non programming background, how can I learn machine learning by self?	 However, the machine-learning-based VAD is still far from its practical use. One significant problem is that we are not sure whether the VAD model trained in a given source corpus is still powerful in a target corpus which might have a different distribution with the source corpus.  In this paper, we try to deal with the aforementioned problem by a novel learning method -- transfer learning.  Generally, transfer learning tries to make the model trained with one or multiple source tasks generalizes well on different but related target tasks, so that the performance gap between the source tasks and the target tasks can be lowered. See \cite{pan2010survey} for an excellent survey on transfer learning. In respect of \textit{different hypothesis} on whether the source data or target data is manually labeled, the transfer learning technologies can be categorized into four groups \cite{pan2010survey}.	 However, the machine-learning-based VAD is still far from its practical use. One significant problem is that we are not sure whether the VAD model trained in a given source corpus is still powerful in a target corpus which might have a different distribution with the source corpus.  In this paper, we try to deal with the aforementioned problem by a novel learning method -- transfer learning.  Generally, transfer learning tries to make the model trained with one or multiple source tasks generalizes well on different but related target tasks, so that the performance gap between the source tasks and the target tasks can be lowered. See \cite{pan2010survey} for an excellent survey on transfer learning. In respect of \textit{different hypothesis} on whether the source data or target data is manually labeled, the transfer learning technologies can be categorized into four groups \cite{pan2010survey}.	score:401
How Can I use Deep Learning on Voice classification/Similarity?	 Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning ({\it BC learning}). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds.  We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher's criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes.	 Deep learning methods have achieved high performance in sound recognition tasks. Deciding how to feed the training data is important for further performance improvement. We propose a novel learning method for deep sound recognition: Between-Class learning ({\it BC learning}). Our strategy is to learn a discriminative feature space by recognizing the between-class sounds as between-class sounds.  We generate between-class sounds by mixing two sounds belonging to different classes with a random ratio. We then input the mixed sound to the model and train the model to output the mixing ratio. The advantages of BC learning are not limited only to the increase in variation of the training data; BC learning leads to an enlargement of Fisher's criterion in the feature space and a regularization of the positional relationship among the feature distributions of the classes.	score:331
How Can I use Deep Learning on Voice classification/Similarity?	 	The success of deep-learning lies in constructing feature spaces where in competing classes of objects, sounds, etc. can be shattered using high-dimensional hyperplanes. The classifier relies on the accumulation of representation in the final convolution layer of a deep neural network. Often times, the classifier performance increases as one incorporates information from earlier layers of the neural network.  Such a structural constraint has been imposed on certain deep learning architectures via the \textit{inception module}. In addition to decreasing the computational effort, utilization of $1 \times 1$ convolution filters enables the dimensionality of feature map to be immensely reduced; in tandem with pooling, the dimensionality reduces even further.	 	The success of deep-learning lies in constructing feature spaces where in competing classes of objects, sounds, etc. can be shattered using high-dimensional hyperplanes. The classifier relies on the accumulation of representation in the final convolution layer of a deep neural network. Often times, the classifier performance increases as one incorporates information from earlier layers of the neural network.  Such a structural constraint has been imposed on certain deep learning architectures via the \textit{inception module}. In addition to decreasing the computational effort, utilization of $1 \times 1$ convolution filters enables the dimensionality of feature map to be immensely reduced; in tandem with pooling, the dimensionality reduces even further.	score:386
How Can I use Deep Learning on Voice classification/Similarity?	  Deep learning~\cite{Bengio-et-al-2015-Book,eosl} refers to a set of machine learning algorithms that utilize large neural networks with many hidden layers (also referred to as Deep Neural Networks (DNNs) for feature generation, learning, classification and prediction.    Deep learning is extensively used by many online and mobile services, such as the voice recognition and dialog systems of Siri, the Google Assistant, Amazon's Alexa and Microsoft Cortana, as well as the image classification systems in Google Photo and Facebook.  We believe that deep learning has many applications within the automotive industry, such as computer vision for autonomous driving and robotics, optimizations in the manufacturing process (e.\,g.\ monitoring for quality issues), and connected vehicle and infotainment services (e.\,g.\ voice recognition systems).     The landscape of infrastructure and tools for training and deploying deep neural networks is evolving rapidly.	  Deep learning~\cite{Bengio-et-al-2015-Book,eosl} refers to a set of machine learning algorithms that utilize large neural networks with many hidden layers (also referred to as Deep Neural Networks (DNNs) for feature generation, learning, classification and prediction.    Deep learning is extensively used by many online and mobile services, such as the voice recognition and dialog systems of Siri, the Google Assistant, Amazon's Alexa and Microsoft Cortana, as well as the image classification systems in Google Photo and Facebook.  We believe that deep learning has many applications within the automotive industry, such as computer vision for autonomous driving and robotics, optimizations in the manufacturing process (e.\,g.\ monitoring for quality issues), and connected vehicle and infotainment services (e.\,g.\ voice recognition systems).     The landscape of infrastructure and tools for training and deploying deep neural networks is evolving rapidly.	score:386
How Can I use Deep Learning on Voice classification/Similarity?	  Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks.  In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.  	Classification is one of the most important machine learning tasks. Besides classification algorithms, the performance of classifier is heavily dependent on the set of data representations on which they are applied.	  Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks.  In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.  	Classification is one of the most important machine learning tasks. Besides classification algorithms, the performance of classifier is heavily dependent on the set of data representations on which they are applied.	score:393
How Can I use Deep Learning on Voice classification/Similarity?	 Deep Learning shows very good performance when trained on large labeled data sets. The problem of training a deep net on a few or one sample per class requires a different learning approach which can generalize to unseen classes using only a few representatives of these classes. This problem has previously been approached by meta-learning. Here we propose a novel meta-learner which shows state-of-the-art performance on common benchmarks for one/few shot classification.  Our model features three novel components: First is a feed-forward embedding that takes random class support samples  (after a customary CNN embedding) and transfers them to a better class representation in terms of a classification problem. Second is a novel attention mechanism, inspired by competitive learning, which causes class representatives to compete with each other to become a temporary class prototype with respect to the query point.	 Deep Learning shows very good performance when trained on large labeled data sets. The problem of training a deep net on a few or one sample per class requires a different learning approach which can generalize to unseen classes using only a few representatives of these classes. This problem has previously been approached by meta-learning. Here we propose a novel meta-learner which shows state-of-the-art performance on common benchmarks for one/few shot classification.  Our model features three novel components: First is a feed-forward embedding that takes random class support samples  (after a customary CNN embedding) and transfers them to a better class representation in terms of a classification problem. Second is a novel attention mechanism, inspired by competitive learning, which causes class representatives to compete with each other to become a temporary class prototype with respect to the query point.	score:413
What are some good books for machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:457
What are some good books for machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:474
What are some good books for machine learning?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:494
What are some good books for machine learning?	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	 It relies on coupling the training of multiple models and combining their predictions into a single output. This coupling is performed by encouraging the models to agree with each other on their predictions, while training. We show how the proposed framework is inspired by human learning, in contrast with other approaches commonly used in machine learning.  Finally, we provide experimental results which exhibit how our framework successfully manages to outperform cross-validation and other ensemble methods that do not couple the training of the models.  \begin{figure}[t!]  \centering      \includegraphics[width=0.5\textwidth, trim=9.2cm 2.5cm 1.5cm 6cm, clip]{diagram.pdf}   \caption{Illustration of the agreement-based learning framework that we propose.	score:501
What are some good books for machine learning?	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	score:505
What are the best books for studying machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:403
What are the best books for studying machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:440
What are the best books for studying machine learning?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:449
What are the best books for studying machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:450
What are the best books for studying machine learning?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:455
Which version of Python is more often used in machine learning, Python 2.7 or Python 3.0+?	    In this paper we present \pydci, a new implementation of the DCI method written in Python and built on top of the \texttt{SciPy} stack and \texttt{scikit-learn} toolkit. Python has become the preferred programming language for computer scientists. In the fields of machine learning and data mining its use has also been promoted by the appearance of Python-based environments such as \texttt{SciPy} and \texttt{scikit-learn}, whose potential and ease of use have attracted the interest of practitioners.  Our reimplementation is thus in line with these trends.  With respect to \jadci, \pydci\ introduces a few modifications in the way DCI is implemented that, although subtle, bring about a significant improvement in the effectiveness of the method.  The rest of this paper is structured as follows. In Section \ref{sec:changes} we describe the main modifications to DCI that our new implementation introduces.	    In this paper we present \pydci, a new implementation of the DCI method written in Python and built on top of the \texttt{SciPy} stack and \texttt{scikit-learn} toolkit. Python has become the preferred programming language for computer scientists. In the fields of machine learning and data mining its use has also been promoted by the appearance of Python-based environments such as \texttt{SciPy} and \texttt{scikit-learn}, whose potential and ease of use have attracted the interest of practitioners.  Our reimplementation is thus in line with these trends.  With respect to \jadci, \pydci\ introduces a few modifications in the way DCI is implemented that, although subtle, bring about a significant improvement in the effectiveness of the method.  The rest of this paper is structured as follows. In Section \ref{sec:changes} we describe the main modifications to DCI that our new implementation introduces.	score:427
Which version of Python is more often used in machine learning, Python 2.7 or Python 3.0+?	} on the convergence guarantee of PSP-based SGD algorithm, but also implement a full-featured distributed learning framework called Actor System and perform intensive evaluation atop of it. 	Barrier synchronisation is critical in many distributed machine learning algorithms. In general, there are three major ways to coordinate how the nodes in a system should progress in iterative learning algorithms: Bulk synchronous parallel (BSP) \cite{Valiant:1990:BMP:79173. 79181}, Stale synchronous parallel (SSP) \cite{NIPS2011_4247, NIPS2013_4894, tseng1991rate}, and Asynchronous parallel (ASP) \cite{Low:2012:DGF:2212351.2212354}.  Even though these synchronisation methods have attracted a lot of attentions lately in the distributed machine learning community, they have recurred several times in the distributed computing literature in the past decades.	} on the convergence guarantee of PSP-based SGD algorithm, but also implement a full-featured distributed learning framework called Actor System and perform intensive evaluation atop of it. 	Barrier synchronisation is critical in many distributed machine learning algorithms. In general, there are three major ways to coordinate how the nodes in a system should progress in iterative learning algorithms: Bulk synchronous parallel (BSP) \cite{Valiant:1990:BMP:79173. 79181}, Stale synchronous parallel (SSP) \cite{NIPS2011_4247, NIPS2013_4894, tseng1991rate}, and Asynchronous parallel (ASP) \cite{Low:2012:DGF:2212351.2212354}.  Even though these synchronisation methods have attracted a lot of attentions lately in the distributed machine learning community, they have recurred several times in the distributed computing literature in the past decades.	score:442
Which version of Python is more often used in machine learning, Python 2.7 or Python 3.0+?	  To address this question,  we include data sets in which some of the outcomes are replaced by an {\tt NA} value according to some underlying pre-determined model (built on top of the features).  In both sets, each simulation file is associated with meta-data indicating the type of simulation that generated it. This includes information such as the number of covariates,  the number of confounding covariates (that affect both treatment assignment and outcome),  and the levels of non-linearity.  This information can be used to further analyze the variables affecting performance.  The scoring code is in Python (versions 2.7 or $\ge 3.4$) and is publicly available  as an open-source (Apache 2.0 license) github repository at \href{https://github.com/IBM-HRL-MLHLS/IBM-Causality-Benchmarking-Framework} {https://github.com/IBM-HRL-MLHLS/IBM-Causality-Benchmarking-Framework}. A full usage description is available in the code repository.	 This information can be used to further analyze the variables affecting performance.  The scoring code is in Python (versions 2.7 or $\ge 3.4$) and is publicly available  as an open-source (Apache 2.0 license) github repository at \href{https://github.com/IBM-HRL-MLHLS/IBM-Causality-Benchmarking-Framework} {https://github.com/IBM-HRL-MLHLS/IBM-Causality-Benchmarking-Framework}. A full usage description is available in the code repository.	score:447
Which version of Python is more often used in machine learning, Python 2.7 or Python 3.0+?	   Gaussian process (GP) predictors are an important component of many   Bayesian approaches to machine learning. However, even a   straightforward implementation of Gaussian process regression (GPR)   requires $O(n^2)$ space and $O(n^3)$ time for a dataset of $n$   examples. Several approximation methods have been proposed, but   there is a lack of understanding of the relative merits of the   different approximations, and in what situations they are most   useful.   We recommend assessing the quality of the predictions   obtained as a function of the compute time taken, and comparing to   standard baselines (e.g., Subset of Data and FITC)\@.   We empirically investigate four different approximation algorithms on   four different prediction problems, and make our code available to   encourage future comparisons.	   Gaussian process (GP) predictors are an important component of many   Bayesian approaches to machine learning. However, even a   straightforward implementation of Gaussian process regression (GPR)   requires $O(n^2)$ space and $O(n^3)$ time for a dataset of $n$   examples. Several approximation methods have been proposed, but   there is a lack of understanding of the relative merits of the   different approximations, and in what situations they are most   useful.   We recommend assessing the quality of the predictions   obtained as a function of the compute time taken, and comparing to   standard baselines (e.g., Subset of Data and FITC)\@.   We empirically investigate four different approximation algorithms on   four different prediction problems, and make our code available to   encourage future comparisons.	score:448
Which version of Python is more often used in machine learning, Python 2.7 or Python 3.0+?	 In this paper, a framework for testing Deep Neural Network (DNN) design in Python is presented. First, big data, machine learning (ML), and Artificial Neural Networks (ANNs) are discussed to familiarize the reader with the importance of such a system. Next, the benefits and detriments of implementing such a system in Python are presented. Lastly, the specifics of the system are explained, and some experimental results are presented to prove the effectiveness of the system.  	In recent years, the amount of data that is produced annually has increased dramatically {[}1{]}, {[}2{]}. The availability of this data has prompted researchers to publish large amounts of literature that study how we can make sense of this data, and, in turn, how can we handle this data in a computationally efficient manner {[}3{]}, {[}4{]}, {[}5{]}, {[}6{]}.	 In this paper, a framework for testing Deep Neural Network (DNN) design in Python is presented. First, big data, machine learning (ML), and Artificial Neural Networks (ANNs) are discussed to familiarize the reader with the importance of such a system. Next, the benefits and detriments of implementing such a system in Python are presented. Lastly, the specifics of the system are explained, and some experimental results are presented to prove the effectiveness of the system.  	In recent years, the amount of data that is produced annually has increased dramatically {[}1{]}, {[}2{]}. The availability of this data has prompted researchers to publish large amounts of literature that study how we can make sense of this data, and, in turn, how can we handle this data in a computationally efficient manner {[}3{]}, {[}4{]}, {[}5{]}, {[}6{]}.	score:457
what steps should I follow to learn machine learning?	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.    \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:401
what steps should I follow to learn machine learning?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:414
what steps should I follow to learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:420
what steps should I follow to learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:420
what steps should I follow to learn machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:426
What data science and machine learning career opportunities are there at Cisco?	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	score:331
What data science and machine learning career opportunities are there at Cisco?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	score:334
What data science and machine learning career opportunities are there at Cisco?	 Fairness, through its many forms and definitions, has become an important issue facing the machine learning community. In this work, we consider how to incorporate group fairness constraints in kernel regression methods, applicable to Gaussian processes, support vector machines, neural network regression and decision tree regression. Further, we focus on examining the effect of incorporating these constraints in decision tree regression, with direct applications to random forests and boosted trees amongst other widespread popular inference techniques.  We show that the order of complexity of memory and computation is preserved for such models and tightly bound the expected perturbations to the model in terms of the number of leaves of the trees. Importantly, the approach works on trained models and hence can be easily applied to models in current use and group labels are only required on training data.	 Fairness, through its many forms and definitions, has become an important issue facing the machine learning community. In this work, we consider how to incorporate group fairness constraints in kernel regression methods, applicable to Gaussian processes, support vector machines, neural network regression and decision tree regression. Further, we focus on examining the effect of incorporating these constraints in decision tree regression, with direct applications to random forests and boosted trees amongst other widespread popular inference techniques.  We show that the order of complexity of memory and computation is preserved for such models and tightly bound the expected perturbations to the model in terms of the number of leaves of the trees. Importantly, the approach works on trained models and hence can be easily applied to models in current use and group labels are only required on training data.	score:340
What data science and machine learning career opportunities are there at Cisco?	 It relies on several research areas including statistics and machine learning. Usually, machine learning techniques are developed around flat data representation (i.e., matrix form) and are known as propositional learning approaches. However, due to the development of communication and storage technologies, data management practices have taken further aspects.  Data can present a very large number of dimensions, with several different types of entities. With the growing interest in extracting patterns from such data representation, relational data mining approaches have emerged with the interest of finding patterns in a given relational database~\cite{datamining01} and Statistical Relational Learning (SRL) has emerged as an area of machine learning that enables effective and robust reasoning about relational data structures~\cite{SL07}.	 It relies on several research areas including statistics and machine learning. Usually, machine learning techniques are developed around flat data representation (i.e., matrix form) and are known as propositional learning approaches. However, due to the development of communication and storage technologies, data management practices have taken further aspects.  Data can present a very large number of dimensions, with several different types of entities. With the growing interest in extracting patterns from such data representation, relational data mining approaches have emerged with the interest of finding patterns in a given relational database~\cite{datamining01} and Statistical Relational Learning (SRL) has emerged as an area of machine learning that enables effective and robust reasoning about relational data structures~\cite{SL07}.	score:340
What data science and machine learning career opportunities are there at Cisco?	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	score:343
What data science and machine learning career opportunities are there at Google?	  	Machine learning serves as the backbone for a wide variety of cognitive tasks such as image classification, object recognition, and natural language processing. Today, applications can leverage state-of-the-art machine learning models by using cloud services that offer machine learning as a service~\cite{azure-ml, google-ai, aws-ml}. To handle large traffic, such service providers typically use a distributed setup with a large number of interconnected servers (compute nodes).   It is well-known that such a distributed compute infrastructure faces a number of unavailability events~\cite{jeff-dean-failures,rashmi2014hitchhiker,asterisxoring}. First, these clusters are typically built out of commodity components making failures the norm rather than the exception. Second, various factors including load imbalance and resource contention cause transient slowdowns.	  	Machine learning serves as the backbone for a wide variety of cognitive tasks such as image classification, object recognition, and natural language processing. Today, applications can leverage state-of-the-art machine learning models by using cloud services that offer machine learning as a service~\cite{azure-ml, google-ai, aws-ml}. To handle large traffic, such service providers typically use a distributed setup with a large number of interconnected servers (compute nodes).   It is well-known that such a distributed compute infrastructure faces a number of unavailability events~\cite{jeff-dean-failures,rashmi2014hitchhiker,asterisxoring}. First, these clusters are typically built out of commodity components making failures the norm rather than the exception. Second, various factors including load imbalance and resource contention cause transient slowdowns.	score:348
What data science and machine learning career opportunities are there at Google?	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	score:351
What data science and machine learning career opportunities are there at Google?	  We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's {\em Federated Learning}. Formally, we focus on a decentralized system that consists of a parameter server and $m$ working machines; each working machine keeps $N/m$ data samples, where $N$ is the total number of samples.  In each iteration, up to $q$ of the $m$ working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system.  Additionally, the sets of faulty machines may be different across iterations.   Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension $d$, despite the interruption of the Byzantine attacks.	 Google has been intensively testing this new paradigm in their recent projects such as {\em Gboard}~\cite{federatedlearningblog}, the Google Keyboard. Compared to ``training within cloud'', Federated Learning faces the following three key challenges: \begin{itemize} \item Security:  The devices of the recruited data owners can be easily reprogrammed and completely controlled by external attackers, and thus behave adversarially.    \item Small local datasets versus high model complexity: While the total number of data samples over all data owners may be large, each individual owner may keep only a small amount of data, which by itself is insufficient for learning a complex model.  \item Communication constraints:  Data transmission between the recruited devices and the cloud may suffer from high latency and low-throughout.	score:353
What data science and machine learning career opportunities are there at Google?	2666468}. This framework has been implemented in   practical systems such as Google's {\em Federated Learning} \cite{konevcny2015federated,federatedlearningblog}, wherein Google tries to learn a model with the training data kept confidential on the users' mobile devices.  We refer to this new learning framework as {\em learning with external workers}.  In contrast to the traditional learning framework under which models are trained within data-centers, in {\em learning with external workers} the learner faces    serious {\em security} risk: (1) some external workers may be highly unreliable or even be malicious (hacked by the system adversary);   (2) the learner lacks enough administrative power over those external workers.	2666468}. This framework has been implemented in   practical systems such as Google's {\em Federated Learning} \cite{konevcny2015federated,federatedlearningblog}, wherein Google tries to learn a model with the training data kept confidential on the users' mobile devices.  We refer to this new learning framework as {\em learning with external workers}.  In contrast to the traditional learning framework under which models are trained within data-centers, in {\em learning with external workers} the learner faces    serious {\em security} risk: (1) some external workers may be highly unreliable or even be malicious (hacked by the system adversary);   (2) the learner lacks enough administrative power over those external workers.	score:358
What data science and machine learning career opportunities are there at Google?	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	 However, it is challenging to perform distributed machine learning on resource-constrained MEC systems.   In this paper, we address the problem of how to efficiently utilize the limited computation and communication resources at the edge for the optimal learning performance.  We consider a typical edge computing architecture where edge nodes are interconnected with the remote cloud via network elements, such as gateways and routers, as illustrated in Fig. ~\ref{fig:architecture}. The raw data is collected and stored at multiple edge nodes, and a machine learning model is trained from the distributed data \emph{without} sending the raw data from the nodes to a central place. This variant of distributed machine learning (model training) from a federation of edge nodes is known as \emph{federated learning}~\cite{GoogleFederatedLearningBlog,mcmahan2016communication,WirelessNetworkIntelligence}.	score:361
Which Linux distribution do people usually use for machine learning?	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	score:459
Which Linux distribution do people usually use for machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:467
Which Linux distribution do people usually use for machine learning?	  	\paragraph{Background.} The tremendous recent progress in machine learning can be explained in part through the availability of extremely vast amounts of data, but also through improved computational support for executing machine learning tasks at scale. Perhaps surprisingly, some of the main algorithms driving this progress are have been known in some form or another for a very long time.  One such tool is the classic stochastic gradient descent (SGD) optimization algorithm~\cite{RM51}, introduced by Robbins and Munro in the 1950s, variants of which are currently the tool of choice for optimization in a variety of settings, such as image classification and speech recognition via neural networks, but also in fundamental data processing tools such as regression.	  	\paragraph{Background.} The tremendous recent progress in machine learning can be explained in part through the availability of extremely vast amounts of data, but also through improved computational support for executing machine learning tasks at scale. Perhaps surprisingly, some of the main algorithms driving this progress are have been known in some form or another for a very long time.  One such tool is the classic stochastic gradient descent (SGD) optimization algorithm~\cite{RM51}, introduced by Robbins and Munro in the 1950s, variants of which are currently the tool of choice for optimization in a variety of settings, such as image classification and speech recognition via neural networks, but also in fundamental data processing tools such as regression.	score:491
Which Linux distribution do people usually use for machine learning?	  In terms of training computers to perform similar tasks, deep neural networks have demonstrated superior performance among machine learning models (\cite{lecun98, alexnet, resnet}). However, these networks have been trained dramatically differently from a learning child, requiring labels for every training example, following a purely supervised training scheme.  Neural networks are defined by huge amounts of parameters to be optimized. Therefore, a plethora of labeled training data is required, which might be costly and time consuming to obtain.  It is desirable to train machine learning models without labels (unsupervisedly) or with only some fraction of the data labeled (semi-supervisedly).  Recently, efforts have been made to train neural networks in an unsupervised or semi-supervised manner yielding promising results.	  In terms of training computers to perform similar tasks, deep neural networks have demonstrated superior performance among machine learning models (\cite{lecun98, alexnet, resnet}). However, these networks have been trained dramatically differently from a learning child, requiring labels for every training example, following a purely supervised training scheme.  Neural networks are defined by huge amounts of parameters to be optimized. Therefore, a plethora of labeled training data is required, which might be costly and time consuming to obtain.  It is desirable to train machine learning models without labels (unsupervisedly) or with only some fraction of the data labeled (semi-supervisedly).  Recently, efforts have been made to train neural networks in an unsupervised or semi-supervised manner yielding promising results.	score:492
Which Linux distribution do people usually use for machine learning?	 	Machine learning has a long history of being applied to networks for multifarious tasks, such as network classification~\cite{snbgge08}, prediction of protein binding~\cite{adwf15}, etc. Thanks to the advancement of technologies such as the Internet and database management systems, the amount of data that are available for machine learning algorithms have been growing tremendously over the past decade.  Among these datasets, a huge fraction can be modeled as networks, such as web networks, brain networks, citation networks, street networks, etc.~\cite{xskk18}. Therefore, improving machine learning algorithms on networks has become even more important.  However, due to the discrete and sparse nature of networks, it is often difficult to apply machine learning directly to them.  To resolve this issue, one major school of thought to approach networks using machine learning is via network embeddings~\cite{gf18}. A network embedding consists of a real number-valued Euclidean vector representation for each node in the network. These vectors can then be fed into machine learning algorithms for various classification and regression tasks.	 They play essential roles in modern software package management systems. For example, in most cases, when a user installs a software package on a modern GNU/Linux distribution via its software package management system (which is the most common way to install software), an SPDN is queried so that the software package management system installs necessary dependencies of that software package.  Unfortunately, to the best of our knowledge, SPDNs have not hitherto been studied in the context of network embeddings (despite that they have been studied in the context of data mining for other purposes, e.g., \cite{smbbgmpa17,dmc18})---the huge literature of network embeddings have been largely focusing on social networks, citation networks, and web networks (e.	score:492
How do I start learning machine learning and data science using python?	      \tiny   \section{Keywords:} machine learning, statistical learning, neuroimaging, scikit-learn, Python 	Interest in applying statistical machine learning to neuroimaging data analysis is growing. Neuroscientists use it as a powerful, albeit complex, tool for statistical inference. The tools are developed by computer scientists who may lack a deep understanding of the neuroscience questions.  This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.  We discuss the use of the scikit-learn toolkit as it is a reference machine learning tool and has and a variety of algorithms that is matched by few packages, but also because it is implemented in Python, and thus dovetails nicely in the rich neuroimaging Python ecosystem.  This paper explores a few applications of statistical learning to resolve common neuroimaging needs, detailing the corresponding code, the choice of the methods, and the underlying assumptions.  We discuss not only prediction scores, but also the interpretability of the results, which leads us to explore the internal model of various methods.  Importantly, the GitHub repository of the paper\footnote{\url{http://www.github.com/AlexandreAbraham/frontiers2013}} provides complete scripts to generate figures.  The scope of this paper is not to present a neuroimaging-specific library, but rather code patterns related to scikit-learn.	      \tiny   \section{Keywords:} machine learning, statistical learning, neuroimaging, scikit-learn, Python 	Interest in applying statistical machine learning to neuroimaging data analysis is growing. Neuroscientists use it as a powerful, albeit complex, tool for statistical inference. The tools are developed by computer scientists who may lack a deep understanding of the neuroscience questions.  This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.  We discuss the use of the scikit-learn toolkit as it is a reference machine learning tool and has and a variety of algorithms that is matched by few packages, but also because it is implemented in Python, and thus dovetails nicely in the rich neuroimaging Python ecosystem.  This paper explores a few applications of statistical learning to resolve common neuroimaging needs, detailing the corresponding code, the choice of the methods, and the underlying assumptions.  We discuss not only prediction scores, but also the interpretability of the results, which leads us to explore the internal model of various methods.  Importantly, the GitHub repository of the paper\footnote{\url{http://www.github.com/AlexandreAbraham/frontiers2013}} provides complete scripts to generate figures.  The scope of this paper is not to present a neuroimaging-specific library, but rather code patterns related to scikit-learn.	score:439
How do I start learning machine learning and data science using python?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	score:440
How do I start learning machine learning and data science using python?	    In this paper we present \pydci, a new implementation of the DCI method written in Python and built on top of the \texttt{SciPy} stack and \texttt{scikit-learn} toolkit. Python has become the preferred programming language for computer scientists. In the fields of machine learning and data mining its use has also been promoted by the appearance of Python-based environments such as \texttt{SciPy} and \texttt{scikit-learn}, whose potential and ease of use have attracted the interest of practitioners.  Our reimplementation is thus in line with these trends.  With respect to \jadci, \pydci\ introduces a few modifications in the way DCI is implemented that, although subtle, bring about a significant improvement in the effectiveness of the method.  The rest of this paper is structured as follows. In Section \ref{sec:changes} we describe the main modifications to DCI that our new implementation introduces.	    In this paper we present \pydci, a new implementation of the DCI method written in Python and built on top of the \texttt{SciPy} stack and \texttt{scikit-learn} toolkit. Python has become the preferred programming language for computer scientists. In the fields of machine learning and data mining its use has also been promoted by the appearance of Python-based environments such as \texttt{SciPy} and \texttt{scikit-learn}, whose potential and ease of use have attracted the interest of practitioners.  Our reimplementation is thus in line with these trends.  With respect to \jadci, \pydci\ introduces a few modifications in the way DCI is implemented that, although subtle, bring about a significant improvement in the effectiveness of the method.  The rest of this paper is structured as follows. In Section \ref{sec:changes} we describe the main modifications to DCI that our new implementation introduces.	score:446
How do I start learning machine learning and data science using python?	 Also, Python is particularly common in this domain since it allows to easily express mathematical computations, such as matrix multiplication~\cite[p. 13]{harrington-why-python}.  Aside from monetary gains from Machine Learning, there are also approaches to solve the mystery of intelligence: Google DeepMind's website explains the mission as to \enquote{solve intelligence} and \enquote{use it to make the world a better place}.  This is to be achieved by teaching the machine to make decisions as humans do - in any situation whatsoever~\cite{deepmind-wired}. In mid-March, Google DeepMind's AlphaGo program beat the world champion of Go, Lee Se-dol, 4-1 in a series of 5 games. For the move selection~\cite{distbelief-alphago-moves}, they initially used DistBelief~\cite{distbelief}, Google's initial system for training and inference which already exhibited some of TensorFlow's aspects like scalability and distributed computations~\cite{tensorflow2015-whitepaper}.	 Also, Python is particularly common in this domain since it allows to easily express mathematical computations, such as matrix multiplication~\cite[p. 13]{harrington-why-python}.  Aside from monetary gains from Machine Learning, there are also approaches to solve the mystery of intelligence: Google DeepMind's website explains the mission as to \enquote{solve intelligence} and \enquote{use it to make the world a better place}.  This is to be achieved by teaching the machine to make decisions as humans do - in any situation whatsoever~\cite{deepmind-wired}. In mid-March, Google DeepMind's AlphaGo program beat the world champion of Go, Lee Se-dol, 4-1 in a series of 5 games. For the move selection~\cite{distbelief-alphago-moves}, they initially used DistBelief~\cite{distbelief}, Google's initial system for training and inference which already exhibited some of TensorFlow's aspects like scalability and distributed computations~\cite{tensorflow2015-whitepaper}.	score:450
How do I start learning machine learning and data science using python?	 In the context of deep learning, declarative programming is useful in specifying the computation structure in neural network configurations, while imperative programming are more natural for  parameter updates and interactive debugging. We also took the effort to embed into multiple host languages, including C++, Python, R, Go and Julia.  Despite the support of multiple languages and combination of different programming paradigm, we are able to fuse the execution to the same backend engine.  The engine tracks data dependencies across computation graphs and imperative operations, and schedules them efficiently jointly. We aggressively reduce memory footprint, performing in-place update and memory space reuse whenever possible. Finally, we designed a compact communication API so that a MXNet program runs on multiple machines with little change.	 In the context of deep learning, declarative programming is useful in specifying the computation structure in neural network configurations, while imperative programming are more natural for  parameter updates and interactive debugging. We also took the effort to embed into multiple host languages, including C++, Python, R, Go and Julia.  Despite the support of multiple languages and combination of different programming paradigm, we are able to fuse the execution to the same backend engine.  The engine tracks data dependencies across computation graphs and imperative operations, and schedules them efficiently jointly. We aggressively reduce memory footprint, performing in-place update and memory space reuse whenever possible. Finally, we designed a compact communication API so that a MXNet program runs on multiple machines with little change.	score:452
Where can I find projects on machine learning projects using MATLAB?	 However, these methods require a line search step, which can be problematic when the cost function cannot be efficiently evaluated, a typical scenario where SGD is used. Still, they are applied with successes to machine learning problems like neural network training, and are made available in standard toolboxes, e.g., the Matlab neural network toolbox \cite{matlab}.  The highly specialized Hessian-free neural network training methods in \cite{Martens2012_hessian_free} represent the state of the art in this direction. There are attempts to adapt the deterministic quasi-Newton methods to stochastic optimization \cite{stochastic_newton1,convex_quasi_newton,stochastic_newton2}. However, due to the existence of gradient noise and infeasibility of line search in stochastic optimization, the resultant methods either impose strong restrictions such as convexity on the target problems, or are significantly more complicated than SGD.	 However, these methods require a line search step, which can be problematic when the cost function cannot be efficiently evaluated, a typical scenario where SGD is used. Still, they are applied with successes to machine learning problems like neural network training, and are made available in standard toolboxes, e.g., the Matlab neural network toolbox \cite{matlab}.  The highly specialized Hessian-free neural network training methods in \cite{Martens2012_hessian_free} represent the state of the art in this direction. There are attempts to adapt the deterministic quasi-Newton methods to stochastic optimization \cite{stochastic_newton1,convex_quasi_newton,stochastic_newton2}. However, due to the existence of gradient noise and infeasibility of line search in stochastic optimization, the resultant methods either impose strong restrictions such as convexity on the target problems, or are significantly more complicated than SGD.	score:400
Where can I find projects on machine learning projects using MATLAB?	  Many machine learning algorithms have been analyzed by smoothed analysis, such as k-means clustering \citep{arthur2009k} and Support Vector Machines \citep{Blum:2002:SAP:545381.545499,spielman2009smoothed}, however, to the best of our knowledge, there is no previous research on using machine learning algorithms to predict the SC of sorting algorithms.   Due to recent modular smoothed analysis results, we can compute the SC of Quicksort exactly, for medium size inputs (e.g., $N \leq 3000$). For other sorting algorithms, currently the SC can only be computed using its original definition, and thus, due to computational requirements, only for small input sizes (e.g., $N \leq 100$), limiting our understanding of the general SC behaviour.	  Many machine learning algorithms have been analyzed by smoothed analysis, such as k-means clustering \citep{arthur2009k} and Support Vector Machines \citep{Blum:2002:SAP:545381.545499,spielman2009smoothed}, however, to the best of our knowledge, there is no previous research on using machine learning algorithms to predict the SC of sorting algorithms.   Due to recent modular smoothed analysis results, we can compute the SC of Quicksort exactly, for medium size inputs (e.g., $N \leq 3000$). For other sorting algorithms, currently the SC can only be computed using its original definition, and thus, due to computational requirements, only for small input sizes (e.g., $N \leq 100$), limiting our understanding of the general SC behaviour.	score:400
Where can I find projects on machine learning projects using MATLAB?	   Many machine learning tasks, such as learning with invariance and policy evaluation in reinforcement learning, can be characterized as problems of \emph{learning from conditional distributions}. In such problems, each sample $x$ itself is associated with a conditional distribution $p(z|x)$ represented by samples $\{z_i\}_{i=1}^M$, and the goal is to learn a function $f$ that links these conditional distributions to target values $y$.  These learning problems become very challenging when we only have limited samples or in the extreme case only one sample from each conditional distribution. Commonly used approaches either assume that $z$ is independent of $x$, or require an overwhelmingly large samples from each conditional distribution.     To address these challenges, we propose a novel approach which employs a new \emph{min-max reformulation} of the learning from conditional distribution problem.	   Many machine learning tasks, such as learning with invariance and policy evaluation in reinforcement learning, can be characterized as problems of \emph{learning from conditional distributions}. In such problems, each sample $x$ itself is associated with a conditional distribution $p(z|x)$ represented by samples $\{z_i\}_{i=1}^M$, and the goal is to learn a function $f$ that links these conditional distributions to target values $y$.  These learning problems become very challenging when we only have limited samples or in the extreme case only one sample from each conditional distribution. Commonly used approaches either assume that $z$ is independent of $x$, or require an overwhelmingly large samples from each conditional distribution.     To address these challenges, we propose a novel approach which employs a new \emph{min-max reformulation} of the learning from conditional distribution problem.	score:401
Where can I find projects on machine learning projects using MATLAB?	 Similar ideas have recently been explored in the context of convex optimization \cite{donti2017task}, but to our knowledge ours is the first attempt to train machine learning systems for performance on \emph{combinatorial} decision-making problems. Combinatorial settings raise new technical challenges because the optimization problem is discrete. However, machine learning systems (e. g., deep neural networks) are often trained via gradient descent.     Our first contribution is a general framework for training machine learning models via their performance on combinatorial problems. The starting point is to relax the combinatorial problem to a continuous one. Then, we analytically differentiate the optimal solution to the continuous problem as a function of the model's predictions.	 Similar ideas have recently been explored in the context of convex optimization \cite{donti2017task}, but to our knowledge ours is the first attempt to train machine learning systems for performance on \emph{combinatorial} decision-making problems. Combinatorial settings raise new technical challenges because the optimization problem is discrete. However, machine learning systems (e. g., deep neural networks) are often trained via gradient descent.     Our first contribution is a general framework for training machine learning models via their performance on combinatorial problems. The starting point is to relax the combinatorial problem to a continuous one. Then, we analytically differentiate the optimal solution to the continuous problem as a function of the model's predictions.	score:410
Where can I find projects on machine learning projects using MATLAB?	   Currently, many organizations or institutes adopt machine learning models trained on historical data to automatically make decisions, including hiring, lending and policing \cite{Joseph2016Fairness}. However, many studies have shown that machine learning models have biased performance against the \textit{protected group} \cite{Beutel2017Data,Binns2017Fairness,Bolukbasi2016Man}.  In principle, if a dataset has discrimination against the protected group, the predictive model simply trained on the dataset will incur discrimination.   Many approaches aim to mitigate discrimination from historical datasets. A general requirement of modifying datasets is to preserve the data utility while removing the discrimination. Some methods mainly modify the labels of the dataset \cite{Zhang2017Causal,Kamiran2009Classifying}.	   Currently, many organizations or institutes adopt machine learning models trained on historical data to automatically make decisions, including hiring, lending and policing \cite{Joseph2016Fairness}. However, many studies have shown that machine learning models have biased performance against the \textit{protected group} \cite{Beutel2017Data,Binns2017Fairness,Bolukbasi2016Man}.  In principle, if a dataset has discrimination against the protected group, the predictive model simply trained on the dataset will incur discrimination.   Many approaches aim to mitigate discrimination from historical datasets. A general requirement of modifying datasets is to preserve the data utility while removing the discrimination. Some methods mainly modify the labels of the dataset \cite{Zhang2017Causal,Kamiran2009Classifying}.	score:414
What is the batch size in the Allen Jaipur Centre for IITJEE?	   Due to the intractability of the likelihood function, RBMs are usually learned using the contrastive divergence~(CD) algorithm~\citep{hinton2002training, tieleman2008PCD},  which approximates the gradient of the likelihood using a Gibbs sampler.     One practical problem when using a RBM is that we need to decide the size of the hidden layer (number of hidden units) before performing learning, and it can be challenging to decide what is the optimal size.   One simple heuristic is to search the `best'' number of hidden units using cross validation or testing likelihood within a pre-defined candidate set.  Unfortunately, this is extremely time consuming; it involves running a full training algorithm (e.g., CD) for each possible size, and thus we can only search over a relatively small set of sizes using this approach.	   Due to the intractability of the likelihood function, RBMs are usually learned using the contrastive divergence~(CD) algorithm~\citep{hinton2002training, tieleman2008PCD},  which approximates the gradient of the likelihood using a Gibbs sampler.     One practical problem when using a RBM is that we need to decide the size of the hidden layer (number of hidden units) before performing learning, and it can be challenging to decide what is the optimal size.   One simple heuristic is to search the `best'' number of hidden units using cross validation or testing likelihood within a pre-defined candidate set.  Unfortunately, this is extremely time consuming; it involves running a full training algorithm (e.g., CD) for each possible size, and thus we can only search over a relatively small set of sizes using this approach.	score:385
What is the batch size in the Allen Jaipur Centre for IITJEE?	 The idea is to arrange sufficient distance between the subsets. Consider composition with an outer Reed-Solomon (RS) code of rate $1-2\delta$ near one, for an overall rate $(1\!-\!2\delta)R$.  The alphabet of the RS code  is taken to be of size $B$. Interpret its codewords  as providing the sequence of labels $j_1,j_2,\ldots,j_L$ of the terms selected from the sections.  The RS codelength $L$ is taken to be either $B-1$ or $B$ using a standard  extension.  RS code properties as in \cite{MacWilliamsSloane1977} guarantee correction of any fraction of section mistakes less than $\delta$.  For advocacy of code concatenation  see \cite{Forney1966}.   As a consequence of our result for the inner code, the composite code makes no mistakes, except in an event inheriting the exponentially small probability in $L/(\log B)^2$.	 The idea is to arrange sufficient distance between the subsets. Consider composition with an outer Reed-Solomon (RS) code of rate $1-2\delta$ near one, for an overall rate $(1\!-\!2\delta)R$.  The alphabet of the RS code  is taken to be of size $B$. Interpret its codewords  as providing the sequence of labels $j_1,j_2,\ldots,j_L$ of the terms selected from the sections.  The RS codelength $L$ is taken to be either $B-1$ or $B$ using a standard  extension.  RS code properties as in \cite{MacWilliamsSloane1977} guarantee correction of any fraction of section mistakes less than $\delta$.  For advocacy of code concatenation  see \cite{Forney1966}.   As a consequence of our result for the inner code, the composite code makes no mistakes, except in an event inheriting the exponentially small probability in $L/(\log B)^2$.	score:388
What is the batch size in the Allen Jaipur Centre for IITJEE?	 In contrast, we directly approximates $\{\dist{p}{\cdot}\vert p \in P\}$, hence the size of the coreset is the product of $f_1(d)$ (which accounts for the shattering dimension of $\{\dist{p}{\cdot} \vert p \in P\}$) and $f_2(j,k)$ (which accounts for the total sensitivity).  \end{comment}  {\bf \noindent Organization of this paper:} In this article, we focus on the construction that establishes small total sensitivity for various shape fitting problems, and the size of the resulting coreset.  For clarity, we omit the description of algorithms for computing such bounds on sensitivity. Efficient algorithms result from the construction using a methodology that is now well-understood. Also because the weights for points in the coreset are nonnegative, the coreset lend itself to streaming settings, where points arrive one by one as $p_1,p_2,\cdots$~\cite{Har-Peled:2004:CKK:1007352.	     The sizes of the coresets in this paper are somewhat larger than the size of coresets in \cite{DBLP:conf/stoc/FeldmanL11}. Roughly speaking, the size of the coreset in \cite{DBLP:conf/stoc/FeldmanL11} is $f_1(d)+f_2(j,k)$, where $f_1(d)$ (respectively $f_2(j,k)$) is a function depending only on $d$ (respectively $j$ and $k$) for the $(j,k)$-projective clustering problem, while the coreset size in our paper is $f_1(d)\cdot f_2(j,k)$.	score:391
What is the batch size in the Allen Jaipur Centre for IITJEE?	 To the best of our knowledge no lowerbounds scaling exponentially with the dimension are known for analog deep neural networks of depths more than $2$. ~\\ \\ In what follows, the {\em depth} of a circuit will be the length of the longest path from the output node to an input variable, and the {\em size} of a circuit will be the total number of gates in the circuit.	 To the best of our knowledge no lowerbounds scaling exponentially with the dimension are known for analog deep neural networks of depths more than $2$. ~\\ \\ In what follows, the {\em depth} of a circuit will be the length of the longest path from the output node to an input variable, and the {\em size} of a circuit will be the total number of gates in the circuit.	score:393
What is the batch size in the Allen Jaipur Centre for IITJEE?	 However, as shown in \mysec{universal}, higher orders of interactions are needed for universal consistency, i.e., to adapt to the potential high complexity of the interactions between the relevant variables; we need to potentially allow $2^p$ of them for $p$ variables (for all possible subsets of the $p$ variables). Theoretical results suggest that with appropriate assumptions,  sparse methods such as greedy methods and methods based on the $\ell_1$-norm would be able to deal correctly with $2^p$ features if $p$ is of the order of the number of observations $n$~\citep{martin,cs2,zhang_greedy}.	 However, as shown in \mysec{universal}, higher orders of interactions are needed for universal consistency, i.e., to adapt to the potential high complexity of the interactions between the relevant variables; we need to potentially allow $2^p$ of them for $p$ variables (for all possible subsets of the $p$ variables). Theoretical results suggest that with appropriate assumptions,  sparse methods such as greedy methods and methods based on the $\ell_1$-norm would be able to deal correctly with $2^p$ features if $p$ is of the order of the number of observations $n$~\citep{martin,cs2,zhang_greedy}.	score:393
How can machine learning help the humanity in the future?	 \end{itemize}  Our work is intended as a preliminary step towards building probabilistic kernel machines that encapsulate human-like support  and inductive biases. Since state of the art machine learning  methods perform conspicuously poorly on a number of extrapolation  problems which would be easy for humans \citep{wilson2014thesis},  such efforts have the potential to help automate machine learning  and improve performance on a wide range of tasks -- including settings which are difficult for humans to process (e.	 \end{itemize}  Our work is intended as a preliminary step towards building probabilistic kernel machines that encapsulate human-like support  and inductive biases. Since state of the art machine learning  methods perform conspicuously poorly on a number of extrapolation  problems which would be easy for humans \citep{wilson2014thesis},  such efforts have the potential to help automate machine learning  and improve performance on a wide range of tasks -- including settings which are difficult for humans to process (e.	score:458
How can machine learning help the humanity in the future?	 Machine Learning has become very famous in today's world  which assist to identifying the patterns from the raw data. Technological advancement has led to substantial improvement in Machine Learning which, thus helping to improve prediction. Current Machine Learning models are based on Classical Theory, which can be replaced by Quantum Theory to improve the effectiveness of the model.    In the previous work, we developed binary classifier inspired by Quantum Detection Theory. In this extended abstract, our main goal is to develop multi class classifier. We generally use the terminology multinomial classification or multi-class classification when we have a classification problem for classifying observations or instances into one of three or more classes.	 Machine Learning has become very famous in today's world  which assist to identifying the patterns from the raw data. Technological advancement has led to substantial improvement in Machine Learning which, thus helping to improve prediction. Current Machine Learning models are based on Classical Theory, which can be replaced by Quantum Theory to improve the effectiveness of the model.    In the previous work, we developed binary classifier inspired by Quantum Detection Theory. In this extended abstract, our main goal is to develop multi class classifier. We generally use the terminology multinomial classification or multi-class classification when we have a classification problem for classifying observations or instances into one of three or more classes.	score:480
How can machine learning help the humanity in the future?	 Machines, not humans, are the world's dominant knowledge accumulators but humans remain the dominant decision makers. Interpreting and disseminating the knowledge accumulated by machines requires expertise, time, and is prone to failure. The problem of how best to convey accumulated knowledge from computers to humans is a critical bottleneck in the broader application of machine learning.  We propose an approach based on human teaching where the problem is formalized as selecting a small subset of the data that will, with high probability, lead the human user to the correct inference. This approach, though successful for modeling human learning in simple laboratory experiments, has failed to achieve broader relevance due to challenges in formulating general and scalable algorithms.	 This paradigm creates a bottleneck where the knowledge accumulated by machines depends on highly trained human experts to interpret, and to conveying to humans. This remains a barrier to the broader usefulness of machine learning. While machines can communicate perfectly among themselves by exchanging bits, humans communicate with data---they teach.  The purposeful selection of data plays a featured role in theories of cognition \citep{sperber1986relevance}, cognitive development \citep{Gergely2007}, and culture \citep{Tomasello2005}. In each of these cases, teaching is conceived of as purposeful, rather than random, selection of small set of examples, with the goal of facilitating accurate inferences about a body of knowledge.	score:485
How can machine learning help the humanity in the future?	 It relies on coupling the training of multiple models and combining their predictions into a single output. This coupling is performed by encouraging the models to agree with each other on their predictions, while training. We show how the proposed framework is inspired by human learning, in contrast with other approaches commonly used in machine learning.  Finally, we provide experimental results which exhibit how our framework successfully manages to outperform cross-validation and other ensemble methods that do not couple the training of the models.  \begin{figure}[t!]  \centering      \includegraphics[width=0.5\textwidth, trim=9.2cm 2.5cm 1.5cm 6cm, clip]{diagram.pdf}   \caption{Illustration of the agreement-based learning framework that we propose.	 It relies on coupling the training of multiple models and combining their predictions into a single output. This coupling is performed by encouraging the models to agree with each other on their predictions, while training. We show how the proposed framework is inspired by human learning, in contrast with other approaches commonly used in machine learning.  Finally, we provide experimental results which exhibit how our framework successfully manages to outperform cross-validation and other ensemble methods that do not couple the training of the models.  \begin{figure}[t!]  \centering      \includegraphics[width=0.5\textwidth, trim=9.2cm 2.5cm 1.5cm 6cm, clip]{diagram.pdf}   \caption{Illustration of the agreement-based learning framework that we propose.	score:487
How can machine learning help the humanity in the future?	    A major concern in our society about automatic decision systems  is their reliability: if decisions are made by a trained classifier  instead of a person, how can we be sure that the system works  reliably now, and that it will continue to do so in the future?   Formal verification techniques, as they are common for other  safety-critical software components, such as device drivers, are  still in their infancy for machine learning.  Instead, quality  control for trained system typically relies on extensive testing,  making use of data that 1) was not used during training, and 2)  reflects the expected situation at prediction time.   If a system works well on a sufficiently large amount of data  fulfilling 1) and 2), practical experience as well as machine  learning theory tell us that it will also work well in the future.	    A major concern in our society about automatic decision systems  is their reliability: if decisions are made by a trained classifier  instead of a person, how can we be sure that the system works  reliably now, and that it will continue to do so in the future?   Formal verification techniques, as they are common for other  safety-critical software components, such as device drivers, are  still in their infancy for machine learning.  Instead, quality  control for trained system typically relies on extensive testing,  making use of data that 1) was not used during training, and 2)  reflects the expected situation at prediction time.   If a system works well on a sufficiently large amount of data  fulfilling 1) and 2), practical experience as well as machine  learning theory tell us that it will also work well in the future.	score:492
What are the best books and resources for learning machine learning as a developer ?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:337
What are the best books and resources for learning machine learning as a developer ?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:360
What are the best books and resources for learning machine learning as a developer ?	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	 However, researchers often have to rely on their intuition and expertise, and spend a significant amount of time fine-tuning their models. This is especially true for {\em deep learning} which is arguably the most popular area of research in machine learning, at present. We propose a new learning framework, {\em agreement-based learning}, which prevents many of the pitfalls associated with model selection.  It relies on coupling the training of multiple models and combining their predictions into a single output. This coupling is performed by encouraging the models to agree with each other on their predictions, while training. We show how the proposed framework is inspired by human learning, in contrast with other approaches commonly used in machine learning.	score:390
What are the best books and resources for learning machine learning as a developer ?	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	score:394
What are the best books and resources for learning machine learning as a developer ?	 provide users a rationale for why a prediction was made using textual and visual components of the data, and/or producing counter-factual knowledge of what would happen were the components different.  \cut{  Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field, with many of the state of the art models being functionally black boxes.    Humans are always the ones who train and deploy machine learning models, and very often are the final consumers of its predictions.   Among other indicators such as accuracy and example predictions, a user's trust is directly impacted by how much they understand the model's behavior, as opposed to seeing it as a black box.     Among other indicators such as accuracy and example predictions, a user's trust is directly impacted by how much they understand the model's behavior, as opposed to seeing it as a black box.	  	As machine learning becomes a crucial component of an ever-growing number of user-facing applications, \emph{interpretable machine learning} has become an increasingly important area of research for a number of reasons.  First, as humans are the ones who train, deploy, and often use the predictions of machine learning models in the real world, it is of utmost importance for them to be able to trust the model.    Apart from indicators such as accuracy on sample instances, a user's trust is directly impacted by how much they can understand and predict the model's behavior, as opposed to treating it as a black box. Second, a system designer who understands why their model is making predictions is certainly better equipped to improve it by means of feature engineering, parameter tuning, or even by replacing the model with a different one.	score:398
What are the good books and resources to learn machine learning?	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.    \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:363
What are the good books and resources to learn machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:403
What are the good books and resources to learn machine learning?	  An intuitive description of the paper's results is this: From prior and corresponding choice of learning machinery (Section \ref{sec:class-learn-probl}), we construct statements about the \emph{dynamics of the learning process} (Section \ref{sec:optim-contr-learn}). The learning machine itself provides a probabilistic description of the dynamics of the physical system.  Combining both dynamics yields a \emph{joint} system, which we aim to control optimally. Doing so amounts to simultaneously controlling exploration (controlling the learning system) and exploitation (controlling the physical system).  Because large parts of the analysis rely on concepts from optimal control theory, this paper will use notation from that field.	  An intuitive description of the paper's results is this: From prior and corresponding choice of learning machinery (Section \ref{sec:class-learn-probl}), we construct statements about the \emph{dynamics of the learning process} (Section \ref{sec:optim-contr-learn}). The learning machine itself provides a probabilistic description of the dynamics of the physical system.  Combining both dynamics yields a \emph{joint} system, which we aim to control optimally. Doing so amounts to simultaneously controlling exploration (controlling the learning system) and exploitation (controlling the physical system).  Because large parts of the analysis rely on concepts from optimal control theory, this paper will use notation from that field.	score:404
What are the good books and resources to learn machine learning?	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	score:409
What are the good books and resources to learn machine learning?	 Machine Learning can allow us to make proper decisions from this vast amount of data by allowing the computer to learn statistically from the large data set and make predictions for new instances based on what has been learnt previously. Also, machines can be trained to learn by feeding it with examples where it then makes decisions based on the carefully selected examples provided.   To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.  The machine learns from the inputs (examples) and makes predictions based on the examples provided.} \item{\textit{Unsupervised Learning:} In unsupervised learning, the machine is left to learn from the data provided to it in order to discover patterns that can be used to make predictions eventually.} \item{\textit{Reinforcement Learning:} Reinforcement learning, for example, a computer learning to play a game, is the process whereby the computer learns in a dynamic environment to perform a certain task without explicitly being told if it is close to achieving the goal.	  To understand how machine learning systems work, we can classify them into three broad categories based on the nature of the learning system. These learning systems as described by \cite{Russell2009} are:\\  \begin{enumerate}  \item {\textit{Supervised Learning:} In supervised learning, the machine or computer is given a set of examples to learn from.	score:415
When is the syntel off campus drive for 2016 batch?	 We use a dataset of ten million ride requests from four major U.S. cities to show that the requests exhibit significant self-similarity. We then propose distributed online learning algorithms for the real-time vehicle placement problem and bound their expected performance under this observed self-similarity. 	In the past five years, ride-sharing platforms like Uber and Lyft have become a significant means of transportation, accounting for nearly two billion rides in 2016.   Given this popularity, ride-sharing companies have turned their attention towards the problem of optimizing rider experience. In particular, they have begun to examine ways to minimize rider wait times. Future vehicular technologies like autonomous cars can also benefit from algorithms to reduce wait times.  Reducing wait times to below two minutes is a challenging problem that requires real-time vehicle placement.	 We use a dataset of ten million ride requests from four major U.S. cities to show that the requests exhibit significant self-similarity. We then propose distributed online learning algorithms for the real-time vehicle placement problem and bound their expected performance under this observed self-similarity. 	In the past five years, ride-sharing platforms like Uber and Lyft have become a significant means of transportation, accounting for nearly two billion rides in 2016.   Given this popularity, ride-sharing companies have turned their attention towards the problem of optimizing rider experience. In particular, they have begun to examine ways to minimize rider wait times. Future vehicular technologies like autonomous cars can also benefit from algorithms to reduce wait times.  Reducing wait times to below two minutes is a challenging problem that requires real-time vehicle placement.	score:477
When is the syntel off campus drive for 2016 batch?	 Yet $\varepsilon$-greedy is a poor exploration strategy and for environments with sparse rewards it is quite ineffective (for example the Atari game `Montezuma's Revenge'): it just takes too long until the agent randomwalks into the first reward.   More sophisticated exploration strategies have been proposed: using information gain about the environment ~\citep{SGS:2011infogain,OLH:2013ksa,HCDSDA:2016explore} or pseudo-count~\citep{BSOSSM:2016explore}.  In practice, these exploration strategies are employed by adding an exploration bonus (`intrinsic motivation') to the reward signal~\citep{Schmidhuber:2010everything}. While the methods above require the agent to have a model of its environment and formalize the strategy `explore by going to where the model has high uncertainty,' there are also model-free strategies like the automatic discovery of options proposed by \citet{MB:2016options}.	 Yet $\varepsilon$-greedy is a poor exploration strategy and for environments with sparse rewards it is quite ineffective (for example the Atari game `Montezuma's Revenge'): it just takes too long until the agent randomwalks into the first reward.   More sophisticated exploration strategies have been proposed: using information gain about the environment ~\citep{SGS:2011infogain,OLH:2013ksa,HCDSDA:2016explore} or pseudo-count~\citep{BSOSSM:2016explore}.  In practice, these exploration strategies are employed by adding an exploration bonus (`intrinsic motivation') to the reward signal~\citep{Schmidhuber:2010everything}. While the methods above require the agent to have a model of its environment and formalize the strategy `explore by going to where the model has high uncertainty,' there are also model-free strategies like the automatic discovery of options proposed by \citet{MB:2016options}.	score:494
When is the syntel off campus drive for 2016 batch?	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.  For example, the next destination people want to go to often depends on what other places they have gone to and how long they have stayed in each place in the past. When the next clinical visit \citep{Choi16} will occur for a patient depends on the time of the most recent visits and the respective duration between them. Therefore, the temporal information of events and the interval between them are crucial to the event sequence prediction in general.  However, how to effectively use and represent time in sequence prediction still largely remains under explored.  A natural and straightforward solution is to bring time as an additional input into an existing sequence model (e.g., recurrent neural networks). However, it is notoriously challenging for recurrent neural networks to directly handle continuous input that has a wide value range, as what is shown in our experiments.	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.  For example, the next destination people want to go to often depends on what other places they have gone to and how long they have stayed in each place in the past. When the next clinical visit \citep{Choi16} will occur for a patient depends on the time of the most recent visits and the respective duration between them. Therefore, the temporal information of events and the interval between them are crucial to the event sequence prediction in general.  However, how to effectively use and represent time in sequence prediction still largely remains under explored.  A natural and straightforward solution is to bring time as an additional input into an existing sequence model (e.g., recurrent neural networks). However, it is notoriously challenging for recurrent neural networks to directly handle continuous input that has a wide value range, as what is shown in our experiments.	score:494
When is the syntel off campus drive for 2016 batch?	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	score:495
When is the syntel off campus drive for 2016 batch?	 For instance, the Internet streaming media company Netflix held the Netflix Prize competition in 2006, to find a better program to predict user preferences. Kaggle, an online platform, hosts regularly competitions since 2010.  These competitions are usually prediction problems. The participant is given the independent variable $X$, and then he is required to predict the dependent variable $Y$.  Usually, the host divides his data into three data sets: \emph{training}, \emph{validation} and \emph{test}. The training dataset is fully available: every participant (having a competition account) can download it and observe its $Y$ as well as its $X$. This allows them to build their models. The validation data set is partially available to participants: they can only observe its $X$.	 For instance, the Internet streaming media company Netflix held the Netflix Prize competition in 2006, to find a better program to predict user preferences. Kaggle, an online platform, hosts regularly competitions since 2010.  These competitions are usually prediction problems. The participant is given the independent variable $X$, and then he is required to predict the dependent variable $Y$.  Usually, the host divides his data into three data sets: \emph{training}, \emph{validation} and \emph{test}. The training dataset is fully available: every participant (having a competition account) can download it and observe its $Y$ as well as its $X$. This allows them to build their models. The validation data set is partially available to participants: they can only observe its $X$.	score:497
When is the Musigma off campus drive for 2016 batch?	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	score:476
When is the Musigma off campus drive for 2016 batch?	 We use a dataset of ten million ride requests from four major U.S. cities to show that the requests exhibit significant self-similarity. We then propose distributed online learning algorithms for the real-time vehicle placement problem and bound their expected performance under this observed self-similarity. 	In the past five years, ride-sharing platforms like Uber and Lyft have become a significant means of transportation, accounting for nearly two billion rides in 2016.   Given this popularity, ride-sharing companies have turned their attention towards the problem of optimizing rider experience. In particular, they have begun to examine ways to minimize rider wait times. Future vehicular technologies like autonomous cars can also benefit from algorithms to reduce wait times.  Reducing wait times to below two minutes is a challenging problem that requires real-time vehicle placement.	 We use a dataset of ten million ride requests from four major U.S. cities to show that the requests exhibit significant self-similarity. We then propose distributed online learning algorithms for the real-time vehicle placement problem and bound their expected performance under this observed self-similarity. 	In the past five years, ride-sharing platforms like Uber and Lyft have become a significant means of transportation, accounting for nearly two billion rides in 2016.   Given this popularity, ride-sharing companies have turned their attention towards the problem of optimizing rider experience. In particular, they have begun to examine ways to minimize rider wait times. Future vehicular technologies like autonomous cars can also benefit from algorithms to reduce wait times.  Reducing wait times to below two minutes is a challenging problem that requires real-time vehicle placement.	score:500
When is the Musigma off campus drive for 2016 batch?	 In particular, to generate skills representing the expertise of the candidates, our approach estimates expertise scores of each candidate on the skills he or she might have and then aggregates the scores across the candidates to determine the most representative ones. To generate company attribute, our approach exploits co-viewing relationships amongst companies to discover companies similar to input candidates' companies.  To rank the final results, we propose a ranking function taking both query and input candidates into account. As the query increasingly deviates from the input candidates, the ranking function gradually focuses more on the impact of the query than it of the input candidates.  The new talent search paradigm is expected to be the next generation of LinkedIn Talent Solution. As of this writing, the product is being launched to a few pilot customers. We plan to ramp it up to other external customers in early 2016.	 To rank the final results, we propose a ranking function taking both query and input candidates into account. As the query increasingly deviates from the input candidates, the ranking function gradually focuses more on the impact of the query than it of the input candidates.  The new talent search paradigm is expected to be the next generation of LinkedIn Talent Solution. As of this writing, the product is being launched to a few pilot customers. We plan to ramp it up to other external customers in early 2016.	score:504
When is the Musigma off campus drive for 2016 batch?	 We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target.	 We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target.	score:508
When is the Musigma off campus drive for 2016 batch?	 We shall however take the original ResNet as our starting point.    Shortly before the introduction of ResNets, batch normalization was introduced by  \cite{ioffe2015batch}.  Batch normalization was employed by \cite{he2016deep} in their definition of ResNets and indeed  many if not most CNN implementations today make use of batch normalization. The goal of batch normalization according to the authors was to accelerate the model training process by allowing for effective training with higher learning rates, as well as reducing the dependence on  parameter initialization.	 We shall however take the original ResNet as our starting point.    Shortly before the introduction of ResNets, batch normalization was introduced by  \cite{ioffe2015batch}.  Batch normalization was employed by \cite{he2016deep} in their definition of ResNets and indeed  many if not most CNN implementations today make use of batch normalization. The goal of batch normalization according to the authors was to accelerate the model training process by allowing for effective training with higher learning rates, as well as reducing the dependence on  parameter initialization.	score:510
How can machine learning be used in quantatative finance?	  Continuing the above example, suppose that in a particular protected community $S$, on average, individuals are financially disadvantaged and are unlikely to repay a loan.  A machine-learning algorithm that aims to optimize the institution's returns might devote resources to learning outside of $S$ -- where there is more opportunity for gains in utility -- and assign a fixed, low probability to all $i \in S$.   Such an algorithm would discriminate against the {\em qualified} members of $S$. If $S$ is an underrepresented subpopulation, this form of discrimination has the potential to amplify $S$'s underrepresentation by refusing to approve members that are capable of repaying the loan.    \subsection{Overview of our contributions} Focusing on such concerns, we investigate a notion we call \emph{multicalibration}.	  Continuing the above example, suppose that in a particular protected community $S$, on average, individuals are financially disadvantaged and are unlikely to repay a loan.  A machine-learning algorithm that aims to optimize the institution's returns might devote resources to learning outside of $S$ -- where there is more opportunity for gains in utility -- and assign a fixed, low probability to all $i \in S$.   Such an algorithm would discriminate against the {\em qualified} members of $S$. If $S$ is an underrepresented subpopulation, this form of discrimination has the potential to amplify $S$'s underrepresentation by refusing to approve members that are capable of repaying the loan.    \subsection{Overview of our contributions} Focusing on such concerns, we investigate a notion we call \emph{multicalibration}.	score:388
How can machine learning be used in quantatative finance?	 Inference in many other semiparametric contexts can be readily obtained. We demonstrate the effectiveness of deep learning with a Monte Carlo analysis and an empirical application to direct mail marketing. 	Statistical machine learning methods are being rapidly integrated into the social and medical sciences. Economics is no exception, and there has been a recent surge of research that applies and explores machine learning methods in the context of econometric modeling, particularly in ``big data'' settings.	 Inference in many other semiparametric contexts can be readily obtained. We demonstrate the effectiveness of deep learning with a Monte Carlo analysis and an empirical application to direct mail marketing. 	Statistical machine learning methods are being rapidly integrated into the social and medical sciences. Economics is no exception, and there has been a recent surge of research that applies and explores machine learning methods in the context of econometric modeling, particularly in ``big data'' settings.	score:393
How can machine learning be used in quantatative finance?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:393
How can machine learning be used in quantatative finance?	 Machine Learning has become very famous in today's world  which assist to identifying the patterns from the raw data. Technological advancement has led to substantial improvement in Machine Learning which, thus helping to improve prediction. Current Machine Learning models are based on Classical Theory, which can be replaced by Quantum Theory to improve the effectiveness of the model.    In the previous work, we developed binary classifier inspired by Quantum Detection Theory. In this extended abstract, our main goal is to develop multi class classifier. We generally use the terminology multinomial classification or multi-class classification when we have a classification problem for classifying observations or instances into one of three or more classes.	 Machine Learning has become very famous in today's world  which assist to identifying the patterns from the raw data. Technological advancement has led to substantial improvement in Machine Learning which, thus helping to improve prediction. Current Machine Learning models are based on Classical Theory, which can be replaced by Quantum Theory to improve the effectiveness of the model.    In the previous work, we developed binary classifier inspired by Quantum Detection Theory. In this extended abstract, our main goal is to develop multi class classifier. We generally use the terminology multinomial classification or multi-class classification when we have a classification problem for classifying observations or instances into one of three or more classes.	score:398
How can machine learning be used in quantatative finance?	  Quantum machine learning has received significant attention in recent years, and promising progress has been made in the development of quantum algorithms to speed up traditional machine learning tasks. In this work, however, we focus on investigating the information-theoretic upper bounds of sample complexity---how many training  samples are sufficient to predict the future behaviour of an unknown target function.  This kind of problem is, arguably, one of the most fundamental problems in statistical learning theory and the bounds for practical settings can be completely characterised by a simple measure of complexity.  Our main result in the paper is that, for learning an unknown quantum measurement, the upper bound, given by the fat-shattering dimension, is linearly proportional to the dimension of the underlying Hilbert space.	 `Quantum Computational Learning'  investigates how quantum machines can serve to accelerate the ML process to improve computational efficiency, or to reduce sample complexity by transforming classical training data into special sets of quantum states. In this line of research, both the input space $\mathcal{X}$ and output space $\mathcal{Y}$ are classical.  On the other hand, `Quantum Statistical Learning' studies the inference of unknown quantum states, operations, or hidden structure in the quantum system. We term the quantum version of classical statistical/stochastic model as `Quantum Stochastic Model'.} \label{table:survey}  \tikzstyle{block} =  [  rectangle  , draw    , text width=15.5em  , text centered    , rounded corners  , minimum height=3em  ]    \tikzstyle{block2} =  [  rectangle  , draw    , text width=10.	score:399
How can one start developing a chat bot using machine learning and natural language processing from scratch?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:316
How can one start developing a chat bot using machine learning and natural language processing from scratch?	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	score:326
How can one start developing a chat bot using machine learning and natural language processing from scratch?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:327
How can one start developing a chat bot using machine learning and natural language processing from scratch?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:327
How can one start developing a chat bot using machine learning and natural language processing from scratch?	  Instead of the conventional method of teaching computers how to play chess by giving them hardcoded rules, this project is an attempt to use machine learning to figure out how to play chess through self-play, and have them derive their own rules from the games.  Using multiple deep artificial neural networks trained in a temporal-difference reinforcement learning framework, we use machine learning to assist the engine in making decisions in a few places - \begin{itemize} \item Statically evaluating positions - estimating how good a position is without looking further \item Deciding which branches are most "interesting" in any given position, and should be searched further, as well as which branches to discard \item Ordering moves - determining which moves to search before others, which significantly affects efficiency of searches \end{itemize}  Using artificial neural networks as a substitute for "intuition", we hope to create a machine that can play chess more efficiently.	  Instead of the conventional method of teaching computers how to play chess by giving them hardcoded rules, this project is an attempt to use machine learning to figure out how to play chess through self-play, and have them derive their own rules from the games.  Using multiple deep artificial neural networks trained in a temporal-difference reinforcement learning framework, we use machine learning to assist the engine in making decisions in a few places - \begin{itemize} \item Statically evaluating positions - estimating how good a position is without looking further \item Deciding which branches are most "interesting" in any given position, and should be searched further, as well as which branches to discard \item Ordering moves - determining which moves to search before others, which significantly affects efficiency of searches \end{itemize}  Using artificial neural networks as a substitute for "intuition", we hope to create a machine that can play chess more efficiently.	score:330
Are we living in a matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	}   In this paper, we extend the theory of low-rank matrix completion to a collection of multiple and heterogeneous matrices. We first consider general matrix completion setting where we assume that for each matrix its  entries are sampled from natural exponential distributions~\citep{lehmCase98}. In this setting, we may have Gaussian distribution for continuous data; Bernoulli for binary data; Poisson for count-data, etc.  In a second part, we relax the assumption of exponential family distribution for the noise and we do not assume any specific model for the observations.  This approach is more popular and widely used in machine learning.  The proposed estimation procedure is based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix.	score:440
Are we living in a matrix?	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	 The rank in our algorithm is estimated as the number of negative eigenvalues of an associated Bethe Hessian matrix \cite{saade2014spectral,saade2015spectral}, and the corresponding eigenvectors are used as an initial condition for the local optimization of a cost function commonly considered in matrix completion (see e.g. \cite{kmo10}). In particular, in the random matrix setting, we show that MaCBetH detects the rank of a large $n\times m$ matrix from $C(r) r \sqrt{nm}$ entries, where $C(r)$ is a small constant, see Fig. ~\ref{fig:transition}, and $C(r) \to 1$ as $r\to \infty$. The corresponding RMSE is evaluated empirically, and in the regime close to $C(r) r \sqrt{nm}$, it compares very favorably to existing approaches, in particular to OptSpace \cite{kmo10}.  \medskip This contribution is organized as follows. First, in Sec. \ref{sec:def} we define the problem and present generally our approach in the context of existing works.	score:462
Are we living in a matrix?	  Organizing the weight vectors of all classes as rows of a single matrix, this is equivalent to requiring sparsity of the \emph{columns} of the matrix.  In this paper we describe and analyze an efficient algorithm for learning a multiclass predictor whose corresponding matrix of weights has a small number of non-zero columns. We formally prove that if there exists an accurate matrix with a number of non-zero columns that grows sub-linearly with the number of classes, then our algorithm will also learn such a matrix.  We apply our algorithm to natural multiclass learning problems and demonstrate its advantages over previously proposed state-of-the-art methods.  Our algorithm is a generalization of the forward greedy selection approach to sparsity in columns. An alternative approach, which has recently been studied in \cite{QuattoniCaCoDa09, Duchi}, generalizes the $\ell_1$ norm based approach, and relies on mixed-norms.	  Organizing the weight vectors of all classes as rows of a single matrix, this is equivalent to requiring sparsity of the \emph{columns} of the matrix.  In this paper we describe and analyze an efficient algorithm for learning a multiclass predictor whose corresponding matrix of weights has a small number of non-zero columns. We formally prove that if there exists an accurate matrix with a number of non-zero columns that grows sub-linearly with the number of classes, then our algorithm will also learn such a matrix.  We apply our algorithm to natural multiclass learning problems and demonstrate its advantages over previously proposed state-of-the-art methods.  Our algorithm is a generalization of the forward greedy selection approach to sparsity in columns. An alternative approach, which has recently been studied in \cite{QuattoniCaCoDa09, Duchi}, generalizes the $\ell_1$ norm based approach, and relies on mixed-norms.	score:465
Are we living in a matrix?	 Second, our model for exchangeable matrices can be seen as a generalization of the \emph{deep sets} architecture \citep{zaheer_deepsets}, where a set can be seen as a one-dimensional exchangeable array.   In what follows, we begin by introducing our parameter-sharing approach in \cref{sec:layer}, considering the cases of both dense and sparse matrices.    In \cref{sec:architectures} we study two architectures for matrix completion that use an exchangeable matrix layer. In particular the factorized autoencoding model provides a powerful alternative to commonly used matrix factorization methods; notably, it does not require retraining to be evaluated on previously unseen data.  In \cref{sec:empirical} we present extensive results on benchmark matrix completion datasets. We generalize our approach to higher-dimensional tensors in \cref{sec:tensors}.	   The parameter-sharing patterns for the weight matrix of the higher dimensional arrays can be produced via the \textit{Kronecker product} of the weight matrix for the 1D array (\ie vector).} \label{fig:parameter_matrix} \end{figure}       When the matrix models the interaction between the members of the same group, one could further constrain permutations to be identical across both rows and columns.   An example of such a \textit{jointly exchangeable matrix} \mbox{\citep{orbanz2015bayesian}},  modelling the interaction of the nodes in a graph, is the adjacency matrix deployed by graph convolution. Our approach reduces to graph convolution in the special case of 2D arrays with this additional parameter-sharing constraint, but also applies to arbitrary matrices and higher dimensional arrays.	score:474
Are we living in a matrix?	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	score:484
What are some challenges for machine learning algorithms for big data analysis?	 Experiments show that proposed schemes significantly improves algorithm performance and increase feature selection quality. 	Almost all business and scientific problems nowadays involve processing huge amounts of data with machine learning algorithms. Due to its universal applicability, machine learning became one of the most promising and researched scientific domains.  In particular, it has application in bioinformatics~\citep{bolon2014review,saeys2007review}, as giant amounts of data about gene expression of different organisms are obtained in this field. In order to filter data noise and reduce model complexity, it is necessary to select the most relevant features. Techniques and methods achieving this goal are called feature selection.	 Experiments show that proposed schemes significantly improves algorithm performance and increase feature selection quality. 	Almost all business and scientific problems nowadays involve processing huge amounts of data with machine learning algorithms. Due to its universal applicability, machine learning became one of the most promising and researched scientific domains.  In particular, it has application in bioinformatics~\citep{bolon2014review,saeys2007review}, as giant amounts of data about gene expression of different organisms are obtained in this field. In order to filter data noise and reduce model complexity, it is necessary to select the most relevant features. Techniques and methods achieving this goal are called feature selection.	score:290
What are some challenges for machine learning algorithms for big data analysis?	  	\subsection{Motivation}  The size of the acquired raw data currently poses a challenge to current state-of-the-art information processing systems. Since many machine learning algorithms' computational efficiency scale with the complexity of the data, machine learning researchers have introduced a family of algorithms for \textit{dimensionality reduction} to address this issue.  Dimensionality reduction algorithms construct a concise representation of high-dimensional data on a lower-dimensional subspace, with as minimal loss of intrinsic information as possible. This representation is often referred to as a low-dimensional \textit{embedding}.  The canonical approach in statistics for constructing a linear embedding is principal components analysis (PCA) \cite{moore1981principal}.	  	\subsection{Motivation}  The size of the acquired raw data currently poses a challenge to current state-of-the-art information processing systems. Since many machine learning algorithms' computational efficiency scale with the complexity of the data, machine learning researchers have introduced a family of algorithms for \textit{dimensionality reduction} to address this issue.  Dimensionality reduction algorithms construct a concise representation of high-dimensional data on a lower-dimensional subspace, with as minimal loss of intrinsic information as possible. This representation is often referred to as a low-dimensional \textit{embedding}.  The canonical approach in statistics for constructing a linear embedding is principal components analysis (PCA) \cite{moore1981principal}.	score:310
What are some challenges for machine learning algorithms for big data analysis?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:313
What are some challenges for machine learning algorithms for big data analysis?	  Much of machine learning has focused on studying classes of models with limited expressiveness in order to develop tractable algorithms for modeling large data sets.  Our investigation takes a different approach and explores how learning might proceed in an expressive class of models with a focus on identifying abstract patterns from small amounts of data.   We represent generative models as programs in a probabilistic programming language. A probabilistic program represents a probability distribution, and each evaluation of the program results in a sample from the distribution. We implement these programs in a subset of the probabilistic programming language Church \cite{N.D.Goodman:2008:f2a0d}.   These programs can have parameterized functions and recursion, which allow for natural representation of ``long-range'' dependencies and recursive patterns.	  Much of machine learning has focused on studying classes of models with limited expressiveness in order to develop tractable algorithms for modeling large data sets.  Our investigation takes a different approach and explores how learning might proceed in an expressive class of models with a focus on identifying abstract patterns from small amounts of data.   We represent generative models as programs in a probabilistic programming language. A probabilistic program represents a probability distribution, and each evaluation of the program results in a sample from the distribution. We implement these programs in a subset of the probabilistic programming language Church \cite{N.D.Goodman:2008:f2a0d}.   These programs can have parameterized functions and recursion, which allow for natural representation of ``long-range'' dependencies and recursive patterns.	score:316
What are some challenges for machine learning algorithms for big data analysis?	      	According to the definitions of Gartner \cite{Gartner2014} and De Maro et al. \cite{Maro2016Bigdata}, big data refer to information assets with characteristics high volume, high velocity, and/or high variety, and the transformation from big data to value requires specific analytical methods. Currently, machine learning methods are used as the main tools for big data analytics, which emphasize algorithms instead of statistical analysis.  Statistical Analysis (SA) is the process of deducing population properties and draw conclusions by analysis of data. Typically, the SA process consists of exploratory data analysis (EDA), model building, parameter estimation, hypothesis testing, interval estimation, prediction, and model diagnostics. Not only does the process depend on available computing system and software, it also heavily depends on the analyst.	      	According to the definitions of Gartner \cite{Gartner2014} and De Maro et al. \cite{Maro2016Bigdata}, big data refer to information assets with characteristics high volume, high velocity, and/or high variety, and the transformation from big data to value requires specific analytical methods. Currently, machine learning methods are used as the main tools for big data analytics, which emphasize algorithms instead of statistical analysis.  Statistical Analysis (SA) is the process of deducing population properties and draw conclusions by analysis of data. Typically, the SA process consists of exploratory data analysis (EDA), model building, parameter estimation, hypothesis testing, interval estimation, prediction, and model diagnostics. Not only does the process depend on available computing system and software, it also heavily depends on the analyst.	score:319
Where can I find Astrophysical data for data analysis or machine learning?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:390
Where can I find Astrophysical data for data analysis or machine learning?	 	How can one do meaningful statistical inference and machine learning when data are \emph{re-used} across analyses? The situation is common in empirical science, especially as data sets get bigger and more complex. For example, analysts often clean the data and perform various exploratory analyses---visualizations, computing descriptive statistics---before selecting how data will be treated.  Many times the main analysis also proceeds in stages, with some sort of feature selection followed by inference using the selected features. In such settings, the analyses performed in later stages are chosen \emph{adaptively} based on the results of earlier stages that used the same data. Adaptivity comes into even sharper relief when data are shared across multiple studies, and the choice of the research question in subsequent studies may depend on the outcomes of earlier ones.	 	How can one do meaningful statistical inference and machine learning when data are \emph{re-used} across analyses? The situation is common in empirical science, especially as data sets get bigger and more complex. For example, analysts often clean the data and perform various exploratory analyses---visualizations, computing descriptive statistics---before selecting how data will be treated.  Many times the main analysis also proceeds in stages, with some sort of feature selection followed by inference using the selected features. In such settings, the analyses performed in later stages are chosen \emph{adaptively} based on the results of earlier stages that used the same data. Adaptivity comes into even sharper relief when data are shared across multiple studies, and the choice of the research question in subsequent studies may depend on the outcomes of earlier ones.	score:396
Where can I find Astrophysical data for data analysis or machine learning?	  	For many tasks, it is useful to analyse the geometric shapes of geospatial objects, such as in quality assessment or enrichment of map data \citep{fan2014quality} or such as the classification of topographical objects \citep{keyes1999fourier}. Machine learning is increasingly used in geospatial analysis tasks. Machine learning can learn from data by extracting patterns \citep[2]{Goodfellow-et-al-2016}.  For example, machine learning can be applied to classify building types \citep{xu2017quality}, analyse wildfires \citep{Araya2016}, traffic safety \citep{Effati2015}, cluster spatial objects \citep{Hagenauer2016}, detect aircraft shapes \citep{wu2016shape} or classify road sections \citep{Andrasik2016}: tasks that extend beyond standard GIS processing operations.	  	For many tasks, it is useful to analyse the geometric shapes of geospatial objects, such as in quality assessment or enrichment of map data \citep{fan2014quality} or such as the classification of topographical objects \citep{keyes1999fourier}. Machine learning is increasingly used in geospatial analysis tasks. Machine learning can learn from data by extracting patterns \citep[2]{Goodfellow-et-al-2016}.  For example, machine learning can be applied to classify building types \citep{xu2017quality}, analyse wildfires \citep{Araya2016}, traffic safety \citep{Effati2015}, cluster spatial objects \citep{Hagenauer2016}, detect aircraft shapes \citep{wu2016shape} or classify road sections \citep{Andrasik2016}: tasks that extend beyond standard GIS processing operations.	score:407
Where can I find Astrophysical data for data analysis or machine learning?	  Many machine learning algorithms have been analyzed by smoothed analysis, such as k-means clustering \citep{arthur2009k} and Support Vector Machines \citep{Blum:2002:SAP:545381.545499,spielman2009smoothed}, however, to the best of our knowledge, there is no previous research on using machine learning algorithms to predict the SC of sorting algorithms.   Due to recent modular smoothed analysis results, we can compute the SC of Quicksort exactly, for medium size inputs (e.g., $N \leq 3000$). For other sorting algorithms, currently the SC can only be computed using its original definition, and thus, due to computational requirements, only for small input sizes (e.g., $N \leq 100$), limiting our understanding of the general SC behaviour.	  Many machine learning algorithms have been analyzed by smoothed analysis, such as k-means clustering \citep{arthur2009k} and Support Vector Machines \citep{Blum:2002:SAP:545381.545499,spielman2009smoothed}, however, to the best of our knowledge, there is no previous research on using machine learning algorithms to predict the SC of sorting algorithms.   Due to recent modular smoothed analysis results, we can compute the SC of Quicksort exactly, for medium size inputs (e.g., $N \leq 3000$). For other sorting algorithms, currently the SC can only be computed using its original definition, and thus, due to computational requirements, only for small input sizes (e.g., $N \leq 100$), limiting our understanding of the general SC behaviour.	score:411
Where can I find Astrophysical data for data analysis or machine learning?	      	According to the definitions of Gartner \cite{Gartner2014} and De Maro et al. \cite{Maro2016Bigdata}, big data refer to information assets with characteristics high volume, high velocity, and/or high variety, and the transformation from big data to value requires specific analytical methods. Currently, machine learning methods are used as the main tools for big data analytics, which emphasize algorithms instead of statistical analysis.  Statistical Analysis (SA) is the process of deducing population properties and draw conclusions by analysis of data. Typically, the SA process consists of exploratory data analysis (EDA), model building, parameter estimation, hypothesis testing, interval estimation, prediction, and model diagnostics. Not only does the process depend on available computing system and software, it also heavily depends on the analyst.	      	According to the definitions of Gartner \cite{Gartner2014} and De Maro et al. \cite{Maro2016Bigdata}, big data refer to information assets with characteristics high volume, high velocity, and/or high variety, and the transformation from big data to value requires specific analytical methods. Currently, machine learning methods are used as the main tools for big data analytics, which emphasize algorithms instead of statistical analysis.  Statistical Analysis (SA) is the process of deducing population properties and draw conclusions by analysis of data. Typically, the SA process consists of exploratory data analysis (EDA), model building, parameter estimation, hypothesis testing, interval estimation, prediction, and model diagnostics. Not only does the process depend on available computing system and software, it also heavily depends on the analyst.	score:415
What is good way to understand carrier phrase in NLP when building diphone database?	 Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout's training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated.  In this work, we first formulate dropout as a tractable approximation of some latent variable model,  leading to a clean view of parameter sharing and enabling further theoretical analysis.   Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we  are able to formally characterize. Algorithmically, we show that our proposed measure of the inference  gap can be used to regularize the standard dropout training objective, resulting in an \emph{explicit}  control of the gap. Our method is as simple and efficient as standard dropout.	 Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout's training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated.  In this work, we first formulate dropout as a tractable approximation of some latent variable model,  leading to a clean view of parameter sharing and enabling further theoretical analysis.   Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we  are able to formally characterize. Algorithmically, we show that our proposed measure of the inference  gap can be used to regularize the standard dropout training objective, resulting in an \emph{explicit}  control of the gap. Our method is as simple and efficient as standard dropout.	score:358
What is good way to understand carrier phrase in NLP when building diphone database?	 	Building multilingual processing systems is a challenging task. Every NLP task involves different stages of preprocessing and calculating intermediate representations that will serve as features for later stages. These stages vary in complexity and requirements for each individual language. Despite recent momentum towards developing multilingual tools \cite{CoNLL2007,CoNLL2009,CoNLL2012}, most of NLP research still focuses on rich resource languages.  Common NLP systems and tools rely heavily on English specific features and they are infrequently tested on multiple datasets. This makes them hard to port to new languages and tasks \cite{Blitzer06domain}.  A serious bottleneck in the current approach for developing multilingual systems is the requirement of familiarity with each language under consideration.	 	Building multilingual processing systems is a challenging task. Every NLP task involves different stages of preprocessing and calculating intermediate representations that will serve as features for later stages. These stages vary in complexity and requirements for each individual language. Despite recent momentum towards developing multilingual tools \cite{CoNLL2007,CoNLL2009,CoNLL2012}, most of NLP research still focuses on rich resource languages.  Common NLP systems and tools rely heavily on English specific features and they are infrequently tested on multiple datasets. This makes them hard to port to new languages and tasks \cite{Blitzer06domain}.  A serious bottleneck in the current approach for developing multilingual systems is the requirement of familiarity with each language under consideration.	score:380
What is good way to understand carrier phrase in NLP when building diphone database?	 In this work, we are interested in deep learning approaches for NLP sequence tagging tasks in which the goal is to label each token of the given input sentence.    Recurrent neural networks constitute one important class of naturally deep architecture that has been applied to many sequential prediction tasks. In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens.  With this view, they have been successfully applied to tasks such as language modeling~\cite{mikolovLanguage}, and spoken language understanding~\cite{slurnn}. Since  classical recurrent neural networks only incorporate information from the past (i.e.\ preceding tokens),  bidirectional variants have been proposed to incorporate information from both the past and the future (i.	 In this work, we are interested in deep learning approaches for NLP sequence tagging tasks in which the goal is to label each token of the given input sentence.    Recurrent neural networks constitute one important class of naturally deep architecture that has been applied to many sequential prediction tasks. In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens.  With this view, they have been successfully applied to tasks such as language modeling~\cite{mikolovLanguage}, and spoken language understanding~\cite{slurnn}. Since  classical recurrent neural networks only incorporate information from the past (i.e.\ preceding tokens),  bidirectional variants have been proposed to incorporate information from both the past and the future (i.	score:387
What is good way to understand carrier phrase in NLP when building diphone database?	 However, this advantage comes with a caveat that deep RL requires a practically prohibitive amount of training data and computational time to successfully learn policies across high-dimensional, continuous state and continuous action domains, thus hindering its application in robotics.  Deep RL methods are commonly used in conjunction with a binary success/failure reward function.  Such a reward mechanism allows for the agent to discover optimal policies intended for successful completion of the task, thus eliminating the need for expert reward specification or the danger of inaccurate reward design~\cite{ng1999policy}. In high-dimensional continuous control, a characteristic property of most practical robots, a binary reward system presents a challenge when the task is complex and the goal states are rarely encountered.	 However, this advantage comes with a caveat that deep RL requires a practically prohibitive amount of training data and computational time to successfully learn policies across high-dimensional, continuous state and continuous action domains, thus hindering its application in robotics.  Deep RL methods are commonly used in conjunction with a binary success/failure reward function.  Such a reward mechanism allows for the agent to discover optimal policies intended for successful completion of the task, thus eliminating the need for expert reward specification or the danger of inaccurate reward design~\cite{ng1999policy}. In high-dimensional continuous control, a characteristic property of most practical robots, a binary reward system presents a challenge when the task is complex and the goal states are rarely encountered.	score:390
What is good way to understand carrier phrase in NLP when building diphone database?	 In this paper, we propose  deep learning architectures (FNN, CNN and LSTM)  to forecast a regression model for time dependent data. These algorithm's are designed to handle  Floating Car Data (FCD) historic speeds to predict road traffic data. For this we aggregate the speeds into the network inputs in an innovative way. We compare the RMSE thus obtained with the results of a simpler physical model, and show that the latter achieves better RMSE accuracy.  We also propose a new indicator, which evaluates the algorithms improvement when compared to a benchmark prediction. We conclude by questionning the interest of using deep learning methods for this specific regression task.  \vspace{0.3cm}  \noindent \textit{AMS subject classifications:}   \vspace{0.3cm}  \noindent \textit{Keywords}: Deep Learning, Feedforward Neural Network, Convolutional Neural Network, Recurrent Neural-Network, Long Short Term Memory, time series.	 In this paper, we propose  deep learning architectures (FNN, CNN and LSTM)  to forecast a regression model for time dependent data. These algorithm's are designed to handle  Floating Car Data (FCD) historic speeds to predict road traffic data. For this we aggregate the speeds into the network inputs in an innovative way. We compare the RMSE thus obtained with the results of a simpler physical model, and show that the latter achieves better RMSE accuracy.  We also propose a new indicator, which evaluates the algorithms improvement when compared to a benchmark prediction. We conclude by questionning the interest of using deep learning methods for this specific regression task.  \vspace{0.3cm}  \noindent \textit{AMS subject classifications:}   \vspace{0.3cm}  \noindent \textit{Keywords}: Deep Learning, Feedforward Neural Network, Convolutional Neural Network, Recurrent Neural-Network, Long Short Term Memory, time series.	score:392
A glass slab is placed in the path of convergent light. The point of convergence of light:?	 In the model, the path from input to output is randomly activated, and the corresponding input unit constrains the weights along the path into the form of a $p$-weight interaction glass model. A critical value of the perturbation is observed to separate a spin glass regime from a paramagnetic regime, with the transition being of the first order. The paramagnetic phase is conjectured to have a poor  generalization performance.    	Deep learning (e.g., with many layers of neural networks) works very well in areas from speech recognition, image classification, to drug discovery, medical image analysis, particle discovery, automatic game playing, and many others~\cite{DL-2016}. This is due to the available large dataset for training and efficient hardware design such as GPU to accelerate training, rather than breakthrough in theoretical foundations.	 In the model, the path from input to output is randomly activated, and the corresponding input unit constrains the weights along the path into the form of a $p$-weight interaction glass model. A critical value of the perturbation is observed to separate a spin glass regime from a paramagnetic regime, with the transition being of the first order. The paramagnetic phase is conjectured to have a poor  generalization performance.    	Deep learning (e.g., with many layers of neural networks) works very well in areas from speech recognition, image classification, to drug discovery, medical image analysis, particle discovery, automatic game playing, and many others~\cite{DL-2016}. This is due to the available large dataset for training and efficient hardware design such as GPU to accelerate training, rather than breakthrough in theoretical foundations.	score:441
A glass slab is placed in the path of convergent light. The point of convergence of light:?	 The user provides a textual description: ``The afternoon light flooded the little room from the window, shining the ground in front of a brown bookshelf made of wood. Besides the bookshelf lies a sofa with light-colored cushions. There is a blue carpet in front of the sofa, and a clock with dark contours above it...''. The system modifies the virtual environment into the target image on the right. } \label{fig:VR}  \end{figure} \begin{figure}[t] \centering \includegraphics[width=0.33\linewidth, height = 1.8cm]{fig/00296_orginal_sketch.jpg} \includegraphics[width=0.33\linewidth, height = 1.8cm]{fig/00296_bw.jpg}  \includegraphics[width=0.33\linewidth, height = 1.8cm]{fig/image_00296.jpg}   \caption{Left: sketch image. Middle: grayscale image. Right: color image (from \cite{Nilsback08}).	 The user provides a textual description: ``The afternoon light flooded the little room from the window, shining the ground in front of a brown bookshelf made of wood. Besides the bookshelf lies a sofa with light-colored cushions. There is a blue carpet in front of the sofa, and a clock with dark contours above it...''. The system modifies the virtual environment into the target image on the right. } \label{fig:VR}  \end{figure} \begin{figure}[t] \centering \includegraphics[width=0.33\linewidth, height = 1.8cm]{fig/00296_orginal_sketch.jpg} \includegraphics[width=0.33\linewidth, height = 1.8cm]{fig/00296_bw.jpg}  \includegraphics[width=0.33\linewidth, height = 1.8cm]{fig/image_00296.jpg}   \caption{Left: sketch image. Middle: grayscale image. Right: color image (from \cite{Nilsback08}).	score:462
A glass slab is placed in the path of convergent light. The point of convergence of light:?	 A time series to be analysed could, for instance, represent the repeated measurement, over many months or years, of the flux of radiation (optical light or radio waves) from a very distant quasar, a very bright source of light usually a few billion light-years away.  Some of these quasars appear as a set of multiple nearby images on the sky, due to the fact that the trajectory of light coming from the source gets bent as it passes a massive galaxy on the way (the ``lens''), and, as a result, the observer receives the light from various directions, resulting in the detection of several images \cite{Kochanek:2004:THC,Saha:2000:GL}.	 A time series to be analysed could, for instance, represent the repeated measurement, over many months or years, of the flux of radiation (optical light or radio waves) from a very distant quasar, a very bright source of light usually a few billion light-years away.  Some of these quasars appear as a set of multiple nearby images on the sky, due to the fact that the trajectory of light coming from the source gets bent as it passes a massive galaxy on the way (the ``lens''), and, as a result, the observer receives the light from various directions, resulting in the detection of several images \cite{Kochanek:2004:THC,Saha:2000:GL}.	score:474
A glass slab is placed in the path of convergent light. The point of convergence of light:?	 Support vector clustering (SVC) is a versatile clustering technique that is able to identify clusters of arbitrary shapes by exploiting the kernel trick. However, one hurdle that restricts the application of SVC lies in its sensitivity to the kernel parameter and the trade-off parameter. Although many extensions of SVC have been developed, to the best of our knowledge, there is still no algorithm that is able to effectively estimate the two crucial parameters in SVC without supervision.  In this paper, we propose a novel support vector clustering approach termed ensemble-driven support vector clustering (EDSVC), which for the first time tackles the automatic parameter estimation problem for SVC based on ensemble learning, and is capable of producing robust clustering results in a purely unsupervised manner. Experimental results on multiple real-world datasets demonstrate the effectiveness of our approach.  	Support vector clustering (SVC) is a flexible clustering technique which is inspired by support vector machines (SVM) \cite{Ben-Hur2001}. In SVC, the input data points are first mapped from the original space to a high-dimensional kernel space, where a sphere that encloses most of the data points is constructed. When mapped back to the original space, the sphere is split into several components, each of which encloses a set of data points and forms a cluster in the clustering result.  With the nonlinear mapping, SVC has two major advantages over the other clustering techniques. First, it is capable of identifying clusters of arbitrary shapes. Second, the number of clusters can be obtained automatically in SVC and needn't to be specified in advance.  Ben-Hur et al. \cite{Ben-Hur2001} for the first time introduced the SVC technique, which consists of two phases, i.	 Support vector clustering (SVC) is a versatile clustering technique that is able to identify clusters of arbitrary shapes by exploiting the kernel trick. However, one hurdle that restricts the application of SVC lies in its sensitivity to the kernel parameter and the trade-off parameter. Although many extensions of SVC have been developed, to the best of our knowledge, there is still no algorithm that is able to effectively estimate the two crucial parameters in SVC without supervision.  In this paper, we propose a novel support vector clustering approach termed ensemble-driven support vector clustering (EDSVC), which for the first time tackles the automatic parameter estimation problem for SVC based on ensemble learning, and is capable of producing robust clustering results in a purely unsupervised manner. Experimental results on multiple real-world datasets demonstrate the effectiveness of our approach.  	Support vector clustering (SVC) is a flexible clustering technique which is inspired by support vector machines (SVM) \cite{Ben-Hur2001}. In SVC, the input data points are first mapped from the original space to a high-dimensional kernel space, where a sphere that encloses most of the data points is constructed. When mapped back to the original space, the sphere is split into several components, each of which encloses a set of data points and forms a cluster in the clustering result.  With the nonlinear mapping, SVC has two major advantages over the other clustering techniques. First, it is capable of identifying clusters of arbitrary shapes. Second, the number of clusters can be obtained automatically in SVC and needn't to be specified in advance.  Ben-Hur et al. \cite{Ben-Hur2001} for the first time introduced the SVC technique, which consists of two phases, i.	score:481
A glass slab is placed in the path of convergent light. The point of convergence of light:?	 These discoveries might be useful not only to provide new information but to outline observations, which might require further and deeper investigation. In particular, our research detects anomalies in photometric time series data (light-curves).   For this work, each light-curve is described by 13 variability characteristics (period, amplitude, color, etc. ) termed \textit{features} \citep{Kim2011,Pichara2012}, which  have been used  for classification. It is worth noting that the method developed in this paper is not only applicable to time-series data but could also be used for any type of data that need to be inspected for anomalies. In addition to this advantage, the fact that it can be applied to big data, makes this algorithm suitable for almost any outlier detection problem.   Many outlier detection methods have been proposed in astronomy. Most of them are unsupervised techniques, where the assumption is made that there is no information about the set of light-curves or their types \citep{Connolly2010}. One of these approaches considers a point-by-point comparison of every pair of light-curves in the data base by using  correlation coefficient \citep{Protopapas2008}.  Other techniques search for anomalies in lower-dimensional subspaces of the data in order to deal with the massive number of objects or the large quantity of features that describe them \citep{Henrion2012, Connolly2010}. Clustering methods are equally applied in the astronomical outlier detection area aiming to find clusters of new variability classes \citep{Bhattacharyya, Rebbapragada2008}.	 These discoveries might be useful not only to provide new information but to outline observations, which might require further and deeper investigation. In particular, our research detects anomalies in photometric time series data (light-curves).   For this work, each light-curve is described by 13 variability characteristics (period, amplitude, color, etc. ) termed \textit{features} \citep{Kim2011,Pichara2012}, which  have been used  for classification. It is worth noting that the method developed in this paper is not only applicable to time-series data but could also be used for any type of data that need to be inspected for anomalies. In addition to this advantage, the fact that it can be applied to big data, makes this algorithm suitable for almost any outlier detection problem.   Many outlier detection methods have been proposed in astronomy. Most of them are unsupervised techniques, where the assumption is made that there is no information about the set of light-curves or their types \citep{Connolly2010}. One of these approaches considers a point-by-point comparison of every pair of light-curves in the data base by using  correlation coefficient \citep{Protopapas2008}.  Other techniques search for anomalies in lower-dimensional subspaces of the data in order to deal with the massive number of objects or the large quantity of features that describe them \citep{Henrion2012, Connolly2010}. Clustering methods are equally applied in the astronomical outlier detection area aiming to find clusters of new variability classes \citep{Bhattacharyya, Rebbapragada2008}.	score:483
Which is better for working in AI / machine learning, a joint degree in mathematics and philosophy or a degree in computer science?	 In mathematical programming and optimization, disciplined programming \cite{gb08} and AMPL \cite{opac-b1123349} have been developed. Finally,  the  DARPA project \emph{Probabilistic Programming for Advancing Machine Learning}  is motivated in the same declarative spirit.\footnote{\texttt{http://www.darpa.mil/program/probabilistic-programming-for-advancing-machine-learning}} Across  disciplines in AI, it has  become increasingly clear that taming the model building process, admitting reusable descriptions in expressive languages,  and providing general but powerful inference engines is essential.	 The semantics of programs is defined in terms of first-order logic structures with semiring labels, which allows us to freely combine and integrate problems from different AI disciplines and represent non-standard problems over unbounded domains.                                     	AI applications, such as robotics and logistics, rely on a variety of disciplines such as logic, probabilistic reasoning, machine learning and mathematical programming.   These applications are often described in a combination of natural and mathematical language, and need to be engineered for the individual application.    Declarative formalisms and methods are ubiquitous in AI as they enable re-use and descriptive clarity.   Initial approaches, such as that by   Kowalski~\cite{DBLP:conf/ifip/Kowalski74},  were rooted in logic, but they have eventually engendered an impressive family of languages.	score:330
Which is better for working in AI / machine learning, a joint degree in mathematics and philosophy or a degree in computer science?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:335
Which is better for working in AI / machine learning, a joint degree in mathematics and philosophy or a degree in computer science?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:346
Which is better for working in AI / machine learning, a joint degree in mathematics and philosophy or a degree in computer science?	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	score:349
Which is better for working in AI / machine learning, a joint degree in mathematics and philosophy or a degree in computer science?	   This paper reviews machine learning applications and approaches to detection, classification and control of intelligent materials and structures with embedded distributed computation elements.  The purpose of this survey is to identify desired tasks to be performed in each type of material or structure (e.g., damage detection in composites), identify and compare common approaches to learning such tasks, and investigate models and training paradigms used.   Machine learning approaches and common temporal features used in the domains of structural health monitoring, morphable aircraft, wearable computing and robotic skins are explored.  As the ultimate goal of this research is to incorporate the approaches described in this survey into a robotic material paradigm, the potential for adapting the computational models used in these applications, and corresponding training algorithms, to an amorphous network of computing nodes is considered.	g., sirens, gun fire, etc.).    Machine learning is one promising approach to developing intelligent behavior in robotic materials.  Broadly speaking, machine learning approaches attempt to learn a particular general task, such as identifying a pattern or learning a function, from a set of provided examples.  Typically, the physical response of a material to some stimulus, such as the vibrations generated from an impact, is only well defined for very simple structures (e. g., rectangular planar composite panels).  More complex geometries, such as those found in real world structures, require computationally intensive numerical modeling approaches, such as finite element methods (FEM) \cite{zienkiewicz2005finite}.  Machine learning approaches, in contrast, utilize a set of examples to \emph{learn} how a structure responds to an external stimulus from several sample stimuli.	score:352
What is the Linnaean system of classification used for?	 Empirical studies partially demonstrate the resistance of \(k\)-nearest neighbor to noise \citep{Kusner:Tyree:Weinberger:Agrawal2014,Tarlow:Swersky:Swersky:Charlin:Sutskever:Zemel2013}, whereas there is a paucity of deep understanding.  This work focuses on binary classification in the presence of random classification noises \citep{Angluin:Laird1988}, that is, each observed label has been flipped with certain probability instead of seeing the ground-truth label, and training data of each class are contaminated by samples from the other class.	 Empirical studies partially demonstrate the resistance of \(k\)-nearest neighbor to noise \citep{Kusner:Tyree:Weinberger:Agrawal2014,Tarlow:Swersky:Swersky:Charlin:Sutskever:Zemel2013}, whereas there is a paucity of deep understanding.  This work focuses on binary classification in the presence of random classification noises \citep{Angluin:Laird1988}, that is, each observed label has been flipped with certain probability instead of seeing the ground-truth label, and training data of each class are contaminated by samples from the other class.	score:407
What is the Linnaean system of classification used for?	 We study the interaction between the statistical sample complexity of $\Fcal$ and its combinatorial structure. We introduce a new version of sample compression schemes and show that it characterizes EMX learnability for a wide family of classes. However, we show that for the class of finite subsets of the real line, the existence of such compression schemes is independent of set theory.  We conclude that the learnability of that class with respect to the family of probability distributions of countable support is independent of the set theory ZFC axioms.  \medskip  We also explore the existence of  a ``VC-dimension-like'' parameter that captures learnability in this setting. Our results imply that that there exist no ``finitary" combinatorial parameter that characterizes EMX learnability in a way similar to the VC-dimension based characterization of binary valued classification problems.	 We study the interaction between the statistical sample complexity of $\Fcal$ and its combinatorial structure. We introduce a new version of sample compression schemes and show that it characterizes EMX learnability for a wide family of classes. However, we show that for the class of finite subsets of the real line, the existence of such compression schemes is independent of set theory.  We conclude that the learnability of that class with respect to the family of probability distributions of countable support is independent of the set theory ZFC axioms.  \medskip  We also explore the existence of  a ``VC-dimension-like'' parameter that captures learnability in this setting. Our results imply that that there exist no ``finitary" combinatorial parameter that characterizes EMX learnability in a way similar to the VC-dimension based characterization of binary valued classification problems.	score:408
What is the Linnaean system of classification used for?	 Namely, what functions reside in the conjugate kernel space, how rich it is, and how good those functions are as predictors. From an empirical perspective, in \citep{daniely2017random}, it is shown that for standard  convolutional networks the conjugate class contains functions whose performance is close to the performance of the function that is actually learned by the network.  This is based on experiments on the standard CIFAR-10 dataset.  From a theoretical perspective, we list below a few implications that demonstrate the richness of the conjugate kernel space. These implications are valid for fully connected networks of any depth between $2$ and $\log(n)$, where $n$ is the input dimension. Likewise, they are also valid for convolutional networks of any depth between $2$ and $\log(n)$, and with constantly many convolutional layers.	 Namely, what functions reside in the conjugate kernel space, how rich it is, and how good those functions are as predictors. From an empirical perspective, in \citep{daniely2017random}, it is shown that for standard  convolutional networks the conjugate class contains functions whose performance is close to the performance of the function that is actually learned by the network.  This is based on experiments on the standard CIFAR-10 dataset.  From a theoretical perspective, we list below a few implications that demonstrate the richness of the conjugate kernel space. These implications are valid for fully connected networks of any depth between $2$ and $\log(n)$, where $n$ is the input dimension. Likewise, they are also valid for convolutional networks of any depth between $2$ and $\log(n)$, and with constantly many convolutional layers.	score:411
What is the Linnaean system of classification used for?	 However, in neural networks with small datasets, we can present all the possible stimuli to the network. We consider a neuron to be selective if it is selective over all the data it is reasonable to expect the network to differentiate between. For example, it is reasonable to do the test over all training data, and it is reasonable to do it over all test and all verification data or even other data of a similar form (such as different photos of the same class), and choosing what constitutes a reasonable set of data is a decision to be made by the experimenters and reviewers.  In this work, we chose to use a simple pattern classification task, rather than an image recognition task, as we could then test the NN with all possible patterns.     \begin{figure}[h]  \includegraphics[width = 3in]{124.pdf}  \includegraphics[width = 3in]{430.pdf}  \caption{Examples of interpretable local codes found in a distributed network. Left: a selectively on unit with a selectivity of $\sim +0.	 However, in neural networks with small datasets, we can present all the possible stimuli to the network. We consider a neuron to be selective if it is selective over all the data it is reasonable to expect the network to differentiate between. For example, it is reasonable to do the test over all training data, and it is reasonable to do it over all test and all verification data or even other data of a similar form (such as different photos of the same class), and choosing what constitutes a reasonable set of data is a decision to be made by the experimenters and reviewers.  In this work, we chose to use a simple pattern classification task, rather than an image recognition task, as we could then test the NN with all possible patterns.     \begin{figure}[h]  \includegraphics[width = 3in]{124.pdf}  \includegraphics[width = 3in]{430.pdf}  \caption{Examples of interpretable local codes found in a distributed network. Left: a selectively on unit with a selectivity of $\sim +0.	score:414
What is the Linnaean system of classification used for?	  To enable principled analytic pipelines where the inherent geometry of the data is respected in an \emph{end-to-end} fashion, we generalize linear support vector classifiers, which are one of the most widely-used methods for classification, to data points in hyperbolic space. Despite the complexities of hyperbolic distance calculation, we prove that support vector classification in hyperbolic space can in fact be performed by solving a simple optimization problem that resembles the Euclidean formulation of SVM, elucidating the close connection between the two.	  To enable principled analytic pipelines where the inherent geometry of the data is respected in an \emph{end-to-end} fashion, we generalize linear support vector classifiers, which are one of the most widely-used methods for classification, to data points in hyperbolic space. Despite the complexities of hyperbolic distance calculation, we prove that support vector classification in hyperbolic space can in fact be performed by solving a simple optimization problem that resembles the Euclidean formulation of SVM, elucidating the close connection between the two.	score:415
What are some good, active blogs for computational linguistics or NLP?	 On the other hand, multilabel  instances occur very commonly in the Internet: in many blog applications, blog posts for example, can be categorized with an arbitrary number of labels.  Furthermore, in collaborative environments (like folksonomies, \cite{vanderwal}) where users can add tags, multilabel is an ordinary process. Due to this fact, sometimes the word ``tag'' or ``label'' is used instead of ``category'', they are all synonyms.   More recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions \cite{trohidis,yang11},  scene or image categorization \cite{dimou,su11}, protein and gene function prediction \cite{pandey,valentini11} or medical diagnosis \cite{taylor10}.  Although each instance has a given set of associated labels, and they are assigned as a whole, the normal approach to solve this problem is just  ignore this fact and concentrate in obtaining good solutions to the individual binary problems (i.	 On the other hand, multilabel  instances occur very commonly in the Internet: in many blog applications, blog posts for example, can be categorized with an arbitrary number of labels.  Furthermore, in collaborative environments (like folksonomies, \cite{vanderwal}) where users can add tags, multilabel is an ordinary process. Due to this fact, sometimes the word ``tag'' or ``label'' is used instead of ``category'', they are all synonyms.   More recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions \cite{trohidis,yang11},  scene or image categorization \cite{dimou,su11}, protein and gene function prediction \cite{pandey,valentini11} or medical diagnosis \cite{taylor10}.  Although each instance has a given set of associated labels, and they are assigned as a whole, the normal approach to solve this problem is just  ignore this fact and concentrate in obtaining good solutions to the individual binary problems (i.	score:415
What are some good, active blogs for computational linguistics or NLP?	 	Artificial Neural Networks (or Neural Nets (NN)) are a powerful and versatile machine learning algorithm. They act as a universal function approximator, and can be used in classification and regression problems\cite{goodfellow,bishop,rojas}. However, despite their successes, a problem that persists is long training times of the NN. This paper addresses that problem by proposing a different parametrization of the hyperplanes used in the algorithm.   This paper demonstrates the new approach on five autoencoder examples. It allows the training to proceed much more rapidly, using in some cases only 1/8-th the number of epochs of the existing approach to reach a given training error value. Results are given for individual runs, averaged runs, and the "epoch speedup". Also, this different parametrization is more intuitive, making it easier to understand how the parameters work, and thus provides insight into how one might best initialize a NN.	 	Artificial Neural Networks (or Neural Nets (NN)) are a powerful and versatile machine learning algorithm. They act as a universal function approximator, and can be used in classification and regression problems\cite{goodfellow,bishop,rojas}. However, despite their successes, a problem that persists is long training times of the NN. This paper addresses that problem by proposing a different parametrization of the hyperplanes used in the algorithm.   This paper demonstrates the new approach on five autoencoder examples. It allows the training to proceed much more rapidly, using in some cases only 1/8-th the number of epochs of the existing approach to reach a given training error value. Results are given for individual runs, averaged runs, and the "epoch speedup". Also, this different parametrization is more intuitive, making it easier to understand how the parameters work, and thus provides insight into how one might best initialize a NN.	score:418
What are some good, active blogs for computational linguistics or NLP?	 These scorers are based on readily available natural language processing (NLP) tools which can produce scores indicating: (a) how formal the generated text is, (b)  whether the generated text is fluent, and most importantly, (c) whether the generated text carries similar  semantics as the input. This framework is trained in multiple iterations, where each iteration is comprised of two phases of \textbf{(i) exploration} and \textbf{(ii) exploitation}.  In the exploration phase, the decoder randomly samples candidate texts for given inputs, and with the help of scorers, automatically produces training data for controllable generation. In the exploitation phase, the encoder-decoder is retrained with the examples thus generated.   For experiments, we prepare a mixture of unlabeled informal texts with low readability grade.	 These scorers are based on readily available natural language processing (NLP) tools which can produce scores indicating: (a) how formal the generated text is, (b)  whether the generated text is fluent, and most importantly, (c) whether the generated text carries similar  semantics as the input. This framework is trained in multiple iterations, where each iteration is comprised of two phases of \textbf{(i) exploration} and \textbf{(ii) exploitation}.  In the exploration phase, the decoder randomly samples candidate texts for given inputs, and with the help of scorers, automatically produces training data for controllable generation. In the exploitation phase, the encoder-decoder is retrained with the examples thus generated.   For experiments, we prepare a mixture of unlabeled informal texts with low readability grade.	score:419
What are some good, active blogs for computational linguistics or NLP?	 Despite its simplicity, BoW works surprisingly well for many tasks~\citep{wang2012baselines}. However, by treating words and phrases as unique and discrete symbols, BoW often fails to capture the similarity between words or phrases and also suffers from sparsity and high dimensionality.   Recent works on using neural networks to learn distributed vector representations of words have gained great popularity.  The well celebrated Word2Vec~\citep{mikolov2013efficient}, by learning to predict the target word using its neighboring words, maps words of similar meanings to nearby points in the continuous vector space.  The surprisingly simple model has succeeded in generating high-quality word embeddings for tasks such as language modeling, text understanding and machine translation.	 Despite its simplicity, BoW works surprisingly well for many tasks~\citep{wang2012baselines}. However, by treating words and phrases as unique and discrete symbols, BoW often fails to capture the similarity between words or phrases and also suffers from sparsity and high dimensionality.   Recent works on using neural networks to learn distributed vector representations of words have gained great popularity.  The well celebrated Word2Vec~\citep{mikolov2013efficient}, by learning to predict the target word using its neighboring words, maps words of similar meanings to nearby points in the continuous vector space.  The surprisingly simple model has succeeded in generating high-quality word embeddings for tasks such as language modeling, text understanding and machine translation.	score:429
What are some good, active blogs for computational linguistics or NLP?	 However, there exist biases and potential issues in using raw citations to compare the impact of scientific articles without accounting for other factors which may affect citation patterns. These factors include time from publication, journal profile, article type and social network of authors \citep{Bornmann2008}. A well-known and highly relevant factor is the variation in citation practices among different disciplines \citep{Garfield1979}.  Articles in certain disciplines (e.g. Social Science and Mathematics) are typically much less cited than others (e.g. Molecular Biology and Immunology) and comparing articles using raw citation counts would be inappropriate. To address this issue, different procedures of normalizing the citation counts with respect to some reference standard have been proposed \citep[e.	 However, there exist biases and potential issues in using raw citations to compare the impact of scientific articles without accounting for other factors which may affect citation patterns. These factors include time from publication, journal profile, article type and social network of authors \citep{Bornmann2008}. A well-known and highly relevant factor is the variation in citation practices among different disciplines \citep{Garfield1979}.  Articles in certain disciplines (e.g. Social Science and Mathematics) are typically much less cited than others (e.g. Molecular Biology and Immunology) and comparing articles using raw citation counts would be inappropriate. To address this issue, different procedures of normalizing the citation counts with respect to some reference standard have been proposed \citep[e.	score:429
I am performing convolution in Python involving some really small numbers. Is there a way to do convolution without losing precision?	   Second, existing deep reinforcement learning (RL) models~\cite{silver2016mastering} often require several costly episodes of trial and error to converge, even with a small action space, and our large action space would exacerbate this problem. To efficiently discover all relationships and attributes in a small number of steps, we introduce a novel variation-structured traversal scheme over the action graph which constructs small, adaptive action sets $\Delta_a, \Delta_p, \Delta_c$ for each step based on the current state and historical actions: $\Delta_a$ contains candidate attributes to describe an object; $\Delta_p$ contains candidate predicates for relating a pair of objects; and $\Delta_c$ contains new object instances to mine in the next step.	   Second, existing deep reinforcement learning (RL) models~\cite{silver2016mastering} often require several costly episodes of trial and error to converge, even with a small action space, and our large action space would exacerbate this problem. To efficiently discover all relationships and attributes in a small number of steps, we introduce a novel variation-structured traversal scheme over the action graph which constructs small, adaptive action sets $\Delta_a, \Delta_p, \Delta_c$ for each step based on the current state and historical actions: $\Delta_a$ contains candidate attributes to describe an object; $\Delta_p$ contains candidate predicates for relating a pair of objects; and $\Delta_c$ contains new object instances to mine in the next step.	score:279
I am performing convolution in Python involving some really small numbers. Is there a way to do convolution without losing precision?	    The resulting objective is separately convex in gating and prediction functions. We propose an alternating minimization scheme that is guaranteed to converge since with appropriate choice of loss-functions (for instance, logistic loss), each optimization step amounts to a probabilistic approximation/projection (I-projection/M-projection) onto a probability space. While our method can be recursively applied in multiple stages to successively approximate the adaptive system obtained in the previous stage, thereby refining accuracy-cost trade-off, we observe that on benchmark datasets even a single stage of our method outperforms state-of-art in accuracy-cost performance.	    The resulting objective is separately convex in gating and prediction functions. We propose an alternating minimization scheme that is guaranteed to converge since with appropriate choice of loss-functions (for instance, logistic loss), each optimization step amounts to a probabilistic approximation/projection (I-projection/M-projection) onto a probability space. While our method can be recursively applied in multiple stages to successively approximate the adaptive system obtained in the previous stage, thereby refining accuracy-cost trade-off, we observe that on benchmark datasets even a single stage of our method outperforms state-of-art in accuracy-cost performance.	score:291
I am performing convolution in Python involving some really small numbers. Is there a way to do convolution without losing precision?	 Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hidden-units required to achieve a particular performance. Our close to state-of-the-art results for MNIST and NORB suggest that the ease of use and accuracy of the ELM algorithm for designing a single-hidden-layer neural network classifier should cause it to be given greater consideration either as a standalone method for simpler problems, or as the final classification stage in deep neural networks applied to more difficult problems.  	The current renaissance in the field of neural networks is a direct result of the success of various types of deep network in tackling difficult classification and regression problems on large datasets.  It may be said to have been initiated by the development of Convolutional Neural Networks (CNN) by LeCun and colleagues in the late 1990s~\cite{LeCun.	 Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hidden-units required to achieve a particular performance. Our close to state-of-the-art results for MNIST and NORB suggest that the ease of use and accuracy of the ELM algorithm for designing a single-hidden-layer neural network classifier should cause it to be given greater consideration either as a standalone method for simpler problems, or as the final classification stage in deep neural networks applied to more difficult problems.  	The current renaissance in the field of neural networks is a direct result of the success of various types of deep network in tackling difficult classification and regression problems on large datasets.  It may be said to have been initiated by the development of Convolutional Neural Networks (CNN) by LeCun and colleagues in the late 1990s~\cite{LeCun.	score:296
I am performing convolution in Python involving some really small numbers. Is there a way to do convolution without losing precision?	  We find that \cpmetric is able to learn the metric function with high accuracy, outperforming existing approximation algorithms on both the regression and classification tasks using less computation time.  This increased performance over existing direct approximation algorithms persists even when \cpmetric is trained with only a small number of samples compared to the dimension of the solution space, indicating the network generalizes well.   	Preferences are central to individual and group decision making by both computer systems and humans.  Due to this central role in decision making the study of representing \cite{RVW11a}, learning \cite{FuHu10a}, and reasoning \cite{DHKP11a,PTV15a} with preferences is a focus of study within computer science and in many other disciplines including psychology and sociology \cite{goldsmith2009preference}.	  We find that \cpmetric is able to learn the metric function with high accuracy, outperforming existing approximation algorithms on both the regression and classification tasks using less computation time.  This increased performance over existing direct approximation algorithms persists even when \cpmetric is trained with only a small number of samples compared to the dimension of the solution space, indicating the network generalizes well.   	Preferences are central to individual and group decision making by both computer systems and humans.  Due to this central role in decision making the study of representing \cite{RVW11a}, learning \cite{FuHu10a}, and reasoning \cite{DHKP11a,PTV15a} with preferences is a focus of study within computer science and in many other disciplines including psychology and sociology \cite{goldsmith2009preference}.	score:304
I am performing convolution in Python involving some really small numbers. Is there a way to do convolution without losing precision?	 We seek to encode the dynamics of the system into a loss function, such that the learning algorithm can learn to produce correct solutions to the future state of the system without having to observe any labeled data. We develop a convolutional kernel which encodes the constraint that must be satisfied by any steady-state solution to the heat flow problem, and we use this kernel to determine the loss function.  By seeking to minimize this loss, the network learns to satisfy the differential equations of heat transport, effectively learning the underlying physics of the system despite never explicitly being shown the outcomes for any given initial condition.  While the physical system examined throughout this paper is 2-D heat transport, the methods developed are extremely general and can be applied to any system defined by partial differential equations and theoretically capable of being solved by the finite difference method (even if it is not practical to do so).	 We seek to encode the dynamics of the system into a loss function, such that the learning algorithm can learn to produce correct solutions to the future state of the system without having to observe any labeled data. We develop a convolutional kernel which encodes the constraint that must be satisfied by any steady-state solution to the heat flow problem, and we use this kernel to determine the loss function.  By seeking to minimize this loss, the network learns to satisfy the differential equations of heat transport, effectively learning the underlying physics of the system despite never explicitly being shown the outcomes for any given initial condition.  While the physical system examined throughout this paper is 2-D heat transport, the methods developed are extremely general and can be applied to any system defined by partial differential equations and theoretically capable of being solved by the finite difference method (even if it is not practical to do so).	score:305
What kind of machine learning models are used to predict ratings?	 This is a preliminary study that emphasizes the fact that in many applications where  predictive machine learning is at the service of competition, much can be gained from practical (back-testing) optimization of the model compared to static prediction improvement.  	\noindent Machine learning is often used in competitive scenarios: Participants learn and fit static models, and those models compete in a shared platform.  The common assumption is that in order to win a competition one has to have the best learning model, i.e., the model with the smallest out-sample error. Is that necessarily true? The main theme of this paper is to raise this important question and propose a theoretical model to answer it. \\  \noindent In competitive machine learning, every player has a predictive model for the target, and the closest, the fastest or the most reliable model wins.	 Machine learning is often used in competitive scenarios: Participants learn and fit static models, and those models compete in a shared platform. The common assumption is that in order to win a competition one has to have the best predictive model, i.e., the model with the smallest out-sample error. Is that necessarily true?  Does the best theoretical predictive model for a target always yield the best reward in a competition?  If not, can one take the best model and purposefully change it into a theoretically inferior model which in practice results in a higher competitive edge? How does that modification look like? And finally, if all participants modify their prediction models towards the  best practical performance, who benefits the most? players with  inferior models, or those with theoretical superiority?	score:300
What kind of machine learning models are used to predict ratings?	 	Many problems in machine learning can be seen as structured output prediction tasks, where one would like to predict a set of labels with rich internal structure \cite{struct_pred_book}. This general framework has proved useful for a wide range of applications from computer vision, natural language processing, computational biology, and others. In order to achieve high prediction accuracy, the parameters of structured predictors are learned from training data.  One of the most effective and commonly used approaches for this supervised learning task is \emph{Structural SVM}, a method that generalizes binary SVM to structured outputs \cite{tsochantaridis2004support}. Since the structured error is non-convex, \citet{tsochantaridis2004support} propose to replace it with a convex surrogate loss function. They formulate two such surrogates, known as \emph{margin} and \emph{slack rescaling}.	 	Many problems in machine learning can be seen as structured output prediction tasks, where one would like to predict a set of labels with rich internal structure \cite{struct_pred_book}. This general framework has proved useful for a wide range of applications from computer vision, natural language processing, computational biology, and others. In order to achieve high prediction accuracy, the parameters of structured predictors are learned from training data.  One of the most effective and commonly used approaches for this supervised learning task is \emph{Structural SVM}, a method that generalizes binary SVM to structured outputs \cite{tsochantaridis2004support}. Since the structured error is non-convex, \citet{tsochantaridis2004support} propose to replace it with a convex surrogate loss function. They formulate two such surrogates, known as \emph{margin} and \emph{slack rescaling}.	score:310
What kind of machine learning models are used to predict ratings?	 Machine learning is often used in competitive scenarios: Participants learn and fit static models, and those models compete in a shared platform. The common assumption is that in order to win a competition one has to have the best predictive model, i.e., the model with the smallest out-sample error. Is that necessarily true?  Does the best theoretical predictive model for a target always yield the best reward in a competition?  If not, can one take the best model and purposefully change it into a theoretically inferior model which in practice results in a higher competitive edge? How does that modification look like? And finally, if all participants modify their prediction models towards the  best practical performance, who benefits the most? players with  inferior models, or those with theoretical superiority?	 Machine learning is often used in competitive scenarios: Participants learn and fit static models, and those models compete in a shared platform. The common assumption is that in order to win a competition one has to have the best predictive model, i.e., the model with the smallest out-sample error. Is that necessarily true?  Does the best theoretical predictive model for a target always yield the best reward in a competition?  If not, can one take the best model and purposefully change it into a theoretically inferior model which in practice results in a higher competitive edge? How does that modification look like? And finally, if all participants modify their prediction models towards the  best practical performance, who benefits the most? players with  inferior models, or those with theoretical superiority?	score:313
What kind of machine learning models are used to predict ratings?	 A main focus of machine learning  research has been improving the generalization accuracy and efficiency of prediction models. Many models such as SVM, random forest, and deep neural nets have been proposed and achieved great success. However, what emerges as missing in many applications is actionability, i.e., the ability to turn prediction results into actions.  For example, in applications such as customer relationship management, clinical prediction, and advertisement, the users need not only accurate prediction, but also actionable instructions which can transfer an input to a desirable goal (e.g., higher profit repays, lower  morbidity rates, higher ads hit rates). Existing effort in deriving such actionable knowledge is few and limited to simple action models which restricted to only change one attribute for each action.	 A main focus of machine learning  research has been improving the generalization accuracy and efficiency of prediction models. Many models such as SVM, random forest, and deep neural nets have been proposed and achieved great success. However, what emerges as missing in many applications is actionability, i.e., the ability to turn prediction results into actions.  For example, in applications such as customer relationship management, clinical prediction, and advertisement, the users need not only accurate prediction, but also actionable instructions which can transfer an input to a desirable goal (e.g., higher profit repays, lower  morbidity rates, higher ads hit rates). Existing effort in deriving such actionable knowledge is few and limited to simple action models which restricted to only change one attribute for each action.	score:323
What kind of machine learning models are used to predict ratings?	 	As a flexible nonparametric tool, kernel ridge regression (KRR) has gained popularity in many application fields, such as machine learning, and visualization; see e.g., \cite{orsenigo2012kernel}. Besides the estimation of the predictive mean, an exploration of the \textit{predictive variance} is also important for statistical inference. Predictive variances can be used for inference, for example, to build confidence intervals; or to select the most informative data points in active learning.  There are two sources of uncertainty in the predictive variance: the noise in the data and the uncertainty in the estimation of the target function. However, calculating the second uncertainty is challenging in KRR on a large data set, since the computational burden increases dramatically with respect to the size of the training set. For example, for $n$ data points, the time and space complexity of kernel ridge regression (KRR) are of $O(n^3)$ and $O(n^2)$ respectively.	 	As a flexible nonparametric tool, kernel ridge regression (KRR) has gained popularity in many application fields, such as machine learning, and visualization; see e.g., \cite{orsenigo2012kernel}. Besides the estimation of the predictive mean, an exploration of the \textit{predictive variance} is also important for statistical inference. Predictive variances can be used for inference, for example, to build confidence intervals; or to select the most informative data points in active learning.  There are two sources of uncertainty in the predictive variance: the noise in the data and the uncertainty in the estimation of the target function. However, calculating the second uncertainty is challenging in KRR on a large data set, since the computational burden increases dramatically with respect to the size of the training set. For example, for $n$ data points, the time and space complexity of kernel ridge regression (KRR) are of $O(n^3)$ and $O(n^2)$ respectively.	score:331
When will be the next batch of Accenture after november 4th 2016?	 We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. \footnote{The original manuscript has been submitted to ICASSP 2014 conference on November 4, 2013 and it has been rejected due to having content on the reference only 5th page. This version has been slightly edited to reflect the latest experimental results. } 	Unlike feedforward neural networks (FFNN) such as deep neural networks (DNNs), the architecture of recurrent neural networks (RNNs) have cycles feeding the activations from previous time steps as input to the network to make a decision for the current input. The activations from the previous time step are stored in the internal state of the network and they provide indefinite temporal contextual information in contrast to the fixed contextual windows used as inputs in FFNNs.	 We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. \footnote{The original manuscript has been submitted to ICASSP 2014 conference on November 4, 2013 and it has been rejected due to having content on the reference only 5th page. This version has been slightly edited to reflect the latest experimental results. } 	Unlike feedforward neural networks (FFNN) such as deep neural networks (DNNs), the architecture of recurrent neural networks (RNNs) have cycles feeding the activations from previous time steps as input to the network to make a decision for the current input. The activations from the previous time step are stored in the internal state of the network and they provide indefinite temporal contextual information in contrast to the fixed contextual windows used as inputs in FFNNs.	score:470
When will be the next batch of Accenture after november 4th 2016?	 	Survival analysis, also known as time-to-event analysis aims to predict the first time of the occurrence of a stochastic event, conditioned on a set of features. An example in the case of medical data is the time of death or a graft failure after an operation. In cases where the time of event for many samples is missing because the event wasn't observed, this can be framed as a particular type of semi-supervised learning where part of the target values are referred to as right-censored.	 	Survival analysis, also known as time-to-event analysis aims to predict the first time of the occurrence of a stochastic event, conditioned on a set of features. An example in the case of medical data is the time of death or a graft failure after an operation. In cases where the time of event for many samples is missing because the event wasn't observed, this can be framed as a particular type of semi-supervised learning where part of the target values are referred to as right-censored.	score:479
When will be the next batch of Accenture after november 4th 2016?	    	\footnotetext{An earlier version of this paper was published on openreview.net on November 4th, 2016.}  Much of the recent progress in computer vision can be attributed to the availability of large labelled datasets and deep neural networks capable of absorbing large amounts of information. While many practical problems can now be solved, the requirement for big (labelled) data is a fundamentally unsatisfactory state of affairs.  Human beings are able to learn new concepts with very few labels, and mimicking this ability is an important challenge for artificial intelligence research. From an applied perspective, improving the statistical efficiency of deep learning is vital because in many domains (e.g. medical image analysis), acquiring large amounts of labelled data is costly.	    	\footnotetext{An earlier version of this paper was published on openreview.net on November 4th, 2016.}  Much of the recent progress in computer vision can be attributed to the availability of large labelled datasets and deep neural networks capable of absorbing large amounts of information. While many practical problems can now be solved, the requirement for big (labelled) data is a fundamentally unsatisfactory state of affairs.  Human beings are able to learn new concepts with very few labels, and mimicking this ability is an important challenge for artificial intelligence research. From an applied perspective, improving the statistical efficiency of deep learning is vital because in many domains (e.g. medical image analysis), acquiring large amounts of labelled data is costly.	score:480
When will be the next batch of Accenture after november 4th 2016?	 Unlike the usual approach, the devised event-driven platform is able to predict a congestion up to $4$ minutes before it really happens. Proactive decision making can then be established leading to significant improvement of traffic conditions. 	Congestion can be defined as a situation when traffic is moving at speed below the designed capacity of a roadway \cite{Downs2004} or as a state of traffic flow on a transportation facility characterized by high densities and low speeds, relative to some chosen reference state \cite{BovySalomon2002}.  It results of various root causes (e.g. traffic incidents, work zones, weather, special events, physical bottlenecks), often interacting with one another \cite{Downs2004} and induces excess delays, reduced safety, and increased environmental pollution due to stop-and-go behaviour. One approach to tackle congestion could be to increase the capacity of the traffic infrastructure by constructing new roads.	 Unlike the usual approach, the devised event-driven platform is able to predict a congestion up to $4$ minutes before it really happens. Proactive decision making can then be established leading to significant improvement of traffic conditions. 	Congestion can be defined as a situation when traffic is moving at speed below the designed capacity of a roadway \cite{Downs2004} or as a state of traffic flow on a transportation facility characterized by high densities and low speeds, relative to some chosen reference state \cite{BovySalomon2002}.  It results of various root causes (e.g. traffic incidents, work zones, weather, special events, physical bottlenecks), often interacting with one another \cite{Downs2004} and induces excess delays, reduced safety, and increased environmental pollution due to stop-and-go behaviour. One approach to tackle congestion could be to increase the capacity of the traffic infrastructure by constructing new roads.	score:483
When will be the next batch of Accenture after november 4th 2016?	  By incorporating these rules into current health-care a patient can be highlighted as susceptible to a future illness based on past or current illnesses, gender and year of birth. This knowledge has the ability to greatly improve health-care and reduce health-care costs.  	A patient's medical state continuously changes over time, for instance, one day they may be `healthy' and the next they may be `suffering from a cold'.  It is common for medical states to develop gradually over time and/or be dependent on previous medical states.  For example, before developing illness A, many patients may previously have illnesses B and C.  If these temporal associations between illnesses can be learned, they can be used to highlight patients that have illnesses B and C as being susceptible to developing illness A.	  By incorporating these rules into current health-care a patient can be highlighted as susceptible to a future illness based on past or current illnesses, gender and year of birth. This knowledge has the ability to greatly improve health-care and reduce health-care costs.  	A patient's medical state continuously changes over time, for instance, one day they may be `healthy' and the next they may be `suffering from a cold'.  It is common for medical states to develop gradually over time and/or be dependent on previous medical states.  For example, before developing illness A, many patients may previously have illnesses B and C.  If these temporal associations between illnesses can be learned, they can be used to highlight patients that have illnesses B and C as being susceptible to developing illness A.	score:490
When will be the next batch of on boarding in Accenture after november 4th 2016?	 Unlike the usual approach, the devised event-driven platform is able to predict a congestion up to $4$ minutes before it really happens. Proactive decision making can then be established leading to significant improvement of traffic conditions. 	Congestion can be defined as a situation when traffic is moving at speed below the designed capacity of a roadway \cite{Downs2004} or as a state of traffic flow on a transportation facility characterized by high densities and low speeds, relative to some chosen reference state \cite{BovySalomon2002}.  It results of various root causes (e.g. traffic incidents, work zones, weather, special events, physical bottlenecks), often interacting with one another \cite{Downs2004} and induces excess delays, reduced safety, and increased environmental pollution due to stop-and-go behaviour. One approach to tackle congestion could be to increase the capacity of the traffic infrastructure by constructing new roads.	 Unlike the usual approach, the devised event-driven platform is able to predict a congestion up to $4$ minutes before it really happens. Proactive decision making can then be established leading to significant improvement of traffic conditions. 	Congestion can be defined as a situation when traffic is moving at speed below the designed capacity of a roadway \cite{Downs2004} or as a state of traffic flow on a transportation facility characterized by high densities and low speeds, relative to some chosen reference state \cite{BovySalomon2002}.  It results of various root causes (e.g. traffic incidents, work zones, weather, special events, physical bottlenecks), often interacting with one another \cite{Downs2004} and induces excess delays, reduced safety, and increased environmental pollution due to stop-and-go behaviour. One approach to tackle congestion could be to increase the capacity of the traffic infrastructure by constructing new roads.	score:475
When will be the next batch of on boarding in Accenture after november 4th 2016?	 We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. \footnote{The original manuscript has been submitted to ICASSP 2014 conference on November 4, 2013 and it has been rejected due to having content on the reference only 5th page. This version has been slightly edited to reflect the latest experimental results. } 	Unlike feedforward neural networks (FFNN) such as deep neural networks (DNNs), the architecture of recurrent neural networks (RNNs) have cycles feeding the activations from previous time steps as input to the network to make a decision for the current input. The activations from the previous time step are stored in the internal state of the network and they provide indefinite temporal contextual information in contrast to the fixed contextual windows used as inputs in FFNNs.	 We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. \footnote{The original manuscript has been submitted to ICASSP 2014 conference on November 4, 2013 and it has been rejected due to having content on the reference only 5th page. This version has been slightly edited to reflect the latest experimental results. } 	Unlike feedforward neural networks (FFNN) such as deep neural networks (DNNs), the architecture of recurrent neural networks (RNNs) have cycles feeding the activations from previous time steps as input to the network to make a decision for the current input. The activations from the previous time step are stored in the internal state of the network and they provide indefinite temporal contextual information in contrast to the fixed contextual windows used as inputs in FFNNs.	score:488
When will be the next batch of on boarding in Accenture after november 4th 2016?	       \item In 2016, Amazon.com used software to determine the areas of the US which it would offer free same-day delivery to\footnote{\url{http://www.businessinsider.com/how-algorithms-can-be-racist-2016-4}}. It turned out that the software inadvertently prevented minority neighbourhoods from participating in the program, often when every surrounding neighbourhood was allowed.	       \item In 2016, Amazon.com used software to determine the areas of the US which it would offer free same-day delivery to\footnote{\url{http://www.businessinsider.com/how-algorithms-can-be-racist-2016-4}}. It turned out that the software inadvertently prevented minority neighbourhoods from participating in the program, often when every surrounding neighbourhood was allowed.	score:489
When will be the next batch of on boarding in Accenture after november 4th 2016?	    	\footnotetext{An earlier version of this paper was published on openreview.net on November 4th, 2016.}  Much of the recent progress in computer vision can be attributed to the availability of large labelled datasets and deep neural networks capable of absorbing large amounts of information. While many practical problems can now be solved, the requirement for big (labelled) data is a fundamentally unsatisfactory state of affairs.  Human beings are able to learn new concepts with very few labels, and mimicking this ability is an important challenge for artificial intelligence research. From an applied perspective, improving the statistical efficiency of deep learning is vital because in many domains (e.g. medical image analysis), acquiring large amounts of labelled data is costly.	    	\footnotetext{An earlier version of this paper was published on openreview.net on November 4th, 2016.}  Much of the recent progress in computer vision can be attributed to the availability of large labelled datasets and deep neural networks capable of absorbing large amounts of information. While many practical problems can now be solved, the requirement for big (labelled) data is a fundamentally unsatisfactory state of affairs.  Human beings are able to learn new concepts with very few labels, and mimicking this ability is an important challenge for artificial intelligence research. From an applied perspective, improving the statistical efficiency of deep learning is vital because in many domains (e.g. medical image analysis), acquiring large amounts of labelled data is costly.	score:492
When will be the next batch of on boarding in Accenture after november 4th 2016?	 In this case, the agent has to maximize its expected return over an indefinite period, possibly infinite. However, it can be desirable to still use time limits in order to diversify the agent's experience. For example, starting from highly diverse states can avoid converging to suboptimal policies that are limited to a fraction of the state space. In Section \ref{sec:time-unlimited}, we show that in order to learn good policies that continue beyond the time limit, it is important to differentiate between the terminations that are due to time limits and those from the environment.  Specifically, for bootstrapping methods, we argue to bootstrap at states where termination is due to time limits, or more generally any other causes than the environmental ones. We refer to this approach as \textit{partial-episode bootstrapping} (PEB) and demonstrate that it can significantly improve the performance of agents.  We evaluate the impact of these considerations on a range of novel and popular benchmark domains using tabular Q-learning and Proximal Policy Optimization (PPO), a modern deep reinforcement learning \citep{Arulkumaran:2017drlbrief, Henderson:2017matters} algorithm which has recently been used to achieve state-of-the-art performance in many domains \citep{Schulman:2017ppo, Heess:2017emergence}.	 Specifically, for bootstrapping methods, we argue to bootstrap at states where termination is due to time limits, or more generally any other causes than the environmental ones. We refer to this approach as \textit{partial-episode bootstrapping} (PEB) and demonstrate that it can significantly improve the performance of agents.  We evaluate the impact of these considerations on a range of novel and popular benchmark domains using tabular Q-learning and Proximal Policy Optimization (PPO), a modern deep reinforcement learning \citep{Arulkumaran:2017drlbrief, Henderson:2017matters} algorithm which has recently been used to achieve state-of-the-art performance in many domains \citep{Schulman:2017ppo, Heess:2017emergence}.	score:498
How can I learn machine learning better?	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	score:393
How can I learn machine learning better?	 Our accelerated algorithm  performs much better than the original regularized sub-sampled Newton in experiments, which validates our theory empirically. Besides, the accelerated regularized sub-sampled Newton has good performance comparable to or even better than classical algorithms.  	Optimization has become an increasingly popular issue in machine learning.  Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.	 Our accelerated algorithm  performs much better than the original regularized sub-sampled Newton in experiments, which validates our theory empirically. Besides, the accelerated regularized sub-sampled Newton has good performance comparable to or even better than classical algorithms.  	Optimization has become an increasingly popular issue in machine learning.  Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.	score:399
How can I learn machine learning better?	  The main adopted principles in this work are based on the belief that an intelligent machine should be able to learn simple patterns from a few number of examples, and to use the learned models in learning more complex ones. It should also be able to interact with human users, and thus we need a communication algorithm to make teaching machines easier and we also need a technique for tuning hyperparameters that can be easily done by non-experts.    A variety of recent works have tried to learn simple patterns (algorithms). For example, \citet{zaremba:2015} used an RNN-based controller, trained using Q-learning, that interacts with the environment through a set of interfaces selected manually for each task. However, they failed to find one controller suitable for all tasks, and the learned models overfit the length of input in some tasks.	  The main adopted principles in this work are based on the belief that an intelligent machine should be able to learn simple patterns from a few number of examples, and to use the learned models in learning more complex ones. It should also be able to interact with human users, and thus we need a communication algorithm to make teaching machines easier and we also need a technique for tuning hyperparameters that can be easily done by non-experts.    A variety of recent works have tried to learn simple patterns (algorithms). For example, \citet{zaremba:2015} used an RNN-based controller, trained using Q-learning, that interacts with the environment through a set of interfaces selected manually for each task. However, they failed to find one controller suitable for all tasks, and the learned models overfit the length of input in some tasks.	score:412
How can I learn machine learning better?	 Many classical problems in machine learning and statistics can be analysed in this framework. On the machine learning side, multiple instance learning \citep{dietterich97solving,ray01multiple,dooly02multiple} can be thought of in this way,  where each instance in a labeled bag is an i.i.d.\ (independent identically distributed) sample from a distribution.  On the statistical side, tasks might include point estimation of statistics on a  distribution without closed form analytical expressions (e.g., its entropy or a hyperparameter).  \tb{Intuitive description of our goal:} Let us start with a somewhat informal definition of the distribution regression problem and an intuitive phrasing of  our goals. Suppose that our data consist of $\b{z}=\{(x_i,y_i)\}_{i=1}^l$, where $x_i$ is a probability distribution, $y_i$ is its label (in the simplest case $y_i\in \R$ or $y_i\in\R^d$) and each $(x_i,y_i)$  pair is i.	 Many classical problems in machine learning and statistics can be analysed in this framework. On the machine learning side, multiple instance learning \citep{dietterich97solving,ray01multiple,dooly02multiple} can be thought of in this way,  where each instance in a labeled bag is an i.i.d.\ (independent identically distributed) sample from a distribution.  On the statistical side, tasks might include point estimation of statistics on a  distribution without closed form analytical expressions (e.g., its entropy or a hyperparameter).  \tb{Intuitive description of our goal:} Let us start with a somewhat informal definition of the distribution regression problem and an intuitive phrasing of  our goals. Suppose that our data consist of $\b{z}=\{(x_i,y_i)\}_{i=1}^l$, where $x_i$ is a probability distribution, $y_i$ is its label (in the simplest case $y_i\in \R$ or $y_i\in\R^d$) and each $(x_i,y_i)$  pair is i.	score:413
How can I learn machine learning better?	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.    \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.	score:415
What is convergence in finite element analysis?	   The mathematical theory of steerable representations reveals a \emph{type system} in which any steerable representation is a composition of elementary \emph{feature types}, each one associated with a particular kind of symmetry.   We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively.     	\footnotetext{An earlier version of this paper was published on openreview.net on November 4th, 2016.}  Much of the recent progress in computer vision can be attributed to the availability of large labelled datasets and deep neural networks capable of absorbing large amounts of information. While many practical problems can now be solved, the requirement for big (labelled) data is a fundamentally unsatisfactory state of affairs.	  We show that any steerable representation is a composition of elementary \emph{feature types}. Each elementary feature can be steered independently of the others, and captures a distinct characteristic of the input that has an invariant or ``objective'' meaning. This doctrine of ``observer-independent quantities'' was put forward by \cite[ch.~1.4]{Weyl1939} and is used throughout physics.  It has been applied to vision and representation learning by \cite{Kanatani1990, Cohen2013}.  The mentioned type system puts constraints on the network weights and architecture. Specifically, since an equivariant filter bank is required to map given input feature types to given output feature types, the number of parameters required by such a filter bank is reduced.	score:381
What is convergence in finite element analysis?	 In summary, the contributions of this paper are as follows: \begin{itemize}   \item \textbf{Analyses and formalizations of shape features for smart meter readings:}  Rigorous analyses and definitions of shape features are presented by exploring the prior knowledge with respect to individual device consumption patterns.   We use the results of the analyses to guide the learning of basis functions, and show how it can help improve the disaggregation performance.     \item \textbf{Design of a Bayesian discriminative sparse coding model:}  A Bayesian sparse coding with Laplace prior is learned for each device, and then we combine these trained models together to achieve the disaggregation dictionaries.  The discriminative capability of the disaggregation dictionaries is improved by adapting the bases to the aggregated data.	 In summary, the contributions of this paper are as follows: \begin{itemize}   \item \textbf{Analyses and formalizations of shape features for smart meter readings:}  Rigorous analyses and definitions of shape features are presented by exploring the prior knowledge with respect to individual device consumption patterns.   We use the results of the analyses to guide the learning of basis functions, and show how it can help improve the disaggregation performance.     \item \textbf{Design of a Bayesian discriminative sparse coding model:}  A Bayesian sparse coding with Laplace prior is learned for each device, and then we combine these trained models together to achieve the disaggregation dictionaries.  The discriminative capability of the disaggregation dictionaries is improved by adapting the bases to the aggregated data.	score:384
What is convergence in finite element analysis?	 We provide convergence analyses, and compare their performance against benchmark algorithms in the literature. The computational experiment shows that the proposed algorithms consistently perform best. 	Principal component analysis (PCA) is a technique to find orthonormal vectors, which are a linear combination of the attributes of the data, that explain the variance structure of the data \cite{Jolliffe:2002}.  Since a few orthonormal vectors usually explain most of the variance, PCA is often used to reduce dimension of the data by keeping only a few of the orthonormal vectors. These orthonormal vectors are called \textit{principal components} (PCs).   For dimensionality reduction, we are given target dimension p, the number of PCs. To measure accuracy, given $p$ principal components, first, the original data is projected into the lower dimension using the PCs.	 We provide convergence analyses, and compare their performance against benchmark algorithms in the literature. The computational experiment shows that the proposed algorithms consistently perform best. 	Principal component analysis (PCA) is a technique to find orthonormal vectors, which are a linear combination of the attributes of the data, that explain the variance structure of the data \cite{Jolliffe:2002}.  Since a few orthonormal vectors usually explain most of the variance, PCA is often used to reduce dimension of the data by keeping only a few of the orthonormal vectors. These orthonormal vectors are called \textit{principal components} (PCs).   For dimensionality reduction, we are given target dimension p, the number of PCs. To measure accuracy, given $p$ principal components, first, the original data is projected into the lower dimension using the PCs.	score:385
What is convergence in finite element analysis?	 (c) How does the distributed implementation fare in terms of computational complexity? and (d) What is the rate of convergence in the case of iterative algorithms?   In this paper, we focus on solving a system of linear equations in a distributed fashion, which is one of the most fundamental problems in numerical computation, and lies at the heart of many algorithms in  engineering and the sciences.  In particular, we consider the setting in which a taskmaster intends to solve a large-scale system of equations with the help of a set of computing machines/cores (Figure~\ref{fig:1}).  This problem can in general be cast as an optimization problem with a cost function that is separable in the data (but not in the variables) \footnote{Solving a system of linear equations, $Ax=b$, can be set up as the optimization problem $\min_x\|Ax-b\|^2=\min_x \sum_i \|(Ax)_i-b_i\|^2$.	 (c) How does the distributed implementation fare in terms of computational complexity? and (d) What is the rate of convergence in the case of iterative algorithms?   In this paper, we focus on solving a system of linear equations in a distributed fashion, which is one of the most fundamental problems in numerical computation, and lies at the heart of many algorithms in  engineering and the sciences.  In particular, we consider the setting in which a taskmaster intends to solve a large-scale system of equations with the help of a set of computing machines/cores (Figure~\ref{fig:1}).  This problem can in general be cast as an optimization problem with a cost function that is separable in the data (but not in the variables) \footnote{Solving a system of linear equations, $Ax=b$, can be set up as the optimization problem $\min_x\|Ax-b\|^2=\min_x \sum_i \|(Ax)_i-b_i\|^2$.	score:386
What is convergence in finite element analysis?	 The method relies on a minimization problem, which is carefully motivated in Section \ref{sec:problem}. Therein, we also discuss the question of what is a suitable analysis operator for image reconstruction, and how to antagonize overfitting the operator to a subset of the training samples. An efficient geometric conjugate gradient method on the so-called oblique manifold is proposed in Section \ref{sec:manicg} for learning the analysis operator.  Furthermore, in Section \ref{sec:image_rec} we explain how to apply the local patch based analysis operator to achieve global reconstruction results. Section \ref{sec:experments} sheds some light on the influence of the parameters required by GOAL and how to select them, and compares our method to other analysis operator learning techniques. The quality of the operator learned by GOAL on natural image patches is further investigated in terms of image denoising, inpainting, and single image super-resolution.	 The method relies on a minimization problem, which is carefully motivated in Section \ref{sec:problem}. Therein, we also discuss the question of what is a suitable analysis operator for image reconstruction, and how to antagonize overfitting the operator to a subset of the training samples. An efficient geometric conjugate gradient method on the so-called oblique manifold is proposed in Section \ref{sec:manicg} for learning the analysis operator.  Furthermore, in Section \ref{sec:image_rec} we explain how to apply the local patch based analysis operator to achieve global reconstruction results. Section \ref{sec:experments} sheds some light on the influence of the parameters required by GOAL and how to select them, and compares our method to other analysis operator learning techniques. The quality of the operator learned by GOAL on natural image patches is further investigated in terms of image denoising, inpainting, and single image super-resolution.	score:396
I have a GRE score of 168+151. What's my best shot at a college in the US for machine learning/data analytics?	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:481
I have a GRE score of 168+151. What's my best shot at a college in the US for machine learning/data analytics?	 In a real-world data set there is always the possibility, rather high in our opinion, that different features may have different degrees of relevance. Most machine learning algorithms deal with this fact by either selecting or deselecting features in the data preprocessing phase. However, we maintain that even among relevant features there may be different degrees of relevance, and this should be taken into account during the clustering process.   With over 50 years of history, K-Means is arguably the most popular partitional clustering algorithm there is. The first K-Means based clustering algorithm to compute feature weights was designed just over 30 years ago. Various such algorithms have been designed since but there has not been, to our knowledge, a survey integrating empirical evidence of cluster recovery ability, common flaws, and possible directions for future research.	 In a real-world data set there is always the possibility, rather high in our opinion, that different features may have different degrees of relevance. Most machine learning algorithms deal with this fact by either selecting or deselecting features in the data preprocessing phase. However, we maintain that even among relevant features there may be different degrees of relevance, and this should be taken into account during the clustering process.   With over 50 years of history, K-Means is arguably the most popular partitional clustering algorithm there is. The first K-Means based clustering algorithm to compute feature weights was designed just over 30 years ago. Various such algorithms have been designed since but there has not been, to our knowledge, a survey integrating empirical evidence of cluster recovery ability, common flaws, and possible directions for future research.	score:483
I have a GRE score of 168+151. What's my best shot at a college in the US for machine learning/data analytics?	 Crowdsourcing~\cite{Lease11,GuoWSH14} has been considered to acquire cheap labels from general-level human intelligence. However, current crowdsourcing mechanisms can still be applied to relatively simple and well-defined tasks, and it is still a challenge for applying machine learning to the labels for more diverse and more specific data~\cite{Lease11}.  \end{itemize}     We use some specific open problems from natural language processing and computer vision to further illustrate the above problems.  {\bf Example 1: text semantics and topics.}   Text semantic similarity/relatedness is one of the fundamental problem in natural language processing. Regarding to different levels of text span, e.g., word, phrase, sentence, or document, there are different ways to compute the similarity/relatedness~\cite{jurgens-pilehvar:2015:EMNLP-Tutorials}.	 Crowdsourcing~\cite{Lease11,GuoWSH14} has been considered to acquire cheap labels from general-level human intelligence. However, current crowdsourcing mechanisms can still be applied to relatively simple and well-defined tasks, and it is still a challenge for applying machine learning to the labels for more diverse and more specific data~\cite{Lease11}.  \end{itemize}     We use some specific open problems from natural language processing and computer vision to further illustrate the above problems.  {\bf Example 1: text semantics and topics.}   Text semantic similarity/relatedness is one of the fundamental problem in natural language processing. Regarding to different levels of text span, e.g., word, phrase, sentence, or document, there are different ways to compute the similarity/relatedness~\cite{jurgens-pilehvar:2015:EMNLP-Tutorials}.	score:488
I have a GRE score of 168+151. What's my best shot at a college in the US for machine learning/data analytics?	  Machine-learning methods that can learn from sequences, such as LSTMs \ or recurrent neural nets \cite{williams_gradient-based_1995,hochreiter_gradient_2001}, offer more power.  It is possible to traverse the local neighborhood of a node in a graph in some order (pre-, post-, or in-order), and encode the neighborhood in a sequence of features complete with markers to denote edge traversals, and then feed this sequence to an LSTM.	  Machine-learning methods that can learn from sequences, such as LSTMs \ or recurrent neural nets \cite{williams_gradient-based_1995,hochreiter_gradient_2001}, offer more power.  It is possible to traverse the local neighborhood of a node in a graph in some order (pre-, post-, or in-order), and encode the neighborhood in a sequence of features complete with markers to denote edge traversals, and then feed this sequence to an LSTM.	score:493
I have a GRE score of 168+151. What's my best shot at a college in the US for machine learning/data analytics?	90\%$ accuracy without the coupler detector and $91.10\%$ with it. Comparing to the previous version the new classifier has optimized algorithms for a shield and a trailer couplers detection, a correlational detector and an updated fusion method for binary detectors aggregation.  Creating rules of this type is a painstaking job requiring creative approach. It would be interesting to develop an automatic method where a machine learning algorithm could replace a human expert. This approach potentially could also produce a higher classification quality. Therefore, in this paper we solve the problem of creating a method for automating an AVC synthesis and minimizing a human involvement.	90\%$ accuracy without the coupler detector and $91.10\%$ with it. Comparing to the previous version the new classifier has optimized algorithms for a shield and a trailer couplers detection, a correlational detector and an updated fusion method for binary detectors aggregation.  Creating rules of this type is a painstaking job requiring creative approach. It would be interesting to develop an automatic method where a machine learning algorithm could replace a human expert. This approach potentially could also produce a higher classification quality. Therefore, in this paper we solve the problem of creating a method for automating an AVC synthesis and minimizing a human involvement.	score:493
By definition, what is a determinant of a matrix?	  In many applications, we have to recover a matrix from only a small number of observed entries, for example collaborative filtering for recommender systems. This problem is often called matrix completion, where missing entries or outliers are presented at arbitrary location in the measurement matrix. Matrix completion has been used in a wide range of problems such as collaborative filtering \cite{candes:emc, chen:lrmr}, structure-from-motion \cite{eriksson:lrma, zheng:rma}, click prediction \cite{yu:cp}, tag recommendation \cite{wang:at}, and face reconstruction \cite{meng:cwm}.	  In many applications, we have to recover a matrix from only a small number of observed entries, for example collaborative filtering for recommender systems. This problem is often called matrix completion, where missing entries or outliers are presented at arbitrary location in the measurement matrix. Matrix completion has been used in a wide range of problems such as collaborative filtering \cite{candes:emc, chen:lrmr}, structure-from-motion \cite{eriksson:lrma, zheng:rma}, click prediction \cite{yu:cp}, tag recommendation \cite{wang:at}, and face reconstruction \cite{meng:cwm}.	score:382
By definition, what is a determinant of a matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	 The method proposed in \cite{jain2013} is based on non-convex matrix factorization. In \cite{fithian2018}, the authors consider a general framework for reduced-rank modeling of matrix-valued data. They use a generalized weighted nuclear norm penalty where the matrix is multiplied by positive semidefinite matrices $P$ and $Q$ which depend on the matrix of features.  In \cite{agarwal2011}, the authors introduce a per-item user covariate logistic regression model augmenting  with user-specific random effects. Their approach is based on a multilevel hierarchical model.  In the case of the heterogeneous data coming from different sources, these approaches can be applied  for recovering each  source separately. In contrast, our approach aims at collecting all the available information in a single matrix which results in faster rates of convergence.	score:394
By definition, what is a determinant of a matrix?	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	score:394
By definition, what is a determinant of a matrix?	   One can attempt to solve  the robust tensor problem  in \eqref{eqn:robust-def}  using matrix methods. In other words, robust matrix PCA can be applied either to  each matrix slice of the tensor, or to the matrix obtained by flattening the tensor. However, such matrix methods ignore the  tensor algebraic constraints or the CP  rank constraints, which differ from the matrix rank constraints.  There are however a number of challenges to incorporating the tensor CP rank   constraints. Enforcing a given  tensor rank   is NP-hard~\cite{hillar2013most}, unlike the matrix case, where low rank projections can be computed efficiently. Moreover, finding the best convex relaxation of the tensor CP rank is also NP-hard~\cite{hillar2013most}, unlike   the matrix case, where the convex relaxation of the rank, \viz the nuclear norm,  can be computed efficiently.	   One can attempt to solve  the robust tensor problem  in \eqref{eqn:robust-def}  using matrix methods. In other words, robust matrix PCA can be applied either to  each matrix slice of the tensor, or to the matrix obtained by flattening the tensor. However, such matrix methods ignore the  tensor algebraic constraints or the CP  rank constraints, which differ from the matrix rank constraints.  There are however a number of challenges to incorporating the tensor CP rank   constraints. Enforcing a given  tensor rank   is NP-hard~\cite{hillar2013most}, unlike the matrix case, where low rank projections can be computed efficiently. Moreover, finding the best convex relaxation of the tensor CP rank is also NP-hard~\cite{hillar2013most}, unlike   the matrix case, where the convex relaxation of the rank, \viz the nuclear norm,  can be computed efficiently.	score:394
By definition, what is a determinant of a matrix?	  Of particular interest here is when that data-dependent sampling process selects rows or columns from the input matrix according to a probability  distribution that depends on the empirical statistical leverage scores of  that matrix. This recently-developed approach of \emph{algorithmic leveraging} has been  applied to matrix-based problems that are of interest in large-scale data  analysis, e. g., least-squares approximation~\cite{DMM06,DMMS07_FastL2_NM10},  least absolute deviations regression~\cite{CDMMMW12_TR,MM12_TR}, and  low-rank matrix approximation~\cite{CUR_PNAS,CW12sparse_TR}.  Typically, the leverage scores are computed approximately~\cite{DMMW12_JMLR,CDMMMW12_TR}, or otherwise a random  projection~\cite{AC10,CDMMMW12_TR} is used to precondition by approximately  uniformizing them~\cite{DMMS07_FastL2_NM10,AMT10,MSM11_TR}.	  Of particular interest here is when that data-dependent sampling process selects rows or columns from the input matrix according to a probability  distribution that depends on the empirical statistical leverage scores of  that matrix. This recently-developed approach of \emph{algorithmic leveraging} has been  applied to matrix-based problems that are of interest in large-scale data  analysis, e. g., least-squares approximation~\cite{DMM06,DMMS07_FastL2_NM10},  least absolute deviations regression~\cite{CDMMMW12_TR,MM12_TR}, and  low-rank matrix approximation~\cite{CUR_PNAS,CW12sparse_TR}.  Typically, the leverage scores are computed approximately~\cite{DMMW12_JMLR,CDMMMW12_TR}, or otherwise a random  projection~\cite{AC10,CDMMMW12_TR} is used to precondition by approximately  uniformizing them~\cite{DMMS07_FastL2_NM10,AMT10,MSM11_TR}.	score:396
What is the determinant of a one-by-one matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	 $1$-bit matrix completion was first studied in~\cite{davenport14}, where the observed entries are assumed to be sampled uniformly at random. This problem was also studied among others by~\citep{cai2013jmlr-onebitmaxnorm,klopp2015EJS-adaptive-onebit,alquier2017}.  Matrix completion with exponential family noise (for a single matrix) was previously considered in~\cite{pmlr-v40-lafond15} and~\cite{gunasekar2014jmlr}.   In these papers authors assume sampling with replacement where there can be multiple observations for the same entry.  In the present paper, we consider more natural setting for matrix completion where each entry may be observed at most once. Our result improves the known results on $1$-bit matrix completion  and on matrix completion with exponential family noise.	score:306
What is the determinant of a one-by-one matrix?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:344
What is the determinant of a one-by-one matrix?	 	Suppose we observe a subset of the entries of an unknown low-rank matrix~$M$, can we recover the matrix~$M$ knowing this subset alone? This problem, called Matrix Completion, is of fundamental interest in a number of fields including statistics, machine learning, signal processing and theoretical computer science. It is widely applicable to the design of recommender systems as popularized by the famous Netflix Prize.   We are interested in understanding the compuational complexity of Matrix Completion.  Much of the theory of Matrix Completion revolves around a beautiful line of positive results. These results show that under certain assumptions there is a natural semidefinite relaxation that solves the problem efficiently even if the number of visible entries is asymptotically much smaller than the total number of entries~\cite{CandesR09,CandesT10,Recht11}.	 	Suppose we observe a subset of the entries of an unknown low-rank matrix~$M$, can we recover the matrix~$M$ knowing this subset alone? This problem, called Matrix Completion, is of fundamental interest in a number of fields including statistics, machine learning, signal processing and theoretical computer science. It is widely applicable to the design of recommender systems as popularized by the famous Netflix Prize.   We are interested in understanding the compuational complexity of Matrix Completion.  Much of the theory of Matrix Completion revolves around a beautiful line of positive results. These results show that under certain assumptions there is a natural semidefinite relaxation that solves the problem efficiently even if the number of visible entries is asymptotically much smaller than the total number of entries~\cite{CandesR09,CandesT10,Recht11}.	score:360
What is the determinant of a one-by-one matrix?	   One can attempt to solve  the robust tensor problem  in \eqref{eqn:robust-def}  using matrix methods. In other words, robust matrix PCA can be applied either to  each matrix slice of the tensor, or to the matrix obtained by flattening the tensor. However, such matrix methods ignore the  tensor algebraic constraints or the CP  rank constraints, which differ from the matrix rank constraints.  There are however a number of challenges to incorporating the tensor CP rank   constraints. Enforcing a given  tensor rank   is NP-hard~\cite{hillar2013most}, unlike the matrix case, where low rank projections can be computed efficiently. Moreover, finding the best convex relaxation of the tensor CP rank is also NP-hard~\cite{hillar2013most}, unlike   the matrix case, where the convex relaxation of the rank, \viz the nuclear norm,  can be computed efficiently.	   One can attempt to solve  the robust tensor problem  in \eqref{eqn:robust-def}  using matrix methods. In other words, robust matrix PCA can be applied either to  each matrix slice of the tensor, or to the matrix obtained by flattening the tensor. However, such matrix methods ignore the  tensor algebraic constraints or the CP  rank constraints, which differ from the matrix rank constraints.  There are however a number of challenges to incorporating the tensor CP rank   constraints. Enforcing a given  tensor rank   is NP-hard~\cite{hillar2013most}, unlike the matrix case, where low rank projections can be computed efficiently. Moreover, finding the best convex relaxation of the tensor CP rank is also NP-hard~\cite{hillar2013most}, unlike   the matrix case, where the convex relaxation of the rank, \viz the nuclear norm,  can be computed efficiently.	score:368
What is the determinant of a one-by-one matrix?	    This work is inspired by \cite{charact}, where the analysis on Grassmannian manifold is proposed for a single-view matrix. Specifically, in \cite{charact}  a novel approach is proposed to consider the rank factorization of a matrix and to treat each observed entry as a polynomial in terms of the entries of the components of the rank factorization.  Then, under the genericity assumption, the algebraic independence among the mentioned polynomials is studied. In this paper, we consider the low-Tucker-rank tensor and follow the general approach that is similar to that in \cite{charact}. We mention some of the main differences: (i) geometry of the manifold, (ii) the equivalence class for the core tensor and consequently (iii) the canonical core tensor, (iv) structure of the polynomials, etc.	    This work is inspired by \cite{charact}, where the analysis on Grassmannian manifold is proposed for a single-view matrix. Specifically, in \cite{charact}  a novel approach is proposed to consider the rank factorization of a matrix and to treat each observed entry as a polynomial in terms of the entries of the components of the rank factorization.  Then, under the genericity assumption, the algebraic independence among the mentioned polynomials is studied. In this paper, we consider the low-Tucker-rank tensor and follow the general approach that is similar to that in \cite{charact}. We mention some of the main differences: (i) geometry of the manifold, (ii) the equivalence class for the core tensor and consequently (iii) the canonical core tensor, (iv) structure of the polynomials, etc.	score:375
Was Khizr Khan paid over 100k by CNN and NBC and 175k by the DNC?	 To overcome the limitations of existing CS cameras, we propose a multi-rate CNN and a synthesizing RNN to improve the trade-off between compression ratio (CR) and spatial-temporal resolution of the reconstructed videos. The experiment results demonstrate that CSVideoNet significantly outperforms state-of-the-art approaches. Without any pre/post-processing, we achieve a 25dB Peak signal-to-noise ratio (PSNR) recovery quality at 100x CR, with a frame rate of 125 fps on a Titan X GPU.  Due to the feedforward and high-data-concurrency natures of CSVideoNet, it can take advantage of GPU acceleration to achieve three orders of magnitude speed-up over conventional iterative-based approaches. We share the source code at https://github.com/PSCLab-ASU/CSVideoNet.  	High-frame-rate cameras are capable of capturing videos at frame rates over 100 frames per second (fps).  These devices were originally developed for research purposes, e.g., to characterize events that occur at a rate that traditional cameras are incapable of recording in physical and biological science. Some high-frame-rate cameras, such as Photron SA1, SA3, are capable of recording high resolution still images of ephemeral events such as a supersonic flying bullet or an exploding balloon with negligible motion blur and image distortion artifacts.	 To overcome the limitations of existing CS cameras, we propose a multi-rate CNN and a synthesizing RNN to improve the trade-off between compression ratio (CR) and spatial-temporal resolution of the reconstructed videos. The experiment results demonstrate that CSVideoNet significantly outperforms state-of-the-art approaches. Without any pre/post-processing, we achieve a 25dB Peak signal-to-noise ratio (PSNR) recovery quality at 100x CR, with a frame rate of 125 fps on a Titan X GPU.  Due to the feedforward and high-data-concurrency natures of CSVideoNet, it can take advantage of GPU acceleration to achieve three orders of magnitude speed-up over conventional iterative-based approaches. We share the source code at https://github.com/PSCLab-ASU/CSVideoNet.  	High-frame-rate cameras are capable of capturing videos at frame rates over 100 frames per second (fps).  These devices were originally developed for research purposes, e.g., to characterize events that occur at a rate that traditional cameras are incapable of recording in physical and biological science. Some high-frame-rate cameras, such as Photron SA1, SA3, are capable of recording high resolution still images of ephemeral events such as a supersonic flying bullet or an exploding balloon with negligible motion blur and image distortion artifacts.	score:431
Was Khizr Khan paid over 100k by CNN and NBC and 175k by the DNC?	  We implement D-PCN on CIFAR-100~\cite{krizhevsky2009learning} and ImageNet32x32~\cite{chrabaszcz2017downsampled} datasets with NIN~\cite{lin2013network}, ResNet~\cite{he2016deep}, WRN~\cite{zagoruyko2016wide}, ResNeXt~\cite{xie2016aggregated} and DenseNet~\cite{huang2017densely}. In experiments, the performance of D-PCN ascends greatly compared with single base CNN.  In particular, our method has outperformed all advanced related approaches which use multiple subnetworks on CIFAR-100. We also apply Grad-CAM~\cite{selvaraju2016grad} to visualize D-PCN with VGG16~\cite{simonyan2014very} on a fine-grained classification dataset, Stanford Dogs~\cite{khosla2011novel}, and the result verifies our motivation. In addition, we introduce D-PCN into FCN~\cite{long2015fully} on PASCAL VOC 2012 segmentation task, and experiment result demonstrates that D-PCN can also improve accuracy of segmentation.	  We implement D-PCN on CIFAR-100~\cite{krizhevsky2009learning} and ImageNet32x32~\cite{chrabaszcz2017downsampled} datasets with NIN~\cite{lin2013network}, ResNet~\cite{he2016deep}, WRN~\cite{zagoruyko2016wide}, ResNeXt~\cite{xie2016aggregated} and DenseNet~\cite{huang2017densely}. In experiments, the performance of D-PCN ascends greatly compared with single base CNN.  In particular, our method has outperformed all advanced related approaches which use multiple subnetworks on CIFAR-100. We also apply Grad-CAM~\cite{selvaraju2016grad} to visualize D-PCN with VGG16~\cite{simonyan2014very} on a fine-grained classification dataset, Stanford Dogs~\cite{khosla2011novel}, and the result verifies our motivation. In addition, we introduce D-PCN into FCN~\cite{long2015fully} on PASCAL VOC 2012 segmentation task, and experiment result demonstrates that D-PCN can also improve accuracy of segmentation.	score:446
Was Khizr Khan paid over 100k by CNN and NBC and 175k by the DNC?	  We tested our method on feedforward networks trained on 100 sequential MNIST permutations \cite{goodfellow2013empirical} and on the ImageNet dataset \cite{imagenet_cvpr09} split into 100 sequential tasks. XdG or synaptic stabilization \cite{zenke2017continual,kirkpatrick2017overcoming}, when used alone, is partially effective at alleviating forgetting across the 100 tasks.	  We tested our method on feedforward networks trained on 100 sequential MNIST permutations \cite{goodfellow2013empirical} and on the ImageNet dataset \cite{imagenet_cvpr09} split into 100 sequential tasks. XdG or synaptic stabilization \cite{zenke2017continual,kirkpatrick2017overcoming}, when used alone, is partially effective at alleviating forgetting across the 100 tasks.	score:448
Was Khizr Khan paid over 100k by CNN and NBC and 175k by the DNC?	8}{$81$}}  \put (-2,6) {\scalebox{.8}{$100$}}  \put (62,95) {\scalebox{.8}{$81$}}  \put (62,6) {\scalebox{.8}{$100$}}  \put (68,-12) {\scalebox{1}{(d)}}   \end{overpic}\hspace{.4cm}  \begin{overpic}[ width=0.165\textwidth,  tics=10]{intronet2sparse-eps-converted-to.pdf} \put (33,100) {$\vdots$}  \put (33,3) {$\vdots$}   \end{overpic}\\  \caption{Net-Trim pruning performance on classification of points within nested spirals; (a) left: the weighted adjacency matrix relating the two hidden layers after training; right: the adjacency matrix after the application of Net-Trim causing more than 93\% of the weights to vanish; (b) partial network topology relating neurons 81 to 100 of the hidden layers, before and after retraining; (c) left: the adjacency matrix after training the network with dropout and $\ell_1$ regularization; right: Net-Trim is yet able to find a model which is over 7 times sparser than the model on the left; (d) partial network topology before and after retraining for panel (c)}\label{figintro} \end{figure}  Net-Trim is particularly useful when the number of training samples is limited.	8}{$81$}}  \put (-2,6) {\scalebox{.8}{$100$}}  \put (62,95) {\scalebox{.8}{$81$}}  \put (62,6) {\scalebox{.8}{$100$}}  \put (68,-12) {\scalebox{1}{(d)}}   \end{overpic}\hspace{.4cm}  \begin{overpic}[ width=0.165\textwidth,  tics=10]{intronet2sparse-eps-converted-to.pdf} \put (33,100) {$\vdots$}  \put (33,3) {$\vdots$}   \end{overpic}\\  \caption{Net-Trim pruning performance on classification of points within nested spirals; (a) left: the weighted adjacency matrix relating the two hidden layers after training; right: the adjacency matrix after the application of Net-Trim causing more than 93\% of the weights to vanish; (b) partial network topology relating neurons 81 to 100 of the hidden layers, before and after retraining; (c) left: the adjacency matrix after training the network with dropout and $\ell_1$ regularization; right: Net-Trim is yet able to find a model which is over 7 times sparser than the model on the left; (d) partial network topology before and after retraining for panel (c)}\label{figintro} \end{figure}  Net-Trim is particularly useful when the number of training samples is limited.	score:449
Was Khizr Khan paid over 100k by CNN and NBC and 175k by the DNC?	  In \textsection\ref{sec:fabolas}, we introduce our new Bayesian optimization method \fabolas{} for hyperparameter optimization on large datasets. In each iteration, \fabolas{} chooses the configuration $x$ and dataset size $N_{sub}$ predicted to yield most information about the loss-minimizing configuration on the full dataset per \emph{unit time spent}.  In \textsection\ref{sec:experiments}, a broad range of experiments with support vector machines and various deep neural networks show \fabolas{} often identifies good hyperparameter settings 10 to 100 times faster than state-of-the-art Bayesian optimization methods acting on the full dataset as well as Hyperband.	  In \textsection\ref{sec:fabolas}, we introduce our new Bayesian optimization method \fabolas{} for hyperparameter optimization on large datasets. In each iteration, \fabolas{} chooses the configuration $x$ and dataset size $N_{sub}$ predicted to yield most information about the loss-minimizing configuration on the full dataset per \emph{unit time spent}.  In \textsection\ref{sec:experiments}, a broad range of experiments with support vector machines and various deep neural networks show \fabolas{} often identifies good hyperparameter settings 10 to 100 times faster than state-of-the-art Bayesian optimization methods acting on the full dataset as well as Hyperband.	score:458
Is sitesmatrix.com a scam?	 With the standard model in online advertising, it is assumed that each advertisement is associated with a click-through-rate (CTR), which is the number of clicks per view. Since web sites receive revenue from clicks on advertisements, it is natural to maximize it, which can be considered as an instance of an MP-MAB problem in which advertisements and clicks correspond to arms and rewards, respectively.    \vspace{-0.5em}  \item \textbf{Example 2 (channel selection in cognitive radio networks \cite{DBLP:conf/infocom/HuangLD08}):} a cognitive radio is an adaptive scheme for allocating channels, such as wireless network spectrums. There are two kinds of users: primary and secondary. Unlike primary users, secondary users do not have primary access to a channel but can take advantage of the vacancies in primary access and opportunistically exploit instantaneous spectrum availability when primary users are idle.	 With the standard model in online advertising, it is assumed that each advertisement is associated with a click-through-rate (CTR), which is the number of clicks per view. Since web sites receive revenue from clicks on advertisements, it is natural to maximize it, which can be considered as an instance of an MP-MAB problem in which advertisements and clicks correspond to arms and rewards, respectively.    \vspace{-0.5em}  \item \textbf{Example 2 (channel selection in cognitive radio networks \cite{DBLP:conf/infocom/HuangLD08}):} a cognitive radio is an adaptive scheme for allocating channels, such as wireless network spectrums. There are two kinds of users: primary and secondary. Unlike primary users, secondary users do not have primary access to a channel but can take advantage of the vacancies in primary access and opportunistically exploit instantaneous spectrum availability when primary users are idle.	score:486
Is sitesmatrix.com a scam?	 Variational methods are typically fast, and often produce high-quality approximations. However, when the variational approximations are poor, estimates can be correspondingly worse.  MCMC strategies, such as Gibbs sampling, simulate a Markov chain whose stationary distribution is the target distribution. Inference queries are then answered by the samples drawn from the Markov chain.  In principle, MCMC will be arbitrarily accurate if run long enough. The principal difficulty is that the time for the Markov chain to converge to its stationary distribution, or the ``mixing time'', can be exponential in the number of variables.  This paper is inspired by a recent hybrid approach for Ising models \citep{Domke2013ProjectingIsingModel}.	 Variational methods are typically fast, and often produce high-quality approximations. However, when the variational approximations are poor, estimates can be correspondingly worse.  MCMC strategies, such as Gibbs sampling, simulate a Markov chain whose stationary distribution is the target distribution. Inference queries are then answered by the samples drawn from the Markov chain.  In principle, MCMC will be arbitrarily accurate if run long enough. The principal difficulty is that the time for the Markov chain to converge to its stationary distribution, or the ``mixing time'', can be exponential in the number of variables.  This paper is inspired by a recent hybrid approach for Ising models \citep{Domke2013ProjectingIsingModel}.	score:499
Is sitesmatrix.com a scam?	 One of the early attempts at addressing this issue was by \cite{ElHihi1995} who proposed a hierarchical recurrent neural network which introduced several levels of state variables, working at different time scales. Various other architectures were developed based on these principles \cite{chang2017dilated,campos2018skip}. Another well-known approach to addressing this challenges is the Long Short-Term Memory (LSTM) introduced by \cite{hochreiter1997lstm}.  The LSTM architecture models LDDs by enforcing constant error flow through \emph{constant error carousels} within special units. More recently, attention and memory augmented networks has shown to deliver good performance in modeling LDDs \cite{Merity2016,Graves2014,Salton2017}, and transformers use self-attention to model LDDs \cite{vaswani_2017,Dai2019}.	 One of the early attempts at addressing this issue was by \cite{ElHihi1995} who proposed a hierarchical recurrent neural network which introduced several levels of state variables, working at different time scales. Various other architectures were developed based on these principles \cite{chang2017dilated,campos2018skip}. Another well-known approach to addressing this challenges is the Long Short-Term Memory (LSTM) introduced by \cite{hochreiter1997lstm}.  The LSTM architecture models LDDs by enforcing constant error flow through \emph{constant error carousels} within special units. More recently, attention and memory augmented networks has shown to deliver good performance in modeling LDDs \cite{Merity2016,Graves2014,Salton2017}, and transformers use self-attention to model LDDs \cite{vaswani_2017,Dai2019}.	score:513
Is sitesmatrix.com a scam?	 The femto cell seems to be the answer to this problematic. In this work\footnote{This work is supported by the COMET project AWARE. http://www.ftw.at/news/project-start-for-aware-ftw}, we  focus on the problem of sharing femto access between a same mobile operator's customers.  This problem can be modeled as a game where service requesters customers (SRCs) and service providers customers (SPCs) are the players.     This work addresses the sharing femto access problem considering only one SPC using game theory tools. We consider that SRCs are static and have some similar and regular connection behavior. We also note that the SPC and each SRC have a software embedded respectively on its femto access, user equipment (UE).  After each connection requested by a SRC, its software will learn the strategy increasing its gain knowing that no information about the other SRCs strategies is given.	   \paragraph{\bf Related works} \leavevmode\  \label{sec:relatedWorks} The problem of sharing bandwidth and pricing has been already addressed by Dusit Niyato et al \cite{papa}, then modeled as a game. The challenging problem in this context is that bandwidth sharing requires a “peaceful” co-existence of both primary and secondary users. The femto access sharing we present in this article is similar but takes also into account both of SRCs and SPCs profiles.     The  potential games introduced by Rosenthal~\cite{rosenthal73}   are classical games  having at least  one pure Nash equilibrium.  These games have a potential function such that each of its local optimums corresponds to a pure Nash equilibrium. This property has been used   for congestion game in general (see \cite{AlgoGameTheory} for a survey), with Resource Reuse in a wireless context (see \cite{citation:6}) and for a real-time spectrum sharing problem with QoS provisioning~\cite{citation:7}.	score:515
Is sitesmatrix.com a scam?	g. GSP \citep{srikant1996mining}, PSP \citep{masseglia1998psp}, and SPADE \citep{zaki2001spade}. These methods, however, had a critical nontrivial computation that was addressed by PrefixSpan \citep{han2001prefixspan,chiu2004efficient}, and SPAM \citep{ayres2002sequential}. While some of these methods are more suitable for sequences of item sets, most of their feature representations can lead to poor accuracy.   \citet{wang2005introduction} extracted features from protein sequences using a 2-gram encoding method and 6-letter exchange group methods to find the global similarity. They used this with a neural network model. Some user-defined variables like \textit{len}, \textit{mut}, and \textit{occur} were also used to find the local similarities. \citet{wu2006universal} enlarged 2-gram encoding to an n-gram to improve the accuracy.	g. GSP \citep{srikant1996mining}, PSP \citep{masseglia1998psp}, and SPADE \citep{zaki2001spade}. These methods, however, had a critical nontrivial computation that was addressed by PrefixSpan \citep{han2001prefixspan,chiu2004efficient}, and SPAM \citep{ayres2002sequential}. While some of these methods are more suitable for sequences of item sets, most of their feature representations can lead to poor accuracy.   \citet{wang2005introduction} extracted features from protein sequences using a 2-gram encoding method and 6-letter exchange group methods to find the global similarity. They used this with a neural network model. Some user-defined variables like \textit{len}, \textit{mut}, and \textit{occur} were also used to find the local similarities. \citet{wu2006universal} enlarged 2-gram encoding to an n-gram to improve the accuracy.	score:519
How did your class (whole batch) frustrated your teacher for her to shout, "You are the worst batch ever?"	 The cloud vanishes and the learner is then asked to reconstruct it from memory. She then becomes the teacher by  passing on her own cloud to the next learner, who likewise, looks at it for a while, and then tries to reconstruct it from memory, etc.  Surprisingly, iterating this process a mere nine times leads the last learner  in the sequence to draw a cloud that regresses to the line $Y=X$; in other words, teaching about descending lines iteratively has precisely the opposite effect!  In fact the initial picture is essentially is irrelevant. A random cloud of points will also lead to $Y=X$.   Unlike the Quenya scenario, where the bias toward English is not unexpected, the cat and line experiments both  reveal a hidden prior among the participants.  Humans seem to love cats and possess a strong positive correlation bias; it is easy to speculate why.	 Of course, it is easy to think of situations where this feature would be highly desirable (eg, school teaching, social transmission of norms, legends, jokes, etc.) We show how keeping the length of the training sessions growing slightly allows iterated learning to be sustained in perpetuity.  In the first part of the paper, we characterize iterated learnability in geometric terms and show how a slight, steady increase in the lengths of the learning sessions ensures self-sustainability for any discrete language class.  In the second part, we tackle the nondiscrete case and investigate self-sustainability for iterated Bayesian linear regression. In all cases, self-sustainability requires making the underlying Markov process time-inhomogeneous in order to stay out of equilibrium.  This gives us an opportunity to offer a few thoughts on the growing importance of non-equilibrium in natural algorithms.	score:380
How did your class (whole batch) frustrated your teacher for her to shout, "You are the worst batch ever?"	 	Large hierarchical classification with more than 10000 categories is a challenging task (\cite{partalas2015lshtc}). Traditionally, hierarchical classification can be done by training a classifier on the flattened labels (\cite{babbar2013flat}) or by training a classifier at each hierarchical node (\cite{silla2011survey}), whereby each hierarchical node is a decision maker of which subsequent node to route to.	 	Large hierarchical classification with more than 10000 categories is a challenging task (\cite{partalas2015lshtc}). Traditionally, hierarchical classification can be done by training a classifier on the flattened labels (\cite{babbar2013flat}) or by training a classifier at each hierarchical node (\cite{silla2011survey}), whereby each hierarchical node is a decision maker of which subsequent node to route to.	score:383
How did your class (whole batch) frustrated your teacher for her to shout, "You are the worst batch ever?"	 Then, the network is left to determine how to change its policy in order to match the expert behaviour. This technique is known as imitation learning (IL) \citep{Subramanian2016,Hester2017a,Le2018,Andersen2018,Nair2017,Zhang2018,Gao2018}. Imitation learning provides the agent with prior knowledge about effective strategies for behaving in the world. Combining TD RL with IL allows an agent to learn from it’s own experiences, and helps to avoid situations where the skill of an agent is limited by the skill of the teacher. Learning via imitation can either be the goal itself, or an auxiliary task that is used to help achieve another goal by bootstrapping off the behaviour of an expert	 Then, the network is left to determine how to change its policy in order to match the expert behaviour. This technique is known as imitation learning (IL) \citep{Subramanian2016,Hester2017a,Le2018,Andersen2018,Nair2017,Zhang2018,Gao2018}. Imitation learning provides the agent with prior knowledge about effective strategies for behaving in the world. Combining TD RL with IL allows an agent to learn from it’s own experiences, and helps to avoid situations where the skill of an agent is limited by the skill of the teacher. Learning via imitation can either be the goal itself, or an auxiliary task that is used to help achieve another goal by bootstrapping off the behaviour of an expert	score:386
How did your class (whole batch) frustrated your teacher for her to shout, "You are the worst batch ever?"	 The nuisance transformations can give rise to many `degrees of freedom' even in a constrained task such as face recognition (\emph{e.g.} pose, age-variation, illumination etc.). Explicitly factoring them out leads to improvements in recognition performance as found in \cite{pal2016discriminative, leibo2014subtasks, hinton1987learning}. It has also been shown that that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced \cite{AnselmiLRMTP13}.  To this end, the study of invariant representations and machinery built on the concept of explicit invariance is important.      \textbf{Invariance through Data Augmentation.} Many approaches in the past have enforced invariance by generating transformed \textit{labelled} training samples in some form such as \cite{Poggio92recognitionand, scholkopf2002learning, scholkopf1998, Niyogi98incorporatingprior, Reisert_2008, Haasdonk07invariantkernel}.	 The nuisance transformations can give rise to many `degrees of freedom' even in a constrained task such as face recognition (\emph{e.g.} pose, age-variation, illumination etc.). Explicitly factoring them out leads to improvements in recognition performance as found in \cite{pal2016discriminative, leibo2014subtasks, hinton1987learning}. It has also been shown that that features that are explicitly invariant to intra-class transformations allow the sample complexity of the recognition problem to be reduced \cite{AnselmiLRMTP13}.  To this end, the study of invariant representations and machinery built on the concept of explicit invariance is important.      \textbf{Invariance through Data Augmentation.} Many approaches in the past have enforced invariance by generating transformed \textit{labelled} training samples in some form such as \cite{Poggio92recognitionand, scholkopf2002learning, scholkopf1998, Niyogi98incorporatingprior, Reisert_2008, Haasdonk07invariantkernel}.	score:389
How did your class (whole batch) frustrated your teacher for her to shout, "You are the worst batch ever?"	 However, this slows down learning~\cite{LeCun+98backprop}.  \subsubsection{Compositionality} \label{sec:compositionality}  In the context of natural language it can be argued that character-level and word-level language modeling are fundamentally different.  Word-level models rely on the principle of compositionality~\cite{sep-compositionality} and can learn to represent the meaning of a sentence by combining the semantic representations of its constituent words~\cite{mitchell2008vector}.  This mapping is arguably `smooth': If words have similar semantics, the sentences they form are likely to have a similar meaning and representation as well.  A character-level model performs a second, different task: Mapping a sequence of characters to the semantic representation of a morpheme. The principle of compositionality does not apply in this case.	 However, this slows down learning~\cite{LeCun+98backprop}.  \subsubsection{Compositionality} \label{sec:compositionality}  In the context of natural language it can be argued that character-level and word-level language modeling are fundamentally different.  Word-level models rely on the principle of compositionality~\cite{sep-compositionality} and can learn to represent the meaning of a sentence by combining the semantic representations of its constituent words~\cite{mitchell2008vector}.  This mapping is arguably `smooth': If words have similar semantics, the sentences they form are likely to have a similar meaning and representation as well.  A character-level model performs a second, different task: Mapping a sequence of characters to the semantic representation of a morpheme. The principle of compositionality does not apply in this case.	score:391
What is SVM^struct MATLAB?	   In contrast to ordinary support vector machines (SVMs)~\cite{VapnikCortesSVM}, which only predict single values, \eg a class label, SSVMs are designed such that, in principle, they can predict arbitrary structured objects.   However, this flexibility comes at a cost: training an SSVM requires  solving a more difficult optimization problem than training an  ordinary SVM.   In particular, SSVM training requires repeated runs of the structured  prediction step (the so called \emph{max-oracle}) across the training  set.   Each of these steps is an optimization problem itself, \eg finding  the minimum energy labeling of a graph,  and often computationally costly.   In fact, the more challenging the problem is, the more the max-oracle  calls become a computational bottleneck.	     Structural support vector machines (SSVMs) are amongst the      best performing models for structured computer vision tasks,      such as semantic image segmentation or human pose estimation.        Training SSVMs, however, is computationally costly, because it      requires repeated calls to a structured prediction subroutine      (called \emph{max-oracle}), which has to solve an optimization      problem itself, \eg a graph cut.         In this work, we introduce a new algorithm for SSVM training that     is more efficient than earlier techniques when the max-oracle is      computationally expensive, as it is frequently the case in computer     vision tasks. The main idea is to (i) combine the recent stochastic      Block-Coordinate Frank-Wolfe algorithm with efficient hyperplane      caching, and (ii) use an automatic selection rule for deciding whether      to call the exact max-oracle or to rely on an approximate one based     on the cached hyperplanes.	score:477
What is SVM^struct MATLAB?	 Many machine learning tasks can be formulated in terms of predicting structured outputs. In frameworks such as the structured support vector machine (SVM-Struct) and the structured perceptron, discriminative functions are learned by iteratively applying efficient maximum a posteriori (MAP) decoding. However, maximum likelihood estimation (MLE) of probabilistic models over these same structured spaces requires computing partition functions, which is generally intractable.  This paper presents a method for learning discrete exponential family models using the Bethe approximation to the MLE. Remarkably, this problem also reduces to iterative (MAP) decoding. This connection emerges by combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a convex dual objective which circumvents the intractable partition function.	 Many machine learning tasks can be formulated in terms of predicting structured outputs. In frameworks such as the structured support vector machine (SVM-Struct) and the structured perceptron, discriminative functions are learned by iteratively applying efficient maximum a posteriori (MAP) decoding. However, maximum likelihood estimation (MLE) of probabilistic models over these same structured spaces requires computing partition functions, which is generally intractable.  This paper presents a method for learning discrete exponential family models using the Bethe approximation to the MLE. Remarkably, this problem also reduces to iterative (MAP) decoding. This connection emerges by combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a convex dual objective which circumvents the intractable partition function.	score:484
What is SVM^struct MATLAB?	 Code has been made available at:~\url{https://github.com/gaobb/FR-TSVM}. 	Support vector machine~(SVM), invented by Vapnik~\cite{Vapnik98}, is a very popular machine learning algorithm. Due to the adoption structural risk minimization principle, SVM is capable of handling excellently with many classification and regression problems, such as machine fault-diagnosis~\cite{Widodo07}, image identification~\cite{Bernd01}, text classification~\cite{Joachims98}, biomedicine~\cite{El-Naqa02} and financial forecast~\cite{Trafalis00}, \emph{etc. }  It has become a prominent highlight of machine learning research. However, the application of SVM is occasionally restricted by some practical issues, especially computational speed and model robustness. To overcome these limitations, some solutions have also been proposed.  Mangasarian~\emph{et al.}~\cite{Mangasarian06} changed the proximal planes which are originally parallel to each other to generate a maximal spaced separating hyperplane into nonparallel ones, and proposed a generalized eigenvalue proximal SVM (GEPSVM).  The GEPSVM speed-ups training with slighter accurate performance since it is solved by a pair of simple generalized eigenvalue problems. Following this concept, Jayadeva \emph{et al}.~\cite{Jayadeva07} proposed twin SVM (TSVM). Its objective is turned to be optimized by placing the non-parallel proximal planes as closely as possible to their corresponding instances' cluster and as far as possible from their adversary instances' cluster (as shown in Fig.	 We first briefly review the basics of classical SVM and its related works, including FSVM and TSVM in Section~\ref{sec:bg}. Then, Section~\ref{sec:app} proposes FR-TSVM approach, including fuzzy membership function construction, linear and nonlinear FR-TSVM models and their optimization algorithms. After that, experimental results are reported in Section~\ref{sec:exp}.  Some preliminary results have been published in a conference presentation~\cite{gao2015coordinate}.  \begin{figure*}  \centering  \subfloat[TSVM]  {\includegraphics[width= 0.45\textwidth]{tsvm-gi}\label{fig:GI-a}}  \subfloat[FR-TSVM]  {\includegraphics[width= 0.45\textwidth]{fr-tsvm-gi}\label{fig:GI-b}}  \caption{Geometric interpretation of linear TSVM and FR-TSVM for binary classification.	score:485
What is SVM^struct MATLAB?	  As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?  random forest (RF)? etc.), what parameters to use (e.g., how many decision trees in a RF? how many hidden layers in an ANN? etc.), and so on. Experienced machine learning practitioners often have good intuitions on what choices are appropriate for the problem domain, but some practitioners can easily spend several weeks tinkering with model parameters and data transformations until the pipeline achieves an acceptable level of performance.	  As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?  random forest (RF)? etc.), what parameters to use (e.g., how many decision trees in a RF? how many hidden layers in an ANN? etc.), and so on. Experienced machine learning practitioners often have good intuitions on what choices are appropriate for the problem domain, but some practitioners can easily spend several weeks tinkering with model parameters and data transformations until the pipeline achieves an acceptable level of performance.	score:488
What is SVM^struct MATLAB?	 As a result, while a description of an admissible structure can be represented efficiently, the number of possible valid structures is exponentially larger than these representations. As such, viewing structured prediction tasks as classification problems results in exponentially large numbers of labels. This is why classical machine learning has to employ techniques such as generalization of support vector machines (SVM) to structured SVMs (SSVM) \cite{yu2011improved} and generative models \cite{sohn2015learning} to solve structured prediction problems.   The inherently combinatorial nature of structured prediction tasks gives rise to piecewise smooth models, as in the case of SVMs. However, predicting a structured output involves the prediction of vectors, rather than simply scalar value assignments. Therefore, the combinatorial aspect of the task is especially egregious in structured prediction. For example, in structured SVMs (SSVM), the number of pieces in the piecewise smooth model is often exponentially large in terms of the dimension of prediction vectors.   The underlying optimization problem in structured prediction tasks is of the form \begin{equation}\label{eq:objective}   \min_w\ \frac{1}{n}\sum_{i=1}^{n} g_i(w),   \quad\mbox{where}\quad   g_i(w) = \max_{y\in \mathcal Y} f_i(y, w)\,, \end{equation} where $f_i$ are strongly convex with Lipschitz continuous gradients, and $\mathcal Y$ is a finite set.	 As a result, while a description of an admissible structure can be represented efficiently, the number of possible valid structures is exponentially larger than these representations. As such, viewing structured prediction tasks as classification problems results in exponentially large numbers of labels. This is why classical machine learning has to employ techniques such as generalization of support vector machines (SVM) to structured SVMs (SSVM) \cite{yu2011improved} and generative models \cite{sohn2015learning} to solve structured prediction problems.   The inherently combinatorial nature of structured prediction tasks gives rise to piecewise smooth models, as in the case of SVMs. However, predicting a structured output involves the prediction of vectors, rather than simply scalar value assignments. Therefore, the combinatorial aspect of the task is especially egregious in structured prediction. For example, in structured SVMs (SSVM), the number of pieces in the piecewise smooth model is often exponentially large in terms of the dimension of prediction vectors.   The underlying optimization problem in structured prediction tasks is of the form \begin{equation}\label{eq:objective}   \min_w\ \frac{1}{n}\sum_{i=1}^{n} g_i(w),   \quad\mbox{where}\quad   g_i(w) = \max_{y\in \mathcal Y} f_i(y, w)\,, \end{equation} where $f_i$ are strongly convex with Lipschitz continuous gradients, and $\mathcal Y$ is a finite set.	score:491
I'm a CS student. How can I have a great career in NLP without going to graduate school?	 Taken together, we found out that $Q$-based learning approaches rarely work out-of-the-box, and require long training time and advanced reward shaping.    A radically different approach has been introduced by Schmidhuber~\cite{schmidhuber1991reinforcement}, who tackled the RL problem using a recurrent neural network (RNN). Following \cite{schmidhuber1991reinforcement}, there have been several additional algorithms that rely on RNNs for RL problems.  For example,  Backer~\cite{bakker2001reinforcement} proposed to tackle the RL problem using recurrent networks with the LSTM architecture. His approach still relies on the value function. Sch{\"a}fer~\cite{schafer2008reinforcement} used RNN to model the dynamics of partially observed MDPs. Again, he still relies on explicitly modeling the Markovian structure.	 Taken together, we found out that $Q$-based learning approaches rarely work out-of-the-box, and require long training time and advanced reward shaping.    A radically different approach has been introduced by Schmidhuber~\cite{schmidhuber1991reinforcement}, who tackled the RL problem using a recurrent neural network (RNN). Following \cite{schmidhuber1991reinforcement}, there have been several additional algorithms that rely on RNNs for RL problems.  For example,  Backer~\cite{bakker2001reinforcement} proposed to tackle the RL problem using recurrent networks with the LSTM architecture. His approach still relies on the value function. Sch{\"a}fer~\cite{schafer2008reinforcement} used RNN to model the dynamics of partially observed MDPs. Again, he still relies on explicitly modeling the Markovian structure.	score:425
I'm a CS student. How can I have a great career in NLP without going to graduate school?	 This type of learning process is modeled on the way human education and cognition functions. For instance, students will start by learning easier concepts (e.g., Linear Equations) before moving on to more complex ones (e.g., Differential Equations) in the mathematics curriculum. Self-paced learning can also be finely explained in a robust learning manner, where uncorrupted data samples are likely to be used for training earlier in the process than corrupted data.     In recent years, self-paced learning \cite{kumar2010self} has received widespread attention for various applications in machine learning, such as image classification \cite{AAAI159750}, event detection \cite{jiang2014easy,Zhang:2017:SEF:3132847.3132996} and object tracking \cite{supancic2013self,Zhang:2016:BSD:3061053.3061115}. A wide assortment of \textit{SPL}-based methods \cite{Pi:2016:SBL:3060832.	 This type of learning process is modeled on the way human education and cognition functions. For instance, students will start by learning easier concepts (e.g., Linear Equations) before moving on to more complex ones (e.g., Differential Equations) in the mathematics curriculum. Self-paced learning can also be finely explained in a robust learning manner, where uncorrupted data samples are likely to be used for training earlier in the process than corrupted data.     In recent years, self-paced learning \cite{kumar2010self} has received widespread attention for various applications in machine learning, such as image classification \cite{AAAI159750}, event detection \cite{jiang2014easy,Zhang:2017:SEF:3132847.3132996} and object tracking \cite{supancic2013self,Zhang:2016:BSD:3061053.3061115}. A wide assortment of \textit{SPL}-based methods \cite{Pi:2016:SBL:3060832.	score:428
I'm a CS student. How can I have a great career in NLP without going to graduate school?	 This poses even further challenges to the educator. In fact, it also creates challenges to the learners themselves in terms of how to interact with colleagues who studied different disciplines, and are potentially accustomed to different learning methods and materials.  In this paper, I will focus on the challenges posed to the educator. I will use the term \textit{educator} to refer to the lecturer, professor, or teacher; and the term \textit{learner} to refer to the students or members of the learning cohort.   I reflect on my own experiences of teaching on a MSc level course created specifically for DS. Established in 2014, the DS MSc course at Lancaster University was one of the first of its kind and continues to draw a large number of learners from all over the world.  The cohort consists of 50-70 learners that primarily come from different backgrounds.	 This poses even further challenges to the educator. In fact, it also creates challenges to the learners themselves in terms of how to interact with colleagues who studied different disciplines, and are potentially accustomed to different learning methods and materials.  In this paper, I will focus on the challenges posed to the educator. I will use the term \textit{educator} to refer to the lecturer, professor, or teacher; and the term \textit{learner} to refer to the students or members of the learning cohort.   I reflect on my own experiences of teaching on a MSc level course created specifically for DS. Established in 2014, the DS MSc course at Lancaster University was one of the first of its kind and continues to draw a large number of learners from all over the world.  The cohort consists of 50-70 learners that primarily come from different backgrounds.	score:429
I'm a CS student. How can I have a great career in NLP without going to graduate school?	   Dropout has been witnessed with great success in training deep neural networks by independently zeroing  out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression.  However, the independent  sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial  sampling for dropout, i. e., sampling features or neurons according to  a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial  dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution.	   Dropout has been witnessed with great success in training deep neural networks by independently zeroing  out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression.  However, the independent  sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial  sampling for dropout, i. e., sampling features or neurons according to  a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial  dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution.	score:431
I'm a CS student. How can I have a great career in NLP without going to graduate school?	 The verified certificate keeps information about the performance of a student in a course (or in a chain of courses) and can be used to justify a student's quality to potential employers. So, the verified certificate should have {\em reliable information} about the student performance in the courses she has participated in. Even though the means to guarantee this in the traditional University system is well-established, achieving this in a MOOC is a challenge.   The big issue is in the massive student participation. Of course, the Internet provides tools so that organizing exams with huge numbers of students is logistically feasible. But what about assessment and grading? As the most popular courses attract $50\,000$ students or more and the vision of MOOCs enthusiasts is for millions of students per course, is grading of assignments or exams possible?	 The verified certificate keeps information about the performance of a student in a course (or in a chain of courses) and can be used to justify a student's quality to potential employers. So, the verified certificate should have {\em reliable information} about the student performance in the courses she has participated in. Even though the means to guarantee this in the traditional University system is well-established, achieving this in a MOOC is a challenge.   The big issue is in the massive student participation. Of course, the Internet provides tools so that organizing exams with huge numbers of students is logistically feasible. But what about assessment and grading? As the most popular courses attract $50\,000$ students or more and the vision of MOOCs enthusiasts is for millions of students per course, is grading of assignments or exams possible?	score:432
What can you do with machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:447
What can you do with machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:452
What can you do with machine learning?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:474
What can you do with machine learning?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:477
What can you do with machine learning?	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	score:480
What can u do with machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:450
What can u do with machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:458
What can u do with machine learning?	 Machine learning is a thriving part of computer science. There are many efficient approaches to machine learning that do not provide strong theoretical guarantees, and a beautiful general learning theory. Unfortunately, machine learning approaches that give strong theoretical guarantees have not been efficient enough to be applicable.  In this paper we introduce a logical approach to machine learning.  Models are represented by tuples of logical formulas and inputs and outputs are logical structures. We present our framework together with several applications where we evaluate it using SAT and SMT solvers. We argue that this approach to machine learning is particularly suited to bridge the gap between efficiency and theoretical soundness.  We exploit results from descriptive complexity theory to prove strong theoretical guarantees for our approach.	 One would hope for much stronger theoretical guarantees of success, but no widely used machine learning methods provide them, so it is difficult to know when they will work.  One key question faced by any machine learning system is: what kind of models will it generate? In neural networks one asks whether the architecture is feed-forward or recurrent and how many layers it has.  In genetic programming one asks for the program representation and which functions are built-in. In general, each machine learning system must make this choice. If the class of generated models or programs is too broad, it might be impossible to learn them efficiently. If it is too narrow, it might not suffice for the task at hand. To solve a different task, one might need a different kind of model.	score:464
What can u do with machine learning?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:466
What can u do with machine learning?	  Machine learning has become pervasive in multiple domains, impacting a wide variety of applications, such as knowledge discovery and data mining, natural language processing, information retrieval, computer vision, social and health informatics, ubiquitous computing, etc.  Two essential problems of machine learning are how to generate features and how to acquire labels for machines to learn.   Particularly, labeling large amount of data for each domain-specific problem can be very time consuming and costly.  It has become a key obstacle in making learning protocols realistic in applications.  In this paper, we will discuss how to use the existing general-purpose world knowledge to enhance machine learning processes, by enriching the features or reducing the labeling work.	   	Machine learning has become pervasive in multiple domains, impacting a wide variety of applications, such as knowledge discovery and data mining, natural language processing, information retrieval, computer vision, social and health informatics, ubiquitous computing, etc. Two major problems of machine learning in practice are how to generate or extract features from data and how to acquire labels for machines to learn.  There have been many studies about feature engineering and labeling work reduction in the past decades. \begin{itemize}  \item {\bf Feature extraction and representation}. Feature engineering, such as handcrafting features for domain dependent problems, has been recognized as a key problem in applications (such as Kaggle\footnote{\url{https://www.kaggle.	score:467
Which is best book for machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:447
Which is best book for machine learning?	 Supervised machine learning techniques are very useful for solving sensing problems. In this paper we describe a machine learning algorithmic framework for the planning part. Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) --- see \cite{bertsekas1995dynamic,kaelbling1996reinforcement,sutton1998reinforcement,szepesvari2010algorithms} for a general overview and \cite{kober2013reinforcement} for a comprehensive review of reinforcement learning in robotics.     Typically, RL is performed in a sequence of consecutive rounds. At round $t$, the planner (a.k.a. the agent) observes a state, $s_t \in S$, which represents the agent as well as the environment. It then should decide on an action $a_t \in A$.  After performing the action, the agent receives an immediate reward, $r_t \in \reals$, and is moved to a new state, $s_{t+1}$.	 Supervised machine learning techniques are very useful for solving sensing problems. In this paper we describe a machine learning algorithmic framework for the planning part. Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) --- see \cite{bertsekas1995dynamic,kaelbling1996reinforcement,sutton1998reinforcement,szepesvari2010algorithms} for a general overview and \cite{kober2013reinforcement} for a comprehensive review of reinforcement learning in robotics.     Typically, RL is performed in a sequence of consecutive rounds. At round $t$, the planner (a.k.a. the agent) observes a state, $s_t \in S$, which represents the agent as well as the environment. It then should decide on an action $a_t \in A$.  After performing the action, the agent receives an immediate reward, $r_t \in \reals$, and is moved to a new state, $s_{t+1}$.	score:464
Which is best book for machine learning?	  Many machine learning algorithms have been analyzed by smoothed analysis, such as k-means clustering \citep{arthur2009k} and Support Vector Machines \citep{Blum:2002:SAP:545381.545499,spielman2009smoothed}, however, to the best of our knowledge, there is no previous research on using machine learning algorithms to predict the SC of sorting algorithms.   Due to recent modular smoothed analysis results, we can compute the SC of Quicksort exactly, for medium size inputs (e.g., $N \leq 3000$). For other sorting algorithms, currently the SC can only be computed using its original definition, and thus, due to computational requirements, only for small input sizes (e.g., $N \leq 100$), limiting our understanding of the general SC behaviour.	  Many machine learning algorithms have been analyzed by smoothed analysis, such as k-means clustering \citep{arthur2009k} and Support Vector Machines \citep{Blum:2002:SAP:545381.545499,spielman2009smoothed}, however, to the best of our knowledge, there is no previous research on using machine learning algorithms to predict the SC of sorting algorithms.   Due to recent modular smoothed analysis results, we can compute the SC of Quicksort exactly, for medium size inputs (e.g., $N \leq 3000$). For other sorting algorithms, currently the SC can only be computed using its original definition, and thus, due to computational requirements, only for small input sizes (e.g., $N \leq 100$), limiting our understanding of the general SC behaviour.	score:470
Which is best book for machine learning?	 Machine learning methods can avoid the manual hand crafting of robotic models, and instead learn these models directly from the data streams acquired by the robot during operation. Furthermore, machine learning can generalize better on larger state space of the model and take into account nonlinearities that are most of the times neglected by classical physics-based approaches \cite{nguyen2011model}.    Model learning has been proven to be an efficient methodology in various robotic domains such as inverse kinematics and dynamics control, robot manipulation, locomotion or navigation. However learning these models may not always be straightforward and is still being faced with several challenges \cite{nguyen2011model}. For most applications, model learning is regarded as a regression problem mapping the robot's states and actions.	 Machine learning methods can avoid the manual hand crafting of robotic models, and instead learn these models directly from the data streams acquired by the robot during operation. Furthermore, machine learning can generalize better on larger state space of the model and take into account nonlinearities that are most of the times neglected by classical physics-based approaches \cite{nguyen2011model}.    Model learning has been proven to be an efficient methodology in various robotic domains such as inverse kinematics and dynamics control, robot manipulation, locomotion or navigation. However learning these models may not always be straightforward and is still being faced with several challenges \cite{nguyen2011model}. For most applications, model learning is regarded as a regression problem mapping the robot's states and actions.	score:478
Which is best book for machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	score:482
When would be the next Infosys batch after November?	  In order to solve the first question, we will use computational machine learning techniques to estimate the fill level of each container. Supported by historical data (past data), the system generates predictions (future data) to decide when a container should be collected. This prediction could be performed long term or short term, considering seasonality, weekends, and holidays.  We want to highlight that, as far as we know, the fill-level prediction of all containers individually (fine grain) is a feature, which until now, has not been implemented in any tool for waste collection management. Additionally, our software system is able to interact with an Internet of Things (IoT) system to obtain the actual filling of the containers equipped with volumetric sensors.	  In order to solve the first question, we will use computational machine learning techniques to estimate the fill level of each container. Supported by historical data (past data), the system generates predictions (future data) to decide when a container should be collected. This prediction could be performed long term or short term, considering seasonality, weekends, and holidays.  We want to highlight that, as far as we know, the fill-level prediction of all containers individually (fine grain) is a feature, which until now, has not been implemented in any tool for waste collection management. Additionally, our software system is able to interact with an Internet of Things (IoT) system to obtain the actual filling of the containers equipped with volumetric sensors.	score:408
When would be the next Infosys batch after November?	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.	score:415
When would be the next Infosys batch after November?	 In the quickest change detection problems, a change occurs in the sensing environment at an unknown time and the aim is to detect the change as soon as possible with the minimal level of false alarms based on the measurements that become available sequentially over time. After obtaining measurements at a given time, decision maker either declares a change or waits for the next time interval to have further measurements.  In general, as the desired detection accuracy increases, detection speed decreases. Hence, the stopping time, at which a change is declared, should be chosen to optimally balance the tradeoff between the detection speed and the detection accuracy.  If the probability density functions (pdfs) of meter measurements for the pre-change, i.e., normal system operation, and the post-change, i.	 In the quickest change detection problems, a change occurs in the sensing environment at an unknown time and the aim is to detect the change as soon as possible with the minimal level of false alarms based on the measurements that become available sequentially over time. After obtaining measurements at a given time, decision maker either declares a change or waits for the next time interval to have further measurements.  In general, as the desired detection accuracy increases, detection speed decreases. Hence, the stopping time, at which a change is declared, should be chosen to optimally balance the tradeoff between the detection speed and the detection accuracy.  If the probability density functions (pdfs) of meter measurements for the pre-change, i.e., normal system operation, and the post-change, i.	score:437
When would be the next Infosys batch after November?	      The fact that languages and knowledge can change over time also makes it difficult to simply rely on a large corpus.~Media produce an endless stream of new knowledge every day (e.g., the results of baseball games played yesterday) that is even changing over time. Furthermore, a good language model should exercise some level of reasoning. For example, it may be possible to observe many occurrences of Barack Obama's year of birth and thus able to predict it in a correlated context.  However, one would not expect current language models to predict, with a proper reasoning, the blank in ``\text{Barack Obama's age is \_\_\_\_}" even if it is only a simple reformulation of the knowledge on the year of birth\footnote{We do not investigate the reasoning ability in this paper but highlight this example because the explicit representation of facts would help to handle such examples.	      The fact that languages and knowledge can change over time also makes it difficult to simply rely on a large corpus.~Media produce an endless stream of new knowledge every day (e.g., the results of baseball games played yesterday) that is even changing over time. Furthermore, a good language model should exercise some level of reasoning. For example, it may be possible to observe many occurrences of Barack Obama's year of birth and thus able to predict it in a correlated context.  However, one would not expect current language models to predict, with a proper reasoning, the blank in ``\text{Barack Obama's age is \_\_\_\_}" even if it is only a simple reformulation of the knowledge on the year of birth\footnote{We do not investigate the reasoning ability in this paper but highlight this example because the explicit representation of facts would help to handle such examples.	score:440
When would be the next Infosys batch after November?	 In their experiments, TS is also more robust to delayed or batched feedback (delayed feedback means that the result of a play of an arm may become available only after some time delay, but we are required to make immediate decisions for which arm to play next) than the other methods.   A possible explanation may be that TS is a randomized algorithm and so it is unlikely to  get trapped in an early bad decision during the delay.   Microsoft's adPredictor (\cite{GraepelCBH10}) for CTR prediction of search ads on Bing uses the idea of Thompson Sampling.      It has been suggested (\cite{CLi}) that despite being easy to implement and being competitive to the state of the art methods, the reason TS is not very popular in literature could be its lack of strong theoretical analysis.	 In their experiments, TS is also more robust to delayed or batched feedback (delayed feedback means that the result of a play of an arm may become available only after some time delay, but we are required to make immediate decisions for which arm to play next) than the other methods.   A possible explanation may be that TS is a randomized algorithm and so it is unlikely to  get trapped in an early bad decision during the delay.   Microsoft's adPredictor (\cite{GraepelCBH10}) for CTR prediction of search ads on Bing uses the idea of Thompson Sampling.      It has been suggested (\cite{CLi}) that despite being easy to implement and being competitive to the state of the art methods, the reason TS is not very popular in literature could be its lack of strong theoretical analysis.	score:442
When does infosys mysore training start its next batch after october?	  In order to solve the first question, we will use computational machine learning techniques to estimate the fill level of each container. Supported by historical data (past data), the system generates predictions (future data) to decide when a container should be collected. This prediction could be performed long term or short term, considering seasonality, weekends, and holidays.  We want to highlight that, as far as we know, the fill-level prediction of all containers individually (fine grain) is a feature, which until now, has not been implemented in any tool for waste collection management. Additionally, our software system is able to interact with an Internet of Things (IoT) system to obtain the actual filling of the containers equipped with volumetric sensors.	  In order to solve the first question, we will use computational machine learning techniques to estimate the fill level of each container. Supported by historical data (past data), the system generates predictions (future data) to decide when a container should be collected. This prediction could be performed long term or short term, considering seasonality, weekends, and holidays.  We want to highlight that, as far as we know, the fill-level prediction of all containers individually (fine grain) is a feature, which until now, has not been implemented in any tool for waste collection management. Additionally, our software system is able to interact with an Internet of Things (IoT) system to obtain the actual filling of the containers equipped with volumetric sensors.	score:391
When does infosys mysore training start its next batch after october?	 However, the training phase continues to be a bottleneck, where the training data must be processed serially over thousands of individual training runs, thus leading to long training times.  This situation is only exacerbated by recent computer architecture trends where clock speeds are stagnant, and future speedups are available only through increased concurrency.     This situation repeats itself for the time-dependent PDE simulation community, where it has been identified as a sequential time-stepping bottleneck \cite{Fa2014}.  In response, interest in parallel-in-time methods, that can parallelize over the time domain, has grown considerably in the last decade.  One of the most well known parallel-in-time methods is Parareal \cite{LiMaTu2001}, which is equivalent to a two-level multigrid method \cite{GaVa2007}.	 However, the training phase continues to be a bottleneck, where the training data must be processed serially over thousands of individual training runs, thus leading to long training times.  This situation is only exacerbated by recent computer architecture trends where clock speeds are stagnant, and future speedups are available only through increased concurrency.     This situation repeats itself for the time-dependent PDE simulation community, where it has been identified as a sequential time-stepping bottleneck \cite{Fa2014}.  In response, interest in parallel-in-time methods, that can parallelize over the time domain, has grown considerably in the last decade.  One of the most well known parallel-in-time methods is Parareal \cite{LiMaTu2001}, which is equivalent to a two-level multigrid method \cite{GaVa2007}.	score:394
When does infosys mysore training start its next batch after october?	 } when we expand the time window of training to past 1 month, 3 months, 6 months and 15 months respectively. The baseline is the model trained using the past 1-week transactions only.     Apparently, the larger the time window (and thus the more data) we use for training, the better the performance is,       implying that we should collect as much data for training as possible.      \begin{figure}[t]   \centering   \includegraphics[width=.4\textwidth]{markov_ri}   \vskip -0.1in   \caption{Relative improvement of recommendation with different time     windows for model training}   \label{fig:markov_ri}   \vskip -0.15in \end{figure}         Hence we have the following dilemma: on one hand, we want to exploit all possible transactions to overcome sparsity; on the other hand, we wish to capture the trend change in recommendation by examining recent transactions only.	 } when we expand the time window of training to past 1 month, 3 months, 6 months and 15 months respectively. The baseline is the model trained using the past 1-week transactions only.     Apparently, the larger the time window (and thus the more data) we use for training, the better the performance is,       implying that we should collect as much data for training as possible.      \begin{figure}[t]   \centering   \includegraphics[width=.4\textwidth]{markov_ri}   \vskip -0.1in   \caption{Relative improvement of recommendation with different time     windows for model training}   \label{fig:markov_ri}   \vskip -0.15in \end{figure}         Hence we have the following dilemma: on one hand, we want to exploit all possible transactions to overcome sparsity; on the other hand, we wish to capture the trend change in recommendation by examining recent transactions only.	score:409
When does infosys mysore training start its next batch after october?	com/code-terminator/DilatedRNN}}. 	Recurrent neural networks (RNNs) have been shown to have remarkable performance on many sequential learning problems.  However, long sequence learning with RNNs remains a challenging problem for the following reasons:  first, memorizing extremely long-term dependencies while maintaining mid- and short-term memory is difficult;  second, training RNNs using back-propagation-through-time is impeded by vanishing and exploding gradients; And lastly, both forward- and back-propagation are performed in a sequential manner, which makes the training time-consuming.	com/code-terminator/DilatedRNN}}. 	Recurrent neural networks (RNNs) have been shown to have remarkable performance on many sequential learning problems.  However, long sequence learning with RNNs remains a challenging problem for the following reasons:  first, memorizing extremely long-term dependencies while maintaining mid- and short-term memory is difficult;  second, training RNNs using back-propagation-through-time is impeded by vanishing and exploding gradients; And lastly, both forward- and back-propagation are performed in a sequential manner, which makes the training time-consuming.	score:412
When does infosys mysore training start its next batch after october?	 After training, activity in the output of this structured representation layer is already a surprisingly good indicator for class activity which only needs to be refined further in standard end-to-end learning (either as improved input or as a new, more accurate output target).  To enforce these activity constraints, one typically needs to drastically favor the activity cost over the reconstruction error.  However, this sometimes had detrimental effects on the training efficiency: the gradient is dominated by the activity cost in this case and thus gradient-based learning methods often set all network weights to zero, thus deactivating all activity (i.e. ignoring the reconstruction error). This often slowed down training considerably. Therefore, we propose a second extension to accelerate training.	 After training, activity in the output of this structured representation layer is already a surprisingly good indicator for class activity which only needs to be refined further in standard end-to-end learning (either as improved input or as a new, more accurate output target).  To enforce these activity constraints, one typically needs to drastically favor the activity cost over the reconstruction error.  However, this sometimes had detrimental effects on the training efficiency: the gradient is dominated by the activity cost in this case and thus gradient-based learning methods often set all network weights to zero, thus deactivating all activity (i.e. ignoring the reconstruction error). This often slowed down training considerably. Therefore, we propose a second extension to accelerate training.	score:417
Which batch should i join last batch (24 aug) of leader or achievers batch (31 aug)?	exe http:\\31.14.136.202\secure.apple.id.login\Apple\login.php http:\\1stopmoney.com\paypal-login-secure\websc.php \end{verbatim} Next, a few examples of malicious file paths: \begin{verbatim} C:\Temp\702D97503A79B0EC69\JUEGOS/Call of Duty 4+Keygen C:\Temp\svchost.vbs C:\DOCUME~1\BASANT~1\LOCALS~1\Temp\WzEC.tmp\fax.doc.exe \end{verbatim} Finally, a few examples of malicious registry keys: \begin{verbatim} HKCU\Software\Microsoft\Windows\CurrentVersion\Run Alpha Antivirus HKCR\Applications\WEBCAM HACKER 1. 0.0.4.EXE HKCR\AppID\bccicabecccag.exe \end{verbatim}  All of these examples appear malicious, or at least suspicious, to the expert eye, leading us to hypothesize that a machine learning system could also infer their maliciousness. It might even be possible to exceed human expert's ability to guess whether these artifacts are malicious, by learning to recognize generalized deceptive patterns observed over tens of millions of malicious artifacts.	exe http:\\31.14.136.202\secure.apple.id.login\Apple\login.php http:\\1stopmoney.com\paypal-login-secure\websc.php \end{verbatim} Next, a few examples of malicious file paths: \begin{verbatim} C:\Temp\702D97503A79B0EC69\JUEGOS/Call of Duty 4+Keygen C:\Temp\svchost.vbs C:\DOCUME~1\BASANT~1\LOCALS~1\Temp\WzEC.tmp\fax.doc.exe \end{verbatim} Finally, a few examples of malicious registry keys: \begin{verbatim} HKCU\Software\Microsoft\Windows\CurrentVersion\Run Alpha Antivirus HKCR\Applications\WEBCAM HACKER 1. 0.0.4.EXE HKCR\AppID\bccicabecccag.exe \end{verbatim}  All of these examples appear malicious, or at least suspicious, to the expert eye, leading us to hypothesize that a machine learning system could also infer their maliciousness. It might even be possible to exceed human expert's ability to guess whether these artifacts are malicious, by learning to recognize generalized deceptive patterns observed over tens of millions of malicious artifacts.	score:434
Which batch should i join last batch (24 aug) of leader or achievers batch (31 aug)?	exe http:\\31.14.136.202\secure.apple.id.login\Apple\login.php http:\\1stopmoney.com\paypal-login-secure\websc.php \end{verbatim} Next, a few examples of malicious file paths: \begin{verbatim} C:\Temp\702D97503A79B0EC69\JUEGOS/Call of Duty 4+Keygen C:\Temp\svchost.vbs C:\DOCUME~1\BASANT~1\LOCALS~1\Temp\WzEC.tmp\fax.doc.exe \end{verbatim} Finally, a few examples of malicious registry keys: \begin{verbatim} HKCU\Software\Microsoft\Windows\CurrentVersion\Run Alpha Antivirus HKCR\Applications\WEBCAM HACKER 1. 0.0.4.EXE HKCR\AppID\bccicabecccag.exe \end{verbatim}  All of these examples appear malicious, or at least suspicious, to the expert eye, leading us to hypothesize that a machine learning system could also infer their maliciousness. It might even be possible to exceed human expert's ability to guess whether these artifacts are malicious, by learning to recognize generalized deceptive patterns observed over tens of millions of malicious artifacts.	exe http:\\31.14.136.202\secure.apple.id.login\Apple\login.php http:\\1stopmoney.com\paypal-login-secure\websc.php \end{verbatim} Next, a few examples of malicious file paths: \begin{verbatim} C:\Temp\702D97503A79B0EC69\JUEGOS/Call of Duty 4+Keygen C:\Temp\svchost.vbs C:\DOCUME~1\BASANT~1\LOCALS~1\Temp\WzEC.tmp\fax.doc.exe \end{verbatim} Finally, a few examples of malicious registry keys: \begin{verbatim} HKCU\Software\Microsoft\Windows\CurrentVersion\Run Alpha Antivirus HKCR\Applications\WEBCAM HACKER 1. 0.0.4.EXE HKCR\AppID\bccicabecccag.exe \end{verbatim}  All of these examples appear malicious, or at least suspicious, to the expert eye, leading us to hypothesize that a machine learning system could also infer their maliciousness. It might even be possible to exceed human expert's ability to guess whether these artifacts are malicious, by learning to recognize generalized deceptive patterns observed over tens of millions of malicious artifacts.	score:434
Which batch should i join last batch (24 aug) of leader or achievers batch (31 aug)?	                   A new algorithm is proposed which accelerates the mini-batch k-means algorithm of~\cite{Sculley:2010:WKC:1772690.1772862} by using the distance bounding approach of~\cite{elkan_2003_kmeansicml}. We argue that, when incorporating distance bounds into a mini-batch algorithm,  already used data should preferentially be reused. To this end we propose using \emph{nested} mini-batches, whereby data in a mini-batch at iteration $t$ is automatically reused at iteration $t+1$.	                   A new algorithm is proposed which accelerates the mini-batch k-means algorithm of~\cite{Sculley:2010:WKC:1772690.1772862} by using the distance bounding approach of~\cite{elkan_2003_kmeansicml}. We argue that, when incorporating distance bounds into a mini-batch algorithm,  already used data should preferentially be reused. To this end we propose using \emph{nested} mini-batches, whereby data in a mini-batch at iteration $t$ is automatically reused at iteration $t+1$.	score:444
Which batch should i join last batch (24 aug) of leader or achievers batch (31 aug)?	  \\  In this paper, we  examine how a deep network can be constructed in a parsimonious manner if we do allow domain knowledge to suggest the compositional structure of the target function as well as the values of the constituent functions. We study the problem of predicting, based on the past few readings of a continuous glucose monitoring (CGM) device, both the blood glucose (BG) level and the rate at which it would be changing 30 minutes after the last reading.  From the point of view of diabetes management, a reliable solution to this problem is of great importance. If a patient has some warning that that his/her BG will rise or drop in the next half hour,  the patient can take certain precautionary measures to prevent this (e.g., administer an insulin injection or take an extra snack) \cite{naumova2012meta,snetselaar2009nutrition}.	  \\  In this paper, we  examine how a deep network can be constructed in a parsimonious manner if we do allow domain knowledge to suggest the compositional structure of the target function as well as the values of the constituent functions. We study the problem of predicting, based on the past few readings of a continuous glucose monitoring (CGM) device, both the blood glucose (BG) level and the rate at which it would be changing 30 minutes after the last reading.  From the point of view of diabetes management, a reliable solution to this problem is of great importance. If a patient has some warning that that his/her BG will rise or drop in the next half hour,  the patient can take certain precautionary measures to prevent this (e.g., administer an insulin injection or take an extra snack) \cite{naumova2012meta,snetselaar2009nutrition}.	score:451
Which batch should i join last batch (24 aug) of leader or achievers batch (31 aug)?	35\%, outperforming all the previous more complex methods. Here we report another substantial improvement: 0.31\% obtained using a committee of MLPs.  	Current automatic handwriting recognition algorithms are already pretty good at learning to recognize handwritten digits.  More than a decade ago, Multilayer Perceptrons or MLPs \citep{Werbos:74, LeCun:85, Rumelhart:86} were among the first classifiers tested on the now famous MNIST handwritten digit recognition benchmark.  Most had few layers or few artificial neurons (units) per layer \citep{LeCun:98}, but apparently back then these were the biggest feasible MLPs, trained when CPU cores were at least 20 times slower than today. A more recent MLP with a single hidden layer of 800 units achieved 0.70\% error \citep{Simard:03}. The latest substantial improvement by others occurred in 2003 \citep{Simard:03} (error rate 0.	35\%, outperforming all the previous more complex methods. Here we report another substantial improvement: 0.31\% obtained using a committee of MLPs.  	Current automatic handwriting recognition algorithms are already pretty good at learning to recognize handwritten digits.  More than a decade ago, Multilayer Perceptrons or MLPs \citep{Werbos:74, LeCun:85, Rumelhart:86} were among the first classifiers tested on the now famous MNIST handwritten digit recognition benchmark.  Most had few layers or few artificial neurons (units) per layer \citep{LeCun:98}, but apparently back then these were the biggest feasible MLPs, trained when CPU cores were at least 20 times slower than today. A more recent MLP with a single hidden layer of 800 units achieved 0.70\% error \citep{Simard:03}. The latest substantial improvement by others occurred in 2003 \citep{Simard:03} (error rate 0.	score:454
What is a matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	 The basic principle of matrix completion  consists in recovering all the entries of an unknown data matrix from incomplete and noisy observations of its entries.  To address the high-dimensionality in matrix completion problem, statistical inference based on low-rank constraint is now an ubiquitous technique for recovering the underlying data matrix.  Thus, matrix completion can be formulated as minimizing the rank of the matrix given a random sample of its entries. However, this rank minimization problem is in general NP-hard due to the combinatorial nature of the rank function~\citep{fazel2001,fazelPhD-2000}. To alleviate this problem and make it tractable, convex relaxation strategies were proposed, e.	score:431
What is a matrix?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:435
What is a matrix?	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	score:502
What is a matrix?	 Section \ref{Fully connected (FC) layer} shows the details of matrix multiplication in fully connected layer. Then, Section \ref{Common explanation on convolutional (CONV) layer} introduces the common explanations about convolutional operations. Section \ref{Converting convolutional operation to matrix multiplication} demonstrates how to convert the convolutional operation to matrix multiplication.  Section \ref{Experiments} shows the result of a simple experiment on training two equivalent networks, a fully connected network and a convolutional neural network.  \textbf{Notation}: In the rest of the note, scalar variables are denoted as non-bold font lowercases, e.g., $c$ and $s$ are scalar values. Matrix and vectors are denoted  by bold font capitals and lowercases respectively. For example, $\mathbf{W} \in \mathbb{R}^{a \times b}$ means a matrix of the shape $a \times b$ and $\mathbf{x} \in \mathbb{R}^{d\times 1}$ means a column vector with $d$ dimensions.	 Section \ref{Fully connected (FC) layer} shows the details of matrix multiplication in fully connected layer. Then, Section \ref{Common explanation on convolutional (CONV) layer} introduces the common explanations about convolutional operations. Section \ref{Converting convolutional operation to matrix multiplication} demonstrates how to convert the convolutional operation to matrix multiplication.  Section \ref{Experiments} shows the result of a simple experiment on training two equivalent networks, a fully connected network and a convolutional neural network.  \textbf{Notation}: In the rest of the note, scalar variables are denoted as non-bold font lowercases, e.g., $c$ and $s$ are scalar values. Matrix and vectors are denoted  by bold font capitals and lowercases respectively. For example, $\mathbf{W} \in \mathbb{R}^{a \times b}$ means a matrix of the shape $a \times b$ and $\mathbf{x} \in \mathbb{R}^{d\times 1}$ means a column vector with $d$ dimensions.	score:509
What is a matrix?	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	score:518
What are some arguments for and against the classification of transgender as a mental illness?	 The performance of the ensemble classifier is evaluated on several multi-class datasets where it demonstrates a superior performance compared to other state-of-the-art classifiers. 	The performance of classifiers can be significantly improved by aggregating the decisions of several classifiers instead of using only a single classifier. This is generally known as ensemble of classifiers, or multiple classifier systems.  The ensemble is obtained by perturbing and combining several individual classifiers \cite{breiman1996bias}. Specifically, it is obtained by perturbing the training set or injecting some randomness in each classifier and aggregating the outputs of the these classifiers in a suitable way.  Decision trees (DT) and Neural networks (NN) are generally used for ensemble generation.	 The performance of the ensemble classifier is evaluated on several multi-class datasets where it demonstrates a superior performance compared to other state-of-the-art classifiers. 	The performance of classifiers can be significantly improved by aggregating the decisions of several classifiers instead of using only a single classifier. This is generally known as ensemble of classifiers, or multiple classifier systems.  The ensemble is obtained by perturbing and combining several individual classifiers \cite{breiman1996bias}. Specifically, it is obtained by perturbing the training set or injecting some randomness in each classifier and aggregating the outputs of the these classifiers in a suitable way.  Decision trees (DT) and Neural networks (NN) are generally used for ensemble generation.	score:273
What are some arguments for and against the classification of transgender as a mental illness?	   However, all of the aforementioned methods aim at improving the \emph{overall} robustness of the classifier. This means that the methods to improve robustness are designed to prevent seed examples in any class from being misclassified as any other class. Achieving such a goal (at least for some definitions of adversarial robustness) requires producing a perfect classifier, and has, unsurprisingly, remained elusive.  Indeed, \citet{mahloujifar2018curse} proved that if the metric probability space is concentrated, overall adversarial robustness is unattainable for any classifier with initial constant error.  We argue that overall robustness may not be the appropriate criteria for measuring system performance in security-sensitive applications, since only certain kinds of adversarial misclassifications pose meaningful threats that provide value for potential adversaries.	   However, all of the aforementioned methods aim at improving the \emph{overall} robustness of the classifier. This means that the methods to improve robustness are designed to prevent seed examples in any class from being misclassified as any other class. Achieving such a goal (at least for some definitions of adversarial robustness) requires producing a perfect classifier, and has, unsurprisingly, remained elusive.  Indeed, \citet{mahloujifar2018curse} proved that if the metric probability space is concentrated, overall adversarial robustness is unattainable for any classifier with initial constant error.  We argue that overall robustness may not be the appropriate criteria for measuring system performance in security-sensitive applications, since only certain kinds of adversarial misclassifications pose meaningful threats that provide value for potential adversaries.	score:285
What are some arguments for and against the classification of transgender as a mental illness?	 They map observed and hidden variables into a vector of score, which are then used as features by classifiers. These methods exploit the superior abilities of generative models in exploiting hidden information and dealing with structured data. However, in these methods, generative models are isolated from the classification process and there is no principled way to tune the generative models as well as the feature mapping to improve classification.  It is desirable to develop a mechanism that can couple the classifier to the generative models to allow fine-tuning of the feature mapping.  Maximum entropy discrimination~\cite{jaakkola1999maxent} provides yet another framework to exploit generative models for classification under the large margin principle. This framework, however, requires deliberately choosing conjugate priors for parameters of the generative models, which limits its application to complex models.	 They map observed and hidden variables into a vector of score, which are then used as features by classifiers. These methods exploit the superior abilities of generative models in exploiting hidden information and dealing with structured data. However, in these methods, generative models are isolated from the classification process and there is no principled way to tune the generative models as well as the feature mapping to improve classification.  It is desirable to develop a mechanism that can couple the classifier to the generative models to allow fine-tuning of the feature mapping.  Maximum entropy discrimination~\cite{jaakkola1999maxent} provides yet another framework to exploit generative models for classification under the large margin principle. This framework, however, requires deliberately choosing conjugate priors for parameters of the generative models, which limits its application to complex models.	score:306
What are some arguments for and against the classification of transgender as a mental illness?	 One may, somewhat arbitrarily, convert a partial ordering   into a full ordering and then apply one of the above algorithms. Doing so ignores   the structure of the partial ordering, however, leading to suboptimal performance   in practice, as confirmed by our experiments. Further, none of these online FDR algorithms,   with the exception of LORD, work under arbitrary dependence.      Fully-ordered testing algorithms have   been derived for model selection by   \cite{g2013false}, \cite{knockoffs} and   \cite{li2017accumulation}. The STAR algorithm of \cite{lei17star},   and its reversed counterpart in \cite{katsevich2018towards}, extend these to the setting of partial orderings by utilizing user interaction.    However, all these algorithms are inherently non-sequential (requiring all p-values up front)    and also need independence of $p$-values, rendering them inapplicable under both   motivations (M1) and (M2).	 One may, somewhat arbitrarily, convert a partial ordering   into a full ordering and then apply one of the above algorithms. Doing so ignores   the structure of the partial ordering, however, leading to suboptimal performance   in practice, as confirmed by our experiments. Further, none of these online FDR algorithms,   with the exception of LORD, work under arbitrary dependence.      Fully-ordered testing algorithms have   been derived for model selection by   \cite{g2013false}, \cite{knockoffs} and   \cite{li2017accumulation}. The STAR algorithm of \cite{lei17star},   and its reversed counterpart in \cite{katsevich2018towards}, extend these to the setting of partial orderings by utilizing user interaction.    However, all these algorithms are inherently non-sequential (requiring all p-values up front)    and also need independence of $p$-values, rendering them inapplicable under both   motivations (M1) and (M2).	score:308
What are some arguments for and against the classification of transgender as a mental illness?	 In this context a better confidence score can improve the final performance of the classifier. The derivation of a good confidence score should therefore be part of the classifier's design, as important as any other component of classifiers' design.  In order to derive a reliable confidence score for NN classifiers, we focus our attention on an empirical observation concerning neural networks trained for classification, which have been shown to demonstrate in parallel useful embedding properties.  Specifically, a common practice these days is to treat one of the upstream layers of a pre-trained network as a representation (or embedding) layer. This layer activation is then used for representing similar objects and train simpler classifiers (such as SVM, or shallower NNs) to perform different tasks, related but not identical to the original task the network had been trained on.	 In this context a better confidence score can improve the final performance of the classifier. The derivation of a good confidence score should therefore be part of the classifier's design, as important as any other component of classifiers' design.  In order to derive a reliable confidence score for NN classifiers, we focus our attention on an empirical observation concerning neural networks trained for classification, which have been shown to demonstrate in parallel useful embedding properties.  Specifically, a common practice these days is to treat one of the upstream layers of a pre-trained network as a representation (or embedding) layer. This layer activation is then used for representing similar objects and train simpler classifiers (such as SVM, or shallower NNs) to perform different tasks, related but not identical to the original task the network had been trained on.	score:312
How is amazon using machine learning to promise delivery date to customer?	 These attacks can affect any deployed system, even if the system is available only as a black box service. Recent advancements in black box \textit{Machine-Learning-as a-Service} providers (such as Amazon AWS\footnote{\url{https://aws.amazon.com/machine-learning/}}, Google Cloud Platform\footnote{\url{https://cloud.google.com/prediction/}} and BigML\footnote{\url{https://bigml. com/}}), promise a new era of flexibility and ubiquity in the usage of machine learning. However, preliminary analysis in \cite{tramer2016stealing} has shown that these services are vulnerable to exploratory attacks, by means of querying the system through their APIs. Membership inference attacks presented in \cite{shokri2016membership}, shows that these services are also vulnerable to data leakage, i.	 These attacks can affect any deployed system, even if the system is available only as a black box service. Recent advancements in black box \textit{Machine-Learning-as a-Service} providers (such as Amazon AWS\footnote{\url{https://aws.amazon.com/machine-learning/}}, Google Cloud Platform\footnote{\url{https://cloud.google.com/prediction/}} and BigML\footnote{\url{https://bigml. com/}}), promise a new era of flexibility and ubiquity in the usage of machine learning. However, preliminary analysis in \cite{tramer2016stealing} has shown that these services are vulnerable to exploratory attacks, by means of querying the system through their APIs. Membership inference attacks presented in \cite{shokri2016membership}, shows that these services are also vulnerable to data leakage, i.	score:372
How is amazon using machine learning to promise delivery date to customer?	 We also use a multimodal autoencoder for comparing products from different language-regions and show preliminary yet promising qualitative results.  	Large e-commerce companies such as Amazon, Alibaba, Flipkart, and Walmart sell billions of products through their websites. Data scientists across these companies try to solve hundreds of machine learning (ML) problems everyday that involve products, e. g., duplicate product detection, product recommendation, safety classification, and price estimation. The first step towards training any ML model usually involves creating feature representation of the relevant entities, i.e., products in this scenario. However, searching through hundreds of data resources within a company, identifying the relevant information, processing and transforming a product related data to a feature vector is a tedious and time consuming process.	  E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell billions of products. Machine learning (ML) algorithms involving products are often used to improve the customer experience and increase revenue, e.g., product similarity, recommendation, and price estimation. The products are required to be represented as features before training an ML algorithm.  In this paper, we propose an approach called MRNet-Product2Vec for creating generic embeddings of products within an e-commerce ecosystem. We learn a dense and low-dimensional embedding where a diverse set of signals related to a product are explicitly injected into its representation. We train a Discriminative Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a product title fed through a Bidirectional RNN and at the output, product labels corresponding to fifteen different tasks are predicted.	score:383
How is amazon using machine learning to promise delivery date to customer?	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	score:392
How is amazon using machine learning to promise delivery date to customer?	   	Over the past few years, providers such as  Google, Microsoft, and Amazon have started to provide customers with access to APIs allowing them to easily embed machine learning tasks into their applications. Organizations can use Machine Learning as a Service (MLaaS) engines to outsource complex tasks, e.g., training classifiers, performing predictions, clustering, etc.   They can also let others query models trained on their data, possibly at a cost.   However, if malicious users were able to recover data used to train these models, the resulting information leakage would create serious issues. In particular, organizations do not have much control over the kind of models and training parameters used by the  platform,  and this might lead to overfitting (i.	   	Over the past few years, providers such as  Google, Microsoft, and Amazon have started to provide customers with access to APIs allowing them to easily embed machine learning tasks into their applications. Organizations can use Machine Learning as a Service (MLaaS) engines to outsource complex tasks, e.g., training classifiers, performing predictions, clustering, etc.   They can also let others query models trained on their data, possibly at a cost.   However, if malicious users were able to recover data used to train these models, the resulting information leakage would create serious issues. In particular, organizations do not have much control over the kind of models and training parameters used by the  platform,  and this might lead to overfitting (i.	score:393
How is amazon using machine learning to promise delivery date to customer?	 Cloud based machine learning services (such as Amazon AWS Machine Learning\footnote{\url{https://aws.amazon.com/machine-learning/}} and Google Cloud Platform\footnote{\url{cloud.google.com/machine-learning}}), which provide APIs for accessing predictive analytics as a service, are also vulnerable to similar black box attacks \citep{tramer2016stealing}.    \begin{figure}[t] \centering \subfloat[An adversary making probes to the black box model \textit{C}, can learn it as \textit{C}', using active learning.]{\includegraphics[width=1\linewidth]{figs/adversarail_model.png}} \\  \subfloat[Example task of attacking behavioral CAPTCHA. Black box model $C$, based on Mouse Speed and Click Time features, is used to detect benign users from bots.	 Cloud based machine learning services (such as Amazon AWS Machine Learning\footnote{\url{https://aws.amazon.com/machine-learning/}} and Google Cloud Platform\footnote{\url{cloud.google.com/machine-learning}}), which provide APIs for accessing predictive analytics as a service, are also vulnerable to similar black box attacks \citep{tramer2016stealing}.    \begin{figure}[t] \centering \subfloat[An adversary making probes to the black box model \textit{C}, can learn it as \textit{C}', using active learning.]{\includegraphics[width=1\linewidth]{figs/adversarail_model.png}} \\  \subfloat[Example task of attacking behavioral CAPTCHA. Black box model $C$, based on Mouse Speed and Click Time features, is used to detect benign users from bots.	score:394
What are the best options for laptops in data science/analysis/machine learning?	 	How can one do meaningful statistical inference and machine learning when data are \emph{re-used} across analyses? The situation is common in empirical science, especially as data sets get bigger and more complex. For example, analysts often clean the data and perform various exploratory analyses---visualizations, computing descriptive statistics---before selecting how data will be treated.  Many times the main analysis also proceeds in stages, with some sort of feature selection followed by inference using the selected features. In such settings, the analyses performed in later stages are chosen \emph{adaptively} based on the results of earlier stages that used the same data. Adaptivity comes into even sharper relief when data are shared across multiple studies, and the choice of the research question in subsequent studies may depend on the outcomes of earlier ones.	 	How can one do meaningful statistical inference and machine learning when data are \emph{re-used} across analyses? The situation is common in empirical science, especially as data sets get bigger and more complex. For example, analysts often clean the data and perform various exploratory analyses---visualizations, computing descriptive statistics---before selecting how data will be treated.  Many times the main analysis also proceeds in stages, with some sort of feature selection followed by inference using the selected features. In such settings, the analyses performed in later stages are chosen \emph{adaptively} based on the results of earlier stages that used the same data. Adaptivity comes into even sharper relief when data are shared across multiple studies, and the choice of the research question in subsequent studies may depend on the outcomes of earlier ones.	score:321
What are the best options for laptops in data science/analysis/machine learning?	  One of the fundamental problems in machine learning is the estimation of a probability distribution from data.  Many techniques have been proposed to study the structure of data, most often building around the assumption that observations lie on a lower-dimensional manifold of high probability. It has been more difficult, however, to exploit this insight to build explicit, tractable density models for high-dimensional data.  In this paper, we introduce the \emph{deep density model} (DDM), a new approach to density estimation. We exploit insights from deep learning to construct a bijective map to a representation space, under which the transformation of the distribution of the data is approximately factorized and has identical and known marginal densities. The simplicity of the latent distribution under the model allows us to feasibly explore it, and the invertibility of the map to characterize contraction of measure across it.	  One of the fundamental problems in machine learning is the estimation of a probability distribution from data.  Many techniques have been proposed to study the structure of data, most often building around the assumption that observations lie on a lower-dimensional manifold of high probability. It has been more difficult, however, to exploit this insight to build explicit, tractable density models for high-dimensional data.  In this paper, we introduce the \emph{deep density model} (DDM), a new approach to density estimation. We exploit insights from deep learning to construct a bijective map to a representation space, under which the transformation of the distribution of the data is approximately factorized and has identical and known marginal densities. The simplicity of the latent distribution under the model allows us to feasibly explore it, and the invertibility of the map to characterize contraction of measure across it.	score:324
What are the best options for laptops in data science/analysis/machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:326
What are the best options for laptops in data science/analysis/machine learning?	  Assessing the performance of a learned model is a crucial part of machine learning. However, in some domains only positive and unlabeled examples are available, which prohibits the use of most standard evaluation metrics. We propose an approach to estimate any metric based on contingency tables, including ROC and PR curves, using only positive and unlabeled data.  Estimating these performance metrics is essentially reduced to estimating the fraction of (latent) positives in the unlabeled set, assuming known positives are a random sample of all positives. We provide theoretical bounds on the quality of our estimates, illustrate the importance of estimating the fraction of positives in the unlabeled set and demonstrate empirically that we are able to reliably estimate ROC and PR curves on real data.	  Assessing the performance of a learned model is a crucial part of machine learning. However, in some domains only positive and unlabeled examples are available, which prohibits the use of most standard evaluation metrics. We propose an approach to estimate any metric based on contingency tables, including ROC and PR curves, using only positive and unlabeled data.  Estimating these performance metrics is essentially reduced to estimating the fraction of (latent) positives in the unlabeled set, assuming known positives are a random sample of all positives. We provide theoretical bounds on the quality of our estimates, illustrate the importance of estimating the fraction of positives in the unlabeled set and demonstrate empirically that we are able to reliably estimate ROC and PR curves on real data.	score:326
What are the best options for laptops in data science/analysis/machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	score:330
Will quantum computers allow machine learning to simulate a human brain?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:329
Will quantum computers allow machine learning to simulate a human brain?	  Quantum machine learning has received significant attention in recent years, and promising progress has been made in the development of quantum algorithms to speed up traditional machine learning tasks. In this work, however, we focus on investigating the information-theoretic upper bounds of sample complexity---how many training  samples are sufficient to predict the future behaviour of an unknown target function.  This kind of problem is, arguably, one of the most fundamental problems in statistical learning theory and the bounds for practical settings can be completely characterised by a simple measure of complexity.  Our main result in the paper is that, for learning an unknown quantum measurement, the upper bound, given by the fat-shattering dimension, is linearly proportional to the dimension of the underlying Hilbert space.	  Quantum machine learning has received significant attention in recent years, and promising progress has been made in the development of quantum algorithms to speed up traditional machine learning tasks. In this work, however, we focus on investigating the information-theoretic upper bounds of sample complexity---how many training  samples are sufficient to predict the future behaviour of an unknown target function.  This kind of problem is, arguably, one of the most fundamental problems in statistical learning theory and the bounds for practical settings can be completely characterised by a simple measure of complexity.  Our main result in the paper is that, for learning an unknown quantum measurement, the upper bound, given by the fat-shattering dimension, is linearly proportional to the dimension of the underlying Hilbert space.	score:332
Will quantum computers allow machine learning to simulate a human brain?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:334
Will quantum computers allow machine learning to simulate a human brain?	  In terms of training computers to perform similar tasks, deep neural networks have demonstrated superior performance among machine learning models (\cite{lecun98, alexnet, resnet}). However, these networks have been trained dramatically differently from a learning child, requiring labels for every training example, following a purely supervised training scheme.  Neural networks are defined by huge amounts of parameters to be optimized. Therefore, a plethora of labeled training data is required, which might be costly and time consuming to obtain.  It is desirable to train machine learning models without labels (unsupervisedly) or with only some fraction of the data labeled (semi-supervisedly).  Recently, efforts have been made to train neural networks in an unsupervised or semi-supervised manner yielding promising results.	  In terms of training computers to perform similar tasks, deep neural networks have demonstrated superior performance among machine learning models (\cite{lecun98, alexnet, resnet}). However, these networks have been trained dramatically differently from a learning child, requiring labels for every training example, following a purely supervised training scheme.  Neural networks are defined by huge amounts of parameters to be optimized. Therefore, a plethora of labeled training data is required, which might be costly and time consuming to obtain.  It is desirable to train machine learning models without labels (unsupervisedly) or with only some fraction of the data labeled (semi-supervisedly).  Recently, efforts have been made to train neural networks in an unsupervised or semi-supervised manner yielding promising results.	score:335
Will quantum computers allow machine learning to simulate a human brain?	 It relies on coupling the training of multiple models and combining their predictions into a single output. This coupling is performed by encouraging the models to agree with each other on their predictions, while training. We show how the proposed framework is inspired by human learning, in contrast with other approaches commonly used in machine learning.  Finally, we provide experimental results which exhibit how our framework successfully manages to outperform cross-validation and other ensemble methods that do not couple the training of the models.  \begin{figure}[t!]  \centering      \includegraphics[width=0.5\textwidth, trim=9.2cm 2.5cm 1.5cm 6cm, clip]{diagram.pdf}   \caption{Illustration of the agreement-based learning framework that we propose.	 It relies on coupling the training of multiple models and combining their predictions into a single output. This coupling is performed by encouraging the models to agree with each other on their predictions, while training. We show how the proposed framework is inspired by human learning, in contrast with other approaches commonly used in machine learning.  Finally, we provide experimental results which exhibit how our framework successfully manages to outperform cross-validation and other ensemble methods that do not couple the training of the models.  \begin{figure}[t!]  \centering      \includegraphics[width=0.5\textwidth, trim=9.2cm 2.5cm 1.5cm 6cm, clip]{diagram.pdf}   \caption{Illustration of the agreement-based learning framework that we propose.	score:337
Why does batch normalization help?	 In other words, a region of the noisy speech spectrogram is detected as noise if it doesn't fit our understanding of what a typical speech spectrogram should look like. Unfortunately, without the use of a separate noise model, noise reduction for speech-shaped noise, such as babble noise, is limited. However, the benefit of the outlier framework is that it removes noise that is strongly inconsistent with the speech dictionary with higher priority. For noise that is more overlapped with speech, it focuses on preserving the underlying speech as much as possible, and thus limits the amount of speech distortion while removing less noise.	 In other words, a region of the noisy speech spectrogram is detected as noise if it doesn't fit our understanding of what a typical speech spectrogram should look like. Unfortunately, without the use of a separate noise model, noise reduction for speech-shaped noise, such as babble noise, is limited. However, the benefit of the outlier framework is that it removes noise that is strongly inconsistent with the speech dictionary with higher priority. For noise that is more overlapped with speech, it focuses on preserving the underlying speech as much as possible, and thus limits the amount of speech distortion while removing less noise.	score:418
Why does batch normalization help?	  We also prove, as far as we know, the first lower bound on the average number of questions to be asked for randomized nearest-neighbor search in this setup (see Section \ref{sec:skd}). Then, in Section \ref{sec:btd}, we ask what can be done if no characterization of the hidden space is known and therefore cannot be used as an input to the algorithms.  In that case, we cannot estimate, or limit, ranks anymore if we have partial rank information. Nevertheless, we develop algorithms that can decompose the space such that dissimilar objects are likely to get separated, and similar objects have the tendency to stay together. This generalizes the notion of randomized $k$-$d$-trees (\cite{DasguptaRPT}) to our setup.	  We also prove, as far as we know, the first lower bound on the average number of questions to be asked for randomized nearest-neighbor search in this setup (see Section \ref{sec:skd}). Then, in Section \ref{sec:btd}, we ask what can be done if no characterization of the hidden space is known and therefore cannot be used as an input to the algorithms.  In that case, we cannot estimate, or limit, ranks anymore if we have partial rank information. Nevertheless, we develop algorithms that can decompose the space such that dissimilar objects are likely to get separated, and similar objects have the tendency to stay together. This generalizes the notion of randomized $k$-$d$-trees (\cite{DasguptaRPT}) to our setup.	score:460
Why does batch normalization help?	 Many core problems in the field of Computer Science, Economic, and Operations Research can be readily expressed in this form, rendering this minimization problem far-reaching. That being said, in its full generality this problem is just too hard to solve or even to approximate. As a consequence,  various structural assumptions on the objective function and the constraints set, along with better-suited optimization algorithms, have been proposed so as to make this problem viable.	 Many core problems in the field of Computer Science, Economic, and Operations Research can be readily expressed in this form, rendering this minimization problem far-reaching. That being said, in its full generality this problem is just too hard to solve or even to approximate. As a consequence,  various structural assumptions on the objective function and the constraints set, along with better-suited optimization algorithms, have been proposed so as to make this problem viable.	score:465
Why does batch normalization help?	 As alternatives, non-parametric methods such as \emph{kernel density estimation} \citep{Inbook:Csorgo+Horvath:1988,book:Brodsky+Darkhovsky:1993} are designed with no particular parametric assumption.  However, they tend to be less accurate in high-dimensional problems because of the so-called \emph{curse of dimensionality} \citep{book:Bellman:1961,book:Vapnik:1998}.   To overcome this difficulty, a new strategy was introduced recently, which estimates the \emph{ratio} of probability densities directly without going through density estimation \citep{book:Sugiyama+etal:2012}. The rationale of this density-ratio estimation idea is that knowing the two densities implies knowing the density ratio, but not vice versa; knowing the ratio does not necessarily imply knowing the two densities because such decomposition is not unique (Figure~\ref{fig:rationale}).	 As alternatives, non-parametric methods such as \emph{kernel density estimation} \citep{Inbook:Csorgo+Horvath:1988,book:Brodsky+Darkhovsky:1993} are designed with no particular parametric assumption.  However, they tend to be less accurate in high-dimensional problems because of the so-called \emph{curse of dimensionality} \citep{book:Bellman:1961,book:Vapnik:1998}.   To overcome this difficulty, a new strategy was introduced recently, which estimates the \emph{ratio} of probability densities directly without going through density estimation \citep{book:Sugiyama+etal:2012}. The rationale of this density-ratio estimation idea is that knowing the two densities implies knowing the density ratio, but not vice versa; knowing the ratio does not necessarily imply knowing the two densities because such decomposition is not unique (Figure~\ref{fig:rationale}).	score:466
Why does batch normalization help?	 While, say, a regret of $\Theta(\sqrt{T})$ is known, we argue that it important to ask what is the provably optimal algorithm for this problem --- both because it leads to natural algorithms, as well as because regret is in fact often comparable in magnitude to the final payoffs and hence is a non-negligible term.  In the basic setting, the result essentially follows from a classical result of Cover from '65.  Here instead, we focus on another standard setting, of time-discounted payoffs, where the final ``stopping time'' is not specified. We exhibit an explicit characterization of the optimal regret for this setting.  To obtain our main result, we show that the optimal payoff functions have to satisfy the Hermite differential equation, and hence are given by the solutions to this equation.	 While, say, a regret of $\Theta(\sqrt{T})$ is known, we argue that it important to ask what is the provably optimal algorithm for this problem --- both because it leads to natural algorithms, as well as because regret is in fact often comparable in magnitude to the final payoffs and hence is a non-negligible term.  In the basic setting, the result essentially follows from a classical result of Cover from '65.  Here instead, we focus on another standard setting, of time-discounted payoffs, where the final ``stopping time'' is not specified. We exhibit an explicit characterization of the optimal regret for this setting.  To obtain our main result, we show that the optimal payoff functions have to satisfy the Hermite differential equation, and hence are given by the solutions to this equation.	score:473
Where can I find the best locality-sensitive hashing implementation for clustering similar instances?	 Saving that pale in comparison to the costs of mismatch between those objectives and the intended use of clustering results. I argue the severity of this problem and describe some recent proposals aiming to address this crucial lacuna. 	Clustering is one of the most basic and useful data processing tasks. It is being routinely applied in a wide variety of applications.  Not surprisingly, there exist many clustering algorithms. However, clustering is an ill defined problem - given a data set, it is not clear what a \emph{correct} clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. In contrast with other common learn- ing tasks, like classification prediction, clustering does not have a well defined ground truth.	 Saving that pale in comparison to the costs of mismatch between those objectives and the intended use of clustering results. I argue the severity of this problem and describe some recent proposals aiming to address this crucial lacuna. 	Clustering is one of the most basic and useful data processing tasks. It is being routinely applied in a wide variety of applications.  Not surprisingly, there exist many clustering algorithms. However, clustering is an ill defined problem - given a data set, it is not clear what a \emph{correct} clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. In contrast with other common learn- ing tasks, like classification prediction, clustering does not have a well defined ground truth.	score:282
Where can I find the best locality-sensitive hashing implementation for clustering similar instances?	 However, if the positions of a few key atoms are set appropriately then certain amino acids will never interact, making it possible to decompose the problem into multiple independent subproblems and solve each separately. A local recursive decomposition algorithm for continuous problems can do exactly this.  We first define local structure and then present our algorithm, RDIS, which (asymptotically) finds the global optimum of a nonconvex function by (R)ecursively (D)ecomposing the function into locally (I)ndependent (S)ubspaces.	 However, if the positions of a few key atoms are set appropriately then certain amino acids will never interact, making it possible to decompose the problem into multiple independent subproblems and solve each separately. A local recursive decomposition algorithm for continuous problems can do exactly this.  We first define local structure and then present our algorithm, RDIS, which (asymptotically) finds the global optimum of a nonconvex function by (R)ecursively (D)ecomposing the function into locally (I)ndependent (S)ubspaces.	score:296
Where can I find the best locality-sensitive hashing implementation for clustering similar instances?	   When time efficiency is more important than predictive power, one could opt for employing specialized approximate algorithms. Locality-sensitive hashing techniques, which are popular for the related problem of nearest neighbor search, would be a good choice. Recently, a locality-sensing hashing method for maximum inner product search has been developed \cite{Shrivastava2014,Shrivastava2015}.  However, in addition to delivering approximate predictions, methods of that kind are usually restricted to finding the top-1 set. Another approximate method would be to cluster the queries $x$ in several groups, for which rankings of targets $y$ can be precomputed by means of predictive indices and related data structures \cite{Agarwal2012,Goel2009}.	   When time efficiency is more important than predictive power, one could opt for employing specialized approximate algorithms. Locality-sensitive hashing techniques, which are popular for the related problem of nearest neighbor search, would be a good choice. Recently, a locality-sensing hashing method for maximum inner product search has been developed \cite{Shrivastava2014,Shrivastava2015}.  However, in addition to delivering approximate predictions, methods of that kind are usually restricted to finding the top-1 set. Another approximate method would be to cluster the queries $x$ in several groups, for which rankings of targets $y$ can be precomputed by means of predictive indices and related data structures \cite{Agarwal2012,Goel2009}.	score:300
Where can I find the best locality-sensitive hashing implementation for clustering similar instances?	 However, in presence of more than a few dozen variables, in order to deal with that many features, or even to simply enumerate those, a certain form of factorization or recursivity is needed. In this paper, we propose to use a hierarchical structure based on directed acyclic graphs, which is natural in our context of non-linear variable selection.   We consider a positive definite kernel that can be expressed as a large sum of positive definite \emph{basis} or \emph{local kernels}.  This exactly corresponds to the situation where a large feature space is the concatenation of  smaller feature spaces, and we aim to do selection among these many kernels (or equivalently feature spaces), which may be done through  multiple kernel learning~\citep{skm}. One major difficulty however is that the number of these smaller kernels is usually exponential in the dimension of the input space and applying multiple kernel learning directly to this decomposition would be intractable.	 However, in presence of more than a few dozen variables, in order to deal with that many features, or even to simply enumerate those, a certain form of factorization or recursivity is needed. In this paper, we propose to use a hierarchical structure based on directed acyclic graphs, which is natural in our context of non-linear variable selection.   We consider a positive definite kernel that can be expressed as a large sum of positive definite \emph{basis} or \emph{local kernels}.  This exactly corresponds to the situation where a large feature space is the concatenation of  smaller feature spaces, and we aim to do selection among these many kernels (or equivalently feature spaces), which may be done through  multiple kernel learning~\citep{skm}. One major difficulty however is that the number of these smaller kernels is usually exponential in the dimension of the input space and applying multiple kernel learning directly to this decomposition would be intractable.	score:303
Where can I find the best locality-sensitive hashing implementation for clustering similar instances?	   Our results show that, in general, applying cross-domain by proximity increases the performance of the majority of the recommenders in terms of relevance. This is the first work, to the best of our knowledge, where so many domains (eight) are combined in the tourism context where a temporal split is used, and thus we expect these results could provide readers with an overall picture of what can be achieved in a real-world environment.  	The great development of location-based social networks (LBSNs) in recent years has encouraged the research into the problem of Point-of-Interest (POI) or venue recommendation, i.e., suggesting new places to visit by analyzing the users' tastes, needs, and movement patterns. Foursquare, Gowalla, or GeoLife are examples of these kind of social networks, where users record \checkins they make to certain venues (restaurants, cinemas, hotels, etc.	   Our results show that, in general, applying cross-domain by proximity increases the performance of the majority of the recommenders in terms of relevance. This is the first work, to the best of our knowledge, where so many domains (eight) are combined in the tourism context where a temporal split is used, and thus we expect these results could provide readers with an overall picture of what can be achieved in a real-world environment.  	The great development of location-based social networks (LBSNs) in recent years has encouraged the research into the problem of Point-of-Interest (POI) or venue recommendation, i.e., suggesting new places to visit by analyzing the users' tastes, needs, and movement patterns. Foursquare, Gowalla, or GeoLife are examples of these kind of social networks, where users record \checkins they make to certain venues (restaurants, cinemas, hotels, etc.	score:303
Where can I find the best locality-sensitive hashing implementation for clustering similar URLs?	 This is because the reconstruction error encourages exact match of samples from the reverse mapping, which may in turn encourage the forward-mapping to keep the sample close to the original domain. Normally, the adversarial objectives  would counter this effect; however, when data from the target domain are scarce, it is very difficult to learn a powerful discriminator that can capture meaningful properties of the target distribution.  Therefore, the resulting mappings learned is likely to be sub-optimal. Importantly, for the learned mapping to be meaningful, it is not necessary to have the exact reconstruction. As long as the `semantic' information is preserved and the `style' matches the corresponding distribution, it would be a valid mapping.   To address this issue, we propose an augmented cyclic adversarial learning model (ACAL) for domain adaptation.	 This is because the reconstruction error encourages exact match of samples from the reverse mapping, which may in turn encourage the forward-mapping to keep the sample close to the original domain. Normally, the adversarial objectives  would counter this effect; however, when data from the target domain are scarce, it is very difficult to learn a powerful discriminator that can capture meaningful properties of the target distribution.  Therefore, the resulting mappings learned is likely to be sub-optimal. Importantly, for the learned mapping to be meaningful, it is not necessary to have the exact reconstruction. As long as the `semantic' information is preserved and the `style' matches the corresponding distribution, it would be a valid mapping.   To address this issue, we propose an augmented cyclic adversarial learning model (ACAL) for domain adaptation.	score:305
Where can I find the best locality-sensitive hashing implementation for clustering similar URLs?	 Thus, our proposed method, called ``Nystr\"om via QR Decomposition,'' can be used with any landmark selection algorithm to find the best rank-$r$ approximation for a given set of landmark points. We also provide intuitive and real-world examples to show the superior performance and efficiency of our method in Section~\ref{sec:nys-eig}.  \item Second, we present a random-projection-type landmark selection algorithm which easily scales to large-scale high-dimensional data sets.  Our proposed ``Randomized Clustered Nystr\"om method'' presented in Section~\ref{sec:random-clustered-nys} performs the K-means clustering algorithm on the random projections of input data points and it requires only two passes over the original data set. Thus our method leads to significant memory and computation savings in comparison with the Clustered Nystr\"om method.	 Thus, our proposed method, called ``Nystr\"om via QR Decomposition,'' can be used with any landmark selection algorithm to find the best rank-$r$ approximation for a given set of landmark points. We also provide intuitive and real-world examples to show the superior performance and efficiency of our method in Section~\ref{sec:nys-eig}.  \item Second, we present a random-projection-type landmark selection algorithm which easily scales to large-scale high-dimensional data sets.  Our proposed ``Randomized Clustered Nystr\"om method'' presented in Section~\ref{sec:random-clustered-nys} performs the K-means clustering algorithm on the random projections of input data points and it requires only two passes over the original data set. Thus our method leads to significant memory and computation savings in comparison with the Clustered Nystr\"om method.	score:306
Where can I find the best locality-sensitive hashing implementation for clustering similar URLs?	 In spite of their contribution to important advances in the area, there are still limitations that need to be addressed. These limitations are mainly related to the metafeatures and the metatarget, which are the focus of this work.   The main limitation regarding metafeatures is that most approaches only describe the recommendation problem using descriptive characteristics of the rating matrix and estimates of performance on samples (i. e. landmarkers), overlooking a wide spectrum of other possibilities. Furthermore, existing papers typically perform a limited comparison between the proposed metafeatures and the ones proposed in other studies. Additionally, there is a lack of studies combining metafeatures from multiple domains in a single collection and validating their individual and combined merits in the same experimental setup.	 In spite of their contribution to important advances in the area, there are still limitations that need to be addressed. These limitations are mainly related to the metafeatures and the metatarget, which are the focus of this work.   The main limitation regarding metafeatures is that most approaches only describe the recommendation problem using descriptive characteristics of the rating matrix and estimates of performance on samples (i. e. landmarkers), overlooking a wide spectrum of other possibilities. Furthermore, existing papers typically perform a limited comparison between the proposed metafeatures and the ones proposed in other studies. Additionally, there is a lack of studies combining metafeatures from multiple domains in a single collection and validating their individual and combined merits in the same experimental setup.	score:309
Where can I find the best locality-sensitive hashing implementation for clustering similar URLs?	 This is natural since the choices of neural network architectures are typically discrete, such as the filter size in CNN and connection topology in RNN cell. However, directly searching the best architecture within discrete space is inefficient given the exponentially growing search space with the number of choices increasing. In this work, we instead propose to \emph{optimize} network architecture by mapping architectures into a continuous vector space (i. e., network embeddings) and conducting optimization in this continuous space via gradient based method. On one hand, similar to the distributed representation of natural language~\cite{w2v,doc2vec}, the continuous representation of an architecture is more compact and efficient in representing its topological information; On the other hand, optimizing in a continuous space is much easier than directly searching within discrete space due to better smoothness.	 This is natural since the choices of neural network architectures are typically discrete, such as the filter size in CNN and connection topology in RNN cell. However, directly searching the best architecture within discrete space is inefficient given the exponentially growing search space with the number of choices increasing. In this work, we instead propose to \emph{optimize} network architecture by mapping architectures into a continuous vector space (i. e., network embeddings) and conducting optimization in this continuous space via gradient based method. On one hand, similar to the distributed representation of natural language~\cite{w2v,doc2vec}, the continuous representation of an architecture is more compact and efficient in representing its topological information; On the other hand, optimizing in a continuous space is much easier than directly searching within discrete space due to better smoothness.	score:310
Where can I find the best locality-sensitive hashing implementation for clustering similar URLs?	 Saving that pale in comparison to the costs of mismatch between those objectives and the intended use of clustering results. I argue the severity of this problem and describe some recent proposals aiming to address this crucial lacuna. 	Clustering is one of the most basic and useful data processing tasks. It is being routinely applied in a wide variety of applications.  Not surprisingly, there exist many clustering algorithms. However, clustering is an ill defined problem - given a data set, it is not clear what a \emph{correct} clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. In contrast with other common learn- ing tasks, like classification prediction, clustering does not have a well defined ground truth.	 Saving that pale in comparison to the costs of mismatch between those objectives and the intended use of clustering results. I argue the severity of this problem and describe some recent proposals aiming to address this crucial lacuna. 	Clustering is one of the most basic and useful data processing tasks. It is being routinely applied in a wide variety of applications.  Not surprisingly, there exist many clustering algorithms. However, clustering is an ill defined problem - given a data set, it is not clear what a \emph{correct} clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. In contrast with other common learn- ing tasks, like classification prediction, clustering does not have a well defined ground truth.	score:311
What is the best programming language for machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:400
What is the best programming language for machine learning?	  Recently, Deep Learning has been showing promising results in various Artificial Intelligence applications like image recognition, natural language processing, language modeling, neural machine translation, etc. Although, in general, it is computationally more expensive as compared to classical machine learning techniques, their results are found to be more effective in some cases.  Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 \& 7 layers architectures with the four different feature sets, and found that  irrespective of the features inputs, the classical RF accuracy outperforms the DNN.	  Recently, Deep Learning has been showing promising results in various Artificial Intelligence applications like image recognition, natural language processing, language modeling, neural machine translation, etc. Although, in general, it is computationally more expensive as compared to classical machine learning techniques, their results are found to be more effective in some cases.  Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 \& 7 layers architectures with the four different feature sets, and found that  irrespective of the features inputs, the classical RF accuracy outperforms the DNN.	score:435
What is the best programming language for machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:437
What is the best programming language for machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:440
What is the best programming language for machine learning?	 	One of the most significant recent developments in machine learning has been the resurgence of ``deep learning'', usually in the form of artificial neural networks. These systems are based on a multi-layered architecture, where the input goes through several transformations, with higher-level concepts derived from lower-level ones. Thus, these systems are considered to be particularly suitable for hard AI tasks, such as computer vision and language processing.	 	One of the most significant recent developments in machine learning has been the resurgence of ``deep learning'', usually in the form of artificial neural networks. These systems are based on a multi-layered architecture, where the input goes through several transformations, with higher-level concepts derived from lower-level ones. Thus, these systems are considered to be particularly suitable for hard AI tasks, such as computer vision and language processing.	score:442
How do I solve the problem: [math] A=\begin{pmatrix} -2 &4 \\   -5& 7 \end{pmatrix}, A^{n}=? [/math]?	 Formally, we are interested in the following optimization problem, which we name as simplicial SymNMF (SSymNMF): \begin{equation} \begin{aligned} & \underset{W}{\text{minimize}} && \frac{1}{4}||P - WW^T||_F^2 \\ & \text{subject to} && W \in \RR_+^{n\times k}, \quad W\mathbf{1}_k = \mathbf{1}_n \end{aligned} \label{equ:sof} \end{equation} where $P\geq 0$ and $P\in\SPD_+^n$ is positive semidefinite.  (\ref{equ:sof}) was proposed as a formulation for probabilistic clustering~\citep{zhao2015sof}. The input matrix $P$ is interpreted as the co-cluster affinity matrix, i.e., entry $P_{ij}$ corresponds to the degree to which we encourage data instances $\mathbf{x}_i$ and $\mathbf{x}_j$ to be in the same cluster. Each row of $W$ then corresponds to the probability distribution of instance $\mathbf{x}_i$ being in different clusters.	 Formally, we are interested in the following optimization problem, which we name as simplicial SymNMF (SSymNMF): \begin{equation} \begin{aligned} & \underset{W}{\text{minimize}} && \frac{1}{4}||P - WW^T||_F^2 \\ & \text{subject to} && W \in \RR_+^{n\times k}, \quad W\mathbf{1}_k = \mathbf{1}_n \end{aligned} \label{equ:sof} \end{equation} where $P\geq 0$ and $P\in\SPD_+^n$ is positive semidefinite.  (\ref{equ:sof}) was proposed as a formulation for probabilistic clustering~\citep{zhao2015sof}. The input matrix $P$ is interpreted as the co-cluster affinity matrix, i.e., entry $P_{ij}$ corresponds to the degree to which we encourage data instances $\mathbf{x}_i$ and $\mathbf{x}_j$ to be in the same cluster. Each row of $W$ then corresponds to the probability distribution of instance $\mathbf{x}_i$ being in different clusters.	score:324
How do I solve the problem: [math] A=\begin{pmatrix} -2 &4 \\   -5& 7 \end{pmatrix}, A^{n}=? [/math]?	  However, recently a relaxation of the optimization problem has been reduced to an eigenvector problem~\cite{shi2000normalized}.  Given the $n\times n$ similarity matrix $\mtx{W}$, one defines the normalized Laplacian\footnote{Note that one may also consider the Laplacian $\mtx{L} = \mtx{D} - \mtx{W}$.} matrix $\mtx{L} \in \R^{n \times n}$ by \begin{equation}\label{eq:L} \mtx{L} = \mtx{D}^{-1/2} (\mtx{D} - \mtx{W})\mtx{D}^{-1/2}, \end{equation} where $\mtx{D} \in \R^{n \times n}$ is the diagonal matrix of degree nodes, \begin{equation}\label{eq:D} \mtx{D}_{ii}=\sum_j \mtx{W}_{ij}.	  However, recently a relaxation of the optimization problem has been reduced to an eigenvector problem~\cite{shi2000normalized}.  Given the $n\times n$ similarity matrix $\mtx{W}$, one defines the normalized Laplacian\footnote{Note that one may also consider the Laplacian $\mtx{L} = \mtx{D} - \mtx{W}$.} matrix $\mtx{L} \in \R^{n \times n}$ by \begin{equation}\label{eq:L} \mtx{L} = \mtx{D}^{-1/2} (\mtx{D} - \mtx{W})\mtx{D}^{-1/2}, \end{equation} where $\mtx{D} \in \R^{n \times n}$ is the diagonal matrix of degree nodes, \begin{equation}\label{eq:D} \mtx{D}_{ii}=\sum_j \mtx{W}_{ij}.	score:326
How do I solve the problem: [math] A=\begin{pmatrix} -2 &4 \\   -5& 7 \end{pmatrix}, A^{n}=? [/math]?	  Consequently, a nonnegative matrix $\mat{A}$ is near $r$-separable if it can be represented as  \begin{equation}  \mat{A} = (\mat{A})_{:\set{K}} \mat{Y} + \mat{N} ,  \end{equation}  where $\mat{N}$ is a noise matrix.    \label[definition]{separableMatrix} \end{definition}   When $\mat{A}$ presents this type of special structure, the NMF problem (now denoted as separable NMF, SNMF) can be simply modeled as \begin{equation}  \min_{\substack{\set{K} \subset \{ 1, \dots, n\} \\ \mat{Y} \in \Real^{r \times n}}} \norm{\mat{A} - (\mat{A})_{:\set{K}} \mat{Y}}{F}^2  \quad  \text{s.	  Consequently, a nonnegative matrix $\mat{A}$ is near $r$-separable if it can be represented as  \begin{equation}  \mat{A} = (\mat{A})_{:\set{K}} \mat{Y} + \mat{N} ,  \end{equation}  where $\mat{N}$ is a noise matrix.    \label[definition]{separableMatrix} \end{definition}   When $\mat{A}$ presents this type of special structure, the NMF problem (now denoted as separable NMF, SNMF) can be simply modeled as \begin{equation}  \min_{\substack{\set{K} \subset \{ 1, \dots, n\} \\ \mat{Y} \in \Real^{r \times n}}} \norm{\mat{A} - (\mat{A})_{:\set{K}} \mat{Y}}{F}^2  \quad  \text{s.	score:329
How do I solve the problem: [math] A=\begin{pmatrix} -2 &4 \\   -5& 7 \end{pmatrix}, A^{n}=? [/math]?	t.} \quad \mathbf{X = XZ + E}.  \label{(1)} \end{align} However rank minimisation is an intractable problem. Therefore LRR actually uses the nuclear norm $\| \cdot \|_*$ (sum of the matrix's singular values) as the closest convex relation \begin{align} \min_{\mathbf{Z, E}} \; \frac{1}{2}\| \mathbf E \|_{\ell} + \| \mathbf{Z} \|_*, \quad \text{s.t.} \quad \mathbf{X = XZ + E},  \label{(2)} \end{align} where $\| \cdot \|_{\ell}$ is a placeholder for the norm most appropriate to the expected noise type.	t.} \quad \mathbf{X = XZ + E}.  \label{(1)} \end{align} However rank minimisation is an intractable problem. Therefore LRR actually uses the nuclear norm $\| \cdot \|_*$ (sum of the matrix's singular values) as the closest convex relation \begin{align} \min_{\mathbf{Z, E}} \; \frac{1}{2}\| \mathbf E \|_{\ell} + \| \mathbf{Z} \|_*, \quad \text{s.t.} \quad \mathbf{X = XZ + E},  \label{(2)} \end{align} where $\| \cdot \|_{\ell}$ is a placeholder for the norm most appropriate to the expected noise type.	score:338
How do I solve the problem: [math] A=\begin{pmatrix} -2 &4 \\   -5& 7 \end{pmatrix}, A^{n}=? [/math]?	   In the case of full-rank matrix completion, a similar nuclear norm minimization problem has been proposed. Suppose $A=Z+N$, where $Z$ is the low-rank matrix that we want to recover, and $N$ is the residual matrix. \citet{MC:Noise:Candes} introduce the following problem \begin{equation} \label{eqn:noise} \begin{array}{ll} \min & \|B\|_* \\ \st & \sqrt{\sum_{(i,j) \in \Omega} (B_{ij}-A_{ij})^2} \leq \delta \end{array} \end{equation} where $\delta$ is an upper bound for $\sqrt{\sum_{(i,j) \in \Omega} N_{ij}^2}$.  The formulation in (\ref{eqn:noise}) also has strong guarantees if $\delta$ is large enough, but optimization  is still a challenge.   On the other hand, practitioners prefer to solve the following nuclear-norm regularized least squares problem \begin{equation} \label{eqn:opt} \min\limits_{B \in \R^{m\times n}} \quad \frac{1}{2} \sum_{(i,j) \in \Omega} (B_{ij}-A_{ij})^2 + \lambda\|B\|_{*}.	   In the case of full-rank matrix completion, a similar nuclear norm minimization problem has been proposed. Suppose $A=Z+N$, where $Z$ is the low-rank matrix that we want to recover, and $N$ is the residual matrix. \citet{MC:Noise:Candes} introduce the following problem \begin{equation} \label{eqn:noise} \begin{array}{ll} \min & \|B\|_* \\ \st & \sqrt{\sum_{(i,j) \in \Omega} (B_{ij}-A_{ij})^2} \leq \delta \end{array} \end{equation} where $\delta$ is an upper bound for $\sqrt{\sum_{(i,j) \in \Omega} N_{ij}^2}$.  The formulation in (\ref{eqn:noise}) also has strong guarantees if $\delta$ is large enough, but optimization  is still a challenge.   On the other hand, practitioners prefer to solve the following nuclear-norm regularized least squares problem \begin{equation} \label{eqn:opt} \min\limits_{B \in \R^{m\times n}} \quad \frac{1}{2} \sum_{(i,j) \in \Omega} (B_{ij}-A_{ij})^2 + \lambda\|B\|_{*}.	score:342
Given [math]A[/math], What is the general formula of [math]A^n[/math]? If [math]A=\begin{pmatrix} -2 &4 \\ -5& 7 \end{pmatrix}[/math], what is the [math]A^n[/math]?	 \end{equation}  where $\mathbf{\mu} = \mathbb{E}[\mathbf{x}]$ is the formalization of the average value and the Covariance matrix $\mathbf{\Sigma} = \mathbb{E}\left[ (\mathbf{x} - \mathbf{\mathbb{E}[\mathbf{x}]})(\mathbf{x} - \mathbf{\mathbb{E}[\mathbf{x}]})^T \right]$ is the generalization of Covariance in $N$-dimensional space. If we assume the data is centered $\sum_{i=1}^{M}\mathbf{x}_i = 0$ for $M$ samples, the information compressed in the Covariance matrix $\mathbf{\Sigma}$ is sufficient to describe the variance of the distribution in all spatial directions; and consequently, the distribution itself: \begin{equation} \label{eq:zero-mean-gaussian} f_X(\mathbf{x};0,\mathbf{\Sigma}) = \frac{1}{(2\pi)^{d/2}|\mathbf{\Sigma}|^{1/2}} \exp \left\lbrace -\frac{1}{2}\mathbf{x}^{T}\Sigma^{-1}\mathbf{x} \right\rbrace.	pdf}   \includegraphics[trim={2cm 1cm 2cm 0.5cm},clip,width=0.49\linewidth]{./figures/new/clustered_dataset.pdf}  \vspace{-5pt}  \caption{\small \textit{Transform-Invariant Non-Parametric Clustering of Covariance Matrices}: \textbf{(left)} Given a dataset of $M$ Covariance matrices; $\mathbf{\Theta} = \{\mathbf{\Sigma}_1, \dots, \mathbf{\Sigma}_M\}$ where  $\mathbf{\Sigma}_i \in \mathds{R}^{N\times N}$ describing data in $\mathbf{x} \in \mathds{R}^{N}$; \textit{with} or \textit{without} a corresponding location $\mu = \{\mu_1, \dots, \mu_M\}$, we assume \textit{structural similarity} between the Covariance matrices (denoted by the colored arrows)  \textbf{(right)} The dataset can be described with $K (3)$ transform-invariant clusters: $\mathbf{\Theta} = \{\textcolor{black}{\mathbf{\Sigma}_2, \mathbf{\Sigma}_3} | \textcolor{black}{\mathbf{\Sigma}_1,\mathbf{\Sigma}_4} |\textcolor{black}{\mathbf{\Sigma}_5,\mathbf{\Sigma}_6}\}$; i.	score:318
Given [math]A[/math], What is the general formula of [math]A^n[/math]? If [math]A=\begin{pmatrix} -2 &4 \\ -5& 7 \end{pmatrix}[/math], what is the [math]A^n[/math]?	 Indeed, suppose that noisy measurements of the sparse signal are taken by a random measurement matrix in the following form: \begin{equation} \label{eq:ncs} \mathbf{y}={\Ab}\mathbf{s}+\mathbf{n}~, \end{equation}  in which, $\mathbf{s}$ is the original ${M\times 1}$ sparse signal, $\mathbf{y}$ is the ${N\times 1}$ vector of measurements, $\mathbf{n}\sim N(0,\sigma_{n}^2I_{N\times N})$ is an ${N\times 1}$ Gaussian noise vector and $\mathbf{A}=[\mathbf{a}_1~~\mathbf{a}_2\dots\mathbf{a}_M]$ is an ${N\times M}$ measurement matrix whose elements are usually generated at random.  More precisely, these elements are independent and identically distributed random variables drawn from some specific distributions (such as Gaussian, Bernoulli, etc.), so that the overall measurement  matrix will be appropriate in the framework of recovery in compressive sampling~\cite{donoho2006compressed,candes2006robust,candes2006compressive,baraniuk2008compressive}.	 Indeed, suppose that noisy measurements of the sparse signal are taken by a random measurement matrix in the following form: \begin{equation} \label{eq:ncs} \mathbf{y}={\Ab}\mathbf{s}+\mathbf{n}~, \end{equation}  in which, $\mathbf{s}$ is the original ${M\times 1}$ sparse signal, $\mathbf{y}$ is the ${N\times 1}$ vector of measurements, $\mathbf{n}\sim N(0,\sigma_{n}^2I_{N\times N})$ is an ${N\times 1}$ Gaussian noise vector and $\mathbf{A}=[\mathbf{a}_1~~\mathbf{a}_2\dots\mathbf{a}_M]$ is an ${N\times M}$ measurement matrix whose elements are usually generated at random.  More precisely, these elements are independent and identically distributed random variables drawn from some specific distributions (such as Gaussian, Bernoulli, etc.), so that the overall measurement  matrix will be appropriate in the framework of recovery in compressive sampling~\cite{donoho2006compressed,candes2006robust,candes2006compressive,baraniuk2008compressive}.	score:322
Given [math]A[/math], What is the general formula of [math]A^n[/math]? If [math]A=\begin{pmatrix} -2 &4 \\ -5& 7 \end{pmatrix}[/math], what is the [math]A^n[/math]?	  The lines subdividing the matrix indicate the coarsest equitable partition. As the core factor of $\tilde A$ we obtain the matrix \[ [\tilde A]=\left( \begin{array}{cc|cc|c}   3&1&0&2&1\\   1&3&2&0&1\\   \hline   6&6&2&2&\infty \end{array} \right). \] Again, the lines subdividing the matrix indicate the coarsest equitable partition. The core factor of $[\tilde A]$, which turns out to be the iterated core factor of $\tilde A$, is \[ \llbracket \tilde A\rrbracket=[[\tilde A]]= \begin{pmatrix}   4&2&1\\   12&4&\infty \end{pmatrix} \] This matrix corresponds to the LP \begin{equation}   \tag{$L'$}   \begin{array}{rl}     \text{min }&(c')^{\tr} x'\\ \text{subject to }&A'x'=b',\;x'\ge 0,   \end{array} \end{equation} where  \[ A'=(4\;\;2),\qquad b'=(1),\qquad c'= \begin{pmatrix}   12\\   4 \end{pmatrix}.	  The lines subdividing the matrix indicate the coarsest equitable partition. As the core factor of $\tilde A$ we obtain the matrix \[ [\tilde A]=\left( \begin{array}{cc|cc|c}   3&1&0&2&1\\   1&3&2&0&1\\   \hline   6&6&2&2&\infty \end{array} \right). \] Again, the lines subdividing the matrix indicate the coarsest equitable partition. The core factor of $[\tilde A]$, which turns out to be the iterated core factor of $\tilde A$, is \[ \llbracket \tilde A\rrbracket=[[\tilde A]]= \begin{pmatrix}   4&2&1\\   12&4&\infty \end{pmatrix} \] This matrix corresponds to the LP \begin{equation}   \tag{$L'$}   \begin{array}{rl}     \text{min }&(c')^{\tr} x'\\ \text{subject to }&A'x'=b',\;x'\ge 0,   \end{array} \end{equation} where  \[ A'=(4\;\;2),\qquad b'=(1),\qquad c'= \begin{pmatrix}   12\\   4 \end{pmatrix}.	score:329
Given [math]A[/math], What is the general formula of [math]A^n[/math]? If [math]A=\begin{pmatrix} -2 &4 \\ -5& 7 \end{pmatrix}[/math], what is the [math]A^n[/math]?	 \item The discovery of many new mathematical identities which offer a significant reduction   in computational complexity for certain expressions. \end{compactitem}   \begin{minipage}{\linewidth} \begin{framed} \begin{flushleft}  \vspace{0mm}  {\bf Example 1:} Assume we are given matrices $A \in \mathbb{R}^{n \times m}$, $B \in \mathbb{R}^{m \times p}$.    We wish to compute the target expression: \texttt{sum(sum(A*B))}, i.e.~:   $\sum_{n,p} AB = \sum_{i = 1}^n \sum_{j = 1}^m \sum_{k = 1}^p A_{i, j} B_{j, k} $  which naively takes $O(nmp)$ time. Our framework is able to discover an efficient version of the formula, that computes the same result in $O(n(m+p))$ time: {\small \texttt{sum((sum(A, 1) * B)', 1)}}.	 \item The discovery of many new mathematical identities which offer a significant reduction   in computational complexity for certain expressions. \end{compactitem}   \begin{minipage}{\linewidth} \begin{framed} \begin{flushleft}  \vspace{0mm}  {\bf Example 1:} Assume we are given matrices $A \in \mathbb{R}^{n \times m}$, $B \in \mathbb{R}^{m \times p}$.    We wish to compute the target expression: \texttt{sum(sum(A*B))}, i.e.~:   $\sum_{n,p} AB = \sum_{i = 1}^n \sum_{j = 1}^m \sum_{k = 1}^p A_{i, j} B_{j, k} $  which naively takes $O(nmp)$ time. Our framework is able to discover an efficient version of the formula, that computes the same result in $O(n(m+p))$ time: {\small \texttt{sum((sum(A, 1) * B)', 1)}}.	score:333
Given [math]A[/math], What is the general formula of [math]A^n[/math]? If [math]A=\begin{pmatrix} -2 &4 \\ -5& 7 \end{pmatrix}[/math], what is the [math]A^n[/math]?	  The vast majority of metric learning approaches are using, as metric, the squared Mahalanobis distance between two $p$-dimensional objects $\mathbf{x}_i$ and $\mathbf{x}_j$ defined by \begin{equation} \label{eq:maha} d^2_A(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i - \mathbf{x}_j)^T A (\mathbf{x}_i-\mathbf{x}_j) \end{equation} where $A$ is a $p$ $\times$ $p$ positive semi-definite (PSD) matrix.  Note that if $A = I$, $d^2_A$ reduces to the squared Euclidean distance. In this setting, the learning task consists in finding a matrix $A$ that is satisfying some given constraints. In order to ensure that (\ref{eq:maha}) defines a proper metric (i.e. a binary function holding the symmetry, triangle inequality and identity properties), $A$ must remain PSD.	  The vast majority of metric learning approaches are using, as metric, the squared Mahalanobis distance between two $p$-dimensional objects $\mathbf{x}_i$ and $\mathbf{x}_j$ defined by \begin{equation} \label{eq:maha} d^2_A(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i - \mathbf{x}_j)^T A (\mathbf{x}_i-\mathbf{x}_j) \end{equation} where $A$ is a $p$ $\times$ $p$ positive semi-definite (PSD) matrix.  Note that if $A = I$, $d^2_A$ reduces to the squared Euclidean distance. In this setting, the learning task consists in finding a matrix $A$ that is satisfying some given constraints. In order to ensure that (\ref{eq:maha}) defines a proper metric (i.e. a binary function holding the symmetry, triangle inequality and identity properties), $A$ must remain PSD.	score:334
What is the advantage of a concrete batching plant?	 Batch renormalization \cite{ioffe2017batch} proposes an affine transformation to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire mini-batch.   However, the BN layer usually causes considerable training overhead for speed and energy consumption in both the forward and backward propagations during training.  On the one hand, the additional computations are quite heavy, especially for resource-limited ASIC devices. When it comes to online learning, i.e., deploying the training process onto terminal devices, the resource problem of BN becomes more salient and has challenged its extensive applications in various scenarios. On the other, the square and root operations introduce strong nonlinearity that makes it difficult to employ low bit-width quantization algorithms.	 Batch renormalization \cite{ioffe2017batch} proposes an affine transformation to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire mini-batch.   However, the BN layer usually causes considerable training overhead for speed and energy consumption in both the forward and backward propagations during training.  On the one hand, the additional computations are quite heavy, especially for resource-limited ASIC devices. When it comes to online learning, i.e., deploying the training process onto terminal devices, the resource problem of BN becomes more salient and has challenged its extensive applications in various scenarios. On the other, the square and root operations introduce strong nonlinearity that makes it difficult to employ low bit-width quantization algorithms.	score:362
What is the advantage of a concrete batching plant?	 We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole.  To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start.	 We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole.  To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start.	score:383
What is the advantage of a concrete batching plant?	 Moreover, the subtasks are independent of each other, which allows them to be learnt in parallel without communication. Another advantage of the proposed method is that, since the task is kept the same, the learning algorithm used to adjust the parameters can also be the same for all stages. Therefore, it can be viewed as a higher level method and is compatible with existing training strategies.  After the proposed pre-training is complete, the obtained smaller neural networks are merged and used as initial condition for the original neural network.  The new method is also straightforward to implement and decreases the number of parameters of the subtasks quadratically in the number of subtasks created, thus being characterized as a highly scalable approach.	 Moreover, the subtasks are independent of each other, which allows them to be learnt in parallel without communication. Another advantage of the proposed method is that, since the task is kept the same, the learning algorithm used to adjust the parameters can also be the same for all stages. Therefore, it can be viewed as a higher level method and is compatible with existing training strategies.  After the proposed pre-training is complete, the obtained smaller neural networks are merged and used as initial condition for the original neural network.  The new method is also straightforward to implement and decreases the number of parameters of the subtasks quadratically in the number of subtasks created, thus being characterized as a highly scalable approach.	score:391
What is the advantage of a concrete batching plant?	 Deep neural networks are powerful and popular learning models that achieve state-of-the-art pattern recognition performance on many computer vision, speech, and language processing tasks. However, these networks have also been shown susceptible to carefully crafted adversarial perturbations which force misclassification of the inputs. Adversarial examples enable adversaries to subvert the expected system behavior leading to undesired consequences and could pose a security risk when these systems are deployed in the real world.   In this work, we focus on deep convolutional neural networks and demonstrate that adversaries can easily craft adversarial examples even without any internal knowledge of the target network. Our attacks treat the network as an oracle (black-box) and only assume that the output of the network can be observed on the probed inputs. Our first attack is based on a simple idea of adding perturbation to a randomly selected single pixel or a small set of them.	 Deep neural networks are powerful and popular learning models that achieve state-of-the-art pattern recognition performance on many computer vision, speech, and language processing tasks. However, these networks have also been shown susceptible to carefully crafted adversarial perturbations which force misclassification of the inputs. Adversarial examples enable adversaries to subvert the expected system behavior leading to undesired consequences and could pose a security risk when these systems are deployed in the real world.   In this work, we focus on deep convolutional neural networks and demonstrate that adversaries can easily craft adversarial examples even without any internal knowledge of the target network. Our attacks treat the network as an oracle (black-box) and only assume that the output of the network can be observed on the probed inputs. Our first attack is based on a simple idea of adding perturbation to a randomly selected single pixel or a small set of them.	score:392
What is the advantage of a concrete batching plant?	 To tackle the issue of high per-iteration complexity of batch (deterministic) ADMM (as a popular first-order optimization method), \citet{wang:oadm}, \citet{suzuki:oadmm} and \citet{ouyang:sadmm} proposed some online or stochastic ADMM algorithms. However, all these variants only achieve the convergence rate of $\mathcal{O}(1/\sqrt{T})$ for general convex problems and $\mathcal{O}(\log T/{T})$ for strongly convex problems, respectively, as compared with the $\mathcal{O}(1/T^2)$ and linear convergence rates of accelerated batch algorithms~\citep{nesterov:cp}, e.	 To tackle the issue of high per-iteration complexity of batch (deterministic) ADMM (as a popular first-order optimization method), \citet{wang:oadm}, \citet{suzuki:oadmm} and \citet{ouyang:sadmm} proposed some online or stochastic ADMM algorithms. However, all these variants only achieve the convergence rate of $\mathcal{O}(1/\sqrt{T})$ for general convex problems and $\mathcal{O}(\log T/{T})$ for strongly convex problems, respectively, as compared with the $\mathcal{O}(1/T^2)$ and linear convergence rates of accelerated batch algorithms~\citep{nesterov:cp}, e.	score:393
Is Haskell a good fit for machine learning problems?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:297
Is Haskell a good fit for machine learning problems?	  Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted much attention due to their low computational cost in each iteration. However, these algorithms might perform poorly especially if it is hard to approximate the Hessian well and efficiently. As far as we know, there is no effective way to handle this problem.  In this paper, we resort to Nesterov's acceleration technique to improve the convergence performance of a class of second-order methods called approximate Newton. We give a theoretical analysis that Nesterov's acceleration technique can improve the convergence performance for approximate Newton just like for first-order methods. We accordingly propose an accelerated regularized sub-sampled Newton.	  Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted much attention due to their low computational cost in each iteration. However, these algorithms might perform poorly especially if it is hard to approximate the Hessian well and efficiently. As far as we know, there is no effective way to handle this problem.  In this paper, we resort to Nesterov's acceleration technique to improve the convergence performance of a class of second-order methods called approximate Newton. We give a theoretical analysis that Nesterov's acceleration technique can improve the convergence performance for approximate Newton just like for first-order methods. We accordingly propose an accelerated regularized sub-sampled Newton.	score:336
Is Haskell a good fit for machine learning problems?	 Many machine learning algorithms are based on the assumption that training examples are drawn independently.  However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects,  and hence share the features of these shared objects.  We show that the classic approach of ignoring this problem potentially can have a harmful effect on the accuracy of statistics, and then consider alternatives.   One of these is to only use independent examples, discarding other information.  However, this is clearly suboptimal.  We analyze sample error bounds in this networked setting, providing significantly improved results.  An important component of our approach is formed by efficient sample weighting schemes, which leads to novel concentration inequalities.	 Many machine learning algorithms are based on the assumption that training examples are drawn independently.  However, this assumption does not hold anymore when learning from a networked sample because two or more training examples may share some common objects,  and hence share the features of these shared objects.  We show that the classic approach of ignoring this problem potentially can have a harmful effect on the accuracy of statistics, and then consider alternatives.   One of these is to only use independent examples, discarding other information.  However, this is clearly suboptimal.  We analyze sample error bounds in this networked setting, providing significantly improved results.  An important component of our approach is formed by efficient sample weighting schemes, which leads to novel concentration inequalities.	score:339
Is Haskell a good fit for machine learning problems?	 At the same time, machine learning---one of the primary tools of the modern data scientist---has experienced a revitalization as academics, businesses, and governments alike discovered new applications for automated algorithms that can learn from data. As a consequence, there has been a growing demand for tools that make machine learning more accessible, scalable, and flexible.  Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.   As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?	 At the same time, machine learning---one of the primary tools of the modern data scientist---has experienced a revitalization as academics, businesses, and governments alike discovered new applications for automated algorithms that can learn from data. As a consequence, there has been a growing demand for tools that make machine learning more accessible, scalable, and flexible.  Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.   As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?	score:339
Is Haskell a good fit for machine learning problems?	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	score:341
What are good machine learning services?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:436
What are good machine learning services?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:454
What are good machine learning services?	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	score:472
What are good machine learning services?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:473
What are good machine learning services?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:476
When will aricent online training (fmfp) for 2016 batch begin?	 Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs.  We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy.  We demonstrate the proposed approaches on four different link prediction tasks. 	Factorization machines (FMs) \cite{fm,libfm} are a supervised learning approach that can use second-order feature combinations efficiently even when the data is very high-dimensional.  The key idea of FMs is to model the weights of feature combinations using a \textit{low-rank} matrix. This has two main benefits. First, FMs can achieve empirical accuracy on a par with polynomial regression or kernel methods but with smaller and faster to evaluate models \cite{fm_icml}. Second, FMs can infer the weights of feature combinations that were not observed in the training set.	 We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy.  We demonstrate the proposed approaches on four different link prediction tasks. 	Factorization machines (FMs) \cite{fm,libfm} are a supervised learning approach that can use second-order feature combinations efficiently even when the data is very high-dimensional.	score:414
When will aricent online training (fmfp) for 2016 batch begin?	 Random Forest (RF) is an ensemble supervised machine learning technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF.  Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofold. First, it investigates how data clustering (a well known diversity technique) can be applied to identify groups of similar decision trees in an RF in order to eliminate redundant trees by selecting a representative from each group (cluster).	 Random Forest (RF) is an ensemble supervised machine learning technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF.  Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofold. First, it investigates how data clustering (a well known diversity technique) can be applied to identify groups of similar decision trees in an RF in order to eliminate redundant trees by selecting a representative from each group (cluster).	score:433
When will aricent online training (fmfp) for 2016 batch begin?	 Even subsequent offerings of a course within a year can have a different population of students, mentors, and - in some cases - instructors. Also, due to the nascent nature of the online learning platforms, many other aspects of a course evolve quickly so that students are frequently being exposed to experimental content modalities or workflow refinements.  In this world of MOOCs, an automated machine which reliably forecasts students' performance early in their coursework would be a valuable tool for making smart decisions about when (and with whom) to make live educational interventions, with the aim of increasing engagement, providing motivation, and empowering students to succeed.   In the last three years, a few deep learning based neural network models for predicting students' future performance have been explored within data mining and learning analytic communities \cite{Mi15, Piech15, Whitehill17, Wang17, Kim18}.	 Even subsequent offerings of a course within a year can have a different population of students, mentors, and - in some cases - instructors. Also, due to the nascent nature of the online learning platforms, many other aspects of a course evolve quickly so that students are frequently being exposed to experimental content modalities or workflow refinements.  In this world of MOOCs, an automated machine which reliably forecasts students' performance early in their coursework would be a valuable tool for making smart decisions about when (and with whom) to make live educational interventions, with the aim of increasing engagement, providing motivation, and empowering students to succeed.   In the last three years, a few deep learning based neural network models for predicting students' future performance have been explored within data mining and learning analytic communities \cite{Mi15, Piech15, Whitehill17, Wang17, Kim18}.	score:449
When will aricent online training (fmfp) for 2016 batch begin?	   Random Forest (RF) is an ensemble classification technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance in terms of predictive accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF.	   Random Forest (RF) is an ensemble classification technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance in terms of predictive accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF.	score:454
When will aricent online training (fmfp) for 2016 batch begin?	 Factorization Machine (FM) is a supervised learning approach with a powerful capability of feature engineering. It yields state-of-the-art performance in various batch learning tasks where all the training data is made available prior to the training. However, in real-world applications where the data arrives sequentially in a streaming manner, the high cost of re-training with batch learning algorithms has posed formidable challenges in the online learning scenario.  The initial challenge is that no prior formulations of FM could fulfill the requirements in Online Convex Optimization (OCO) -- the paramount framework for online learning algorithm design. To address the aforementioned challenge, we invent a new convexification scheme leading to a Compact Convexified FM (CCFM) that seamlessly meets the requirements in OCO.	 Factorization Machine (FM) is a supervised learning approach with a powerful capability of feature engineering. It yields state-of-the-art performance in various batch learning tasks where all the training data is made available prior to the training. However, in real-world applications where the data arrives sequentially in a streaming manner, the high cost of re-training with batch learning algorithms has posed formidable challenges in the online learning scenario.  The initial challenge is that no prior formulations of FM could fulfill the requirements in Online Convex Optimization (OCO) -- the paramount framework for online learning algorithm design. To address the aforementioned challenge, we invent a new convexification scheme leading to a Compact Convexified FM (CCFM) that seamlessly meets the requirements in OCO.	score:454
Can Donald Trump refuse to take any questions from CNN in White house press meetings for the entire duration of his presidency?	  The goal of the \gls{kbc} task is to fill in the missing piece of information into an incomplete triple. For instance, given a query $\langle \text{Donald Trump, president of, ?}\rangle$ one should predict that the target entity is USA.  More formally, given a set of entities $\EntitySpace$ and a set of binary relations $\RelationSpace$ over these entities, a \emph{knowledge base} (sometimes also referred to as a knowledge \emph{graph}) can be specified by a set of triplets $\langle h,r,t \rangle$ where $h,t \in \EntitySpace$ are head and tail entities respectively and $r \in \RelationSpace$ is a relation between them.	  The goal of the \gls{kbc} task is to fill in the missing piece of information into an incomplete triple. For instance, given a query $\langle \text{Donald Trump, president of, ?}\rangle$ one should predict that the target entity is USA.  More formally, given a set of entities $\EntitySpace$ and a set of binary relations $\RelationSpace$ over these entities, a \emph{knowledge base} (sometimes also referred to as a knowledge \emph{graph}) can be specified by a set of triplets $\langle h,r,t \rangle$ where $h,t \in \EntitySpace$ are head and tail entities respectively and $r \in \RelationSpace$ is a relation between them.	score:453
Can Donald Trump refuse to take any questions from CNN in White house press meetings for the entire duration of his presidency?	 To achieve the desired runtime in practice, an implementation would replace this assumption with either a procedure to prepare a state from an arbitrary input vector (where the cost of preparation could be amortized over multiple runs of the algorithm) or a specification of input vectors for which quantum state preparation is easy. Usually QML algorithms abstract away these implementation details, assuming a number of the desired quantum states are already prepared.  The quantum recommendation systems algorithm is unique in that it explicitly comes with a data structure to prepare its states (see Section~\ref{sec:ds}).  These state preparation assumptions are nontrivial: even given ability to query entries of a vector in superposition, preparing states corresponding to arbitrary length-$n$ input vectors is known to take $\Omega(\sqrt{n})$ time (a corollary of quantum search lower bounds \cite{bennett1997strengths}).	 To achieve the desired runtime in practice, an implementation would replace this assumption with either a procedure to prepare a state from an arbitrary input vector (where the cost of preparation could be amortized over multiple runs of the algorithm) or a specification of input vectors for which quantum state preparation is easy. Usually QML algorithms abstract away these implementation details, assuming a number of the desired quantum states are already prepared.  The quantum recommendation systems algorithm is unique in that it explicitly comes with a data structure to prepare its states (see Section~\ref{sec:ds}).  These state preparation assumptions are nontrivial: even given ability to query entries of a vector in superposition, preparing states corresponding to arbitrary length-$n$ input vectors is known to take $\Omega(\sqrt{n})$ time (a corollary of quantum search lower bounds \cite{bennett1997strengths}).	score:466
Can Donald Trump refuse to take any questions from CNN in White house press meetings for the entire duration of his presidency?	  For example, a tweet saying ``\textit{Vote Trump!}'' shows a positive sentiment link from the poster to Donald Trump, and ``\textit{Trump is mad...}'' indicates the opposite case.    For a given sentiment link, we define its \textit{sign} to be positive or negative depending on whether its related content expresses a positive or negative attitude from the generator of the link to the recipient \cite{leskovec2010predicting}, and all such sentiment links form a new network topology called \textit{sentiment network}.	  For example, a tweet saying ``\textit{Vote Trump!}'' shows a positive sentiment link from the poster to Donald Trump, and ``\textit{Trump is mad...}'' indicates the opposite case.    For a given sentiment link, we define its \textit{sign} to be positive or negative depending on whether its related content expresses a positive or negative attitude from the generator of the link to the recipient \cite{leskovec2010predicting}, and all such sentiment links form a new network topology called \textit{sentiment network}.	score:468
Can Donald Trump refuse to take any questions from CNN in White house press meetings for the entire duration of his presidency?	 The proposed NDDR layer combines existing CNN components in a novel way, which possesses clear mathematical interpretability as discriminative dimensionality reduction. Moreover, the use of the existing CNN components is desirable to guarantee the extensibility of our method to various state-of-the-art CNN architectures, where the proposed NDDR layer can be used in a ``plug-and-play'' manner.  The rest of this paper is organized as follows. First, we describe the NDDR layer and propose a novel NDDR-CNN as well as its variant NDDR-CNN-Shortcut for MTL in Sect. \ref{Sect:methods}. After that, we discuss the related works in Sect. \ref{Sect:related_works}, where we show that our method can generalize several state-of-the-art methods, which can be treated as our special cases.	 The proposed NDDR layer combines existing CNN components in a novel way, which possesses clear mathematical interpretability as discriminative dimensionality reduction. Moreover, the use of the existing CNN components is desirable to guarantee the extensibility of our method to various state-of-the-art CNN architectures, where the proposed NDDR layer can be used in a ``plug-and-play'' manner.  The rest of this paper is organized as follows. First, we describe the NDDR layer and propose a novel NDDR-CNN as well as its variant NDDR-CNN-Shortcut for MTL in Sect. \ref{Sect:methods}. After that, we discuss the related works in Sect. \ref{Sect:related_works}, where we show that our method can generalize several state-of-the-art methods, which can be treated as our special cases.	score:481
Can Donald Trump refuse to take any questions from CNN in White house press meetings for the entire duration of his presidency?	 However, we cannot simply apply CNN and LSTM on demand prediction problem. If treating the demand over an entire city as an image and applying CNN on this image, we fail to achieve the best result. We realize including regions with weak correlations to predict a target region actually hurts the performance. To address this issue, we propose a novel local CNN method which only considers spatially nearby regions.  This local CNN method is motivated by the First Law of Geography: ``near things are more related than distant things,''~\cite{tobler1970computer} and it is also supported by observations from real data that demand patterns are more correlated for spatially close regions.  While local CNN method filters weakly correlated remote regions, this fails to consider the case that two locations could be spatially distant but are similar in their demand patterns (i.	 However, we cannot simply apply CNN and LSTM on demand prediction problem. If treating the demand over an entire city as an image and applying CNN on this image, we fail to achieve the best result. We realize including regions with weak correlations to predict a target region actually hurts the performance. To address this issue, we propose a novel local CNN method which only considers spatially nearby regions.  This local CNN method is motivated by the First Law of Geography: ``near things are more related than distant things,''~\cite{tobler1970computer} and it is also supported by observations from real data that demand patterns are more correlated for spatially close regions.  While local CNN method filters weakly correlated remote regions, this fails to consider the case that two locations could be spatially distant but are similar in their demand patterns (i.	score:483
How is machine learning applied to chat bot?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:313
How is machine learning applied to chat bot?	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	score:322
How is machine learning applied to chat bot?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:334
How is machine learning applied to chat bot?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:334
How is machine learning applied to chat bot?	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	score:335
Can I use boosting algorithms with SVM as weak learner?	 We review the concept of Support Vector Machines (SVMs) and discuss examples of their use in a number of scenarios.  Several SVM implementations have been used in HEP and we exemplify this algorithm using the Toolkit for Multivariate Analysis (TMVA) implementation. We discuss examples relevant to HEP including background suppression for $H\to\tau^+\tau^-$ at the LHC with several different kernel functions.   Performance benchmarking leads to the issue of  generalisation of hyper-parameter selection. The avoidance of fine tuning (over  training or over fitting) in MVA hyper-parameter optimisation,  i.e. the ability to ensure generalised performance of an MVA that is independent of the  training, validation and test samples, is of utmost importance.  We discuss this issue and  compare and contrast performance of hold-out and k-fold cross-validation.	 We review the concept of Support Vector Machines (SVMs) and discuss examples of their use in a number of scenarios.  Several SVM implementations have been used in HEP and we exemplify this algorithm using the Toolkit for Multivariate Analysis (TMVA) implementation. We discuss examples relevant to HEP including background suppression for $H\to\tau^+\tau^-$ at the LHC with several different kernel functions.   Performance benchmarking leads to the issue of  generalisation of hyper-parameter selection. The avoidance of fine tuning (over  training or over fitting) in MVA hyper-parameter optimisation,  i.e. the ability to ensure generalised performance of an MVA that is independent of the  training, validation and test samples, is of utmost importance.  We discuss this issue and  compare and contrast performance of hold-out and k-fold cross-validation.	score:384
Can I use boosting algorithms with SVM as weak learner?	 We focus on the AdaBoost algorithm in this paper (also called  boosting together with its variations~\cite{Breiman,Friedman98}).  Boosting algorithms have many advantages over the traditional classification algorithms.     Its asymptotical behavior when combining a large number of weak classifiers is less prone to the overfitting problem. Once trained, a boosting algorithm performs weighted sum on the selected weak classifiers.  This linear summation weakly performs the `and' and `or' operations. In the discrete case, as long as the overall score is above the threshold, a pattern is considered as positive. This may include a combinotory combinations of the conditions. Some weak classifiers may require to be satisfied together (`and'), and some may not as long a subset answer yes (`or').	 We focus on the AdaBoost algorithm in this paper (also called  boosting together with its variations~\cite{Breiman,Friedman98}).  Boosting algorithms have many advantages over the traditional classification algorithms.     Its asymptotical behavior when combining a large number of weak classifiers is less prone to the overfitting problem. Once trained, a boosting algorithm performs weighted sum on the selected weak classifiers.  This linear summation weakly performs the `and' and `or' operations. In the discrete case, as long as the overall score is above the threshold, a pattern is considered as positive. This may include a combinotory combinations of the conditions. Some weak classifiers may require to be satisfied together (`and'), and some may not as long a subset answer yes (`or').	score:411
Can I use boosting algorithms with SVM as weak learner?	 This implies that the use of TK kernels for learning the kernel can obviate the need for selecting candidate kernels in algorithms such as SimpleMKL and parameters such as the bandwidth.  Numerical testing on soft margin Support Vector Machine (SVM) problems show that algorithms using TK kernels outperform other kernel learning algorithms and neural networks.  Furthermore, our results show that when the ratio of the number of training data to features is high, the improvement of TK over MKL increases significantly. 	This paper addresses the problem of the automated selection of an optimal kernel function for a given kernel-based machine learning problem (e.g. soft margin SVM). Kernel functions implicitly define a linear parametrization of nonlinear candidate maps $y=f(x)$ from vectors $x$ to scalars $y$.	 This implies that the use of TK kernels for learning the kernel can obviate the need for selecting candidate kernels in algorithms such as SimpleMKL and parameters such as the bandwidth.  Numerical testing on soft margin Support Vector Machine (SVM) problems show that algorithms using TK kernels outperform other kernel learning algorithms and neural networks.  Furthermore, our results show that when the ratio of the number of training data to features is high, the improvement of TK over MKL increases significantly. 	This paper addresses the problem of the automated selection of an optimal kernel function for a given kernel-based machine learning problem (e.g. soft margin SVM). Kernel functions implicitly define a linear parametrization of nonlinear candidate maps $y=f(x)$ from vectors $x$ to scalars $y$.	score:414
Can I use boosting algorithms with SVM as weak learner?	 Twin support vector machine~(TSVM) is a powerful learning algorithm by solving a pair of small-sized SVM-type problems. However, there are still some specific issues such as low efficiency and weak robustness when it is faced with some applications. In this paper, we propose a Fast and Robust TSVM~(FR-TSVM) to deal with the above issues. In order to strengthen model robustness, we propose an effective fuzzy membership function and reformulate the TSVMs such that different input instances can make different contributions to the learning of the decision hyperplane.	 In additional, some regression model based TSVM are also proposed such as TSVR~\cite{peng2010tsvr}, ~TPSVR\cite{peng2014twin}, $\epsilon$-TSVR~\cite{ye2016weighted}, TWSVR~\cite{khemchandani2016twsvr} and $v$-TWSVR~\cite{rastogi2017nu}. All of the proposed variants share the same merits of TSVM. However, one challenge is that training instances from real-world applications often carry information with significant noise.  These TSVM methods are sensitive to outliers or noises.  Noisy data often can deteriorate the generalization ability of SVM or TSVM. In term of fuzzy theory technique, \emph{noisy information} can be causally converted into the \emph{fuzzy information} to meet fuzzy inference theory. The training of SVM would be too sensitive to noisy inputs if all training instances are treated equivalently at the training stage.  A conceptual way to alleviate this sensitive deterioration is to contract the influence of noisy inputs. This means that a conventional SVM, which intrinsically treats every input instance in equivalence, can be improved by introducing fuzzy membership functions to soften input information. A category of fuzzy SVMs are hence developed, such as Lin and Wang~\cite{Lin02,Lin04}, Wu and Liu~\cite{Wu07}, Inoue and Abe~\cite{Inoue01}, Yang~\textit{et al.	score:415
Can I use boosting algorithms with SVM as weak learner?	Support vector machine (SVM) is one of the most well-known supervised classification methods that has been extensively used in such fields as disease diagnosis, text categorization, and fraud detection. Training nonlinear SVM classifier (such as Gaussian kernel based) requires solving convex quadratic programming (QP) model whose running time can be prohibitive for large-scale instances without using specialized acceleration techniques such as sampling, boosting, and hierarchical training.	Support vector machine (SVM) is one of the most well-known supervised classification methods that has been extensively used in such fields as disease diagnosis, text categorization, and fraud detection. Training nonlinear SVM classifier (such as Gaussian kernel based) requires solving convex quadratic programming (QP) model whose running time can be prohibitive for large-scale instances without using specialized acceleration techniques such as sampling, boosting, and hierarchical training.	score:425
Why can't we use boosting to improve the accuracy of SVM?	 We review the concept of Support Vector Machines (SVMs) and discuss examples of their use in a number of scenarios.  Several SVM implementations have been used in HEP and we exemplify this algorithm using the Toolkit for Multivariate Analysis (TMVA) implementation. We discuss examples relevant to HEP including background suppression for $H\to\tau^+\tau^-$ at the LHC with several different kernel functions.   Performance benchmarking leads to the issue of  generalisation of hyper-parameter selection. The avoidance of fine tuning (over  training or over fitting) in MVA hyper-parameter optimisation,  i.e. the ability to ensure generalised performance of an MVA that is independent of the  training, validation and test samples, is of utmost importance.  We discuss this issue and  compare and contrast performance of hold-out and k-fold cross-validation.   We have extended the SVM functionality and introduced tools to facilitate cross validation in TMVA and present results based on these improvements. 	These proceedings discuss High Energy Physics (HEP) usage of Support Vector Machines (SVMs)~\cite{svm}, and in particular  recent improvements to the functionality of the TMVA~\cite{TMVA} implementation.   These improvements are deployed in the ROOT git repository and from version 6.08. Having discussed this  Machine Learning (ML) algorithm we proceed to raise the issue of generalisation of hyper-parameters (HPs); and in particular  the use of hold-out and cross-validation (CV).  We also discuss the issues of understanding variance on the classification  and performing a hypothesis test to address the issue of identifying if the HPs of a Multivariate Algorithm (MVA) are generalised, having used some method to promote HP generalisation.	 We review the concept of Support Vector Machines (SVMs) and discuss examples of their use in a number of scenarios.  Several SVM implementations have been used in HEP and we exemplify this algorithm using the Toolkit for Multivariate Analysis (TMVA) implementation. We discuss examples relevant to HEP including background suppression for $H\to\tau^+\tau^-$ at the LHC with several different kernel functions.   Performance benchmarking leads to the issue of  generalisation of hyper-parameter selection. The avoidance of fine tuning (over  training or over fitting) in MVA hyper-parameter optimisation,  i.e. the ability to ensure generalised performance of an MVA that is independent of the  training, validation and test samples, is of utmost importance.  We discuss this issue and  compare and contrast performance of hold-out and k-fold cross-validation.   We have extended the SVM functionality and introduced tools to facilitate cross validation in TMVA and present results based on these improvements. 	These proceedings discuss High Energy Physics (HEP) usage of Support Vector Machines (SVMs)~\cite{svm}, and in particular  recent improvements to the functionality of the TMVA~\cite{TMVA} implementation.   These improvements are deployed in the ROOT git repository and from version 6.08. Having discussed this  Machine Learning (ML) algorithm we proceed to raise the issue of generalisation of hyper-parameters (HPs); and in particular  the use of hold-out and cross-validation (CV).  We also discuss the issues of understanding variance on the classification  and performing a hypothesis test to address the issue of identifying if the HPs of a Multivariate Algorithm (MVA) are generalised, having used some method to promote HP generalisation.	score:299
Why can't we use boosting to improve the accuracy of SVM?	 To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.  	Discriminative learning algorithms are typically trained from large collections of vectorial training examples. In many classical learning problems, however, it is arguably more appropriate to represent training data not as individual data points, but as probability distributions. There are, in fact,  multiple reasons why probability distributions may be preferable.	 To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.  	Discriminative learning algorithms are typically trained from large collections of vectorial training examples. In many classical learning problems, however, it is arguably more appropriate to represent training data not as individual data points, but as probability distributions. There are, in fact,  multiple reasons why probability distributions may be preferable.	score:310
Why can't we use boosting to improve the accuracy of SVM?	   In contrast to ordinary support vector machines (SVMs)~\cite{VapnikCortesSVM}, which only predict single values, \eg a class label, SSVMs are designed such that, in principle, they can predict arbitrary structured objects.   However, this flexibility comes at a cost: training an SSVM requires  solving a more difficult optimization problem than training an  ordinary SVM.   In particular, SSVM training requires repeated runs of the structured  prediction step (the so called \emph{max-oracle}) across the training  set.   Each of these steps is an optimization problem itself, \eg finding  the minimum energy labeling of a graph,  and often computationally costly.   In fact, the more challenging the problem is, the more the max-oracle  calls become a computational bottleneck.	   In contrast to ordinary support vector machines (SVMs)~\cite{VapnikCortesSVM}, which only predict single values, \eg a class label, SSVMs are designed such that, in principle, they can predict arbitrary structured objects.   However, this flexibility comes at a cost: training an SSVM requires  solving a more difficult optimization problem than training an  ordinary SVM.   In particular, SSVM training requires repeated runs of the structured  prediction step (the so called \emph{max-oracle}) across the training  set.   Each of these steps is an optimization problem itself, \eg finding  the minimum energy labeling of a graph,  and often computationally costly.   In fact, the more challenging the problem is, the more the max-oracle  calls become a computational bottleneck.	score:310
Why can't we use boosting to improve the accuracy of SVM?	 We further observe that our condition can be empirically verified to judge the quality of the approximation in practice. As a concrete example of the framework, in Section~\ref{sec:luca:th:FK} we describe how kernel methods such as support vector machines (SVMs) can be enhanced to satisfy the fairness constraint. We observe that a particular instance of the fairness constraint for $\epsilon=0$ reduces to an orthogonality constraint.  Moreover, in the linear case, the constraint translates into a preprocessing step that implicitly imposes the fairness requirement on the data, making fair any linear model learned with them. We report numerical experiments using both linear and nonlinear kernels, which indicate that our method improves on the state-of-the-art in four out of five datasets and is competitive on the fifth dataset\footnote{ Additional technical steps and experiments are presented in the supplementary materials.	 We further observe that our condition can be empirically verified to judge the quality of the approximation in practice. As a concrete example of the framework, in Section~\ref{sec:luca:th:FK} we describe how kernel methods such as support vector machines (SVMs) can be enhanced to satisfy the fairness constraint. We observe that a particular instance of the fairness constraint for $\epsilon=0$ reduces to an orthogonality constraint.  Moreover, in the linear case, the constraint translates into a preprocessing step that implicitly imposes the fairness requirement on the data, making fair any linear model learned with them. We report numerical experiments using both linear and nonlinear kernels, which indicate that our method improves on the state-of-the-art in four out of five datasets and is competitive on the fifth dataset\footnote{ Additional technical steps and experiments are presented in the supplementary materials.	score:312
Why can't we use boosting to improve the accuracy of SVM?	 The ML algorithms that we have tried are support vector machines (SVM), SVM and $K$ nearest neighbor hybrid (SVM-KNN), AdaBoost, and asymmetric AdaBoost. Of these four methods, SVM and SVM-KNN have been previously tried for quasar-star classification, but we improve upon the performance (and the justification for using them) by introducing methods of bias-handling.  To the best of our knowledge, AdaBoost and asymmetric AdaBoost have not been previously tried to solve this problem. To contrast the effects of bias that arise due to the imbalance in the data, we have performed the experiments on naturally imbalanced as well as artificially balanced data sets.  The outcome of this research is two-fold. The first, to assert appropriate models for the separation of stars and quasars; and the second, to provide a solid reasoning for selecting these models, and consequently establishing a set of best practices for data scientific research in astronomy.	 The ML algorithms that we have tried are support vector machines (SVM), SVM and $K$ nearest neighbor hybrid (SVM-KNN), AdaBoost, and asymmetric AdaBoost. Of these four methods, SVM and SVM-KNN have been previously tried for quasar-star classification, but we improve upon the performance (and the justification for using them) by introducing methods of bias-handling.  To the best of our knowledge, AdaBoost and asymmetric AdaBoost have not been previously tried to solve this problem. To contrast the effects of bias that arise due to the imbalance in the data, we have performed the experiments on naturally imbalanced as well as artificially balanced data sets.  The outcome of this research is two-fold. The first, to assert appropriate models for the separation of stars and quasars; and the second, to provide a solid reasoning for selecting these models, and consequently establishing a set of best practices for data scientific research in astronomy.	score:318
Why isn't CNN covering the allegations swirling around the Clinton Foundation?	 While their performance is impressive, CNNs are  opaque or ``black box'' in nature, and there is a growing concern that the inability to interpret  their internal actions will hinder human confidence and trust of these systems in practice~\citep{lipton16, doran17}.  A number of current efforts to make CNNs interpretable relates internal node activations to  aspects of the input image.  An aspect may be a particular color or texture pattern, like those processed in early stage CNN feature maps. Aspects may also be broad patterns that define objects (or object parts)  depicted in an image. Semantically meaningful image aspects like pointy ears, paws and whiskers may  lead a human to decide that an image is of a cat, while observing  sand, water, blue sky, and shells in an image may determine that the image depicts a beach.	 While their performance is impressive, CNNs are  opaque or ``black box'' in nature, and there is a growing concern that the inability to interpret  their internal actions will hinder human confidence and trust of these systems in practice~\citep{lipton16, doran17}.  A number of current efforts to make CNNs interpretable relates internal node activations to  aspects of the input image.  An aspect may be a particular color or texture pattern, like those processed in early stage CNN feature maps. Aspects may also be broad patterns that define objects (or object parts)  depicted in an image. Semantically meaningful image aspects like pointy ears, paws and whiskers may  lead a human to decide that an image is of a cat, while observing  sand, water, blue sky, and shells in an image may determine that the image depicts a beach.	score:332
Why isn't CNN covering the allegations swirling around the Clinton Foundation?	 Stochastic Gradient Descent (SGD) is the central workhorse for training modern CNNs. Although giving impressive empirical performance it can be slow to converge. In this paper we explore a novel strategy for training a CNN using an alternation strategy that offers substantial speedups during training. We make the following contributions: (i) replace the ReLU non-linearity within a CNN with positive hard-thresholding, (ii) re-interpret this non-linearity as a binary state vector making the entire CNN linear if the multi-layer support is known, and (iii) demonstrate that under certain conditions a global optima to the CNN can be found through local descent.	 Stochastic Gradient Descent (SGD) is the central workhorse for training modern CNNs. Although giving impressive empirical performance it can be slow to converge. In this paper we explore a novel strategy for training a CNN using an alternation strategy that offers substantial speedups during training. We make the following contributions: (i) replace the ReLU non-linearity within a CNN with positive hard-thresholding, (ii) re-interpret this non-linearity as a binary state vector making the entire CNN linear if the multi-layer support is known, and (iii) demonstrate that under certain conditions a global optima to the CNN can be found through local descent.	score:336
Why isn't CNN covering the allegations swirling around the Clinton Foundation?	   In order to train this CNN in a fully supervised manner, we describe how the automatic alignment of fingerprint images can be used to obtain the required training annotations, which are otherwise missing in all publicly available datasets.   This improves the state-of-the-art recognition results for both partial and full fingerprints in a public benchmark.    To confirm that the observed improvement is due to the adoption of learned descriptors, we conduct an ablation study using the most successful pore descriptors previously used in the literature.   All our code is available    \ifcvprfinal   at \url{https://github.com/gdahia/high-res-fingerprint-recognition}.   \else   in the Supplementary Materials.	   In order to train this CNN in a fully supervised manner, we describe how the automatic alignment of fingerprint images can be used to obtain the required training annotations, which are otherwise missing in all publicly available datasets.   This improves the state-of-the-art recognition results for both partial and full fingerprints in a public benchmark.    To confirm that the observed improvement is due to the adoption of learned descriptors, we conduct an ablation study using the most successful pore descriptors previously used in the literature.   All our code is available    \ifcvprfinal   at \url{https://github.com/gdahia/high-res-fingerprint-recognition}.   \else   in the Supplementary Materials.	score:338
Why isn't CNN covering the allegations swirling around the Clinton Foundation?	 We are concerned with the  following question: How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well? The goal  of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset. To this end, we use Fourier  filtering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical  regularities.  For the SVHN and CIFAR-10 datasets, we present two Fourier filtered variants: a low frequency variant and a randomly filtered  variant. Each of the Fourier filtering schemes is tuned to preserve the recognizability of the objects. Our main finding is that CNNs  exhibit a tendency to latch onto the Fourier image statistics of the training dataset, sometimes exhibiting up to a 28\% generalization gap  across the various test sets.	 We are concerned with the  following question: How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well? The goal  of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset. To this end, we use Fourier  filtering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical  regularities.  For the SVHN and CIFAR-10 datasets, we present two Fourier filtered variants: a low frequency variant and a randomly filtered  variant. Each of the Fourier filtering schemes is tuned to preserve the recognizability of the objects. Our main finding is that CNNs  exhibit a tendency to latch onto the Fourier image statistics of the training dataset, sometimes exhibiting up to a 28\% generalization gap  across the various test sets.	score:341
Why isn't CNN covering the allegations swirling around the Clinton Foundation?	       In this paper we address the interplay between deep neural networks and statistical  anonymization of datasets. We focus on the following fundamental questions: \emph{What conditions can we place to learn anonymized (or sanitized) representations of a dataset in order to minimize the amount of information which could be revealed about the identity of a person?  What is the effect of sanitization on these procedures?} The line of research we investigate  is based on privacy-preserving statistical methods, such as learning differentially private algorithms~\cite{45428}.  The main goal of this framework  is to enable an analyst to learn relevant properties (e.g., regular labels)  of a dataset as a whole while protecting the privacy of the individual contributors (private labels which can identify a person).	       In this paper we address the interplay between deep neural networks and statistical  anonymization of datasets. We focus on the following fundamental questions: \emph{What conditions can we place to learn anonymized (or sanitized) representations of a dataset in order to minimize the amount of information which could be revealed about the identity of a person?  What is the effect of sanitization on these procedures?} The line of research we investigate  is based on privacy-preserving statistical methods, such as learning differentially private algorithms~\cite{45428}.  The main goal of this framework  is to enable an analyst to learn relevant properties (e.g., regular labels)  of a dataset as a whole while protecting the privacy of the individual contributors (private labels which can identify a person).	score:345
Which would be the best classification group for a dolphin?	 In this context a better confidence score can improve the final performance of the classifier. The derivation of a good confidence score should therefore be part of the classifier's design, as important as any other component of classifiers' design.  In order to derive a reliable confidence score for NN classifiers, we focus our attention on an empirical observation concerning neural networks trained for classification, which have been shown to demonstrate in parallel useful embedding properties.  Specifically, a common practice these days is to treat one of the upstream layers of a pre-trained network as a representation (or embedding) layer. This layer activation is then used for representing similar objects and train simpler classifiers (such as SVM, or shallower NNs) to perform different tasks, related but not identical to the original task the network had been trained on.	 In this context a better confidence score can improve the final performance of the classifier. The derivation of a good confidence score should therefore be part of the classifier's design, as important as any other component of classifiers' design.  In order to derive a reliable confidence score for NN classifiers, we focus our attention on an empirical observation concerning neural networks trained for classification, which have been shown to demonstrate in parallel useful embedding properties.  Specifically, a common practice these days is to treat one of the upstream layers of a pre-trained network as a representation (or embedding) layer. This layer activation is then used for representing similar objects and train simpler classifiers (such as SVM, or shallower NNs) to perform different tasks, related but not identical to the original task the network had been trained on.	score:291
Which would be the best classification group for a dolphin?	   In this setting, after finishing the supervised training stage of a classifier, it is possible to continue the learning process of the classifier on new unlabeled data, which may be the on-line unlabeled test samples it is asked to classify. Whenever the classifier encounters a sample on which the certainty that the classification is correct is high, this sample can be used as a training example along with that prediction as replacement for a label.   The crucial question is how can the self-training classifier decide on which of the self-labeled samples it should train on. In other words - when should the predictions of the not yet fully trained classifier be trusted?  In this work, different methods for training a self-training classifier are suggested and their utilities are analyzed. The suggested techniques can be easily implemented on top of any boosting and data augmentation methods, improving the obtained results.	   In this setting, after finishing the supervised training stage of a classifier, it is possible to continue the learning process of the classifier on new unlabeled data, which may be the on-line unlabeled test samples it is asked to classify. Whenever the classifier encounters a sample on which the certainty that the classification is correct is high, this sample can be used as a training example along with that prediction as replacement for a label.   The crucial question is how can the self-training classifier decide on which of the self-labeled samples it should train on. In other words - when should the predictions of the not yet fully trained classifier be trusted?  In this work, different methods for training a self-training classifier are suggested and their utilities are analyzed. The suggested techniques can be easily implemented on top of any boosting and data augmentation methods, improving the obtained results.	score:293
Which would be the best classification group for a dolphin?	  We propose an efficient method to estimate the accuracy of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them.  The proposed method is based on the intuition that: (i) when classifiers agree, they are more likely to be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data.	  We propose an efficient method to estimate the accuracy of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them.  The proposed method is based on the intuition that: (i) when classifiers agree, they are more likely to be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data.	score:299
Which would be the best classification group for a dolphin?	  To this end we developed a new theoretical framework for binary classification, the Strategy for Unsupervised Multiple Method Aggregation (SUMMA), to estimate the performances of base classifiers and an optimal  strategy for ensemble learning from unlabeled data.  	It has long been appreciated that combinations of independent and weak learning methods, in both classification and regression tasks, can be used to make a single strong learning method.  In their work, \citep{dietterich2000ensemble} found this phenomena to be universal for three reasons.  The first reason is statistical, in that real world data is often insufficiently large to uniquely infer a single model.  The second reason is computational limitations, as several commonly used models are non-convex, e.g. neural networks, and consequently often result in a parameter sets that are locally as opposed to globally optimal.	  To this end we developed a new theoretical framework for binary classification, the Strategy for Unsupervised Multiple Method Aggregation (SUMMA), to estimate the performances of base classifiers and an optimal  strategy for ensemble learning from unlabeled data.  	It has long been appreciated that combinations of independent and weak learning methods, in both classification and regression tasks, can be used to make a single strong learning method.  In their work, \citep{dietterich2000ensemble} found this phenomena to be universal for three reasons.  The first reason is statistical, in that real world data is often insufficiently large to uniquely infer a single model.  The second reason is computational limitations, as several commonly used models are non-convex, e.g. neural networks, and consequently often result in a parameter sets that are locally as opposed to globally optimal.	score:318
Which would be the best classification group for a dolphin?	 We then experiment with the discussed ensemble techniques on forty different datasets. The experiments show that TUPSO is by far the best option to use when multiple one-class classifiers exist. Furthermore, we show that TUPSO's classification performance is strongly correlated with that of the actual best ensemble-member.    \subsection{One-Class Ensemble} The main motivation behind the ensemble methodology is to weigh several individual classifiers and combine them to obtain a classifier that outperforms them all.	 We then experiment with the discussed ensemble techniques on forty different datasets. The experiments show that TUPSO is by far the best option to use when multiple one-class classifiers exist. Furthermore, we show that TUPSO's classification performance is strongly correlated with that of the actual best ensemble-member.    \subsection{One-Class Ensemble} The main motivation behind the ensemble methodology is to weigh several individual classifiers and combine them to obtain a classifier that outperforms them all.	score:322
What's the difference between deep learning and machine learning?	 Deep learning has become a powerful and popular tool for a variety of machine learning tasks. However, it is challenging to understand the mechanism of deep learning from a theoretical perspective. In this work, we propose a random active path model to study collective properties of  deep neural networks with binary synapses, under the removal perturbation of connections between layers.  In the model, the path from input to output is randomly activated, and the corresponding input unit constrains the weights along the path into the form of a $p$-weight interaction glass model. A critical value of the perturbation is observed to separate a spin glass regime from a paramagnetic regime, with the transition being of the first order. The paramagnetic phase is conjectured to have a poor  generalization performance.	 Deep learning has become a powerful and popular tool for a variety of machine learning tasks. However, it is challenging to understand the mechanism of deep learning from a theoretical perspective. In this work, we propose a random active path model to study collective properties of  deep neural networks with binary synapses, under the removal perturbation of connections between layers.  In the model, the path from input to output is randomly activated, and the corresponding input unit constrains the weights along the path into the form of a $p$-weight interaction glass model. A critical value of the perturbation is observed to separate a spin glass regime from a paramagnetic regime, with the transition being of the first order. The paramagnetic phase is conjectured to have a poor  generalization performance.	score:319
What's the difference between deep learning and machine learning?	 Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation.  In this pioneering paper, we propose the ``coding criterion'' to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations.	 Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation.  In this pioneering paper, we propose the ``coding criterion'' to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations.	score:331
What's the difference between deep learning and machine learning?	 Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning \emph{can make mistakes} and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems.  We show that in limited contexts the required number of training samples can be low and self-improvement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications.	 Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning \emph{can make mistakes} and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems.  We show that in limited contexts the required number of training samples can be low and self-improvement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications.	score:346
What's the difference between deep learning and machine learning?	 As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks.  In order to improve the performance as well to maintain the low power cost, in this paper we design DLAU, which is a scalable accelerator architecture for large-scale deep learning networks using FPGA as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications.	 The emergence of deep learning speeded up the development of machine learning and artificial intelligence. Consequently, deep learning has become a research hot spot in research organizations \cite{nature}. In general, deep learning uses a multi-layer neural network model to extract high-level features which are a combination of low-level abstractions to find the distributed data features, in order to solve complex problems in machine learning.	score:348
What's the difference between deep learning and machine learning?	 Deep learning, computational neuroscience, and cognitive science have overlapping goals related to understanding intelligence such that perception and behaviour can be simulated in computational systems. In neuroimaging, machine learning methods have been used to test computational models of sensory information processing. Recently, these model comparison techniques have been used to evaluate deep neural networks (DNNs) as models of sensory information processing.	 Deep learning, computational neuroscience, and cognitive science have overlapping goals related to understanding intelligence such that perception and behaviour can be simulated in computational systems. In neuroimaging, machine learning methods have been used to test computational models of sensory information processing. Recently, these model comparison techniques have been used to evaluate deep neural networks (DNNs) as models of sensory information processing.	score:349
Why CNN is so anti-Trump?	  Thus, as presented in~\cite{sexual_orientation}, if CNN is used to leak a very private matter of individuals, it is very hard to prevent CNN from exposing the privacy because we do not know what noise can hamper the recognition of a CNN. Actually, some noise that does not affect humans' recognition does make a huge difference for CNN~\cite{intriguing}.  Also, some representation that does not make any sense for human may work for CNN~\cite{DNN_fool}. One thing obvious is that CNN learns all the things from the data fed into it. This implies what CNN recognizes as a noise comes from the data set. In this paper, we mainly focus on how to add the effective noise that disrupts CNN's recognition for the purpose of privacy protection.   Differential privacy~\cite{cynthia} is the mathematical definition that has been introduced to measure the privacy loss for the domains that handle massive user data such as data mining applications.  Google~\cite{rappor} and Apple~\cite{apple} have adopted the differential privacy techniques into their crowdsourced applications with some data sketching technique to approximate users' private data.	 Thanks to the condition, even with the bad control-knob that keeps oscillating the probabilistic accuracy up and down, we can preserve the privacy of CNNs in a certain level that the condition designates.  This paper is organized as follows: Section~\ref{sec:problem_description} describes the problem that controls the privacy loss of CNN with the IFMs of layers.  In Section~\ref{sec:proposed}, the degree of sanitization is introduced as the boundary condition that the method of decreasing the probabilistic accuracy should satisfy. Also, the IFM approximation scheme that reduces the accuracy and its network-wise control method are proposed. Section~\ref{sec:evaluation} evaluates the proposed scheme on the layers of AlexNet in Caffe~\cite{caffe} CNN framework. Finally, Section~\ref{sec:conclusion} concludes with the summary of our contribution.	score:570
Why CNN is so anti-Trump?	 (\expandafter{\romannumeral2}) CNN has the ability to capture the frequency shift in human speech signal.  Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks \cite{cnn7,cnn8,cnn9,cnn10}. Most of previous application of CNNs in speech recognition only used fewer convolutional layers.  For example, Abdel-Hamid~et~al. \cite{cnn1} used one convolutional layer, one pooling layer and a few full-connected layers. Amodei~et~al. \cite{rnn2} also used only three convolutional layers as the feature preprocessing layers. Some researcher has shown that CNN-based speech recognition which uses raw speech as input can be more robust \cite{cnn11}, and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks \cite{cnn2,cnn3,cnn9,cnn14}.	 (\expandafter{\romannumeral2}) CNN has the ability to capture the frequency shift in human speech signal.  Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks \cite{cnn7,cnn8,cnn9,cnn10}. Most of previous application of CNNs in speech recognition only used fewer convolutional layers.  For example, Abdel-Hamid~et~al. \cite{cnn1} used one convolutional layer, one pooling layer and a few full-connected layers. Amodei~et~al. \cite{rnn2} also used only three convolutional layers as the feature preprocessing layers. Some researcher has shown that CNN-based speech recognition which uses raw speech as input can be more robust \cite{cnn11}, and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks \cite{cnn2,cnn3,cnn9,cnn14}.	score:624
Why CNN is so anti-Trump?	      \item Theoretically demonstrate that any feed forward CNN       employing striding has a mathematically equivalent non-striding       CNN architecture during evaluation.       \item We reinterpret striding as a tool for sharing parameters       along channels, and argue that this connection gives a more       thorough and theoretical explanation for why        striding is still an invaluable tool when designing CNN       architectures.  \end{itemize}	      \item Theoretically demonstrate that any feed forward CNN       employing striding has a mathematically equivalent non-striding       CNN architecture during evaluation.       \item We reinterpret striding as a tool for sharing parameters       along channels, and argue that this connection gives a more       thorough and theoretical explanation for why        striding is still an invaluable tool when designing CNN       architectures.  \end{itemize}	score:625
Why CNN is so anti-Trump?	  To address these challenges and leverage the power from both richness and semantics of these two types of feature representations, in this paper, we propose to integrate the semantic output, i.e., the output from the soft-max layer of CNN models, as well as region proposals to achieve compact yet effective visual representations, namely {\em deep attribute} (DA).  Since the soft-max layer neural codes are the probability response to the categories on which CNNs are trained, it is fairly compact, semantic, and sparse due to insignificant responses to most categories. Briefly, the proposed method contains four key components. \begin{itemize} \vspace{-0.04in} \addtolength{\itemsep}{-0.05in} \small \item[(1)] We introduce region proposals using algorithm like selective search~\cite{uijlings2013selective} or edge-box~\cite{edgebox} from each input image.	  To address these challenges and leverage the power from both richness and semantics of these two types of feature representations, in this paper, we propose to integrate the semantic output, i.e., the output from the soft-max layer of CNN models, as well as region proposals to achieve compact yet effective visual representations, namely {\em deep attribute} (DA).  Since the soft-max layer neural codes are the probability response to the categories on which CNNs are trained, it is fairly compact, semantic, and sparse due to insignificant responses to most categories. Briefly, the proposed method contains four key components. \begin{itemize} \vspace{-0.04in} \addtolength{\itemsep}{-0.05in} \small \item[(1)] We introduce region proposals using algorithm like selective search~\cite{uijlings2013selective} or edge-box~\cite{edgebox} from each input image.	score:630
Why CNN is so anti-Trump?	  There are two principal advantages of a CNN over a fully-connected neural network: (i) sparsity---that each nonlinear convolutional filter acts only on a local patch of the input, and (ii) parameter sharing---that the same filter is applied to each patch.  However, as with most neural networks, the standard approach to training CNNs is based on solving a nonconvex optimization problem that is known to be NP-hard~\cite{blum1992training}.   In practice, researchers use some flavor of stochastic gradient method, in which gradients are computed via backpropagation~\cite{bottou1998online}. This approach has two drawbacks: (i) the rate of convergence, which is at best only to a local optimum, can be slow due to nonconvexity (for instance, see the paper~\cite{fahlman1988empirical}), and (ii) its statistical properties are very difficult to understand, as the actual performance is determined by some combination of the CNN architecture along with the optimization algorithm.	  Second, the statistical properties of CCNN models can be studied in a precise and rigorous manner.  We obtain CCNNs by convexifying two-layer CNNs; doing so requires overcoming two challenges. First, the activation function of a CNN is nonlinear.  In order to address this issue, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS).   This approach is inspired by our earlier work~\cite{zhang2015ell_1}, involving a subset of the current authors, in which we developed this relaxation step for fully-connected neural networks.  Second, the parameter sharing induced by CNNs is crucial to its effectiveness and must be preserved. We show that CNNs with RKHS filters can be parametrized by a low-rank matrix.	score:633
Which one is the most mature Eminem, Bam Margera, Benedict Cumberbatch, Tom Hiddleston or Adam Sandler?	 Then one can use a minimum description length (MDL) argument to show generalization \citet{rissanen1983universal}. Alternatively one can quantitatively measure the "flatness" of a minimum by injecting noise to the network parameters and measuring the stability of the network output. The more a trained network output is stable to noise, the more "flat" the minimum to which it corresponds, and the better it's generalization \citet{shawe1997pac} \citet{mcallester1999some}.  Using this measure of "flatness" \citet{neyshabur2017pac} proposed a GE bound for deep fully connected networks.   The bound of \citet{neyshabur2017pac} depends linearly on the latent ambient dimensionality of the hidden layers. For convolutional architectures while the ambient dimensionality of the convolution operators is huge, the effective number of parameters is much smaller.	 Then one can use a minimum description length (MDL) argument to show generalization \citet{rissanen1983universal}. Alternatively one can quantitatively measure the "flatness" of a minimum by injecting noise to the network parameters and measuring the stability of the network output. The more a trained network output is stable to noise, the more "flat" the minimum to which it corresponds, and the better it's generalization \citet{shawe1997pac} \citet{mcallester1999some}.  Using this measure of "flatness" \citet{neyshabur2017pac} proposed a GE bound for deep fully connected networks.   The bound of \citet{neyshabur2017pac} depends linearly on the latent ambient dimensionality of the hidden layers. For convolutional architectures while the ambient dimensionality of the convolution operators is huge, the effective number of parameters is much smaller.	score:432
Which one is the most mature Eminem, Bam Margera, Benedict Cumberbatch, Tom Hiddleston or Adam Sandler?	 The difference among applicants applied to either a major with test scores of L is clear evidence of discrimination against females. The difference among applicants applied to either a major with test scores of H can be treated as reverse discrimination against males, or tokenism where some strong male applicants are purposefully rejected to refute a claim of discrimination against females.  So, the data publisher cannot make a non-discrimination claim.            The above two examples show that, any quantitative evidence of discrimination must be measured under a meaningful partition. In addition, to ensure non-discrimination, we must show no bias for all meaningful partitions. In this paper, we make use of the causal graphs to identify meaningful partitions and develop discrimination discovery and removal algorithms.	 The difference among applicants applied to either a major with test scores of L is clear evidence of discrimination against females. The difference among applicants applied to either a major with test scores of H can be treated as reverse discrimination against males, or tokenism where some strong male applicants are purposefully rejected to refute a claim of discrimination against females.  So, the data publisher cannot make a non-discrimination claim.            The above two examples show that, any quantitative evidence of discrimination must be measured under a meaningful partition. In addition, to ensure non-discrimination, we must show no bias for all meaningful partitions. In this paper, we make use of the causal graphs to identify meaningful partitions and develop discrimination discovery and removal algorithms.	score:440
Which one is the most mature Eminem, Bam Margera, Benedict Cumberbatch, Tom Hiddleston or Adam Sandler?	 These problems often require balancing several decision-making tradeoffs, such as speed-versus-accuracy, robustness-versus-efficiency, and explore-versus-exploit. The MAB problem is a prototypical example of the explore-versus-exploit tradeoff: choosing between the most informative and seemingly the most rewarding alternative.   In an MAB problem,  a decision-maker sequentially allocates a single resource by repeatedly choosing one among a set of competing alternative arms (options).  These problems have been applied in several interesting areas such as robotic foraging and surveillance~\cite{JRK-AK-PT:78,VS-PR-NEL:13, VS-PR-NEL:14}, acoustic relay positioning for underwater communication~\cite{MYC-JL-FSH:13}, and channel allocation in communication networks~\cite{anandkumar2011distributed}. In a standard MAB problem, a stationary environment is considered, however, many application areas are inherently non-stationary.	 These problems often require balancing several decision-making tradeoffs, such as speed-versus-accuracy, robustness-versus-efficiency, and explore-versus-exploit. The MAB problem is a prototypical example of the explore-versus-exploit tradeoff: choosing between the most informative and seemingly the most rewarding alternative.   In an MAB problem,  a decision-maker sequentially allocates a single resource by repeatedly choosing one among a set of competing alternative arms (options).  These problems have been applied in several interesting areas such as robotic foraging and surveillance~\cite{JRK-AK-PT:78,VS-PR-NEL:13, VS-PR-NEL:14}, acoustic relay positioning for underwater communication~\cite{MYC-JL-FSH:13}, and channel allocation in communication networks~\cite{anandkumar2011distributed}. In a standard MAB problem, a stationary environment is considered, however, many application areas are inherently non-stationary.	score:440
Which one is the most mature Eminem, Bam Margera, Benedict Cumberbatch, Tom Hiddleston or Adam Sandler?	    Thompson sampling is one of the earliest randomized algorithms for multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to Budgeted MAB, where there is random cost for pulling an arm and the total cost is constrained by a budget. We start with the case of Bernoulli bandits, in which the random rewards (costs) of an arm are independently sampled from a Bernoulli distribution.  To implement the Thompson sampling algorithm in this case, at each round, we sample two numbers from the posterior distributions of the reward and cost for each arm, obtain their ratio, select the arm with the maximum ratio, and then update the posterior distributions. We prove that the distribution-dependent regret bound of this algorithm is $O(\ln B)$, where $B$ denotes the budget.	    Thompson sampling is one of the earliest randomized algorithms for multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to Budgeted MAB, where there is random cost for pulling an arm and the total cost is constrained by a budget. We start with the case of Bernoulli bandits, in which the random rewards (costs) of an arm are independently sampled from a Bernoulli distribution.  To implement the Thompson sampling algorithm in this case, at each round, we sample two numbers from the posterior distributions of the reward and cost for each arm, obtain their ratio, select the arm with the maximum ratio, and then update the posterior distributions. We prove that the distribution-dependent regret bound of this algorithm is $O(\ln B)$, where $B$ denotes the budget.	score:446
Which one is the most mature Eminem, Bam Margera, Benedict Cumberbatch, Tom Hiddleston or Adam Sandler?	 To tackle the issue of high per-iteration complexity of batch (deterministic) ADMM (as a popular first-order optimization method), \citet{wang:oadm}, \citet{suzuki:oadmm} and \citet{ouyang:sadmm} proposed some online or stochastic ADMM algorithms. However, all these variants only achieve the convergence rate of $\mathcal{O}(1/\sqrt{T})$ for general convex problems and $\mathcal{O}(\log T/{T})$ for strongly convex problems, respectively, as compared with the $\mathcal{O}(1/T^2)$ and linear convergence rates of accelerated batch algorithms~\citep{nesterov:cp}, e.	 To tackle the issue of high per-iteration complexity of batch (deterministic) ADMM (as a popular first-order optimization method), \citet{wang:oadm}, \citet{suzuki:oadmm} and \citet{ouyang:sadmm} proposed some online or stochastic ADMM algorithms. However, all these variants only achieve the convergence rate of $\mathcal{O}(1/\sqrt{T})$ for general convex problems and $\mathcal{O}(\log T/{T})$ for strongly convex problems, respectively, as compared with the $\mathcal{O}(1/T^2)$ and linear convergence rates of accelerated batch algorithms~\citep{nesterov:cp}, e.	score:449
Where can I find a source to practice the matrix test for the IBM placement exam?	 We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization,  electronic circuits testing and recommendation systems.  In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion  of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection.	 We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization,  electronic circuits testing and recommendation systems.  In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion  of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection.	score:318
Where can I find a source to practice the matrix test for the IBM placement exam?	 This paper describes a new approach, based on linear programming, for computing nonnegative matrix factorizations (NMFs).  The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix $\mtx{X}$, the algorithm identifies a matrix $\mtx{C}$ that satisfies $\mtx{X} \approx \mtx{CX}$ and some linear constraints.   The constraints are chosen to ensure that the matrix $\mtx{C}$ selects features; these features can then be used to find a low-rank NMF of $\mtx{X}$.  A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method extends to more general noise models and leads to efficient, scalable algorithms.	 This paper describes a new approach, based on linear programming, for computing nonnegative matrix factorizations (NMFs).  The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix $\mtx{X}$, the algorithm identifies a matrix $\mtx{C}$ that satisfies $\mtx{X} \approx \mtx{CX}$ and some linear constraints.   The constraints are chosen to ensure that the matrix $\mtx{C}$ selects features; these features can then be used to find a low-rank NMF of $\mtx{X}$.  A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method extends to more general noise models and leads to efficient, scalable algorithms.	score:320
Where can I find a source to practice the matrix test for the IBM placement exam?	  Recent SVD-free matrix factorization formulations have enabled  rank minimization  for systems with millions of rows and columns,   paving  the way for matrix completion in  extremely large-scale applications, such as seismic data interpolation.   In this paper, we consider matrix completion formulations designed to hit a target data-fitting error level provided by the user, and propose an algorithm \black{called LR-BPDN}  that is able to exploit factorized formulations to solve the corresponding optimization problem.	  Recent SVD-free matrix factorization formulations have enabled  rank minimization  for systems with millions of rows and columns,   paving  the way for matrix completion in  extremely large-scale applications, such as seismic data interpolation.   In this paper, we consider matrix completion formulations designed to hit a target data-fitting error level provided by the user, and propose an algorithm \black{called LR-BPDN}  that is able to exploit factorized formulations to solve the corresponding optimization problem.	score:325
Where can I find a source to practice the matrix test for the IBM placement exam?	 Yet the implementation of this strategy is much more delicate and poses significant new challenges when moving from matrices to tensors.  At the core of our method is the initialization of the linear subspaces in which the fibers of a tensor reside. In the matrix case, a natural way to do so is by singular value decomposition, a tool that is no longer available for higher order tensors.  An obvious solution is to unfold tensors into matrices and then applying the usual singular value decomposition based approach. This, however, requires an unnecessarily large sample size. To overcome this problem, we propose an alternative approach to estimating the singular spaces of the matrix unfoldings of a tensor. Our method is based on a carefully constructed estimate of the second moment of appropriate unfolding of a tensor, which can be viewed as a matrix version U-statistics.	 Yet the implementation of this strategy is much more delicate and poses significant new challenges when moving from matrices to tensors.  At the core of our method is the initialization of the linear subspaces in which the fibers of a tensor reside. In the matrix case, a natural way to do so is by singular value decomposition, a tool that is no longer available for higher order tensors.  An obvious solution is to unfold tensors into matrices and then applying the usual singular value decomposition based approach. This, however, requires an unnecessarily large sample size. To overcome this problem, we propose an alternative approach to estimating the singular spaces of the matrix unfoldings of a tensor. Our method is based on a carefully constructed estimate of the second moment of appropriate unfolding of a tensor, which can be viewed as a matrix version U-statistics.	score:335
Where can I find a source to practice the matrix test for the IBM placement exam?	  In many applications, we have to recover a matrix from only a small number of observed entries, for example collaborative filtering for recommender systems. This problem is often called matrix completion, where missing entries or outliers are presented at arbitrary location in the measurement matrix. Matrix completion has been used in a wide range of problems such as collaborative filtering \cite{candes:emc, chen:lrmr}, structure-from-motion \cite{eriksson:lrma, zheng:rma}, click prediction \cite{yu:cp}, tag recommendation \cite{wang:at}, and face reconstruction \cite{meng:cwm}.	 However, all current provable algorithms suffer from superlinear per-iteration cost, which severely limits their applicability to large-scale problems. In this paper, we propose a scalable, provable structured low-rank matrix factorization method to recover low-rank and sparse matrices from missing and grossly corrupted data, i.e., robust matrix completion (RMC) problems, or incomplete and grossly corrupted measurements, i. e., compressive principal component pursuit (CPCP) problems. Specifically, we first present two small-scale matrix trace norm regularized bilinear structured factorization models for RMC and CPCP problems, in which repetitively calculating SVD of a large-scale matrix is replaced by updating two much smaller factor matrices. Then, we apply the alternating direction method of multipliers (ADMM) to efficiently solve the RMC problems.	score:336
What machine learning algorithm can automatically generate sequence images from text?	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	score:274
What machine learning algorithm can automatically generate sequence images from text?	  	Theano was introduced to the machine learning community by \citet{bergstra+al:2010-scipy} as a CPU and GPU mathematical compiler, demonstrating how it can be used to symbolically define mathematical functions, automatically derive gradient expressions, and compile these expressions into executable functions that outperform implementations using other existing tools.   \citet{bergstra+all-Theano-NIPS2011} then demonstrated how Theano could be used to implement Deep Learning models.  In Section~\ref{sec:main_features}, we will briefly expose the main goals and features of Theano. Section~\ref{sec:new_in_theano} will present some of the new features available and measures taken to speed up Theano's implementations.  Section~\ref{sec:benchmarks} compares Theano's performance with that of Torch7~\citep{Torch-2011} on neural network benchmarks, and RNNLM~\citep{Mikolov-Interspeech-2011} on recurrent neural network benchmarks.	  	Theano was introduced to the machine learning community by \citet{bergstra+al:2010-scipy} as a CPU and GPU mathematical compiler, demonstrating how it can be used to symbolically define mathematical functions, automatically derive gradient expressions, and compile these expressions into executable functions that outperform implementations using other existing tools.   \citet{bergstra+all-Theano-NIPS2011} then demonstrated how Theano could be used to implement Deep Learning models.  In Section~\ref{sec:main_features}, we will briefly expose the main goals and features of Theano. Section~\ref{sec:new_in_theano} will present some of the new features available and measures taken to speed up Theano's implementations.  Section~\ref{sec:benchmarks} compares Theano's performance with that of Torch7~\citep{Torch-2011} on neural network benchmarks, and RNNLM~\citep{Mikolov-Interspeech-2011} on recurrent neural network benchmarks.	score:279
What machine learning algorithm can automatically generate sequence images from text?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:298
What machine learning algorithm can automatically generate sequence images from text?	 Another standard  practice in tackling inverse problems like these is integrating priors using domain knowledge in forward modelling. For example, if the algorithm knows what a galaxy should look like or it knows the output needs to have certain properties such as being ``sharp'', it will make more informative decisions when choosing among all possible solutions.  In this paper we demonstrate a method using machine learning to automatically introduce such priors. This method can reliably recover features in images of galaxies. We find that machine learning techniques can go beyond this limitation of deconvolutions --- by training on higher quality data, a machine learning system can learn to recover information from poor quality data by effectively building priors.	 In this paper we demonstrate a method using machine learning to automatically introduce such priors. This method can reliably recover features in images of galaxies. We find that machine learning techniques can go beyond this limitation of deconvolutions --- by training on higher quality data, a machine learning system can learn to recover information from poor quality data by effectively building priors.	score:299
What machine learning algorithm can automatically generate sequence images from text?	 In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning.  The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.          	Recurrent neural networks (RNNs) have shown impressive results in modeling generation tasks that have a sequential structured output form, such as machine translation~\cite{SutskeverVL14,BahdanauCB14}, caption generation~\cite{KarpathyF14,xu2015icml}, and natural language generation~\cite{wensclstm15,Kiddon2016}.	 In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning.  The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.          	Recurrent neural networks (RNNs) have shown impressive results in modeling generation tasks that have a sequential structured output form, such as machine translation~\cite{SutskeverVL14,BahdanauCB14}, caption generation~\cite{KarpathyF14,xu2015icml}, and natural language generation~\cite{wensclstm15,Kiddon2016}.	score:302
Is it enough to complete a machine learning course by Andrew Ng from Coursera to get my first job/internship? If not, what more can be done for the same?	 Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes, but it has not yet been successfully used for automotive applications. There has recently been a revival of interest in the topic, however, driven by the ability of deep learning algorithms to learn good representations of the environment.  Motivated by Google DeepMind's successful demonstrations of learning for games from Breakout to Go, we will propose different methods for autonomous driving using deep reinforcement learning. This is of particular interest as it is difficult to pose autonomous driving as a supervised learning problem as it has a strong interaction with the environment including other vehicles, pedestrians and roadworks.	 Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes, but it has not yet been successfully used for automotive applications. There has recently been a revival of interest in the topic, however, driven by the ability of deep learning algorithms to learn good representations of the environment.  Motivated by Google DeepMind's successful demonstrations of learning for games from Breakout to Go, we will propose different methods for autonomous driving using deep reinforcement learning. This is of particular interest as it is difficult to pose autonomous driving as a supervised learning problem as it has a strong interaction with the environment including other vehicles, pedestrians and roadworks.	score:285
Is it enough to complete a machine learning course by Andrew Ng from Coursera to get my first job/internship? If not, what more can be done for the same?	g.,~\cite{filar1989variance}) and/or computing it is not tractable (e.g.,~\cite{filar1989variance,Mannor11MV}). Although risk-sensitive sequential decision-making has a long history in operations research and finance, it has only recently grabbed attention in the machine learning community. Most of the work on this topic (including those mentioned above) has been in the context of MDPs (when the model of the system is known) and much less work has been done within the reinforcement learning (RL) framework (when the model is unknown and all the information about the system is obtained from the samples resulted from the agent's interaction with the environment).	g.,~\cite{filar1989variance}) and/or computing it is not tractable (e.g.,~\cite{filar1989variance,Mannor11MV}). Although risk-sensitive sequential decision-making has a long history in operations research and finance, it has only recently grabbed attention in the machine learning community. Most of the work on this topic (including those mentioned above) has been in the context of MDPs (when the model of the system is known) and much less work has been done within the reinforcement learning (RL) framework (when the model is unknown and all the information about the system is obtained from the samples resulted from the agent's interaction with the environment).	score:286
Is it enough to complete a machine learning course by Andrew Ng from Coursera to get my first job/internship? If not, what more can be done for the same?	 Generally there exists some training data and the devices follow a learning method designed by the system designer to learn and improve the performance of some specific tasks. Most learning approaches studied in machine learning are non-strategic without the rationality on considering their own benefit. Such non-strategic learning approaches may not be applicable to the scenario where devices are rational and intelligent enough to choose actions to maximize their own benefits instead of following the rule designed by the system designer.   Chinese restaurant process, which is introduced in non-parametric learning methods in machine learning \cite{aldous1985exchangeable}, provides an interesting non-strategic learning method for unbounded number of objects. In Chinese restaurant process, there exists infinite number of tables, where each table has infinite number of seats. There are infinite number of customers entering the restaurant sequentially.	 Generally there exists some training data and the devices follow a learning method designed by the system designer to learn and improve the performance of some specific tasks. Most learning approaches studied in machine learning are non-strategic without the rationality on considering their own benefit. Such non-strategic learning approaches may not be applicable to the scenario where devices are rational and intelligent enough to choose actions to maximize their own benefits instead of following the rule designed by the system designer.   Chinese restaurant process, which is introduced in non-parametric learning methods in machine learning \cite{aldous1985exchangeable}, provides an interesting non-strategic learning method for unbounded number of objects. In Chinese restaurant process, there exists infinite number of tables, where each table has infinite number of seats. There are infinite number of customers entering the restaurant sequentially.	score:294
Is it enough to complete a machine learning course by Andrew Ng from Coursera to get my first job/internship? If not, what more can be done for the same?	  We also note that tools that enable understanding will especially benefit the vast numbers of newcomers to deep learning, who would like to take advantage of off-the-shelf software packages --- like Theano \cite{bergstra2010theano:-a-cpu-and-gpu-math-expression}, Pylearn2 \cite{goodfellow2013pylearn2:-a-machine-learning-research}, Caffe \cite{jia2014caffe:-convolutional-architecture}, and Torch \cite{collobert-2011-torch7:-a-matlab-like-environment} --- in new domains, but who may not have any intuition for why their models work (or do not).  Experts can also benefit as they iterate ideas for new models or when they are searching for good hyperparameters. We thus believe that both experts and newcomers will benefit from tools that provide intuitions about the inner workings of DNNs.  This paper provides two such tools, both of which are open source so that scientists and practitioners can integrate them with their own DNNs to better understand them.	  We also note that tools that enable understanding will especially benefit the vast numbers of newcomers to deep learning, who would like to take advantage of off-the-shelf software packages --- like Theano \cite{bergstra2010theano:-a-cpu-and-gpu-math-expression}, Pylearn2 \cite{goodfellow2013pylearn2:-a-machine-learning-research}, Caffe \cite{jia2014caffe:-convolutional-architecture}, and Torch \cite{collobert-2011-torch7:-a-matlab-like-environment} --- in new domains, but who may not have any intuition for why their models work (or do not).  Experts can also benefit as they iterate ideas for new models or when they are searching for good hyperparameters. We thus believe that both experts and newcomers will benefit from tools that provide intuitions about the inner workings of DNNs.  This paper provides two such tools, both of which are open source so that scientists and practitioners can integrate them with their own DNNs to better understand them.	score:295
Is it enough to complete a machine learning course by Andrew Ng from Coursera to get my first job/internship? If not, what more can be done for the same?	 The success of machine learning on a given task depends on, among other things, which learning algorithm is selected and its associated hyperparameters. Selecting an appropriate learning algorithm and setting its hyperparameters for a given data set can be a challenging task, especially for users who are not experts in machine learning. Previous work has examined using meta-features to predict which learning algorithm and hyperparameters should be used.  However, choosing a set of meta-features that are predictive of algorithm performance is difficult. Here, we propose to apply collaborative filtering techniques to learning algorithm and hyperparameter selection, and find that doing so avoids determining which meta-features to use and outperforms traditional meta-learning approaches in many cases. 		 The success of machine learning on a given task depends on, among other things, which learning algorithm is selected and its associated hyperparameters. Selecting an appropriate learning algorithm and setting its hyperparameters for a given data set can be a challenging task, especially for users who are not experts in machine learning. Previous work has examined using meta-features to predict which learning algorithm and hyperparameters should be used.  However, choosing a set of meta-features that are predictive of algorithm performance is difficult. Here, we propose to apply collaborative filtering techniques to learning algorithm and hyperparameter selection, and find that doing so avoids determining which meta-features to use and outperforms traditional meta-learning approaches in many cases. 		score:297
WIPRO started its training for the 2015 batch before the 8th semester. How can I complete the training before the exams?	 We then present the H2PC algorithm in details in Section 3. Section 4 evaluates our proposed method and shows results for several tasks involving artificial data sampled from known BNs. Then we report, in Section 5, on our experiments on real-world data sets in a multi-label learning context so as to provide empirical support for the proposed methodology. The main theoretical results appear formally  as two theorems (Theorem 8 and 9) in Section 5. Their proofs are established in the Appendix. Finally, Section 6 raises several issues for future work and we conclude in Section 7 with a summary of our contribution.	 We then present the H2PC algorithm in details in Section 3. Section 4 evaluates our proposed method and shows results for several tasks involving artificial data sampled from known BNs. Then we report, in Section 5, on our experiments on real-world data sets in a multi-label learning context so as to provide empirical support for the proposed methodology. The main theoretical results appear formally  as two theorems (Theorem 8 and 9) in Section 5. Their proofs are established in the Appendix. Finally, Section 6 raises several issues for future work and we conclude in Section 7 with a summary of our contribution.	score:417
WIPRO started its training for the 2015 batch before the 8th semester. How can I complete the training before the exams?	 LSTMs can be combined with Connectionist Temporal Classification (CTC) and implicitly perform sequence training over the speech signal on TIMIT \cite{graves-asru-2013}. \cite{chorowski-nips-2014} used GRUs and generated an explicit alignment model between the TIMIT speech sequence data to the phone sequence. In \cite{sak-interspeech-2014} a commercial speech system is trained using a LSTM acoustic model, here the the entire speech sequence is used as the context for classifying context dependent phones. \cite{sak-interspeech-2014b} extend from \cite{sak-interspeech-2014} and applied sequence training on top of LSTMs. Our contribution in this paper is a novel deep RNN acoustic model which is easy to train and archives an 8\% relative improvement over DNNs for the Wall Street Journal (WSJ) corpus.	 LSTMs can be combined with Connectionist Temporal Classification (CTC) and implicitly perform sequence training over the speech signal on TIMIT \cite{graves-asru-2013}. \cite{chorowski-nips-2014} used GRUs and generated an explicit alignment model between the TIMIT speech sequence data to the phone sequence. In \cite{sak-interspeech-2014} a commercial speech system is trained using a LSTM acoustic model, here the the entire speech sequence is used as the context for classifying context dependent phones. \cite{sak-interspeech-2014b} extend from \cite{sak-interspeech-2014} and applied sequence training on top of LSTMs. Our contribution in this paper is a novel deep RNN acoustic model which is easy to train and archives an 8\% relative improvement over DNNs for the Wall Street Journal (WSJ) corpus.	score:419
WIPRO started its training for the 2015 batch before the 8th semester. How can I complete the training before the exams?	 We keep the last 10 days to evaluate how a derived personal valuation (DPV) based system design would perform. Note that this is how we will do a system design update based on DPV, where we learn from previous experiments (for example, the training instances) and use the learning to update system design that would impact future predictions (for example, the test instances).  Suppose, no conclusion could be inferred at the global level using the training instances, in other words, CTR for version 1 is not statistically significantly different from CTR for version 2 with a level of significance, say 5\%. However, using gender of the user as a context gives us more information. Suppose, we find that women, in general, usually like version 2 better more than version 1 (based on CTR from training instances) and men prefer version 1 over version 2 and both these conclusions could be made with a level of significance, say 5\%.	 We keep the last 10 days to evaluate how a derived personal valuation (DPV) based system design would perform. Note that this is how we will do a system design update based on DPV, where we learn from previous experiments (for example, the training instances) and use the learning to update system design that would impact future predictions (for example, the test instances).  Suppose, no conclusion could be inferred at the global level using the training instances, in other words, CTR for version 1 is not statistically significantly different from CTR for version 2 with a level of significance, say 5\%. However, using gender of the user as a context gives us more information. Suppose, we find that women, in general, usually like version 2 better more than version 1 (based on CTR from training instances) and men prefer version 1 over version 2 and both these conclusions could be made with a level of significance, say 5\%.	score:421
WIPRO started its training for the 2015 batch before the 8th semester. How can I complete the training before the exams?	 However, in recent times, DNNs became popular again due to the success of first-order gradient based heuristic algorithms for training. This success started with the work of \cite{HOT06} which presented empirical evidence that if DNNs are initialized properly, then we can find good solutions in reasonable runtime. This work was soon followed by series of early successes of deep learning in natural language processing \cite{CW08}, speech recognition \cite{MDH12} and visual object classification \cite{KSH12}.  It was empirically shown in \cite{CSMBO16} that a sufficiently over-parameterized neural network can be trained to global optimality.   These gradient-based heuristics are not useful for  neural networks  with threshold activation function  as there is no gradient information. Even networks with sigmoid activation function fell out of favor because gradient information is not valuable when input values are large \cite{Hochreiter01gradientflow}.	 However, in recent times, DNNs became popular again due to the success of first-order gradient based heuristic algorithms for training. This success started with the work of \cite{HOT06} which presented empirical evidence that if DNNs are initialized properly, then we can find good solutions in reasonable runtime. This work was soon followed by series of early successes of deep learning in natural language processing \cite{CW08}, speech recognition \cite{MDH12} and visual object classification \cite{KSH12}.  It was empirically shown in \cite{CSMBO16} that a sufficiently over-parameterized neural network can be trained to global optimality.   These gradient-based heuristics are not useful for  neural networks  with threshold activation function  as there is no gradient information. Even networks with sigmoid activation function fell out of favor because gradient information is not valuable when input values are large \cite{Hochreiter01gradientflow}.	score:427
WIPRO started its training for the 2015 batch before the 8th semester. How can I complete the training before the exams?	  The moving averages are far away from the actual statistics in the early stage of training, making the correction of statistics in BRN unreliable.  In the implementation of BRN, two extra parameters are introduced to measure whether this correction can be trusted, which need to be carefully tuned during training.  Moreover, BRN may probably fail on handling the mini-batches with very few examples, \eg less than 8 samples.  In such case, the estimates of the batch sample statistics and moving statistics by either BN or BRN are instable because the means and variances dramatically vary in different training iterations.    In this paper, we present a new normalization method, Batch Kalman Normalization (BKN), for improving and accelerating training of DNNs particularly under the context of micro-batches.	  The moving averages are far away from the actual statistics in the early stage of training, making the correction of statistics in BRN unreliable.  In the implementation of BRN, two extra parameters are introduced to measure whether this correction can be trusted, which need to be carefully tuned during training.  Moreover, BRN may probably fail on handling the mini-batches with very few examples, \eg less than 8 samples.  In such case, the estimates of the batch sample statistics and moving statistics by either BN or BRN are instable because the means and variances dramatically vary in different training iterations.    In this paper, we present a new normalization method, Batch Kalman Normalization (BKN), for improving and accelerating training of DNNs particularly under the context of micro-batches.	score:430
What is the classification of 1-pentanol?	 Using this criteria, one can estimate the classifier's F-score in the semi-supervise paradigm. However, when only positive-labeled instances exist, the recall, $r=Pr[f(x)=1|y=1]$, equals to $Pr[f(x)=1]$ (because $Pr[y=1]=1$), which only measures the fraction of correct classifications on positive test examples, i.e., true-positive rate (TPR). Using the TPR to measure the classification performance makes sense, because the TPR is strongly correlated with the classification accuracy when negative examples are very rare, such as in the case of most one-class problems.	g., the ensemble members' predictions. A vector of meta-features and a classification $k$ comprise a meta-instance, i.e., meta-instance $\equiv <f^{meta}_{1},\dots,f^{meta}_{k},y>$, where $y$ is the real classification of the meta-instance that is identical to the class of the instance used to produce the ensemble members' predictions. A collection of meta-instances comprises the meta-dataset upon which the meta-classifier is trained.    \subsection{Estimating The Classification Quality}  Traditional classifier evaluation metrics, such as true negative and false positive, cannot be computed in the one-class setup, since only positive examples exist. Consequently, measures, such as a classifier's accuracy, precision, AUC, F-score, and Matthew’s correlation coefficient (MCC), cannot be computed, since $accuracy=(TP+\textbf{TN})/(TP+\textbf{TN}+\textbf{FP}+FN)$, $Precision=TP/(TP+\textbf{FP})$ and $F$-$score=2*\textbf{P}*R/(\textbf{P}+R)$, where $P$ is precision and $R$ is recall.	score:468
What is the classification of 1-pentanol?	    The classes $\cH_{k,\theta}$ can be seen as a generalization and extension of the class of $k$-monotone-disjunctions and $r$-of-$k$-formulas. Considering binary instances $x \in \{0,1\}^d$, the class of $k$-monotone-disjunctions corresponds to linear classifiers with binary weights, $w \in \{0,1\}^d$, with $\norm{w}_1\leq k$ and a fixed threshold of $\theta=\half$.   That is, a restriction of $\cH_{k,\half}$ to integer weights and integer instances.  More generally, the class of $r$-of-$k$ formulas (i.e.,~formulas which are true if at least $r$ of a specified $k$ variables are true) corresponds to a similar restriction, but with a threshold of $\theta=r-\half$.  Studying $k$-disjunctions and $r$-of-$k$ formulas, \citet{Littlestone88} presented the efficient Winnow online learning rule, which admits an online mistake bound (in the separable case) of $O(k \log d)$ for $k$-disjunctions and $O(r k \log d)$ for $r$-of-$k$-formulas.	    The classes $\cH_{k,\theta}$ can be seen as a generalization and extension of the class of $k$-monotone-disjunctions and $r$-of-$k$-formulas. Considering binary instances $x \in \{0,1\}^d$, the class of $k$-monotone-disjunctions corresponds to linear classifiers with binary weights, $w \in \{0,1\}^d$, with $\norm{w}_1\leq k$ and a fixed threshold of $\theta=\half$.   That is, a restriction of $\cH_{k,\half}$ to integer weights and integer instances.  More generally, the class of $r$-of-$k$ formulas (i.e.,~formulas which are true if at least $r$ of a specified $k$ variables are true) corresponds to a similar restriction, but with a threshold of $\theta=r-\half$.  Studying $k$-disjunctions and $r$-of-$k$ formulas, \citet{Littlestone88} presented the efficient Winnow online learning rule, which admits an online mistake bound (in the separable case) of $O(k \log d)$ for $k$-disjunctions and $O(r k \log d)$ for $r$-of-$k$-formulas.	score:478
What is the classification of 1-pentanol?	 The prediction outcomes from the level-0 models are pooled for the second-stage learning, where a meta-classifier is trained. The pooled classification outcomes are called level-1 data and the meta-classifier is called the level-1 generalizer.   Ting and Witten \cite{Ting97stackedgeneralization:} showed that for the task of classification, the best practice is to use the predicted class probabilities generated by level-0 models to construct level-1 data.   Essentially, stacking learns a meta-classifier that assigns a set of weights to the class predictions made by individual classifiers. The traditional stacking model assumes the weight of each classifier is constant from instance to instance, which does not hold in general for many relational classifiers on a network. For example, the weighted-vote relational neighbor (wvRN) classifier \cite{Macskassy03asimple} infers a node's label by taking a weighted average of the class membership probabilities of its neighbors.	 The prediction outcomes from the level-0 models are pooled for the second-stage learning, where a meta-classifier is trained. The pooled classification outcomes are called level-1 data and the meta-classifier is called the level-1 generalizer.   Ting and Witten \cite{Ting97stackedgeneralization:} showed that for the task of classification, the best practice is to use the predicted class probabilities generated by level-0 models to construct level-1 data.   Essentially, stacking learns a meta-classifier that assigns a set of weights to the class predictions made by individual classifiers. The traditional stacking model assumes the weight of each classifier is constant from instance to instance, which does not hold in general for many relational classifiers on a network. For example, the weighted-vote relational neighbor (wvRN) classifier \cite{Macskassy03asimple} infers a node's label by taking a weighted average of the class membership probabilities of its neighbors.	score:478
What is the classification of 1-pentanol?	 If one assumes that the instances in positive bags follow a mixture model $\Ptp = (1-\pi) P_1 + \pi P_0$, and the instances are iid according to $P_0$ or $\Ptp$, the setting is that of one-sided label noise.  As mentioned above, classification with label noise is the basis of co-training \citep{blum98}, which is a framework for classifying instances that are represented by two distinct ``views. " The original analysis of co-training considers the ``realizable" case, where labels are a deterministic function of inputs. Our results allow us to state a result for co-training without making this restrictive assumption. This result is presented in Section \ref{sec:cotrain}.  There is also a connection between classification with label noise and class probability estimation.	 If one assumes that the instances in positive bags follow a mixture model $\Ptp = (1-\pi) P_1 + \pi P_0$, and the instances are iid according to $P_0$ or $\Ptp$, the setting is that of one-sided label noise.  As mentioned above, classification with label noise is the basis of co-training \citep{blum98}, which is a framework for classifying instances that are represented by two distinct ``views. " The original analysis of co-training considers the ``realizable" case, where labels are a deterministic function of inputs. Our results allow us to state a result for co-training without making this restrictive assumption. This result is presented in Section \ref{sec:cotrain}.  There is also a connection between classification with label noise and class probability estimation.	score:480
What is the classification of 1-pentanol?	   It is known that the $\ell_0$ penalty is the most essential sparsity measure, while the $\ell_1$ penalty is only a best convex relaxation of $\ell_0$ penalty. Thus, we naturally expect to use the $\ell_0$ penalty to improve the feature selection performance. However, directly putting the $\ell_0$ penalty into the sparse clustering framework~\cite{witten2010} makes the problem intractable.  Even if it is tractable, the solution defined cannot be interpreted. To overcome this difficulty, a new sparse clustering framework using both a $\ell_{\infty}$ and a $\ell_0$ penalty ($\ell_{\infty}/\ell_0$ penalty for short) is proposed. As a realization of this new framework, we develop a $\ell_0$-k-means method for performing clustering. We find that the $\ell_0$-k-means is extremely easy to implement and interpret.	   It is known that the $\ell_0$ penalty is the most essential sparsity measure, while the $\ell_1$ penalty is only a best convex relaxation of $\ell_0$ penalty. Thus, we naturally expect to use the $\ell_0$ penalty to improve the feature selection performance. However, directly putting the $\ell_0$ penalty into the sparse clustering framework~\cite{witten2010} makes the problem intractable.  Even if it is tractable, the solution defined cannot be interpreted. To overcome this difficulty, a new sparse clustering framework using both a $\ell_{\infty}$ and a $\ell_0$ penalty ($\ell_{\infty}/\ell_0$ penalty for short) is proposed. As a realization of this new framework, we develop a $\ell_0$-k-means method for performing clustering. We find that the $\ell_0$-k-means is extremely easy to implement and interpret.	score:481
What is Neuro Linguistic programming (NLP)? What are its uses?	 Intuitively, neural networks are a form of sieve estimation, wherein basis functions of the original variables are used to approximate unknown nonparametric objects. What sets neural nets apart is that the basis functions are themselves learned from the data by optimizing over many flexible combinations of simple functions. It has been known for some time that such networks yield universal approximations \citep{hornik1989multilayer}.  Comprehensive theoretical treatments are given by \citet{white1992artificial} and \citet{Anthony-Bartlett1999_book}. Of particular relevance in this strand of theoretical work is \citet{Chen-White1999_IEEE}, where it was shown that single-layer, sigmoid-based networks could attain sufficiently fast rates for semiparametric inference (see \citet{Chen2007_handbook} for more references).	 Intuitively, neural networks are a form of sieve estimation, wherein basis functions of the original variables are used to approximate unknown nonparametric objects. What sets neural nets apart is that the basis functions are themselves learned from the data by optimizing over many flexible combinations of simple functions. It has been known for some time that such networks yield universal approximations \citep{hornik1989multilayer}.  Comprehensive theoretical treatments are given by \citet{white1992artificial} and \citet{Anthony-Bartlett1999_book}. Of particular relevance in this strand of theoretical work is \citet{Chen-White1999_IEEE}, where it was shown that single-layer, sigmoid-based networks could attain sufficiently fast rates for semiparametric inference (see \citet{Chen2007_handbook} for more references).	score:383
What is Neuro Linguistic programming (NLP)? What are its uses?	  After a couple of pioneer works (\newcite{Yoshua:2001:lm_nips}, \newcite{collobert:2008:icml_nlp}, \newcite{collobert:2011:jmlt_nlp} among others), the use of neural networks for NLP applications is  attracting huge interest in the research community and they are  systematically applied to all NLP tasks.  However, while the use of (deep) neural networks in NLP has shown very good results for many tasks, it seems that they have not yet reached the level to outperform the state-of-the-art by a large margin, as it was observed in computer vision and speech recognition.	  Currently, the main-stream approach is to consider a sentence as a sequence of tokens (characters or words) and to process them with a recurrent neural network (RNN).  Tokens are usually processed in sequential order, from left to right, and the RNN is expected to \textit{``memorize''} the whole sequence in its internal states.  The most popular and successful RNN variant are certainly LSTMs\cite{hochreiter1997long}~--~there are many works which have shown the ability of LSTMs to model long-range dependencies in NLP applications, \eg  \cite{Sundermeyer:2012:is_lstm,Sutskever:2014:nips_nntrans} to name just a few.	score:387
What is Neuro Linguistic programming (NLP)? What are its uses?	  	For diverse NLP classification tasks, such as sentiment and opinion mining, or text-forecasting, in which text documents are used to make predictions about measurable phenomena in the real world~\cite{Kogan2009}, there is a need to generalize over words while simultaneously capturing relational and structural information. Feature engineering for NLP learning tasks can be labor-intensive.  We propose OmniGraph, a novel representation that supports a continuum of features from lexical items, to syntactic dependencies, to frame semantic features. Figure~\ref{fig:capability-feature} illustrates a sentence, the structure of its graph, and a predictive subgraph feature our method discovers that captures semantic and syntactic dependencies (arrows), semantic role information for syntactic arguments (diamonds), and generalizations over lexical items (semantic frame names, shown as rectangles).	  	For diverse NLP classification tasks, such as sentiment and opinion mining, or text-forecasting, in which text documents are used to make predictions about measurable phenomena in the real world~\cite{Kogan2009}, there is a need to generalize over words while simultaneously capturing relational and structural information. Feature engineering for NLP learning tasks can be labor-intensive.  We propose OmniGraph, a novel representation that supports a continuum of features from lexical items, to syntactic dependencies, to frame semantic features. Figure~\ref{fig:capability-feature} illustrates a sentence, the structure of its graph, and a predictive subgraph feature our method discovers that captures semantic and syntactic dependencies (arrows), semantic role information for syntactic arguments (diamonds), and generalizations over lexical items (semantic frame names, shown as rectangles).	score:407
What is Neuro Linguistic programming (NLP)? What are its uses?	  We refer to this process as the delayed sparse randomized SVD (DSSVD) algorithm. We show that CA with DSSVD can be applied to NLP problems.  Neural-network-based approaches are the most popular feature extractors used in NLP. Of these, \verb|word2vec|~\cite{NIPS2013_5021} is well known. Usually, such an approach will involve many parameters, which do not have explicit meanings in most cases.  These parameters have to be tuned by grid searching or manual parameter tuning, which is difficult in the absence of explicit meanings for the parameters. This parameter problem with neural-network-based approaches also gives rise to domain problems. For example, if \verb|word2vec| is tuned for application to restaurant reviews, the tuning may not be appropriate for movie reviews.	 Many applications use such simple representations. However, histogram-based representations cannot make use of information about correlations within the data. CA enables the representation of both histograms and correlations in data.   The most popular problem involving categorical data is natural language processing (NLP). However, CA has not been applied to NLP because most NLP problems involve a large number of categories.  For example, the entire Wikipedia text comprises more than 10,000 different words. Because CA requires excessive memory if applied to numerous categories, CA has not been used for NLP problems involving more than 10,000 categories.  CA is implemented by singular value decomposition (SVD) of a contingency table. In many categorical problems, the contingency table is sparsely populated.	score:410
What is Neuro Linguistic programming (NLP)? What are its uses?	 These scorers are based on readily available natural language processing (NLP) tools which can produce scores indicating: (a) how formal the generated text is, (b)  whether the generated text is fluent, and most importantly, (c) whether the generated text carries similar  semantics as the input. This framework is trained in multiple iterations, where each iteration is comprised of two phases of \textbf{(i) exploration} and \textbf{(ii) exploitation}.  In the exploration phase, the decoder randomly samples candidate texts for given inputs, and with the help of scorers, automatically produces training data for controllable generation. In the exploitation phase, the encoder-decoder is retrained with the examples thus generated.   For experiments, we prepare a mixture of unlabeled informal texts with low readability grade.	 These scorers are based on readily available natural language processing (NLP) tools which can produce scores indicating: (a) how formal the generated text is, (b)  whether the generated text is fluent, and most importantly, (c) whether the generated text carries similar  semantics as the input. This framework is trained in multiple iterations, where each iteration is comprised of two phases of \textbf{(i) exploration} and \textbf{(ii) exploitation}.  In the exploration phase, the decoder randomly samples candidate texts for given inputs, and with the help of scorers, automatically produces training data for controllable generation. In the exploitation phase, the encoder-decoder is retrained with the examples thus generated.   For experiments, we prepare a mixture of unlabeled informal texts with low readability grade.	score:413
What is the difference between machine learning and deep learning?	 Deep learning has become a powerful and popular tool for a variety of machine learning tasks. However, it is challenging to understand the mechanism of deep learning from a theoretical perspective. In this work, we propose a random active path model to study collective properties of  deep neural networks with binary synapses, under the removal perturbation of connections between layers.  In the model, the path from input to output is randomly activated, and the corresponding input unit constrains the weights along the path into the form of a $p$-weight interaction glass model. A critical value of the perturbation is observed to separate a spin glass regime from a paramagnetic regime, with the transition being of the first order. The paramagnetic phase is conjectured to have a poor  generalization performance.	 Deep learning has become a powerful and popular tool for a variety of machine learning tasks. However, it is challenging to understand the mechanism of deep learning from a theoretical perspective. In this work, we propose a random active path model to study collective properties of  deep neural networks with binary synapses, under the removal perturbation of connections between layers.  In the model, the path from input to output is randomly activated, and the corresponding input unit constrains the weights along the path into the form of a $p$-weight interaction glass model. A critical value of the perturbation is observed to separate a spin glass regime from a paramagnetic regime, with the transition being of the first order. The paramagnetic phase is conjectured to have a poor  generalization performance.	score:326
What is the difference between machine learning and deep learning?	 The emergence of deep learning speeded up the development of machine learning and artificial intelligence. Consequently, deep learning has become a research hot spot in research organizations \cite{nature}. In general, deep learning uses a multi-layer neural network model to extract high-level features which are a combination of low-level abstractions to find the distributed data features, in order to solve complex problems in machine learning.	 As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks.  In order to improve the performance as well to maintain the low power cost, in this paper we design DLAU, which is a scalable accelerator architecture for large-scale deep learning networks using FPGA as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications.	score:346
What is the difference between machine learning and deep learning?	 As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks.  In order to improve the performance as well to maintain the low power cost, in this paper we design DLAU, which is a scalable accelerator architecture for large-scale deep learning networks using FPGA as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications.	 As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks.  In order to improve the performance as well to maintain the low power cost, in this paper we design DLAU, which is a scalable accelerator architecture for large-scale deep learning networks using FPGA as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications.	score:348
What is the difference between machine learning and deep learning?	 Over the last decade, deep learning has become a disruptive advance in machine learning, giving new live to the long-standing connectionist paradigm in artificial intelligence. Deep learning methods are ideally suited to large-scale data and, therefore, they should be ideally suited to knowledge discovery in bioinformatics and biomedicine at large.  In this brief paper, we review key aspects of the application of deep learning in bioinformatics and medicine, drawing from the themes covered by the contributions to an ESANN 2018 special session devoted to this topic. 	Deep Learning (DL) \cite{Goodfellow-et-al-2016} has become an increasingly popular Machine Learning (ML) approach in the last decade, as shown in Fig.	 Over the last decade, deep learning has become a disruptive advance in machine learning, giving new live to the long-standing connectionist paradigm in artificial intelligence. Deep learning methods are ideally suited to large-scale data and, therefore, they should be ideally suited to knowledge discovery in bioinformatics and biomedicine at large.  In this brief paper, we review key aspects of the application of deep learning in bioinformatics and medicine, drawing from the themes covered by the contributions to an ESANN 2018 special session devoted to this topic. 	Deep Learning (DL) \cite{Goodfellow-et-al-2016} has become an increasingly popular Machine Learning (ML) approach in the last decade, as shown in Fig.	score:350
What is the difference between machine learning and deep learning?	 Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation.  In this pioneering paper, we propose the ``coding criterion'' to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations.	 Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation.  In this pioneering paper, we propose the ``coding criterion'' to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations.	score:353
What was a good interview question you were asked for a machine learning position? Why?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:287
What was a good interview question you were asked for a machine learning position? Why?	 Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.   As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?	 Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.   As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?	score:323
What was a good interview question you were asked for a machine learning position? Why?	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	score:329
What was a good interview question you were asked for a machine learning position? Why?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:330
What was a good interview question you were asked for a machine learning position? Why?	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	score:331
How do I learn machine learning as a programmer?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:371
How do I learn machine learning as a programmer?	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	score:379
How do I learn machine learning as a programmer?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:387
How do I learn machine learning as a programmer?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:387
How do I learn machine learning as a programmer?	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	 It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, ``learning to learn'' as they do so.  In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal.   Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm.  Our aim is to learn new internal representations as the algorithm learns new target functions,  that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data.	score:388
How can machine learning be used to solve the group food-ordering problem?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:284
How can machine learning be used to solve the group food-ordering problem?	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	score:315
How can machine learning be used to solve the group food-ordering problem?	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	score:316
How can machine learning be used to solve the group food-ordering problem?	 Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.   As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?	 Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.   As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?	score:322
How can machine learning be used to solve the group food-ordering problem?	  A number of heuristics have been developed to help with such choices, but the complicated nature of the geometric relationships involved means these are imperfect and can sometimes make poor choices.   We investigate the use of machine learning (specifically support vector machines) to make such choices instead.    Machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data.  In this paper we apply it in two case studies: the first to select between heuristics for choosing a CAD variable ordering; the second to identify when a CAD problem instance would benefit from Gr\"obner Basis preconditioning.  These appear to be the first such applications of machine learning to Symbolic Computation.  We demonstrate in both cases that the machine learned choice outperforms human developed heuristics.	  A number of heuristics have been developed to help with such choices, but the complicated nature of the geometric relationships involved means these are imperfect and can sometimes make poor choices.   We investigate the use of machine learning (specifically support vector machines) to make such choices instead.    Machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data.  In this paper we apply it in two case studies: the first to select between heuristics for choosing a CAD variable ordering; the second to identify when a CAD problem instance would benefit from Gr\"obner Basis preconditioning.  These appear to be the first such applications of machine learning to Symbolic Computation.  We demonstrate in both cases that the machine learned choice outperforms human developed heuristics.	score:323
What is the percentage of engineers in the 2017 batch at IIM Ahmedabad?	 However in \cite{heckel2015robust}, the threshold is loose and more importantly like \cite{menon2018fast} it can detect only unstructured outliers. The outlier removal algorithm in this work is in spirit a parameter free extension to the work in \cite{rahmani2016coherence} and can detect both structured and unstructured outliers. \subsection{Motivation and proposed approach} The main motivation behind this work is to build parameter free algorithms for robust PCA.  By parameter free we mean an algorithm which does not require the knowledge of parameters such as the dimension of true subspace or the number of outliers in the system nor it has a tuning parameter which has to be tuned according to the data. Tuning parameters in any algorithm present a challenge, as the user then would have to decide either through cross validation  \cite{arlot2010survey} or prior knowledge on how to set them.	 However in \cite{heckel2015robust}, the threshold is loose and more importantly like \cite{menon2018fast} it can detect only unstructured outliers. The outlier removal algorithm in this work is in spirit a parameter free extension to the work in \cite{rahmani2016coherence} and can detect both structured and unstructured outliers. \subsection{Motivation and proposed approach} The main motivation behind this work is to build parameter free algorithms for robust PCA.  By parameter free we mean an algorithm which does not require the knowledge of parameters such as the dimension of true subspace or the number of outliers in the system nor it has a tuning parameter which has to be tuned according to the data. Tuning parameters in any algorithm present a challenge, as the user then would have to decide either through cross validation  \cite{arlot2010survey} or prior knowledge on how to set them.	score:386
What is the percentage of engineers in the 2017 batch at IIM Ahmedabad?	 In addition, prior work has shown that decorrelated activations result in better features~\cite{1961_Barlow_possible,1992_NC_Schmidhuber,2009_NIPS_Bengio} and better generalization~\cite{2016_ICLR_Cogswell,2016_ICDM_Xiong}, suggesting room for further improving Batch Normalization.  In this paper, we propose Decorrelated Batch Normalization, in which we whiten the activations of each layer within a mini-batch.  Let $\mathbf{x}_i \in \mathbb{R}^d$ be the input to a layer for the $i$-th example in a mini-batch of size $m$.  The whitened input is given by {\setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt} \begin{equation} \hat{\mathbf{x}}_i = \Sigma^{-\frac{1}{2}}(\mathbf{x}_i - \mathbf{\mu}), \end{equation} } \hspace{-0.05in}where $\mathbf{\mu}=\frac{1}{m} \sum_{j=1}^{m} \mathbf{x}_j$ is the mini-batch mean and $\Sigma =\frac{1}{m} \sum_{j=1}^{m} (\mathbf{x}_j-\mathbf{\mu})(\mathbf{x}_j-\mathbf{\mu})^T$ is the mini-batch covariance matrix.	 In addition, prior work has shown that decorrelated activations result in better features~\cite{1961_Barlow_possible,1992_NC_Schmidhuber,2009_NIPS_Bengio} and better generalization~\cite{2016_ICLR_Cogswell,2016_ICDM_Xiong}, suggesting room for further improving Batch Normalization.  In this paper, we propose Decorrelated Batch Normalization, in which we whiten the activations of each layer within a mini-batch.  Let $\mathbf{x}_i \in \mathbb{R}^d$ be the input to a layer for the $i$-th example in a mini-batch of size $m$.  The whitened input is given by {\setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt} \begin{equation} \hat{\mathbf{x}}_i = \Sigma^{-\frac{1}{2}}(\mathbf{x}_i - \mathbf{\mu}), \end{equation} } \hspace{-0.05in}where $\mathbf{\mu}=\frac{1}{m} \sum_{j=1}^{m} \mathbf{x}_j$ is the mini-batch mean and $\Sigma =\frac{1}{m} \sum_{j=1}^{m} (\mathbf{x}_j-\mathbf{\mu})(\mathbf{x}_j-\mathbf{\mu})^T$ is the mini-batch covariance matrix.	score:402
What is the percentage of engineers in the 2017 batch at IIM Ahmedabad?	  As researchers try to scale neural network training to ever larger datasets and models, the optimization algorithm itself can be altered. For synchronous SGD there are rapidly diminishing returns \citep{lecun2012efficient,keskar2016large} as the number of workers, and thus the effective batch size, increases and we might hope that algorithms like KFAC \citep{distKFAC} would make better use of large batches. Although a promising direction for research, in this work we focus on what should hopefully be an optimization algorithm agnostic way to improve scalability and reproducibility.	  As researchers try to scale neural network training to ever larger datasets and models, the optimization algorithm itself can be altered. For synchronous SGD there are rapidly diminishing returns \citep{lecun2012efficient,keskar2016large} as the number of workers, and thus the effective batch size, increases and we might hope that algorithms like KFAC \citep{distKFAC} would make better use of large batches. Although a promising direction for research, in this work we focus on what should hopefully be an optimization algorithm agnostic way to improve scalability and reproducibility.	score:407
What is the percentage of engineers in the 2017 batch at IIM Ahmedabad?	 Both categories can be implemented based on various models such as neural networks (NN) \cite{phillips_generalizable_2017}\cite{hu2018probabilistic}, hidden Markov models (HMM) \cite{dong_intention_2017} and Bayes net \cite{schreier_integrated_2016}. Most of the work, however, formulated the future trajectories/actions as either deterministic functions or conditional probabilities of historical and current scene states.  The influence of human drivers' beliefs about the other vehicles future actions are ignored in the prediction framework.  In fact, in interactive driving scenarios, human drivers will actively anticipate and reason about surrounding vehicles' behavior when they decide the next-step actions. This means that besides historical and current scene states, the distribution over all possible trajectories is also influenced by their beliefs about other vehicles' plan.	 Both categories can be implemented based on various models such as neural networks (NN) \cite{phillips_generalizable_2017}\cite{hu2018probabilistic}, hidden Markov models (HMM) \cite{dong_intention_2017} and Bayes net \cite{schreier_integrated_2016}. Most of the work, however, formulated the future trajectories/actions as either deterministic functions or conditional probabilities of historical and current scene states.  The influence of human drivers' beliefs about the other vehicles future actions are ignored in the prediction framework.  In fact, in interactive driving scenarios, human drivers will actively anticipate and reason about surrounding vehicles' behavior when they decide the next-step actions. This means that besides historical and current scene states, the distribution over all possible trajectories is also influenced by their beliefs about other vehicles' plan.	score:407
What is the percentage of engineers in the 2017 batch at IIM Ahmedabad?	 In particular, \cite{Blanchard2017} assumes all the workers can access the whole data sample.  Similar to ours, the concurrent work \cite{alistarh2018byzantine} considers the scenario where data is generated and stored in a distributed fashion at the workers. However, \cite{alistarh2018byzantine} assumes that   in each iteration the workers are able to use \emph{fresh data} to compute the gradients.  However, fresh data in each round implies that the local sample size grows with time, which is not necessarily true in some applications. The fresh data assumption is crucial in their analysis: with fresh data, conditioning on the current model parameter estimator, the local gradients computed at different workers become independent, and the existing analysis of robust mean estimation may suffice.	 In particular, \cite{Blanchard2017} assumes all the workers can access the whole data sample.  Similar to ours, the concurrent work \cite{alistarh2018byzantine} considers the scenario where data is generated and stored in a distributed fashion at the workers. However, \cite{alistarh2018byzantine} assumes that   in each iteration the workers are able to use \emph{fresh data} to compute the gradients.  However, fresh data in each round implies that the local sample size grows with time, which is not necessarily true in some applications. The fresh data assumption is crucial in their analysis: with fresh data, conditioning on the current model parameter estimator, the local gradients computed at different workers become independent, and the existing analysis of robust mean estimation may suffice.	score:408
What do channels refer to in a convolutional neural network?	 \item We introduce a normalized data representation, which can be used in Neural Network algorithms, or other methods.  \item We present a Deep Convolutional Network, which can be able to learn traffic flow of different traffic points. \item Then, we present a Recurrent Neural Network, which, apart from its structure, can do the same as Convolutional Network.  \item Both of these models are able to predict \textit{n}-level traffic prediction for different points of the traffic (e.g. Quiet, light traffic, heavy traffic, congested, etc.) \item They also put up the predicted average speeds on different points of the traffic network based on the speed limits in that point (e.g. 0.65 of speed limit). \item Then, we present a Recurrent Neural Network \end{enumerate}  In the rest of this paper, we start our work by some preliminaries in section \ref{prelim}.	 \item We introduce a normalized data representation, which can be used in Neural Network algorithms, or other methods.  \item We present a Deep Convolutional Network, which can be able to learn traffic flow of different traffic points. \item Then, we present a Recurrent Neural Network, which, apart from its structure, can do the same as Convolutional Network.  \item Both of these models are able to predict \textit{n}-level traffic prediction for different points of the traffic (e.g. Quiet, light traffic, heavy traffic, congested, etc.) \item They also put up the predicted average speeds on different points of the traffic network based on the speed limits in that point (e.g. 0.65 of speed limit). \item Then, we present a Recurrent Neural Network \end{enumerate}  In the rest of this paper, we start our work by some preliminaries in section \ref{prelim}.	score:374
What do channels refer to in a convolutional neural network?	 	Deep neural networks, especially convolutional neural networks (CNNs) \cite{lecun1995convolutional} and recurrent neural networks (RNNs) \cite{elman1991distributed}, have been widely used in a variety of subjects \cite{lecun2015deep}. However, traditional neural network blocks aim to learn the feature representations in a local sense. For example, both convolutional and recurrent operations process a local neighborhood (several nearest neighboring neurons) in either space or time.  Therefore, the long-range dependencies can only be captured when these operations are applied recursively, while those long-range dependencies are sometimes significant in practical learning problems, such as image or video classification, text summarization, and financial market analysis \cite{beran1995long,cont2005long,pipiras2017long,willinger2003long}.   To address the above issue, a nonlocal neural network \cite{wang2017non} has been proposed recently, which is able to improve the performance on a couple of computer vision tasks. In contrast to convolutional or recurrent blocks, nonlocal operations \cite{wang2017non} capture long-range dependencies directly by computing interactions between each pair of positions in the feature space.  Generally speaking, nonlocality is ubiquitous in nature, and the nonlocal models and algorithms have been studied in various domains of physical, biological and social sciences \cite{ buades2005non, coifman2006diffusion, du2012analysis,silling2000reformulation,tadmor2015mathematical}.  In this work, we aim to study the nature of nonlocal networks, namely, what the nonlocal blocks have exactly learned through training on a real-world task.	  To address the above issue, a nonlocal neural network \cite{wang2017non} has been proposed recently, which is able to improve the performance on a couple of computer vision tasks. In contrast to convolutional or recurrent blocks, nonlocal operations \cite{wang2017non} capture long-range dependencies directly by computing interactions between each pair of positions in the feature space.	score:374
What do channels refer to in a convolutional neural network?	 Studying neural connectivity is considered one of the most promising and challenging areas of modern neuroscience. The underpinnings of cognition are hidden in the way neurons interact with each other. However, our experimental methods of studying real neural connections at a microscopic level are still arduous and costly. An efficient alternative is to infer connectivity based on the neuronal activations using computational methods.  A reliable method for network inference, would not only facilitate research of neural circuits without the need of laborious experiments but also reveal insights on the underlying mechanisms of the brain. In this work, we perform a review of methods for neural circuit inference given the activation time series of the neural population.  Approaching it from machine learning perspective, we divide the methodologies into unsupervised and supervised learning.	 Studying neural connectivity is considered one of the most promising and challenging areas of modern neuroscience. The underpinnings of cognition are hidden in the way neurons interact with each other. However, our experimental methods of studying real neural connections at a microscopic level are still arduous and costly. An efficient alternative is to infer connectivity based on the neuronal activations using computational methods.  A reliable method for network inference, would not only facilitate research of neural circuits without the need of laborious experiments but also reveal insights on the underlying mechanisms of the brain. In this work, we perform a review of methods for neural circuit inference given the activation time series of the neural population.  Approaching it from machine learning perspective, we divide the methodologies into unsupervised and supervised learning.	score:380
What do channels refer to in a convolutional neural network?	   This paper presents OptNet, a network architecture that integrates   optimization problems (here, specifically in the form of quadratic programs)   as individual layers in larger end-to-end trainable deep networks.   These layers encode constraints and complex dependencies   between the hidden states that traditional convolutional and   fully-connected layers often cannot capture.    We explore the foundations for such an architecture:   we show how techniques from sensitivity analysis, bilevel   optimization, and implicit differentiation can be used to   exactly differentiate through these layers and with respect   to layer parameters;   we develop a highly efficient solver for these layers that exploits fast   GPU-based batch solves within a primal-dual interior point method, and which   provides backpropagation gradients with virtually no additional cost on top of   the solve;   and we highlight the application of these approaches in several problems.	  Specifically, we build a framework where the output of the $i+1$th layer in a network is the \emph{solution} to a constrained optimization problem based upon previous layers.  This framework naturally encompasses a wide variety of inference problems expressed within a neural network, allowing for the potential of much richer end-to-end training for complex tasks that require such inference procedures.	score:390
What do channels refer to in a convolutional neural network?	 Do convolutional networks really need a fixed feed-forward structure? What if, after identifying the high-level concept of an image, a network could move directly to a layer that can distinguish fine-grained differences? Currently, a network would first need to execute sometimes hundreds of intermediate layers that specialize in unrelated aspects. Ideally, the more a network already knows about an image, the better it should be at deciding which layer to compute next.  In this work, we propose convolutional networks with adaptive inference graphs (ConvNet-AIG) that adaptively define their network topology conditioned on the input image. Following a high-level structure similar to residual networks (ResNets), ConvNet-AIG decides for each input image on the fly which layers are needed. In experiments on ImageNet we show that ConvNet-AIG learns distinct inference graphs for different categories.	 This leads us to the following research question: \emph{Do we really need fixed structures for convolutional networks, or could we assemble network graphs on the fly, conditioned on the input?}   In this work, we propose ConvNet-AIG, a convolutional network that adaptively defines its inference graph conditioned on the input image. Specifically, ConvNet\nobreakdash-AIG learns a set of convolutional layers and decides for each input image which layers are needed.  By learning both general layers useful to all images and expert layers specializing on subsets of categories, it allows to only compute features relevant to the input image. It is worthy to note that ConvNet-AIG does not require special supervision about label hierarchies and relationships to guide layers to specialize.  Figure~\ref{fig:page1} gives an overview of our approach.  ConvNet-AIG (bottom) follows a structure similar to a ResNet (center). The key difference is that for each residual layer, a gate determines whether the layer is needed for the current input image. The main technical challenge is that the gates need to make discrete decisions, which are difficult to integrate into convolutional networks that we would like to train using gradient descent.  To incorporate the discrete decisions, we build upon recent work~\cite{bengio2013estimating,gumbel,concrete} that introduces differentiable approximations for discrete stochastic nodes in neural networks. In particular, we model the gates as discrete random variables over two states: to execute the respective layer or to skip it. Further, we model the gates conditional on the output of the previous layer.	score:393
In multichannel convolutional neural networks, do the weights attached to one channel also get tied into the weights attached to other channels?	           In multi-network embedding, multiple networks represent different types of relations among the same set of nodes (representing person, commodity, gene and so on). There may be potential correlations between different networks. For multi-networks embedding, one of the most challenging task is how to consider the correlation between different networks.  To address this problem, we try to model the correlation between different networks during the feature learning process.     Autoencoder~\citep{Rumelhart1986Learning, Baldi2011Autoencoders} is a typical unsupervised deep learning model, which aims to learning a new encoding representation of input data.  It has been proved that autoencoder can solve these non-linear feature learning problems effectively.	           In multi-network embedding, multiple networks represent different types of relations among the same set of nodes (representing person, commodity, gene and so on). There may be potential correlations between different networks. For multi-networks embedding, one of the most challenging task is how to consider the correlation between different networks.  To address this problem, we try to model the correlation between different networks during the feature learning process.     Autoencoder~\citep{Rumelhart1986Learning, Baldi2011Autoencoders} is a typical unsupervised deep learning model, which aims to learning a new encoding representation of input data.  It has been proved that autoencoder can solve these non-linear feature learning problems effectively.	score:290
In multichannel convolutional neural networks, do the weights attached to one channel also get tied into the weights attached to other channels?	 These problems usually have temporal and/or spatial structure, which makes them amenable to particular neural architectures - Convolutional and Recurrent Neural Networks (CNN \cite{lecun1989backpropagation} and RNN \cite{hochreiter1997long}). Multi-agent interactions are different from machine perception in several ways: \begin{itemize} \item The data is no longer sampled on a spatial or temporal grid.   \item The number of agents changes frequently. \item Systems are quite heterogeneous, there is not a canonical large network that can be used for finetuning. \item Multi-agent systems have an obvious factorization (into point agents), whereas signals such as images and speech do not.   \end{itemize}  To model simple interactions in a physics simulation context, Interaction Networks (INs) were proposed by Battaglia et al.	 These problems usually have temporal and/or spatial structure, which makes them amenable to particular neural architectures - Convolutional and Recurrent Neural Networks (CNN \cite{lecun1989backpropagation} and RNN \cite{hochreiter1997long}). Multi-agent interactions are different from machine perception in several ways: \begin{itemize} \item The data is no longer sampled on a spatial or temporal grid.   \item The number of agents changes frequently. \item Systems are quite heterogeneous, there is not a canonical large network that can be used for finetuning. \item Multi-agent systems have an obvious factorization (into point agents), whereas signals such as images and speech do not.   \end{itemize}  To model simple interactions in a physics simulation context, Interaction Networks (INs) were proposed by Battaglia et al.	score:293
In multichannel convolutional neural networks, do the weights attached to one channel also get tied into the weights attached to other channels?	 The goal of a practitioner is to choose the network architecture most suitable for solving a given problem/application. One is then faced with a `multitude' of architectural choices when trying to decide the specific construction that is likely to work the best. For instance, one may decide to use a network with combinations of convolutional and fully connected layers using rectified linear units where learning is performed using dropout,   or prefer other variations of such a {\it prototypical} construction.   The choice is, at least, in part, governed by the domain knowledge apart from other resources constraints including  the sizes of available datasets and/or the desired convergence (or generalization) of the estimated parameters. Therefore, this interplay of (network) structure and parameter convergence is important to guide the choice of which setup will be most useful.	  We build upon and adapt this analysis by first addressing single-layer networks and unsupervised pretraining, and then,  the more general case of multi-layer dropout networks, followed by convolutional and recurrent neural networks. In each of these cases, once the network structure is tied to the behaviour of the gradients, we analyze the influence of input data statistics on the parameter estimation and convergence.   More importantly, apart from addressing the interplay, the algorithms we present, with minor tweaks, are easily deployable to the standard training pipeline. The bounds natively take into account the standard regularization schemes like dropout and layer-wise pretraining, making them even more useful in practice.      \subsection{The design choice problem} \label{sec:design}  Within the last several years, several variants of network architectures have been proposed with various combinations of fully connected  (e.	score:294
In multichannel convolutional neural networks, do the weights attached to one channel also get tied into the weights attached to other channels?	 However, typical point process based models often considered the impact of peer influence and content on the user participation and neglected other factors. Gamification elements, are among those factors that are neglected, while they have a strong impact on user participation in online services.  In this paper, we propose interdependent multi-dimensional temporal point processes that capture the impact of badges on user participation besides the peer influence and content factors.  We extend the proposed processes to model user actions over the community based question and answering websites, and propose an inference algorithm based on Variational-EM that can efficiently learn the model parameters. Extensive experiments on both synthetic and real data gathered from Stack Overflow show that our inference algorithm learns the parameters efficiently and the proposed method can better predict the user behavior compared to the alternatives.	 However, typical point process based models often considered the impact of peer influence and content on the user participation and neglected other factors. Gamification elements, are among those factors that are neglected, while they have a strong impact on user participation in online services.  In this paper, we propose interdependent multi-dimensional temporal point processes that capture the impact of badges on user participation besides the peer influence and content factors.  We extend the proposed processes to model user actions over the community based question and answering websites, and propose an inference algorithm based on Variational-EM that can efficiently learn the model parameters. Extensive experiments on both synthetic and real data gathered from Stack Overflow show that our inference algorithm learns the parameters efficiently and the proposed method can better predict the user behavior compared to the alternatives.	score:295
In multichannel convolutional neural networks, do the weights attached to one channel also get tied into the weights attached to other channels?	  This effect holds for a range of architectures, conditions, and datasets, ruling out the option that it is specific to a peculiar combination thereof. We discuss and explore various ways of selecting subsets of networks parameters to be learned. To the best of our knowledge, while others have shown analytic properties of randomly weighted networks, we are the first to explore the effects of keeping most of the weights at their randomly initialized values in multiple layers.	  This effect holds for a range of architectures, conditions, and datasets, ruling out the option that it is specific to a peculiar combination thereof. We discuss and explore various ways of selecting subsets of networks parameters to be learned. To the best of our knowledge, while others have shown analytic properties of randomly weighted networks, we are the first to explore the effects of keeping most of the weights at their randomly initialized values in multiple layers.	score:295
Approximation inverse matrix?	   Matrix completion and approximation are popular tools to capture a   user's preferences for recommendation and to approximate missing   data.  Instead of using low-rank factorization we take a drastically   different approach, based on the   simple insight that an additive model of co-clusterings allows   one to approximate matrices efficiently.  This allows us to build a   concise model that, per bit of model    learned, significantly beats all factorization approaches to   matrix approximation.   Even more surprisingly, we find that    summing over small co-clusterings is more effective in   modeling matrices than classic co-clustering, which uses just one   large partitioning of the matrix.     Following Occam's razor principle suggests that the simple structure   induced by our model better captures the latent preferences and   decision making processes present in the real world than classic   co-clustering or matrix factorization.	   Matrix completion and approximation are popular tools to capture a   user's preferences for recommendation and to approximate missing   data.  Instead of using low-rank factorization we take a drastically   different approach, based on the   simple insight that an additive model of co-clusterings allows   one to approximate matrices efficiently.  This allows us to build a   concise model that, per bit of model    learned, significantly beats all factorization approaches to   matrix approximation.   Even more surprisingly, we find that    summing over small co-clusterings is more effective in   modeling matrices than classic co-clustering, which uses just one   large partitioning of the matrix.     Following Occam's razor principle suggests that the simple structure   induced by our model better captures the latent preferences and   decision making processes present in the real world than classic   co-clustering or matrix factorization.	score:382
Approximation inverse matrix?	} 	Matrix factorization is an important tool across applied statistics including psychometrics, econometrics, biostatistics and machine learning.  Despite its popularity, matrix factorization suffers from two fundamental problems.  First, in general the matrix factorization does not have a unique solution.  There may be many unrelated matrices consistent with the observed data.   Second, the number of parameters to estimate may be larger than the number of observations and increasing at a faster rate.  In the language of statistics, estimators based on matrix factorization may neither be identified nor consistently estimated.  In practice, results based on matrix factorization may be neither reliable nor robust.  This paper considers a subset of matrix factorization problems where the matrix factors are assumed to have non-negative elements and for at least one matrix factor, its columns sum to one.	} 	Matrix factorization is an important tool across applied statistics including psychometrics, econometrics, biostatistics and machine learning.  Despite its popularity, matrix factorization suffers from two fundamental problems.  First, in general the matrix factorization does not have a unique solution.  There may be many unrelated matrices consistent with the observed data.   Second, the number of parameters to estimate may be larger than the number of observations and increasing at a faster rate.  In the language of statistics, estimators based on matrix factorization may neither be identified nor consistently estimated.  In practice, results based on matrix factorization may be neither reliable nor robust.  This paper considers a subset of matrix factorization problems where the matrix factors are assumed to have non-negative elements and for at least one matrix factor, its columns sum to one.	score:406
Approximation inverse matrix?	 Low-rank matrix approximations are often used to help scale standard machine learning algorithms to large-scale problems.  Recently, matrix coherence has been used to characterize the ability to extract global information from a subset of matrix entries in the context of these low-rank approximations and other sampling-based algorithms, e.g., matrix completion, robust PCA.   Since coherence is defined in terms of the singular vectors of a matrix and is expensive to compute, the practical significance of these results largely hinges on the following question: \emph{Can we efficiently and accurately estimate the coherence of a matrix?} In this paper we address this question. We propose a novel algorithm for estimating coherence from a small number of columns, formally analyze its behavior, and derive a new coherence-based matrix approximation bound based on this analysis.  We then present extensive experimental results on synthetic and real datasets that corroborate our worst-case theoretical analysis, yet provide strong support for the use of our proposed algorithm whenever low-rank approximation is being considered.  Our algorithm efficiently and accurately estimates matrix coherence across a wide range of datasets, and these coherence estimates are excellent predictors of the effectiveness of sampling-based matrix approximation on a case-by-case basis.	 Low-rank matrix approximations are often used to help scale standard machine learning algorithms to large-scale problems.  Recently, matrix coherence has been used to characterize the ability to extract global information from a subset of matrix entries in the context of these low-rank approximations and other sampling-based algorithms, e.g., matrix completion, robust PCA.   Since coherence is defined in terms of the singular vectors of a matrix and is expensive to compute, the practical significance of these results largely hinges on the following question: \emph{Can we efficiently and accurately estimate the coherence of a matrix?} In this paper we address this question. We propose a novel algorithm for estimating coherence from a small number of columns, formally analyze its behavior, and derive a new coherence-based matrix approximation bound based on this analysis.  We then present extensive experimental results on synthetic and real datasets that corroborate our worst-case theoretical analysis, yet provide strong support for the use of our proposed algorithm whenever low-rank approximation is being considered.  Our algorithm efficiently and accurately estimates matrix coherence across a wide range of datasets, and these coherence estimates are excellent predictors of the effectiveness of sampling-based matrix approximation on a case-by-case basis.	score:416
Approximation inverse matrix?	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	score:418
Approximation inverse matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	 This assumption is restrictive in many applications such as recommendations systems. The theoretical analysis in the present paper is carried out for general sampling distributions.   Similar to our setting, matrix completion with side information explores  the available user data provided by various sources. For instance \cite{jain2013} and \cite{xu2013NIPS} introduce the so-called Inductive Matrix Completion (IMC).	score:419
What is the transpose of an inverse matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	 The basic principle of matrix completion  consists in recovering all the entries of an unknown data matrix from incomplete and noisy observations of its entries.  To address the high-dimensionality in matrix completion problem, statistical inference based on low-rank constraint is now an ubiquitous technique for recovering the underlying data matrix.  Thus, matrix completion can be formulated as minimizing the rank of the matrix given a random sample of its entries. However, this rank minimization problem is in general NP-hard due to the combinatorial nature of the rank function~\citep{fazel2001,fazelPhD-2000}. To alleviate this problem and make it tractable, convex relaxation strategies were proposed, e.	score:375
What is the transpose of an inverse matrix?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:384
What is the transpose of an inverse matrix?	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	g., \cite{Roth1981}) we propose an efficient randomized algorithm that determines whether or not it is possible to uniquely complete an incomplete matrix to a matrix of specified rank $d$. Our proposed algorithm does not attempt to complete the matrix but only determines if a unique completion is possible. We introduce a new matrix, which we call {\em the completion matrix} that serves as the analogue of the rigidity matrix in rigidity theory.  The rank of the completion matrix determines a property which we call infinitesimal completion. Whenever the completion matrix is large and sparse its rank can be efficiently determined using iterative methods such as LSQR \cite{LSQR}. As in rigidity theory, we will also make the distinction between {\em local} completion and {\em global} completion.	score:400
What is the transpose of an inverse matrix?	 A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined \emph{blocks} of columns (or rows) from the matrix.	 A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined \emph{blocks} of columns (or rows) from the matrix.	score:400
What is the transpose of an inverse matrix?	     Our goal in this paper is to propose an analytic method of a model explanation that is applicable to general classification models.     To this end, we introduce the concept of a contribution matrix and an explanation embedding in a constraint space by using a matrix factorization.     We extract a rule-like model explanation from the contribution matrix with the help of the nonnegative matrix factorization.      To validate our method, the experiment results provide with open datasets as well as an industry dataset of a LTE network diagnosis and the results show our method extracts reasonable explanations.  	In recent years, a number of artificial intelligent services have been developed such as defect detection system or diagnosis system for customer services.	     The constraints should be reflected in the explanations of instances.      The main contribution in this paper is that we propose an analytic method of a model explanation that is applicable to general classification models.     We introduce the concept of a contribution matrix and an explanation embedding in a constraint space by using the matrix factorization.      We experimentally show that the embedded explanations are well clustered according to the classification category of instances.      By using the explanation embedding, we extract a model explanation of the rule-like form.     We also perform experiments with open datasets as well as an industrial dataset to show the validity of our approach in practice.     In particular, many of industrial datasets, especially defect detection or diagnosis systems, consist of numerical attributes each of them has its own meaning.     Our method therefore will focus on dataset having numeric attributes.	score:402
What is the star batch of Kota's coaching?	 	The detection and characterization of circumstellar disks and exoplanets requires very high contrasts relative to the host star. Achieving these contrasts requires a combination of telescope and instrument design to suppress the residual diffraction pattern of a star as well as state-of-the-art post-processing techniques to suppress residual systematic flux that remains.  Well-designed instruments are able to stabilize the temporally varying noise in telescope exposures, creating quasi-static features \citep{perrin04, hinkley07, perrin08, soummer07, traub10}. These quasi-static features (speckles), together with the stellar point spread function (PSF), can be empirically modeled and removed by post-processing techniques, revealing circumstellar disks and exoplanets around stars \citep[e.	 	The detection and characterization of circumstellar disks and exoplanets requires very high contrasts relative to the host star. Achieving these contrasts requires a combination of telescope and instrument design to suppress the residual diffraction pattern of a star as well as state-of-the-art post-processing techniques to suppress residual systematic flux that remains.  Well-designed instruments are able to stabilize the temporally varying noise in telescope exposures, creating quasi-static features \citep{perrin04, hinkley07, perrin08, soummer07, traub10}. These quasi-static features (speckles), together with the stellar point spread function (PSF), can be empirically modeled and removed by post-processing techniques, revealing circumstellar disks and exoplanets around stars \citep[e.	score:514
What is the star batch of Kota's coaching?	 	The detection and characterization of circumstellar disks and exoplanets requires very high contrasts relative to the host star. Achieving these contrasts requires a combination of telescope and instrument design to suppress the residual diffraction pattern of a star as well as state-of-the-art post-processing techniques to suppress residual systematic flux that remains.  Well-designed instruments are able to stabilize the temporally varying noise in telescope exposures, creating quasi-static features \citep{perrin04, hinkley07, perrin08, soummer07, traub10}. These quasi-static features (speckles), together with the stellar point spread function (PSF), can be empirically modeled and removed by post-processing techniques, revealing circumstellar disks and exoplanets around stars \citep[e.	 	The detection and characterization of circumstellar disks and exoplanets requires very high contrasts relative to the host star. Achieving these contrasts requires a combination of telescope and instrument design to suppress the residual diffraction pattern of a star as well as state-of-the-art post-processing techniques to suppress residual systematic flux that remains.  Well-designed instruments are able to stabilize the temporally varying noise in telescope exposures, creating quasi-static features \citep{perrin04, hinkley07, perrin08, soummer07, traub10}. These quasi-static features (speckles), together with the stellar point spread function (PSF), can be empirically modeled and removed by post-processing techniques, revealing circumstellar disks and exoplanets around stars \citep[e.	score:514
What is the star batch of Kota's coaching?	 On the other hand, in this big data era, the e-commerce industry takes huge advantages of machine learning techniques to discover customers' preference. For example, notifying a customer of the release of ``Star Wars: The Last Jedi'' if he/she has ever purchased the tickets for ``Star Trek Beyond''; recommending a reader ``A Brief History of Time'' from Stephen Hawking in case there is a ``Relativity: The Special and General Theory'' from Albert Einstein in the shopping cart on Amazon.  The content based recommendation is achieved by analyzing the theme of the items extracted from its text description.   Topic modeling is a collection of algorithms that aim to discover and annotate large archives of documents with thematic information\cite{blei2012probabilistic}.  Usually, general topic modeling algorithms do not require any prior annotations or labeling of the document while the abstraction is the output of the algorithms.	 On the other hand, in this big data era, the e-commerce industry takes huge advantages of machine learning techniques to discover customers' preference. For example, notifying a customer of the release of ``Star Wars: The Last Jedi'' if he/she has ever purchased the tickets for ``Star Trek Beyond''; recommending a reader ``A Brief History of Time'' from Stephen Hawking in case there is a ``Relativity: The Special and General Theory'' from Albert Einstein in the shopping cart on Amazon.  The content based recommendation is achieved by analyzing the theme of the items extracted from its text description.   Topic modeling is a collection of algorithms that aim to discover and annotate large archives of documents with thematic information\cite{blei2012probabilistic}.  Usually, general topic modeling algorithms do not require any prior annotations or labeling of the document while the abstraction is the output of the algorithms.	score:523
What is the star batch of Kota's coaching?	1677085}, and many others. The extension of clustering techniques to multilayer graphs  is a challenging task and several approaches have been proposed so far. See   \cite{Kim:2015:CDM,Sun:2013a,Xu:2013a,zhao:2017b} for an overview.  For instance,   \cite{Dong:2012:Clustering,Dong:2014:Grassmann,Tang:2009:CMG:1674659.1677085,zhao:2017a} rely on matrix factorizations, whereas  \cite{Bacco:2017,Paul:2016b,Peixoto:2015a,Schein:2015:BPT:2783258. 2783414,schein:2016} take a Bayesian inference approach, and~\cite{Kumar:2011,NIPS2011_4360} enforce consistency among layers in the resulting clustering assignment. In   \cite{Mucha:2010a,Paul:2016a,wilson2017community} Newman's modularity~\cite{Newman:2006a} is extended to multilayer graphs.  Recently   \cite{Domenico:2015a,Stanley:2016a} proposed to compress a multilayer graph by combining sets of similar layers (called \textquoteleft strata\textquoteright) to later identify the corresponding communities.	1677085}, and many others. The extension of clustering techniques to multilayer graphs  is a challenging task and several approaches have been proposed so far. See   \cite{Kim:2015:CDM,Sun:2013a,Xu:2013a,zhao:2017b} for an overview.  For instance,   \cite{Dong:2012:Clustering,Dong:2014:Grassmann,Tang:2009:CMG:1674659.1677085,zhao:2017a} rely on matrix factorizations, whereas  \cite{Bacco:2017,Paul:2016b,Peixoto:2015a,Schein:2015:BPT:2783258. 2783414,schein:2016} take a Bayesian inference approach, and~\cite{Kumar:2011,NIPS2011_4360} enforce consistency among layers in the resulting clustering assignment. In   \cite{Mucha:2010a,Paul:2016a,wilson2017community} Newman's modularity~\cite{Newman:2006a} is extended to multilayer graphs.  Recently   \cite{Domenico:2015a,Stanley:2016a} proposed to compress a multilayer graph by combining sets of similar layers (called \textquoteleft strata\textquoteright) to later identify the corresponding communities.	score:539
What is the star batch of Kota's coaching?	  Consider, by way of example, the problem of detecting exoplanets via the so called transit method first proposed by \cite{struve1952proposal}. The luminosity of a star is measured at regular intervals, with the aim of detecting segments of reduced luminosity. These indicate the transit of a planet \citep{Transit} and can naturally be interpreted as collective anomalies.  The light curves are typically preprocessed \citep{Preprocessing} and both the raw and whitened light curves can be accessed online. We have included the whitened light curve of the star Kepler 1132 in Figure \ref{fig:Kepler1132First} to illustrate the nature of this type of data. We note the presence of a global anomaly on day 1550 and the noisy nature of the data.	  Consider, by way of example, the problem of detecting exoplanets via the so called transit method first proposed by \cite{struve1952proposal}. The luminosity of a star is measured at regular intervals, with the aim of detecting segments of reduced luminosity. These indicate the transit of a planet \citep{Transit} and can naturally be interpreted as collective anomalies.  The light curves are typically preprocessed \citep{Preprocessing} and both the raw and whitened light curves can be accessed online. We have included the whitened light curve of the star Kepler 1132 in Figure \ref{fig:Kepler1132First} to illustrate the nature of this type of data. We note the presence of a global anomaly on day 1550 and the noisy nature of the data.	score:542
What is star batch of Allen and do droppers also get into that batch?	  If an agent has already swung several bats, for example, we would hope that it could easily learn to swing a new bat.  Why?  Like many domains, the bat-swinging domain has a low-dimensional representation that affects the system's dynamics in structured ways. The agent's prior experience should allow it to both learn \emph{how} to model related instances of a domain---such as the bat's length, a latent parameter that smoothly changes in the bat's dynamics---and \emph{what} specific model parameters (e. g., lengths) are likely.  Domains with closely-related dynamics are an interesting regime for transfer learning. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP) as a formalization of these types of domains, with two important features.  First, we posit that  there exist a bounded number of latent parameters that, if known, would fully specify the dynamics.	  If an agent has already swung several bats, for example, we would hope that it could easily learn to swing a new bat.  Why?  Like many domains, the bat-swinging domain has a low-dimensional representation that affects the system's dynamics in structured ways. The agent's prior experience should allow it to both learn \emph{how} to model related instances of a domain---such as the bat's length, a latent parameter that smoothly changes in the bat's dynamics---and \emph{what} specific model parameters (e. g., lengths) are likely.  Domains with closely-related dynamics are an interesting regime for transfer learning. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP) as a formalization of these types of domains, with two important features.  First, we posit that  there exist a bounded number of latent parameters that, if known, would fully specify the dynamics.	score:478
What is star batch of Allen and do droppers also get into that batch?	 We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole.  To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start.	 We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole.  To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start.	score:488
What is star batch of Allen and do droppers also get into that batch?	 However, one cannot afford the cost of communications. This paper aims to find an algorithm with acceptable cost of communications and maximum factor of speed-up.  Due to the cost of communications, we can find that traditional methods for $\X$-armed bandits such as StoSOO algorithm \cite{valko2013stochastic} and HOO algorithm \cite{bubeck2011x} can hardly be put into a distributed framework.  These methods all operates in many traversals of a covering tree from the root down to leaves. If two players traverse the covering tree without communication, they cannot know whether they are visiting the same node. Thus, the whole process may be inefficient and uncoordinated unless large number of communication rounds are set in the algorithm.  Motivated by this, we propose a novel distributed algorithm for the $\X$-armed bandit problem with multiple communication rounds.	 However, one cannot afford the cost of communications. This paper aims to find an algorithm with acceptable cost of communications and maximum factor of speed-up.  Due to the cost of communications, we can find that traditional methods for $\X$-armed bandits such as StoSOO algorithm \cite{valko2013stochastic} and HOO algorithm \cite{bubeck2011x} can hardly be put into a distributed framework.  These methods all operates in many traversals of a covering tree from the root down to leaves. If two players traverse the covering tree without communication, they cannot know whether they are visiting the same node. Thus, the whole process may be inefficient and uncoordinated unless large number of communication rounds are set in the algorithm.  Motivated by this, we propose a novel distributed algorithm for the $\X$-armed bandit problem with multiple communication rounds.	score:490
What is star batch of Allen and do droppers also get into that batch?	.. $ , where $\mathbf{d_i}$ denotes a batch. In this case, each batch receives the same training iterations as a batch updates the network exactly once in an epoch. Please note this engineering simplification allows batches to repetitively flow into the network, which is different from the random sampling in Statistics. However, it is known that the gradient variances differentiate batches in the training \cite{johnson2013accelerating}, and gradient updates from the large loss batch contribute more than the small loss ones \cite{simo2015discriminative}.  This suggests that rebalancing the training effort across batches is necessary. SGD fails to consider the issue, and we think this is a problem.  In this paper, we propose Inconsistent Stochastic Gradient Descent (ISGD) to rebalance the training effort among batches. The inconsistency is reflected by the uneven gradient updates on batches. ISGD measures the training status of a batch by the associated loss.  At any iteration $t$, ISGD traces the losses in iterations $[t-n_b, t]$, where $n_b$ is the number of distinct batches in a dataset. These losses assist in constructing a dynamic upper threshold to identify a under-trained batch during the training. If a batch's loss exceeds the threshold, ISGD accelerates the training on the batch by solving a new subproblem that minimizes the discrepancy between the loss of current batch and the mean.  The subproblem also contains a conservative constraint to avoid overshooting by bounding the parameter change. The key idea of the subproblem is to allow additional gradient updates on a under-trained batch while still remaining the proximity to the current network parameters. Empirical experiments demonstrate ISGD, especially at the final stage, performs much better than the baseline method SGD on various mainstream datasets and networks.	.. $ , where $\mathbf{d_i}$ denotes a batch. In this case, each batch receives the same training iterations as a batch updates the network exactly once in an epoch. Please note this engineering simplification allows batches to repetitively flow into the network, which is different from the random sampling in Statistics. However, it is known that the gradient variances differentiate batches in the training \cite{johnson2013accelerating}, and gradient updates from the large loss batch contribute more than the small loss ones \cite{simo2015discriminative}.  This suggests that rebalancing the training effort across batches is necessary. SGD fails to consider the issue, and we think this is a problem.  In this paper, we propose Inconsistent Stochastic Gradient Descent (ISGD) to rebalance the training effort among batches. The inconsistency is reflected by the uneven gradient updates on batches. ISGD measures the training status of a batch by the associated loss.  At any iteration $t$, ISGD traces the losses in iterations $[t-n_b, t]$, where $n_b$ is the number of distinct batches in a dataset. These losses assist in constructing a dynamic upper threshold to identify a under-trained batch during the training. If a batch's loss exceeds the threshold, ISGD accelerates the training on the batch by solving a new subproblem that minimizes the discrepancy between the loss of current batch and the mean.  The subproblem also contains a conservative constraint to avoid overshooting by bounding the parameter change. The key idea of the subproblem is to allow additional gradient updates on a under-trained batch while still remaining the proximity to the current network parameters. Empirical experiments demonstrate ISGD, especially at the final stage, performs much better than the baseline method SGD on various mainstream datasets and networks.	score:496
What is star batch of Allen and do droppers also get into that batch?	 Unlike the standard fixed-point iteration, the proposed stochastic fixed-point iteration outputs an average over its trajectory. Extensive experiments show that, with proper choice of step size for stochastic backward Euler, the proposed algorithm can improve over EM and Mini-batch EM and locate an improved minimum with decreased objective value.    In other words, while Lloyd's algorithm is effective with \emph{a full gradient oracle} we achieve better performance with the weaker \emph{mini-batch gradient oracle}. We are motivated by  recent work by two of the authors \cite{deep_relax} which applied a similar algorithm to accelerate the training of Deep Neural Networks.	 Unlike the standard fixed-point iteration, the proposed stochastic fixed-point iteration outputs an average over its trajectory. Extensive experiments show that, with proper choice of step size for stochastic backward Euler, the proposed algorithm can improve over EM and Mini-batch EM and locate an improved minimum with decreased objective value.    In other words, while Lloyd's algorithm is effective with \emph{a full gradient oracle} we achieve better performance with the weaker \emph{mini-batch gradient oracle}. We are motivated by  recent work by two of the authors \cite{deep_relax} which applied a similar algorithm to accelerate the training of Deep Neural Networks.	score:503
In what ways is CNN biased, if it is?	 Such positive effects could be achieved even by requiring that the judge agrees with the algorithm in 95\% of cases. However, the widespread use of machine learning also poses a major worry. If algorithms themselves can be biased, for some definition of this, then widespread usage may be institutionalizing this bias. Recently, many researchers have raised significant concerns that these algorithms can be biased in problematic ways.   One notable example is the COMPAS system, which has been used to score criminal defendants and judge whether or not they are likely to recidivate. These scores are then used in parole considerations. Thus, ensuring that these scores do not discriminate against certain demographic groups is of utmost importance. In their study of the COMPAS system, Larson, Mattu, Kirchner, and Angwin~\cite{LMKA16} looked at 10,000 criminal defendants and compared their predicted recidivism rates with their actual recidivism rates over a two year period.	 Such positive effects could be achieved even by requiring that the judge agrees with the algorithm in 95\% of cases. However, the widespread use of machine learning also poses a major worry. If algorithms themselves can be biased, for some definition of this, then widespread usage may be institutionalizing this bias. Recently, many researchers have raised significant concerns that these algorithms can be biased in problematic ways.   One notable example is the COMPAS system, which has been used to score criminal defendants and judge whether or not they are likely to recidivate. These scores are then used in parole considerations. Thus, ensuring that these scores do not discriminate against certain demographic groups is of utmost importance. In their study of the COMPAS system, Larson, Mattu, Kirchner, and Angwin~\cite{LMKA16} looked at 10,000 criminal defendants and compared their predicted recidivism rates with their actual recidivism rates over a two year period.	score:357
In what ways is CNN biased, if it is?	 While popular in practice, this algorithm can be brittle as it relies on the Markov chain mixing properly during training: if the sampling process fails to adequately explore the space of allowed configurations (e.g. by getting trapped in regions of high-probability), the negative phase statistics can become biased and cause the learning procedure to diverge.  Great care must therefore be taken in decreasing the learning rate or increasing the number of Gibbs steps to offset the loss of ergodicity incurred by learning.  An attractive and well studied alternative is to augment the Boltzmann distribution with a temperature parameter in order to perform a simulation in the joint temperature-configuration space.	 While popular in practice, this algorithm can be brittle as it relies on the Markov chain mixing properly during training: if the sampling process fails to adequately explore the space of allowed configurations (e.g. by getting trapped in regions of high-probability), the negative phase statistics can become biased and cause the learning procedure to diverge.  Great care must therefore be taken in decreasing the learning rate or increasing the number of Gibbs steps to offset the loss of ergodicity incurred by learning.  An attractive and well studied alternative is to augment the Boltzmann distribution with a temperature parameter in order to perform a simulation in the joint temperature-configuration space.	score:358
In what ways is CNN biased, if it is?	 However, to date, there has been little work exploring exactly which topological structures are being learned in the embeddings process. In this paper, we investigate if graph embeddings are approximating something analogous with traditional vertex level graph features. If such a relationship can be found, it could be used to provide a theoretical insight into how graph embedding approaches function.  We perform this investigation by predicting known topological features, using supervised and unsupervised methods, directly from the embedding space. If a mapping between the embeddings and topological features can be found, then we argue that the structural information encapsulated by the features is represented in the embedding space. To explore this, we present extensive experimental evaluation from five state-of-the-art unsupervised graph embedding techniques, across a range of empirical graph datasets, measuring a selection of topological features.	 However, to date, there has been little work exploring exactly which topological structures are being learned in the embeddings process. In this paper, we investigate if graph embeddings are approximating something analogous with traditional vertex level graph features. If such a relationship can be found, it could be used to provide a theoretical insight into how graph embedding approaches function.  We perform this investigation by predicting known topological features, using supervised and unsupervised methods, directly from the embedding space. If a mapping between the embeddings and topological features can be found, then we argue that the structural information encapsulated by the features is represented in the embedding space. To explore this, we present extensive experimental evaluation from five state-of-the-art unsupervised graph embedding techniques, across a range of empirical graph datasets, measuring a selection of topological features.	score:372
In what ways is CNN biased, if it is?	  \end{itemize} 	Consider a bidder trying to decide how much to bid in an auction (for example, a sponsored search auction). If the auction happens to be the truthful Vickrey-Clarke-Groves auction~\cite{Vickrey61,Clarke71,Groves73}, then the bidder's decision is easy: simply bid your value. If instead, the bidder is participating in a Generalized First-Price (GFP) or Generalized Second-Price (GSP) auction, the optimal strategy is less clear.  Bidders can certainly attempt to compute a Bayes-Nash equilibrium of the associated game and play accordingly, but this is unrealistic due to the need for accurate priors and extensive computation.  Alternatively, the bidders may try to learn a best-response over time (possibly offloading the learning to commercial bid optimizers). We specifically consider bidders who \emph{\lowregret learn}, as empirical work of Nekipelov et al.	  \end{itemize} 	Consider a bidder trying to decide how much to bid in an auction (for example, a sponsored search auction). If the auction happens to be the truthful Vickrey-Clarke-Groves auction~\cite{Vickrey61,Clarke71,Groves73}, then the bidder's decision is easy: simply bid your value. If instead, the bidder is participating in a Generalized First-Price (GFP) or Generalized Second-Price (GSP) auction, the optimal strategy is less clear.  Bidders can certainly attempt to compute a Bayes-Nash equilibrium of the associated game and play accordingly, but this is unrealistic due to the need for accurate priors and extensive computation.  Alternatively, the bidders may try to learn a best-response over time (possibly offloading the learning to commercial bid optimizers). We specifically consider bidders who \emph{\lowregret learn}, as empirical work of Nekipelov et al.	score:385
In what ways is CNN biased, if it is?	  This is analogous to performing a probabilistic binary search, and if there is just a single path being monitored, the approach is similar to related work for single-path bandwidth monitoring.  In our framework, the difference is that we determine both which path to probe and what rate to probe in an active, online fashion, based on the measurements we have already gathered over the entire network.   Simulations and PlanetLab experiments suggest that our framework significantly reduces the amount of probing traffic (by more than 50\%) in comparison to na\"ively probing sequentially on every path, without any loss in accuracy.  The rest of this paper is organized as follows.  In Sect.~\ref{sec:pab}, we introduce the notion of probabilistic available bandwidth.	  This is analogous to performing a probabilistic binary search, and if there is just a single path being monitored, the approach is similar to related work for single-path bandwidth monitoring.  In our framework, the difference is that we determine both which path to probe and what rate to probe in an active, online fashion, based on the measurements we have already gathered over the entire network.   Simulations and PlanetLab experiments suggest that our framework significantly reduces the amount of probing traffic (by more than 50\%) in comparison to na\"ively probing sequentially on every path, without any loss in accuracy.  The rest of this paper is organized as follows.  In Sect.~\ref{sec:pab}, we introduce the notion of probabilistic available bandwidth.	score:386
Is CNN honest?	   A Convolutional Neural Network (CNN) is a variant of a Deep Neural Network (DNN) \cite{110_deeplearning_lenet}. Inspired by the visual cortex of animals, CNNs are applied to state-of-the-art applications, including computer vision and speech recognition \cite{deng2014deep}. However, supervised training of CNNs is computationally demanding and time consuming, and in many cases, several weeks are required to complete a training session.  Often applications are tested with different parameters, and each test requires a full session of training.   Multi-core processors~\cite{Williams:2009} and in particular many-core~\cite{benkner11} processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) \cite{nvidia_gpu} or the Intel Xeon Phi \cite{chrysos2012intel} co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs.	   A Convolutional Neural Network (CNN) is a variant of a Deep Neural Network (DNN) \cite{110_deeplearning_lenet}. Inspired by the visual cortex of animals, CNNs are applied to state-of-the-art applications, including computer vision and speech recognition \cite{deng2014deep}. However, supervised training of CNNs is computationally demanding and time consuming, and in many cases, several weeks are required to complete a training session.  Often applications are tested with different parameters, and each test requires a full session of training.   Multi-core processors~\cite{Williams:2009} and in particular many-core~\cite{benkner11} processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) \cite{nvidia_gpu} or the Intel Xeon Phi \cite{chrysos2012intel} co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs.	score:466
Is CNN honest?	 However, it is difficult for CNN to play a great role in visual tracking. CNN usually consists of a large amount of parameters and needs big dataset to train the network to avoid over-fitting. So far, there are some novel studies~\cite{23,5,27,34} which combine CNN and traditional trackers to achieve the start of the art. Some of them just use CNNs which are trained on ImageNet or other large datasets to extract features. \\ On one hand, tracking seems much easier than detection because it only needs to conduct a binary classification between the target and the background. On the other hand, it is difficult to train the network because of the diversity of objects that we might track. An object may be the target in one video but the background in another. And there is no such amount of data for tracking to train a deep network.	 However, it is difficult for CNN to play a great role in visual tracking. CNN usually consists of a large amount of parameters and needs big dataset to train the network to avoid over-fitting. So far, there are some novel studies~\cite{23,5,27,34} which combine CNN and traditional trackers to achieve the start of the art. Some of them just use CNNs which are trained on ImageNet or other large datasets to extract features. \\ On one hand, tracking seems much easier than detection because it only needs to conduct a binary classification between the target and the background. On the other hand, it is difficult to train the network because of the diversity of objects that we might track. An object may be the target in one video but the background in another. And there is no such amount of data for tracking to train a deep network.	score:480
Is CNN honest?	 \citet{invert-cnn} used upsampling-deconvolutional architectures to invert the hidden activations of feedforward CNNs to the input domain.  In another related work, \citet{what-where} proposed a stacked what-where autoencoder network and demonstrated its promise in unsupervised and semi-supervised settings.  \citet{deconv-recon} showed that CNNs discriminately trained for image classification (e. g., VGGNet~\citep{vggnet}) are almost fully invertible using pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why CNNs are invertible yet.  \textgray{ Our setup is also similar to recent work of Zhang et al. (http://bit.ly/2a6sR) which empirically shows that CNNs are almost fully invertible using pooling switches (it also shows that encouraging this property improves large-scale image classification).	 \citet{invert-cnn} used upsampling-deconvolutional architectures to invert the hidden activations of feedforward CNNs to the input domain.  In another related work, \citet{what-where} proposed a stacked what-where autoencoder network and demonstrated its promise in unsupervised and semi-supervised settings.  \citet{deconv-recon} showed that CNNs discriminately trained for image classification (e. g., VGGNet~\citep{vggnet}) are almost fully invertible using pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why CNNs are invertible yet.  \textgray{ Our setup is also similar to recent work of Zhang et al. (http://bit.ly/2a6sR) which empirically shows that CNNs are almost fully invertible using pooling switches (it also shows that encouraging this property improves large-scale image classification).	score:488
Is CNN honest?	 Deep learning\blfootnote{$^*$ These authors contributed equally to this work}, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a `good' architecture. The existing works tend to focus on reporting CNN architectures that work well  for face recognition rather than investigate the reason.   In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible.  Specifically, we  use public database LFW (Labeled Faces in the Wild) to train CNNs,  unlike most existing CNNs trained on private databases.  We propose three CNN architectures which are the first reported architectures trained using LFW data.	 Deep learning\blfootnote{$^*$ These authors contributed equally to this work}, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a `good' architecture. The existing works tend to focus on reporting CNN architectures that work well  for face recognition rather than investigate the reason.   In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible.  Specifically, we  use public database LFW (Labeled Faces in the Wild) to train CNNs,  unlike most existing CNNs trained on private databases.  We propose three CNN architectures which are the first reported architectures trained using LFW data.	score:492
Is CNN honest?	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	score:495
Why is CNN and The New York Times fueling riots by pushing more negative rhetoric against Trump ?	 In contrast with prior work where extremely large CNNs are used, we exploit the dilated CNN for its flexibility in varying the amount of conditioning context. In the two extremes, depending on the choice of dilation, the CNN decoder can reproduce a simple MLP using a bags of words representation of text, or can reproduce the long-range dependence of recurrent architectures (like an LSTM) by conditioning on the entire history.  Thus, by choosing a dilated CNN as the decoder, we are able to conduct experiments where we vary contextual capacity, finding a sweet spot where the decoder can accurately model text but does not yet overpower the latent representation.  We demonstrate that when this trade-off is correctly managed, textual VAEs can perform substantially better than simple LSTM language models, a finding consistent with recent image modeling experiments using variational lossy autoencoders~\cite{chen2016variational}.	 In contrast with prior work where extremely large CNNs are used, we exploit the dilated CNN for its flexibility in varying the amount of conditioning context. In the two extremes, depending on the choice of dilation, the CNN decoder can reproduce a simple MLP using a bags of words representation of text, or can reproduce the long-range dependence of recurrent architectures (like an LSTM) by conditioning on the entire history.  Thus, by choosing a dilated CNN as the decoder, we are able to conduct experiments where we vary contextual capacity, finding a sweet spot where the decoder can accurately model text but does not yet overpower the latent representation.  We demonstrate that when this trade-off is correctly managed, textual VAEs can perform substantially better than simple LSTM language models, a finding consistent with recent image modeling experiments using variational lossy autoencoders~\cite{chen2016variational}.	score:371
Why is CNN and The New York Times fueling riots by pushing more negative rhetoric against Trump ?	   Specifically, we present two semi-supervised deep convolutional neural network methods, the convolutional encoder-decoder (CNN-Encoder-Decoder) and the convolutional ladder network (CNN-Ladder). The contributions of our work are the following.                   \begin{itemize} \item To our best knowledge, this is the first paper to leverage unlabeled data in CNNs in HAR applications.	   Specifically, we present two semi-supervised deep convolutional neural network methods, the convolutional encoder-decoder (CNN-Encoder-Decoder) and the convolutional ladder network (CNN-Ladder). The contributions of our work are the following.                   \begin{itemize} \item To our best knowledge, this is the first paper to leverage unlabeled data in CNNs in HAR applications.	score:373
Why is CNN and The New York Times fueling riots by pushing more negative rhetoric against Trump ?	 The proposed NDDR layer combines existing CNN components in a novel way, which possesses clear mathematical interpretability as discriminative dimensionality reduction. Moreover, the use of the existing CNN components is desirable to guarantee the extensibility of our method to various state-of-the-art CNN architectures, where the proposed NDDR layer can be used in a ``plug-and-play'' manner.  The rest of this paper is organized as follows. First, we describe the NDDR layer and propose a novel NDDR-CNN as well as its variant NDDR-CNN-Shortcut for MTL in Sect. \ref{Sect:methods}. After that, we discuss the related works in Sect. \ref{Sect:related_works}, where we show that our method can generalize several state-of-the-art methods, which can be treated as our special cases.	 Then, we show that the discriminative dimensionality reduction can be fulfilled by 1 $\times$ 1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use of existing CNN components ensures the end-to-end training and the extensibility of the proposed NDDR layer to various state-of-the-art CNN architectures in a ``plug-and-play'' manner.  The detailed ablation analysis shows that the proposed NDDR layer is easy to train and also robust to different hyperparameters. Experiments on different task sets with various base network architectures demonstrate the promising performance and desirable generalizability of our proposed method. The code of our paper is available at \url{https://github.	score:378
Why is CNN and The New York Times fueling riots by pushing more negative rhetoric against Trump ?	 However, the periodic patterns  spanning multiple time steps  are difficult for the typical attention mechanism to identify, as it usually focuses only on a few time steps.  In temporal pattern attention, we introduce a convolutional neural network (CNN)~\cite{CNN_0,CNN_1} to extract temporal pattern information from each individual variable.  The main contributions of this paper are summarized as follows: \begin{itemize} \item We introduce a new attention concept in which we select the relevant variables as opposed to the relevant time steps.	 However, the periodic patterns  spanning multiple time steps  are difficult for the typical attention mechanism to identify, as it usually focuses only on a few time steps.  In temporal pattern attention, we introduce a convolutional neural network (CNN)~\cite{CNN_0,CNN_1} to extract temporal pattern information from each individual variable.  The main contributions of this paper are summarized as follows: \begin{itemize} \item We introduce a new attention concept in which we select the relevant variables as opposed to the relevant time steps.	score:381
Why is CNN and The New York Times fueling riots by pushing more negative rhetoric against Trump ?	 Then, we show that the discriminative dimensionality reduction can be fulfilled by 1 $\times$ 1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use of existing CNN components ensures the end-to-end training and the extensibility of the proposed NDDR layer to various state-of-the-art CNN architectures in a ``plug-and-play'' manner.  The detailed ablation analysis shows that the proposed NDDR layer is easy to train and also robust to different hyperparameters. Experiments on different task sets with various base network architectures demonstrate the promising performance and desirable generalizability of our proposed method. The code of our paper is available at \url{https://github.	 Then, we show that the discriminative dimensionality reduction can be fulfilled by 1 $\times$ 1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use of existing CNN components ensures the end-to-end training and the extensibility of the proposed NDDR layer to various state-of-the-art CNN architectures in a ``plug-and-play'' manner.  The detailed ablation analysis shows that the proposed NDDR layer is easy to train and also robust to different hyperparameters. Experiments on different task sets with various base network architectures demonstrate the promising performance and desirable generalizability of our proposed method. The code of our paper is available at \url{https://github.	score:383
What are the real life applications of series of numbers and their convergence?	 What is worse is that, the number of censored samples is usually greater than the number of samples that have observed outcomes (\revise{in this paper we name them as ``complete samples''}). Thus, a learning formulation that can fuse both types of samples is needed. Another data challenge is, as a well-known phenomenon in many healthcare applications, we encounter many missing values that result in a challenge for the learning formulation.  We therefore propose a matrix completion approach to overcome this challenge and derive a convex optimization formulation to solve it. Further, with the spatial-temporal matrix representation for the dynamic mHealth data as the input of the prediction model, we investigate the use of a bilinear formalism to reduce the dimensionality of the prediction model.	 What is worse is that, the number of censored samples is usually greater than the number of samples that have observed outcomes (\revise{in this paper we name them as ``complete samples''}). Thus, a learning formulation that can fuse both types of samples is needed. Another data challenge is, as a well-known phenomenon in many healthcare applications, we encounter many missing values that result in a challenge for the learning formulation.  We therefore propose a matrix completion approach to overcome this challenge and derive a convex optimization formulation to solve it. Further, with the spatial-temporal matrix representation for the dynamic mHealth data as the input of the prediction model, we investigate the use of a bilinear formalism to reduce the dimensionality of the prediction model.	score:401
What are the real life applications of series of numbers and their convergence?	   Moreover, there is no real-life golden standard for clustering analysis, since various experts may have different points of views about the same data and express different constraints on the number and size of clusters. Thanks to a visual index, different solutions can be presented with respect to the data. Thus, experts can make a trade-off between their opinion and the best local solutions proposed by the visual index.   Hence, in this paper, we first review existing quality indices that are well-suited to fuzzy clustering, such as~\cite{bezdek1973cluster,chen2001rule,calinski1974dendrite,fukuyama1989new,xie1991validity,zhang2014novel}. Then, we propose an innovative, visual quality index for the well-known Fuzzy C-Means (FCM) method. Moreover, we compare our proposal with state-of-the-art quality indices from the literature on several numerical real-world and artificial datasets.	   Moreover, there is no real-life golden standard for clustering analysis, since various experts may have different points of views about the same data and express different constraints on the number and size of clusters. Thanks to a visual index, different solutions can be presented with respect to the data. Thus, experts can make a trade-off between their opinion and the best local solutions proposed by the visual index.   Hence, in this paper, we first review existing quality indices that are well-suited to fuzzy clustering, such as~\cite{bezdek1973cluster,chen2001rule,calinski1974dendrite,fukuyama1989new,xie1991validity,zhang2014novel}. Then, we propose an innovative, visual quality index for the well-known Fuzzy C-Means (FCM) method. Moreover, we compare our proposal with state-of-the-art quality indices from the literature on several numerical real-world and artificial datasets.	score:404
What are the real life applications of series of numbers and their convergence?	 In particular, we show that various types of VKDEs can be considered as solutions to a class of regularization problems studied in this paper. Experiments on Sloan Digital Sky Survey dataset and High Energy Particle Physics dataset demonstrate the benefits of the proposed framework in real-world applications. 	Anomaly detection is one of the most important tools in all data-driven scientific disciplines.  Data that do not conform to the expected behaviors often bear some interesting characteristics and can help domain experts better understand the problem at hand. However, in the era of data explosion, the anomaly may appear not only in the data themselves, but also as a result of their interactions. The main objective of this paper is to investigate the latter type of anomalies.	 In particular, we show that various types of VKDEs can be considered as solutions to a class of regularization problems studied in this paper. Experiments on Sloan Digital Sky Survey dataset and High Energy Particle Physics dataset demonstrate the benefits of the proposed framework in real-world applications. 	Anomaly detection is one of the most important tools in all data-driven scientific disciplines.  Data that do not conform to the expected behaviors often bear some interesting characteristics and can help domain experts better understand the problem at hand. However, in the era of data explosion, the anomaly may appear not only in the data themselves, but also as a result of their interactions. The main objective of this paper is to investigate the latter type of anomalies.	score:410
What are the real life applications of series of numbers and their convergence?	 Wang et al. \cite{Wang2013199} proposed the discriminative SC method based on multi-manifolds, by learning discriminative class-conditioned codebooks and sparse codes from both data feature spaces and class labels. Though these methods use the class labels, they require that all the training samples are labeled. However, in some real-world applications, there are only very few training samples labeled, while the remaining training samples are unlabeled.  Learning from such a training set is called semi-supervised learning \cite{Bilenko200481}. Semi-supervised learning, compared to the supervised learning, can explore both the labels of the labeled samples and the distribution of the overall data set containing labeled and unlabeled samples. When there are few labeled samples, they are not sufficient to learn an effective classifier using a supervised learning algorithm.	 Wang et al. \cite{Wang2013199} proposed the discriminative SC method based on multi-manifolds, by learning discriminative class-conditioned codebooks and sparse codes from both data feature spaces and class labels. Though these methods use the class labels, they require that all the training samples are labeled. However, in some real-world applications, there are only very few training samples labeled, while the remaining training samples are unlabeled.  Learning from such a training set is called semi-supervised learning \cite{Bilenko200481}. Semi-supervised learning, compared to the supervised learning, can explore both the labels of the labeled samples and the distribution of the overall data set containing labeled and unlabeled samples. When there are few labeled samples, they are not sufficient to learn an effective classifier using a supervised learning algorithm.	score:410
What are the real life applications of series of numbers and their convergence?	 Traditionally, data representations are hand-crafted, with prior knowledge or hypotheses of the human designers. Then the classifiers with the designed representations (or features) are trained by fitting the labeled data, expected to give a good class prediction on test data inputs.  However, the increasing size of data in real world and the variety of learning tasks bring challenges to this traditional paradigm.  Practically, labeled data is rare, but unlabeled data is always abundant. Although there are some less expensive ways to obtain labels, automatically learning representations from data would be more efficient. Furthermore, it has been proved that in some fields automatic representation learning can work better, even if human feature engineering is still powerful.	 Traditionally, data representations are hand-crafted, with prior knowledge or hypotheses of the human designers. Then the classifiers with the designed representations (or features) are trained by fitting the labeled data, expected to give a good class prediction on test data inputs.  However, the increasing size of data in real world and the variety of learning tasks bring challenges to this traditional paradigm.  Practically, labeled data is rare, but unlabeled data is always abundant. Although there are some less expensive ways to obtain labels, automatically learning representations from data would be more efficient. Furthermore, it has been proved that in some fields automatic representation learning can work better, even if human feature engineering is still powerful.	score:415
There is a way to batch replace multiple footage in after effects?	 To better understand the dynamics of point processes, there is an urgent need for joint models of the two processes, which are largely inexistent to date. There are related efforts in linking the time series and event sequence to each other. In fact, one popular way to convert a time series to an event sequence is by detecting multiple events (\emph{e. g.}, based on thresholding the stock price series~\cite{bacry2015hawkes}) from the series data. On the other hand, statistical aggregation (\emph{e.g.}, total number of counts) is often performed on each time interval with equal length to extract aligned time series data from the event sequences. However such a coarse treatment can lead to the key information loss about the actual behavior of the process, or at least in a too early stage.	 To better understand the dynamics of point processes, there is an urgent need for joint models of the two processes, which are largely inexistent to date. There are related efforts in linking the time series and event sequence to each other. In fact, one popular way to convert a time series to an event sequence is by detecting multiple events (\emph{e. g.}, based on thresholding the stock price series~\cite{bacry2015hawkes}) from the series data. On the other hand, statistical aggregation (\emph{e.g.}, total number of counts) is often performed on each time interval with equal length to extract aligned time series data from the event sequences. However such a coarse treatment can lead to the key information loss about the actual behavior of the process, or at least in a too early stage.	score:320
There is a way to batch replace multiple footage in after effects?	 In its simplest form, the traffic flow prediction problem is restricted to predicting a single time-step into the future. Multi-step traffic flow prediction extends this set-up to the case where predicting multiple time-steps into the future based on some finite history is of interest. This problem is significantly more difficult than its single-step variant and is known to suffer from degradation in predictions as the time step increases.  In this paper, two approaches to improve multi-step traffic flow prediction performance in recursive and multi-output settings are introduced. In particular, a model that allows recursive prediction approaches to take into account the temporal context in term of time-step index when making predictions is introduced. In addition, a conditional generative adversarial network-based data augmentation method is proposed to improve prediction performance in the multi-output setting.	 In its simplest form, the traffic flow prediction problem is restricted to predicting a single time-step into the future. Multi-step traffic flow prediction extends this set-up to the case where predicting multiple time-steps into the future based on some finite history is of interest. This problem is significantly more difficult than its single-step variant and is known to suffer from degradation in predictions as the time step increases.  In this paper, two approaches to improve multi-step traffic flow prediction performance in recursive and multi-output settings are introduced. In particular, a model that allows recursive prediction approaches to take into account the temporal context in term of time-step index when making predictions is introduced. In addition, a conditional generative adversarial network-based data augmentation method is proposed to improve prediction performance in the multi-output setting.	score:337
There is a way to batch replace multiple footage in after effects?	 In event detection problem, it is urgent to timely and effectively investigate an emerging new event, which is excluded in the early detection systems. In this way, we can immediately integrate new events into the previous detection system by borrowing the knowledge from past events. Therefore, involvement of constantly emerging labels is very significant for the multi-label learning.  (b) Although there are some tricks to adapt classical multi-label algorithms to handle emerging new labels, they could have various disadvantages. More precisely, independently learning for new labels would neglect the knowledge harvested from past labels; integrating new labels and past labels to re-train a new multi-label model requires a huge computation cost, which thus decreases the scalability of the multi-label system, especially when dealing with large scale scenario.	 In event detection problem, it is urgent to timely and effectively investigate an emerging new event, which is excluded in the early detection systems. In this way, we can immediately integrate new events into the previous detection system by borrowing the knowledge from past events. Therefore, involvement of constantly emerging labels is very significant for the multi-label learning.  (b) Although there are some tricks to adapt classical multi-label algorithms to handle emerging new labels, they could have various disadvantages. More precisely, independently learning for new labels would neglect the knowledge harvested from past labels; integrating new labels and past labels to re-train a new multi-label model requires a huge computation cost, which thus decreases the scalability of the multi-label system, especially when dealing with large scale scenario.	score:340
There is a way to batch replace multiple footage in after effects?	 For the problem studied in this paper, the LP does not admit polynomial-time solutions (unless P=NP).   Side information has also been used to refer to context information in the so-called contextual bandits (see \cite{langford2008epoch,chapelle2011empirical,li2010contextual} and references therein). Under this formulation, context information is revealed at each time, which affects the arm reward distributions.  A contextual bandit problem can thus be viewed as multiple simple bandits, one for each context, that are interleaved in time according to the context stream. The complexity of the problem comes from the coupling of these simple bandits by assuming various models on how context affects the arm reward distributions. The problem is fundamentally~different~from~the~one~studied~here.	 A contextual bandit problem can thus be viewed as multiple simple bandits, one for each context, that are interleaved in time according to the context stream. The complexity of the problem comes from the coupling of these simple bandits by assuming various models on how context affects the arm reward distributions. The problem is fundamentally~different~from~the~one~studied~here.	score:342
There is a way to batch replace multiple footage in after effects?	  Among various distributed learning paradigms, a  simple one is to divide the whole data set into multiple blocks, apply  a base learning algorithm to each block, and then average the results from different blocks \citep{rosenblatt2016optimality, zhang2015divide}. This process, though simple, has some advantages. First, it is computational efficient because the second stage can be easily parallelized.  Second, because no mutual communication is required, the data security or confidentiality can be well protected. Last, recent research shows this method is consistent and sometimes reaches optimal learning rate \citep{zhang2015divide, LGZ16}. Thus its asymptotic effectiveness is theoretically guaranteed.  In distributed learning the performance highly depends on the selection of the base algorithm in the second stage.	  Among various distributed learning paradigms, a  simple one is to divide the whole data set into multiple blocks, apply  a base learning algorithm to each block, and then average the results from different blocks \citep{rosenblatt2016optimality, zhang2015divide}. This process, though simple, has some advantages. First, it is computational efficient because the second stage can be easily parallelized.  Second, because no mutual communication is required, the data security or confidentiality can be well protected. Last, recent research shows this method is consistent and sometimes reaches optimal learning rate \citep{zhang2015divide, LGZ16}. Thus its asymptotic effectiveness is theoretically guaranteed.  In distributed learning the performance highly depends on the selection of the base algorithm in the second stage.	score:344
What is a good Deep Learning framework to implement R-CNN based Object Detection & Tracking?	 LightNet is a \textbf{lightweight}, \textbf{versatile}, \textbf{purely Matlab-based} deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).  The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments.  \textbf{Availability}: the source code and data is available at: \url{https://github.com/yechengxi/LightNet}  	Deep neural networks ~\cite{krizhevsky2012imagenet} have given rise to major advancements in many problems of machine intelligence.	 LightNet is a \textbf{lightweight}, \textbf{versatile}, \textbf{purely Matlab-based} deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).  The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments.  \textbf{Availability}: the source code and data is available at: \url{https://github.com/yechengxi/LightNet}  	Deep neural networks ~\cite{krizhevsky2012imagenet} have given rise to major advancements in many problems of machine intelligence.	score:267
What is a good Deep Learning framework to implement R-CNN based Object Detection & Tracking?	 We  consider a general framework for reducing the number of trainable model parameters in deep learning networks by decomposing linear operators as a product of sums of simpler linear operators. Recently proposed deep learning architectures such as CNN, KFC, Dilated CNN, etc. are special cases of this framework and we illustrate other types of neural network architectures within this framework.  We show that good accuracy on MNIST and Fashion MNIST can be obtained using a relatively small number of trainable parameters. In addition, since implementation of the convolutional layer is resource-heavy, we consider an approach in the transform domain that obviates the need for convolutional layers.  One of the advantages of this general framework over prior approaches is that the number of trainable parameters is not fixed and can be varied arbitrarily.	 We  consider a general framework for reducing the number of trainable model parameters in deep learning networks by decomposing linear operators as a product of sums of simpler linear operators. Recently proposed deep learning architectures such as CNN, KFC, Dilated CNN, etc. are special cases of this framework and we illustrate other types of neural network architectures within this framework.  We show that good accuracy on MNIST and Fashion MNIST can be obtained using a relatively small number of trainable parameters. In addition, since implementation of the convolutional layer is resource-heavy, we consider an approach in the transform domain that obviates the need for convolutional layers.  One of the advantages of this general framework over prior approaches is that the number of trainable parameters is not fixed and can be varied arbitrarily.	score:271
What is a good Deep Learning framework to implement R-CNN based Object Detection & Tracking?	 We  consider a general framework for reducing the number of trainable model parameters in deep learning networks by decomposing linear operators as a product of sums of simpler linear operators. Recently proposed deep learning architectures such as CNN, KFC, Dilated CNN, etc. are special cases of this framework and we illustrate other types of neural network architectures within this framework.  We show that good accuracy on MNIST and Fashion MNIST can be obtained using a relatively small number of trainable parameters. In addition, since implementation of the convolutional layer is resource-heavy, we consider an approach in the transform domain that obviates the need for convolutional layers.  One of the advantages of this general framework over prior approaches is that the number of trainable parameters is not fixed and can be varied arbitrarily.	 We  consider a general framework for reducing the number of trainable model parameters in deep learning networks by decomposing linear operators as a product of sums of simpler linear operators. Recently proposed deep learning architectures such as CNN, KFC, Dilated CNN, etc. are special cases of this framework and we illustrate other types of neural network architectures within this framework.  We show that good accuracy on MNIST and Fashion MNIST can be obtained using a relatively small number of trainable parameters. In addition, since implementation of the convolutional layer is resource-heavy, we consider an approach in the transform domain that obviates the need for convolutional layers.  One of the advantages of this general framework over prior approaches is that the number of trainable parameters is not fixed and can be varied arbitrarily.	score:271
What is a good Deep Learning framework to implement R-CNN based Object Detection & Tracking?	 A MapReduce-based handwriting character recognizer will be designed in this project to verify the efficiency improvement this mechanism will achieve on training and practical large-scale data. Careful discussion and experiment will be developed to illustrate how deep learning algorithm works to train handwritten digits data, how MapReduce is implemented on deep learning neural network, and why this combination accelerates computation.  Besides performance, the scalability and robustness will be mentioned in this report as well. Our system comes with two demonstration software that visually illustrates our handwritten digit recognition/encoding application.  \footnote{K. Sun, X. Wei, G. Jia, R. Wang, and R. Li are with the team of NerveCloud in the course of EEL-6935: Cloud Computing and Storage, Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, 32611 USA (e-mail: ksun@ufl.	 A MapReduce-based handwriting character recognizer will be designed in this project to verify the efficiency improvement this mechanism will achieve on training and practical large-scale data. Careful discussion and experiment will be developed to illustrate how deep learning algorithm works to train handwritten digits data, how MapReduce is implemented on deep learning neural network, and why this combination accelerates computation.  Besides performance, the scalability and robustness will be mentioned in this report as well. Our system comes with two demonstration software that visually illustrates our handwritten digit recognition/encoding application.  \footnote{K. Sun, X. Wei, G. Jia, R. Wang, and R. Li are with the team of NerveCloud in the course of EEL-6935: Cloud Computing and Storage, Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, 32611 USA (e-mail: ksun@ufl.	score:273
What is a good Deep Learning framework to implement R-CNN based Object Detection & Tracking?	  Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems --- the models (often deep networks or wide networks or both) are compute and memory intensive.  \textit{Low-precision numerics} and model compression using \textit{knowledge distillation} are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the \textit{combination} of these two techniques and show that the performance of low-precision networks can be \textit{significantly} improved by using knowledge distillation techniques.	  Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems --- the models (often deep networks or wide networks or both) are compute and memory intensive.  \textit{Low-precision numerics} and model compression using \textit{knowledge distillation} are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the \textit{combination} of these two techniques and show that the performance of low-precision networks can be \textit{significantly} improved by using knowledge distillation techniques.	score:274
Are CNN or RCNN-based frameworks the best in detecting human in photos?	 Results show that features extracted from CNNs are superior to all handcrafted competitors, and furthermore can be compressed to very short codes with negligible loss in performance. Second, this work demonstrates that CNNs trained on non-document images transfer well to document-related tasks. Third, this paper explores a strategy of embedding human knowledge of document structure into CNN architectures, by guiding an ensemble of CNNs toward learning region-specific features.  Interestingly, results show little to no improvement in classification and retrieval after this augmentation, suggesting that a basic holistic CNN may be learning region-specific features (or perhaps better features) automatically. Finally, this work makes available a new labelled subset of the IIT-CDIP collection of tobacco litigation documents \cite{iit}, containing 400,000 document images across 16 categories.	 Inspired by the success of CNNs in other domains, this paper presents an extensive evaluation of CNNs for document classification and retrieval. In the end, it is determined that features extracted from deep CNNs exceed the performance of all popular alternative features on both classification and retrieval, by a large margin. Experiments are also presented on transfer learning, which demonstrate that CNNs trained on object recognition learn features that are surprisingly effective at describing documents.	score:387
Are CNN or RCNN-based frameworks the best in detecting human in photos?	 This work is on highly accurate and robust yet efficient and lightweight landmark localization using binarized CNNs.         Our work is inspired by very recent results of binarized CNN architectures on image classification \cite{rastegari2016xnor, courbariaux2016binarized}. Contrary to these works, we are the first to study the effect of neural network binarization on fine-grained tasks like landmark localization.  Similarly to \cite{rastegari2016xnor, courbariaux2016binarized}, we find that binarization results in performance drop, however to address this we opted to investigate and propose several architectural innovations which led to the introduction of a completely novel hierarchical, parallel and multi-scale residual block, as opposed to investigating ways to improve the binarization process as proposed in \cite{rastegari2016xnor, courbariaux2016binarized}.	 This work is on highly accurate and robust yet efficient and lightweight landmark localization using binarized CNNs.         Our work is inspired by very recent results of binarized CNN architectures on image classification \cite{rastegari2016xnor, courbariaux2016binarized}. Contrary to these works, we are the first to study the effect of neural network binarization on fine-grained tasks like landmark localization.  Similarly to \cite{rastegari2016xnor, courbariaux2016binarized}, we find that binarization results in performance drop, however to address this we opted to investigate and propose several architectural innovations which led to the introduction of a completely novel hierarchical, parallel and multi-scale residual block, as opposed to investigating ways to improve the binarization process as proposed in \cite{rastegari2016xnor, courbariaux2016binarized}.	score:390
Are CNN or RCNN-based frameworks the best in detecting human in photos?	 Various deep learning models have achieved state-of-the-art results on a number of vision-related benchmarks. In most cases, the preferred architecture is a Convolutional Neural Network (CNN). CNN models have been applied successfully to the tasks of image classification~\cite{ImageNet}, image super-resolution~\cite{DRCN}, and video action recognition~\cite{action-ConvNets}, among many others.   CNNs, however, are designed to work for data that can be represented as grids (\textit{e.g.}, videos, images, or audio clips) and do not generalize to graphs -- which have more irregular structure. Due to this limitation, it cannot be applied directly to many real-world problems whose data come in the form of graphs -- social networks~\cite{deepwalk} or citation networks~\cite{ICA} in social network analysis, for instance.	 Various deep learning models have achieved state-of-the-art results on a number of vision-related benchmarks. In most cases, the preferred architecture is a Convolutional Neural Network (CNN). CNN models have been applied successfully to the tasks of image classification~\cite{ImageNet}, image super-resolution~\cite{DRCN}, and video action recognition~\cite{action-ConvNets}, among many others.   CNNs, however, are designed to work for data that can be represented as grids (\textit{e.g.}, videos, images, or audio clips) and do not generalize to graphs -- which have more irregular structure. Due to this limitation, it cannot be applied directly to many real-world problems whose data come in the form of graphs -- social networks~\cite{deepwalk} or citation networks~\cite{ICA} in social network analysis, for instance.	score:391
Are CNN or RCNN-based frameworks the best in detecting human in photos?	   Recently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing~\cite{ATR,Co-CNN,liang2015semantic}. Nevertheless, as demonstrated in many other problems such as object detection~\cite{liang2015towards} and semantic segmentation~\cite{crfasrnn}, the performance of those CNN-based approaches heavily rely on the availability of annotated images for training.  In order to train a human parsing network with potentially practical value in real-word applications, it is highly desired to have a large-scale dataset composed of representative instances with varied clothing appearances, strong articulation, partial (self-)occlusions, truncation at image borders, diverse viewpoints and background clutters. Although there exist training sets for special scenarios such as fashion pictures~\cite{Yamaguchiparsing13,Dongparsing13,ATR,Co-CNN} and people in constrained situations (e.	   Recently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing~\cite{ATR,Co-CNN,liang2015semantic}. Nevertheless, as demonstrated in many other problems such as object detection~\cite{liang2015towards} and semantic segmentation~\cite{crfasrnn}, the performance of those CNN-based approaches heavily rely on the availability of annotated images for training.  In order to train a human parsing network with potentially practical value in real-word applications, it is highly desired to have a large-scale dataset composed of representative instances with varied clothing appearances, strong articulation, partial (self-)occlusions, truncation at image borders, diverse viewpoints and background clutters. Although there exist training sets for special scenarios such as fashion pictures~\cite{Yamaguchiparsing13,Dongparsing13,ATR,Co-CNN} and people in constrained situations (e.	score:392
Are CNN or RCNN-based frameworks the best in detecting human in photos?	 Interestingness prediction has a number of real-world applications. In particular, since the number of images and videos uploaded to the Internet is growing explosively, people are increasingly relying on image/video  recommendation tools to select which ones to view. Given a query, ranking the retrieved data with relevance to the query based on the predicted interestingness would improve  user satisfaction.  Similarly user stickiness can be increased if a media-sharing website such as YouTube can recommend videos that are both relevant and interesting.  Other applications such as web advertising and video summarisation can also benefit. Subjective visual properties such as the above-mentioned ones are useful on their own. But they can also be used as an intermediate representation for other tasks such as visual recognition, e.	 Interestingness prediction has a number of real-world applications. In particular, since the number of images and videos uploaded to the Internet is growing explosively, people are increasingly relying on image/video  recommendation tools to select which ones to view. Given a query, ranking the retrieved data with relevance to the query based on the predicted interestingness would improve  user satisfaction.  Similarly user stickiness can be increased if a media-sharing website such as YouTube can recommend videos that are both relevant and interesting.  Other applications such as web advertising and video summarisation can also benefit. Subjective visual properties such as the above-mentioned ones are useful on their own. But they can also be used as an intermediate representation for other tasks such as visual recognition, e.	score:392
What are the best machine learning blogs or resources available?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:301
What are the best machine learning blogs or resources available?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:345
What are the best machine learning blogs or resources available?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:356
What are the best machine learning blogs or resources available?	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	 Instead, machine learning techniques may be used to teach the robot the concept of cleanliness, and how to assess it from sensory data.  Reinforcement learning (RL) \citep{Sutton1998} is one popular way to teach agents what to do. Here, a reward is given if the agent does something well (and no reward otherwise), and the agent strives to optimise the total amount of reward it receives over its lifetime.  Depending on context, the reward may either be given manually by a human supervisor, or by an automatic computer program that evaluates the agent's performance based on some data. In the related framework of inverse RL (IRL) \citep{Ng2000}, the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function.	score:359
What are the best machine learning blogs or resources available?	 This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.  We discuss the use of the scikit-learn toolkit as it is a reference machine learning tool and has and a variety of algorithms that is matched by few packages, but also because it is implemented in Python, and thus dovetails nicely in the rich neuroimaging Python ecosystem.  This paper explores a few applications of statistical learning to resolve common neuroimaging needs, detailing the corresponding code, the choice of the methods, and the underlying assumptions.	 This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.  We discuss the use of the scikit-learn toolkit as it is a reference machine learning tool and has and a variety of algorithms that is matched by few packages, but also because it is implemented in Python, and thus dovetails nicely in the rich neuroimaging Python ecosystem.  This paper explores a few applications of statistical learning to resolve common neuroimaging needs, detailing the corresponding code, the choice of the methods, and the underlying assumptions.	score:360
I am going to take admission in FIITJEE Kalu Sarai, 2 Years classroom program. How can I get the best batch?	 So far, I have obtained the following results on some publicly available data from the UCI Machine Learning Repository:  1). There seems to be a strong correlation between the sum of those eigenvalues associated with discarded eigenvectors and shrinkages in pair-wise distances. 2). Neither the sum of those eigenvalues nor pair-wise distances have any strong correlations with classification accuracies.   	In this introductory section, I will review the basics of PCA. A good reference on this topic is in \cite{cast95} Essentially, PCA involves two key aspects: transformation of data to a zero-correlation space, and truncation of the data. Depending on the application, one may choose not to truncate the data after the transformation. To proceed, let us denote the data to be transformed by \textbf{column} vectors $\textbf{x}_i$.	 So far, I have obtained the following results on some publicly available data from the UCI Machine Learning Repository:  1). There seems to be a strong correlation between the sum of those eigenvalues associated with discarded eigenvectors and shrinkages in pair-wise distances. 2). Neither the sum of those eigenvalues nor pair-wise distances have any strong correlations with classification accuracies.   	In this introductory section, I will review the basics of PCA. A good reference on this topic is in \cite{cast95} Essentially, PCA involves two key aspects: transformation of data to a zero-correlation space, and truncation of the data. Depending on the application, one may choose not to truncate the data after the transformation. To proceed, let us denote the data to be transformed by \textbf{column} vectors $\textbf{x}_i$.	score:353
I am going to take admission in FIITJEE Kalu Sarai, 2 Years classroom program. How can I get the best batch?	 So far, I have obtained the following results on some publicly available data from the UCI Machine Learning Repository:  1). There seems to be a strong correlation between the sum of those eigenvalues associated with discarded eigenvectors and shrinkages in pair-wise distances. 2). Neither the sum of those eigenvalues nor pair-wise distances have any strong correlations with classification accuracies.   	In this introductory section, I will review the basics of PCA. A good reference on this topic is in \cite{cast95} Essentially, PCA involves two key aspects: transformation of data to a zero-correlation space, and truncation of the data. Depending on the application, one may choose not to truncate the data after the transformation. To proceed, let us denote the data to be transformed by \textbf{column} vectors $\textbf{x}_i$.	 So far, I have obtained the following results on some publicly available data from the UCI Machine Learning Repository:  1). There seems to be a strong correlation between the sum of those eigenvalues associated with discarded eigenvectors and shrinkages in pair-wise distances. 2). Neither the sum of those eigenvalues nor pair-wise distances have any strong correlations with classification accuracies.   	In this introductory section, I will review the basics of PCA. A good reference on this topic is in \cite{cast95} Essentially, PCA involves two key aspects: transformation of data to a zero-correlation space, and truncation of the data. Depending on the application, one may choose not to truncate the data after the transformation. To proceed, let us denote the data to be transformed by \textbf{column} vectors $\textbf{x}_i$.	score:353
I am going to take admission in FIITJEE Kalu Sarai, 2 Years classroom program. How can I get the best batch?	 Besides, we adopt the $l_{2,1}$-norm based loss function, which is robust to the outliers, to achieve robust performance. Different from \cite{robustPCAMAYI}, our algorithm is inductive and can be directly used to map the unseen data which are outside the training set. We name the proposed algorithm Convex Sparse PCA (CSPCA).  The main contributions of this paper can be summarized as follows:  \begin{enumerate} \item We have theoretically proved the equivalence of the classical PCA and low rank regression.	 Besides, we adopt the $l_{2,1}$-norm based loss function, which is robust to the outliers, to achieve robust performance. Different from \cite{robustPCAMAYI}, our algorithm is inductive and can be directly used to map the unseen data which are outside the training set. We name the proposed algorithm Convex Sparse PCA (CSPCA).  The main contributions of this paper can be summarized as follows:  \begin{enumerate} \item We have theoretically proved the equivalence of the classical PCA and low rank regression.	score:364
I am going to take admission in FIITJEE Kalu Sarai, 2 Years classroom program. How can I get the best batch?	 This paper describes an efficient reduction of the learning problem of ranking to binary classification.   The reduction guarantees an average pairwise misranking regret of at most that of the binary classifier regret, improving a recent result of Balcan et al which only guarantees a factor of $2$. Moreover, our reduction applies to a broader class of ranking loss functions, admits a simpler proof, and the expected running time complexity of our algorithm in terms of number of calls to a classifier or preference function is improved from $\Omega(n^2)$ to $O(n \log n)$.   In addition, when the top $k$ ranked elements only are required ($k \ll n$), as in many applications in information extraction or search engines, the time complexity of our algorithm can be further reduced to $O(k \log k + n)$. Our reduction and algorithm are thus practical for realistic applications where the number of points to rank exceeds several thousands.	 Section~\ref{sec:algorithm} describes a simple and efficient algorithm for reducing ranking to binary classification, proves several bounds guaranteeing the quality of the ranking produced by the algorithm, and shows the running-time complexity of our algorithm to be very efficient. In Section~\ref{sec:discussion} we discuss the relationship of the algorithm and its proof with previous related work in combinatorial optimization.  In Section~\ref{sec:lower} we derive a lower bound of factor $2$ on any deterministic reduction from  binary (preference) classification to ranking, implying that our use of a randomized reduction is essentially necessary for the improved guarantees we provide.	score:376
I am going to take admission in FIITJEE Kalu Sarai, 2 Years classroom program. How can I get the best batch?	   Figure~\ref{fig:overall} depicts our experimental framework. Experiments are conducted with ten binary classifiers submitted to the TREC 2011 Crowdsourcing Track\footnote{\url{https://sites.google.com/site/treccrowd}} investigate the following research questions: 1) Can we reliably estimate classifier performance without expert judgments? 2) How much benefit do crowd judgments provide over blind evaluation?  3) Which methods provide the best score and/or rank correlation for each labeled data condition and classifier metric of interest? 4) How robust is evaluation based on {\em\cas} methods to their labeling errors? 5) How effectively can we evaluate outlier classifiers without any judgments?  Results show high score correlation for three of the four classifier metrics considered.	   Figure~\ref{fig:overall} depicts our experimental framework. Experiments are conducted with ten binary classifiers submitted to the TREC 2011 Crowdsourcing Track\footnote{\url{https://sites.google.com/site/treccrowd}} investigate the following research questions: 1) Can we reliably estimate classifier performance without expert judgments? 2) How much benefit do crowd judgments provide over blind evaluation?  3) Which methods provide the best score and/or rank correlation for each labeled data condition and classifier metric of interest? 4) How robust is evaluation based on {\em\cas} methods to their labeling errors? 5) How effectively can we evaluate outlier classifiers without any judgments?  Results show high score correlation for three of the four classifier metrics considered.	score:379
I want to change my batch timing from 2 pm to 8am/11 am for a GS at Vajiram and Ravi. Whom should I go to?	 This approach provides a robust and efficient estimation as we demonstrate in the experiments.    \begin{figure}[t]  \centering{   \includegraphics[width=0.8\linewidth,height=0.7\textheight,keepaspectratio]{robust_approach2}}   \caption{The robust Bayesian approach in Deep-RoK.}  \label{fig:robust_approach} \end{figure}  Our contributions are: (1) A Bayesian approach for on-line and nonlinear approximation of the value function in RMDPs. We connect the robust Bellman TD error to the EKF updates to achieve robust policies in RMDPs;   (2) We propose two algorithms, RTD-DQN and Deep-RoK, to solve large scale RMDPs; (3) We provide theoretical guarantees for our proposed methods; (4) We demonstrate the performance of our two algorithms on a continuous state domain.	 This approach provides a robust and efficient estimation as we demonstrate in the experiments.    \begin{figure}[t]  \centering{   \includegraphics[width=0.8\linewidth,height=0.7\textheight,keepaspectratio]{robust_approach2}}   \caption{The robust Bayesian approach in Deep-RoK.}  \label{fig:robust_approach} \end{figure}  Our contributions are: (1) A Bayesian approach for on-line and nonlinear approximation of the value function in RMDPs. We connect the robust Bellman TD error to the EKF updates to achieve robust policies in RMDPs;   (2) We propose two algorithms, RTD-DQN and Deep-RoK, to solve large scale RMDPs; (3) We provide theoretical guarantees for our proposed methods; (4) We demonstrate the performance of our two algorithms on a continuous state domain.	score:432
I want to change my batch timing from 2 pm to 8am/11 am for a GS at Vajiram and Ravi. Whom should I go to?	8ex]    AP-EVT$_{pf}$ [Theorem \ref{thm:AP-EVT-pf}] & $\Theta(H(\log(nK) + \log(1/\epsilon)) + (1-\delta)                   \tau)$ & $(1+\delta\eta)\sum_{k=1}^K (\sigma_k^2\Delta_k^{-2} + \Delta_k^{-1})$ \\%[0.8ex] \hline \end{tabular} } } \end{center} \end{table*} The main contribution of this paper can be summarized in the following \begin{itemize} \item To the best of our knowledge, the proposed {\rcc \TEV~as well as its parameter free version \TEVf}~is the first thresholding   bandit algorithm using empirical variance.  The theoretical analysis provides   interesting insights when and why using empirical variance could significantly improve the optimal methods only using the first moment estimate. \item {\rcc The proposed algorithms \APTEV~and \APTEVf~allow the asynchronous\footnote{The synchronous parallel implementation can be considered as a special case of asynchronous parallel implementation.	8ex]    AP-EVT$_{pf}$ [Theorem \ref{thm:AP-EVT-pf}] & $\Theta(H(\log(nK) + \log(1/\epsilon)) + (1-\delta)                   \tau)$ & $(1+\delta\eta)\sum_{k=1}^K (\sigma_k^2\Delta_k^{-2} + \Delta_k^{-1})$ \\%[0.8ex] \hline \end{tabular} } } \end{center} \end{table*} The main contribution of this paper can be summarized in the following \begin{itemize} \item To the best of our knowledge, the proposed {\rcc \TEV~as well as its parameter free version \TEVf}~is the first thresholding   bandit algorithm using empirical variance.  The theoretical analysis provides   interesting insights when and why using empirical variance could significantly improve the optimal methods only using the first moment estimate. \item {\rcc The proposed algorithms \APTEV~and \APTEVf~allow the asynchronous\footnote{The synchronous parallel implementation can be considered as a special case of asynchronous parallel implementation.	score:436
I want to change my batch timing from 2 pm to 8am/11 am for a GS at Vajiram and Ravi. Whom should I go to?	  In this paper we present a novel instance segmentation algorithm that extends a fully convolutional network to learn to label objects separately without prediction of regions of interest.  We trained the new algorithm on a challenging CCTV recording of beef cattle, as well as benchmark MS COCO and Pascal VOC datasets. Extensive experimentation showed that our approach outperforms the state-of-the-art solutions by up to 8\% on our data.   	Recently deep convolutional neural networks have made strong contribution in a number of areas in computer vision. In particular, three areas  have seen progress: object detection, class (semantic) segmentation and instance segmentation (mask representation). Object detection algorithms, such as \cite{girshick2014rich,ren2015faster, girshick2014fast}, generate bounding box proposals for \textit{every} object in the image and  classify these proposals.	  In this paper we present a novel instance segmentation algorithm that extends a fully convolutional network to learn to label objects separately without prediction of regions of interest.  We trained the new algorithm on a challenging CCTV recording of beef cattle, as well as benchmark MS COCO and Pascal VOC datasets. Extensive experimentation showed that our approach outperforms the state-of-the-art solutions by up to 8\% on our data.   	Recently deep convolutional neural networks have made strong contribution in a number of areas in computer vision. In particular, three areas  have seen progress: object detection, class (semantic) segmentation and instance segmentation (mask representation). Object detection algorithms, such as \cite{girshick2014rich,ren2015faster, girshick2014fast}, generate bounding box proposals for \textit{every} object in the image and  classify these proposals.	score:436
I want to change my batch timing from 2 pm to 8am/11 am for a GS at Vajiram and Ravi. Whom should I go to?	    We also prove a lower bound showing that no ETC strategy can improve on this result. Surprisingly it is possible to do even better by using a fully sequential strategy inspired by the UCB algorithm for multi-armed bandits \citep{KR95}.  We design a new strategy for which $R^\pi_\mu(T) \sim \log(T)/(2\Delta)$, which improves on the fixed-design strategy by a factor of $8$ and on  SPRT by a factor of $2$.  Again we prove a lower bound showing that no strategy can improve on this result.   For the case where $\Delta$ is unknown, fixed-design strategies are hopeless because there is no reasonable tuning for the exploration budget $n$.  However, it is possible to design an ETC strategy for unknown gaps. Our approach uses a modified fixed-budget best arm  identification (BAI) algorithm in its exploration phase (see e.	    We also prove a lower bound showing that no ETC strategy can improve on this result. Surprisingly it is possible to do even better by using a fully sequential strategy inspired by the UCB algorithm for multi-armed bandits \citep{KR95}.  We design a new strategy for which $R^\pi_\mu(T) \sim \log(T)/(2\Delta)$, which improves on the fixed-design strategy by a factor of $8$ and on  SPRT by a factor of $2$.  Again we prove a lower bound showing that no strategy can improve on this result.   For the case where $\Delta$ is unknown, fixed-design strategies are hopeless because there is no reasonable tuning for the exploration budget $n$.  However, it is possible to design an ETC strategy for unknown gaps. Our approach uses a modified fixed-budget best arm  identification (BAI) algorithm in its exploration phase (see e.	score:438
I want to change my batch timing from 2 pm to 8am/11 am for a GS at Vajiram and Ravi. Whom should I go to?	 We would like to emphasize that a number of new conceptual and technical ideas are required to overcome the various obstacles arising in the multi-dimensional setting.  \medskip  We start with an intuitive explanation of two key ideas that form the basis of our approach.   \paragraph{Sparsity of the Fourier Transform of PMDs.} Since the Fourier Transform (FT) of a PMD is the product of the FTs of its component CRVs, its magnitude is the product of terms each bounded from above by $1. $ Note that each term in the product is strictly less than $1$ except in a small region, unless the component CRV is trivial (i.e., essentially deterministic). Roughly speaking, to establish the sparsity of the FT of PMDs, we proceed as follows: We bound from above the magnitude of the FT by the FT of a Gaussian with the same covariance matrix as our PMD.	  Our new CLT strengthens the CLT of Valiant and Valiant~\cite{VV10b, ValiantValiant:11} by completely removing the dependence on $n$ in the error bound.  \smallskip  Along the way we prove several new structural results of independent interest about PMDs. These include: (i) a robust moment-matching lemma,  roughly stating that two PMDs that approximately agree on their low-degree parameter moments are close in variation distance;  (ii) near-optimal size proper $\eps$-covers for PMDs in total variation distance (constructive upper bound and nearly-matching lower bound).  In addition to Fourier analysis, we employ a number of analytic tools, including the saddlepoint method from complex analysis, that may find other applications. 	\subsection{Background and Motivation} \label{ssec:motiv}  The Poisson Multinomial Distribution (PMD) is the discrete probability distribution of a sum of mutually independent categorical random variables over the same sample space.	score:439
What data science and machine learning career opportunities are there at Yelp?	 With this understanding, we naturally simplify Ladder by eliminating its redundant computation and explain how to choose the parameter and interpret it. We also prove that the sample complexity is cubic to the desired precision of the leaderboard. 	Machine learning competitions have been a popular platform for young students to practice their knowledge, for scientists to apply their expertise, and for industries to solve their data mining problems.  For instance, the Internet streaming media company Netflix held the Netflix Prize competition in 2006, to find a better program to predict user preferences. Kaggle, an online platform, hosts regularly competitions since 2010.  These competitions are usually prediction problems. The participant is given the independent variable $X$, and then he is required to predict the dependent variable $Y$.	 With this understanding, we naturally simplify Ladder by eliminating its redundant computation and explain how to choose the parameter and interpret it. We also prove that the sample complexity is cubic to the desired precision of the leaderboard. 	Machine learning competitions have been a popular platform for young students to practice their knowledge, for scientists to apply their expertise, and for industries to solve their data mining problems.  For instance, the Internet streaming media company Netflix held the Netflix Prize competition in 2006, to find a better program to predict user preferences. Kaggle, an online platform, hosts regularly competitions since 2010.  These competitions are usually prediction problems. The participant is given the independent variable $X$, and then he is required to predict the dependent variable $Y$.	score:337
What data science and machine learning career opportunities are there at Yelp?	 We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application.  	In both the practice of machine learning and the practice of medicine, a serious challenge is presented by disagreements amongst human labels. Machine learning classification models are typically developed on large datasets consisting of $(x_i, y_i)$ (data instance, label) pairs.  These are collected \cite{RussakovskyECCV10,Welinder2010Crowdsourcing} by assigning each raw instance $x_i$ to multiple human evaluators, yielding several labels $y_i^{(1)}, y_i^{(2)},...y_i^{(n_i)}$. Unsurprisingly, these labels often have disagreements amongst them and must be carefully aggregated to give a single target value.   This label disagreement issue becomes a full-fledged clinical problem in the healthcare domain.	 We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application.  	In both the practice of machine learning and the practice of medicine, a serious challenge is presented by disagreements amongst human labels. Machine learning classification models are typically developed on large datasets consisting of $(x_i, y_i)$ (data instance, label) pairs.  These are collected \cite{RussakovskyECCV10,Welinder2010Crowdsourcing} by assigning each raw instance $x_i$ to multiple human evaluators, yielding several labels $y_i^{(1)}, y_i^{(2)},...y_i^{(n_i)}$. Unsurprisingly, these labels often have disagreements amongst them and must be carefully aggregated to give a single target value.   This label disagreement issue becomes a full-fledged clinical problem in the healthcare domain.	score:342
What data science and machine learning career opportunities are there at Yelp?	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	score:345
What data science and machine learning career opportunities are there at Yelp?	      \tiny   \section{Keywords:} machine learning, statistical learning, neuroimaging, scikit-learn, Python 	Interest in applying statistical machine learning to neuroimaging data analysis is growing. Neuroscientists use it as a powerful, albeit complex, tool for statistical inference. The tools are developed by computer scientists who may lack a deep understanding of the neuroscience questions.  This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.  We discuss the use of the scikit-learn toolkit as it is a reference machine learning tool and has and a variety of algorithms that is matched by few packages, but also because it is implemented in Python, and thus dovetails nicely in the rich neuroimaging Python ecosystem.  This paper explores a few applications of statistical learning to resolve common neuroimaging needs, detailing the corresponding code, the choice of the methods, and the underlying assumptions.	      \tiny   \section{Keywords:} machine learning, statistical learning, neuroimaging, scikit-learn, Python 	Interest in applying statistical machine learning to neuroimaging data analysis is growing. Neuroscientists use it as a powerful, albeit complex, tool for statistical inference. The tools are developed by computer scientists who may lack a deep understanding of the neuroscience questions.  This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.  We discuss the use of the scikit-learn toolkit as it is a reference machine learning tool and has and a variety of algorithms that is matched by few packages, but also because it is implemented in Python, and thus dovetails nicely in the rich neuroimaging Python ecosystem.  This paper explores a few applications of statistical learning to resolve common neuroimaging needs, detailing the corresponding code, the choice of the methods, and the underlying assumptions.	score:349
What data science and machine learning career opportunities are there at Yelp?	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	score:352
Should I learn machine learning?	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.    \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	 However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	score:454
Should I learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:455
Should I learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:455
Should I learn machine learning?	 It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, ``learning to learn'' as they do so.  In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal.   Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm.  Our aim is to learn new internal representations as the algorithm learns new target functions,  that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data.	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	score:475
Should I learn machine learning?	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:476
Is machine learning dying?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:418
Is machine learning dying?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:431
Is machine learning dying?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:445
Is machine learning dying?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:445
Is machine learning dying?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:449
What are some applications of machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:411
What are some applications of machine learning?	 Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.   As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?	 Unfortunately, the successful application of these machine learning tools often requires expert knowledge of the tool as well as the target problem, an awareness of all assumptions involved in the analysis, and/or the application of simple exhaustive, brute force search. These requirements can make the application of many machine learning approaches a time-consuming and computationally-demanding endeavor.   As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?	score:432
What are some applications of machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:440
What are some applications of machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:443
What are some applications of machine learning?	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	score:445
What are some applications of machine learning in education?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:431
What are some applications of machine learning in education?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:444
What are some applications of machine learning in education?	 Apart from its fundamental machine learning interest, machine teaching has been applied to domains such as education \cite{rafferty2016faster} and adversarial attacks \cite{mei2015using}. In this paper, we study the machine teaching problem of active sequential machine learners: the learner sequentially chooses queries and the teacher provides responses to them.  Importantly, to steer the learner towards the teaching goal, the teacher needs to appreciate the order of the learner's queries and the effect of the responses on it. Current techniques in machine teaching do not address such interaction. Furthermore, by viewing users as boundedly optimal teachers, and solving the (inverse machine teaching) problem of how to learn from the teacher's responses, our approach provides a way to formulate models of strategically planning users in interactive AI systems.	 Apart from its fundamental machine learning interest, machine teaching has been applied to domains such as education \cite{rafferty2016faster} and adversarial attacks \cite{mei2015using}. In this paper, we study the machine teaching problem of active sequential machine learners: the learner sequentially chooses queries and the teacher provides responses to them.  Importantly, to steer the learner towards the teaching goal, the teacher needs to appreciate the order of the learner's queries and the effect of the responses on it. Current techniques in machine teaching do not address such interaction. Furthermore, by viewing users as boundedly optimal teachers, and solving the (inverse machine teaching) problem of how to learn from the teacher's responses, our approach provides a way to formulate models of strategically planning users in interactive AI systems.	score:452
What are some applications of machine learning in education?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:457
What are some applications of machine learning in education?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:462
What are Kernels in Machine Learning and SVM?	 In this paper, we analyze the Wisconsin Diagnostic Breast Cancer Data using Machine Learning classification techniques, such as the SVM, Bayesian Logistic Regression (Variational Approximation), and K-Nearest-Neighbors. We describe each model, and compare their performance through different measures. We conclude that SVM has the best performance among all other classifiers, while it competes closely with the Bayesian Logistic Regression that is ranked second best method for this dataset.  	Breast Cancer is the most common cancer among women, all around the world. There are two types of breast mass, \textbf{"Benign"} and \textbf{"Malignant"} , where benign is the non-cancerous type and malignant is the cancerous one. According to the World Health Organization, about 502,000 deaths (per year) worldwide are caused by breast cancer. The most effective way to reduce breast cancer deaths is to detect it earlier.  Mammography and a Fine Needle Aspiration biopsy (FNA) are the most common diagnostic tools, which may be time consuming and cost-effective. Thus the automatic diagnosis of breast cancer becomes important, while attracting many researchers from all over the world to find an accurate and reliable diagnosis (classification) method. In Machine Learning content, there are several different methods for classification, such as SVM (Support Vector Machines) which is one of the most popular ones, Bayesian Logistic Regression (which we will use Variational Approximation here), and K-Nearest-Neighbors, etc.	 Mammography and a Fine Needle Aspiration biopsy (FNA) are the most common diagnostic tools, which may be time consuming and cost-effective. Thus the automatic diagnosis of breast cancer becomes important, while attracting many researchers from all over the world to find an accurate and reliable diagnosis (classification) method. In Machine Learning content, there are several different methods for classification, such as SVM (Support Vector Machines) which is one of the most popular ones, Bayesian Logistic Regression (which we will use Variational Approximation here), and K-Nearest-Neighbors, etc.	score:443
What are Kernels in Machine Learning and SVM?	 The kernel support vector machine (SVM) is one of the most widely used  classification methods; however, the amount of computation required  becomes the bottleneck when facing millions of samples.  In this paper, we propose and analyze a novel divide-and-conquer  solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently.   We show theoretically that the support vectors identified by the subproblem  solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering.          In the conquer step, the local solutions from the subproblems are used to initialize a  global coordinate descent solver, which converges quickly as suggested by our analysis.	 The kernel support vector machine (SVM) is one of the most widely used  classification methods; however, the amount of computation required  becomes the bottleneck when facing millions of samples.  In this paper, we propose and analyze a novel divide-and-conquer  solver for kernel SVMs (DC-SVM). In the division step, we partition the kernel SVM problem into smaller subproblems by clustering the data, so that each subproblem can be solved independently and efficiently.   We show theoretically that the support vectors identified by the subproblem  solution are likely to be support vectors of the entire kernel SVM problem, provided that the problem is partitioned appropriately by kernel clustering.          In the conquer step, the local solutions from the subproblems are used to initialize a  global coordinate descent solver, which converges quickly as suggested by our analysis.	score:445
What are Kernels in Machine Learning and SVM?	  Many learning algorithms based on kernel methods and RKHS regularization~\cite{SCH02}, including support vector machines (SVM) and regularized least squares (RLS), have been used with considerable success in a wide range of supervised learning tasks, such as regression and classification. However, these algorithms are,  for the most part, restricted to a RKHS regularization term with an exponent equal to two.  While the regularization hyperparameter has been extensively studied~(see e.g.~\cite{oneto2015support}), the influence of this exponent on the performance of kernel machines remains poorly understood. Studying the effects of varying the exponent of the RKHS regularization on the regularization process and the underlying learning algorithm is the main goal of this research.	  Many learning algorithms based on kernel methods and RKHS regularization~\cite{SCH02}, including support vector machines (SVM) and regularized least squares (RLS), have been used with considerable success in a wide range of supervised learning tasks, such as regression and classification. However, these algorithms are,  for the most part, restricted to a RKHS regularization term with an exponent equal to two.  While the regularization hyperparameter has been extensively studied~(see e.g.~\cite{oneto2015support}), the influence of this exponent on the performance of kernel machines remains poorly understood. Studying the effects of varying the exponent of the RKHS regularization on the regularization process and the underlying learning algorithm is the main goal of this research.	score:446
What are Kernels in Machine Learning and SVM?	 Over the past three decades, many methods based on machine learning and statistical models have been applied to perform this task, such as latent semantic analysis (LSA), support vector machines (SVM), and multinomial naive Bayes (MNB).  The first step in utilizing such methods to categorize textual data is to convert the texts into a vector representation.  One of the most popular text representation models is the bag-of-words model~\cite{salton1975vector}, which represents each document in a collection as a vector in a vector space. Each dimension of the vectors represents a term (e.g., a word, a sequence of words), and its value encodes a weight, which can be how many times the term occurs in the document.	 Over the past three decades, many methods based on machine learning and statistical models have been applied to perform this task, such as latent semantic analysis (LSA), support vector machines (SVM), and multinomial naive Bayes (MNB).  The first step in utilizing such methods to categorize textual data is to convert the texts into a vector representation.  One of the most popular text representation models is the bag-of-words model~\cite{salton1975vector}, which represents each document in a collection as a vector in a vector space. Each dimension of the vectors represents a term (e.g., a word, a sequence of words), and its value encodes a weight, which can be how many times the term occurs in the document.	score:449
What are Kernels in Machine Learning and SVM?	  	Support-vector machines (SVMs) are a class of algorithms that have, in recent years, exhibited superior performance compared to other pattern classification algorithms. There are several formulations of the SVM problem, depending on the specific application of the SVM (e.g., classification, regression, etc.).  One of the difficulties in using SVMs is that building an SVM requires solving a constrained quadratic programming problem, whose size is quadratic in the number of training examples.	 \]  The underlying assumption used is that the kernel matrices are invertible. \subsection{Previous Approaches for Solving Parallel SVMs}         There are several main methods for finding a solution to an SVM problem         on a single-node computer. (See Chapter 10 of~\cite{SS2002}) for a taxonomy         of such methods.) However, since solving an SVM is quadratic in time and         cubic in memory, these methods encounter difficulty when scaling to datasets         that have many examples and support vectors.  The latter two are not synonymous.         A large dataset with many repeated examples might be solved using sub-sampling         approaches, while a highly non-separable dataset with many support vectors will          require an altogether different solution strategy.         The literature covers several attempts at solving SVMs in parallel,         which allow for greater computational power and larger memory size.  In Collobert et al.~\cite{CBB2002}         the SVM solver is parallelized by training multiple SVMs, each on a subset         of the training data, and aggregating the resulting classifiers into a single         classifier. The training data is then redistributed to the classifiers according          their performance and the process is iterated until convergence is reached.           The need to re-divide the data among the SVM classifiers means that the data          must be moved between nodes several times; this rules out the use of an          approach where bandwidth is a concern.         A more low-level approach is taken by Zanghirati et al.~\cite{ZZ2003}, where the quadratic          optimization problem is divided into smaller quadratic programs (similar to the          Active Set methods), each of which is solved on a different node.	score:453
Is using wordnet representing hypernyms a good approach for text classification?	 When modeling the word subspaces, we assume only one occurrence of each word inside the class.   However, as seen in the BOW approach, the frequency of words inside a text is an informative feature that should be considered. In order to introduce this feature in the word subspace modeling and enhance its performance, we further extend the concept of word subspace to the term-frequency (TF) weighted word subspace.    In this extension, we consider a set of weights, which encodes the words frequencies, when performing the PCA. Text classification with TF weighted word subspace can also be performed under the framework of MSM. We show the validity of our modeling through experiments on the  Reuters\footnote{http://www.daviddlewis.com/resources/testcollections/reuters21578} database, an established database for natural language processing tasks.	 Through this concept, it is possible to model text from word vectors while holding semantic information. To incorporate the word frequency directly in the subspace model, we further extend the word subspace to the term-frequency (TF) weighted word subspace. Based on these new concepts, text classification can be performed under the mutual subspace method (MSM) framework.  The validity of our modeling is shown through experiments on the Reuters text database, comparing the results to various state-of-art algorithms.  	Text classification has become an indispensable task due to the rapid growth in the number of texts in digital form available online. It aims to classify different texts, also called documents, into a fixed number of predefined categories, helping to organize data, and making easier for users to find the desired information.	score:247
Is using wordnet representing hypernyms a good approach for text classification?	 Through this concept, it is possible to model text from word vectors while holding semantic information. To incorporate the word frequency directly in the subspace model, we further extend the word subspace to the term-frequency (TF) weighted word subspace. Based on these new concepts, text classification can be performed under the mutual subspace method (MSM) framework.  The validity of our modeling is shown through experiments on the Reuters text database, comparing the results to various state-of-art algorithms.  	Text classification has become an indispensable task due to the rapid growth in the number of texts in digital form available online. It aims to classify different texts, also called documents, into a fixed number of predefined categories, helping to organize data, and making easier for users to find the desired information.	 Through this concept, it is possible to model text from word vectors while holding semantic information. To incorporate the word frequency directly in the subspace model, we further extend the word subspace to the term-frequency (TF) weighted word subspace. Based on these new concepts, text classification can be performed under the mutual subspace method (MSM) framework.  The validity of our modeling is shown through experiments on the Reuters text database, comparing the results to various state-of-art algorithms.  	Text classification has become an indispensable task due to the rapid growth in the number of texts in digital form available online. It aims to classify different texts, also called documents, into a fixed number of predefined categories, helping to organize data, and making easier for users to find the desired information.	score:259
Is using wordnet representing hypernyms a good approach for text classification?	 Task-specific word identification aims to choose the task-related words that best describe a short text. Existing approaches require well-defined seed words or lexical dictionaries (e.g., WordNet), which are often unavailable for many applications such as social discrimination detection and fake review detection. However, we often have a set of labeled short texts where each short text has a task-related class label, e. g., discriminatory or non-discriminatory, specified by users or learned by classification algorithms. In this paper, we focus on identifying task-specific words and phrases from short texts by exploiting their class labels rather than using seed words or lexical dictionaries. We consider the task-specific word and phrase identification as feature learning.	 Task-specific word identification aims to choose the task-related words that best describe a short text. Existing approaches require well-defined seed words or lexical dictionaries (e.g., WordNet), which are often unavailable for many applications such as social discrimination detection and fake review detection. However, we often have a set of labeled short texts where each short text has a task-related class label, e. g., discriminatory or non-discriminatory, specified by users or learned by classification algorithms. In this paper, we focus on identifying task-specific words and phrases from short texts by exploiting their class labels rather than using seed words or lexical dictionaries. We consider the task-specific word and phrase identification as feature learning.	score:262
Is using wordnet representing hypernyms a good approach for text classification?	 Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information.  Based on these document vectors, we introduce a measure of {\it model explanatory power} and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.   	A number of real-world problems related to text data have been studied under the framework of natural language processing (NLP).	 Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information.  Based on these document vectors, we introduce a measure of {\it model explanatory power} and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.   	A number of real-world problems related to text data have been studied under the framework of natural language processing (NLP).	score:262
Is using wordnet representing hypernyms a good approach for text classification?	 In this work we introduce a method for enriching the bag-of-words model by complementing such rare term information with related terms from both general and domain-specific Word Vector models. By reducing sparseness in the bag-of-words models, our enrichment approach achieves improved classification over several baseline classifiers in a variety of text classification problems.   Our approach is also efficient because it requires no change to the linear classifier before or during training, since bag-of-words enrichment applies only to text being classified. 	The bag-of-words model is used as the standard representation of text input for many linear classification models (such as Multinomial Naive Bayes \cite{mccallum1998comparison} and Support Vector Machines \cite{Joachims98}).	 In this work we introduce a method for enriching the bag-of-words model by complementing such rare term information with related terms from both general and domain-specific Word Vector models. By reducing sparseness in the bag-of-words models, our enrichment approach achieves improved classification over several baseline classifiers in a variety of text classification problems.   Our approach is also efficient because it requires no change to the linear classifier before or during training, since bag-of-words enrichment applies only to text being classified. 	The bag-of-words model is used as the standard representation of text input for many linear classification models (such as Multinomial Naive Bayes \cite{mccallum1998comparison} and Support Vector Machines \cite{Joachims98}).	score:268
What are some of the ways one can combine data from images with other diagnostic tests to build a machine learning model that leverages both forms of data?	 Ideally, we would build a sophisticated machine learning model to learn an attractiveness representation for each image by utilizing the pairwise-ranked image pairs. In this way, we leave a relatively simple task to the human judges and let the computer solve the harder problem.   In this paper, we propose an alternative framework to rank image attractiveness using a novel model trained with a large set of side-by-side multi-labeled image pairs.  We collect a large and diverse set of image pairs from a real web index and ask judges to rate which image is more attractive in the pair by choosing one of five labels - ``left better", ``left slightly better", ``equal", ``right slightly better", and ``right better". The ``slightly better" is added according to the feedback from judges who feel this additional label provides comfortable flexibility.	 Ideally, we would build a sophisticated machine learning model to learn an attractiveness representation for each image by utilizing the pairwise-ranked image pairs. In this way, we leave a relatively simple task to the human judges and let the computer solve the harder problem.   In this paper, we propose an alternative framework to rank image attractiveness using a novel model trained with a large set of side-by-side multi-labeled image pairs.  We collect a large and diverse set of image pairs from a real web index and ask judges to rate which image is more attractive in the pair by choosing one of five labels - ``left better", ``left slightly better", ``equal", ``right slightly better", and ``right better". The ``slightly better" is added according to the feedback from judges who feel this additional label provides comfortable flexibility.	score:257
What are some of the ways one can combine data from images with other diagnostic tests to build a machine learning model that leverages both forms of data?	   Humans can learn concepts or recognize items from just a handful of examples, while machines require many more samples to perform the same task. In this paper, we build a computational model to investigate the possibility of this kind of rapid learning. The proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory.   We present a simple and intuitive technique called cognitive discriminative mappings (CDM) to explore the cognitive problem. First, CDM separates and clusters the data instances retrieved from long-term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm.  CDM then maps each sensory data instance to be as close as possible to the median point of the data group with the same class.	   Humans can learn concepts or recognize items from just a handful of examples, while machines require many more samples to perform the same task. In this paper, we build a computational model to investigate the possibility of this kind of rapid learning. The proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory.   We present a simple and intuitive technique called cognitive discriminative mappings (CDM) to explore the cognitive problem. First, CDM separates and clusters the data instances retrieved from long-term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm.  CDM then maps each sensory data instance to be as close as possible to the median point of the data group with the same class.	score:259
What are some of the ways one can combine data from images with other diagnostic tests to build a machine learning model that leverages both forms of data?	 Here, we introduce a simple permutation-equivariant layer for deep learning with set structure, where the primary dataset is a collection of sets, possibly of different sizes.  Note that each instance may have a structure of its own, such as graph, image, or another set.  In typical machine-learning applications, iid assumption implies that the entire data-set itself has a set structure.  Therefore, our special treatment of the set structure is only necessary due to multiplicity of distinct yet homogeneous (data)sets.  Here, we show that a simple parameter-sharing scheme enables a general treatment of sets within supervised and semi-supervised settings.  In the following, after introducing the set layer in \cref{sec:parameter_sharing}, we explore several novel applications.	 Here, we introduce a simple permutation-equivariant layer for deep learning with set structure, where the primary dataset is a collection of sets, possibly of different sizes.  Note that each instance may have a structure of its own, such as graph, image, or another set.  In typical machine-learning applications, iid assumption implies that the entire data-set itself has a set structure.  Therefore, our special treatment of the set structure is only necessary due to multiplicity of distinct yet homogeneous (data)sets.  Here, we show that a simple parameter-sharing scheme enables a general treatment of sets within supervised and semi-supervised settings.  In the following, after introducing the set layer in \cref{sec:parameter_sharing}, we explore several novel applications.	score:265
What are some of the ways one can combine data from images with other diagnostic tests to build a machine learning model that leverages both forms of data?	 The authors demonstrated how an adversary may repurpose a pre-trained ImageNet \cite{imagenet_cvpr09} model for an alternate classification task like classification of MNIST digits or CIFAR-10 images without modifying the network parameters. Since machine learning agents can be reprogrammed to perform unwanted actions as desired by the adversary, such an attack can lead to theft of computational resources such as cloud-hosted machine learning models.  Besides theft of computational resources, the adversary may perform a task that violates the code of ethics of the system provider.    The adversarial reprogramming approach proposed by \cite{reprogramming} trains an additive contribution $\theta$ to the inputs of the neural network to repurpose it for the desired alternate task.   This approach assumes a white-box attack scenario where the adversary has full access to the network's parameters.	 The authors demonstrated how an adversary may repurpose a pre-trained ImageNet \cite{imagenet_cvpr09} model for an alternate classification task like classification of MNIST digits or CIFAR-10 images without modifying the network parameters. Since machine learning agents can be reprogrammed to perform unwanted actions as desired by the adversary, such an attack can lead to theft of computational resources such as cloud-hosted machine learning models.  Besides theft of computational resources, the adversary may perform a task that violates the code of ethics of the system provider.    The adversarial reprogramming approach proposed by \cite{reprogramming} trains an additive contribution $\theta$ to the inputs of the neural network to repurpose it for the desired alternate task.   This approach assumes a white-box attack scenario where the adversary has full access to the network's parameters.	score:266
What are some of the ways one can combine data from images with other diagnostic tests to build a machine learning model that leverages both forms of data?	   There has been growing interest in developing accurate models that can also   be explained to humans.  Unfortunately, if there exist multiple distinct but   accurate models for some dataset, current machine learning methods are   unlikely to find them: standard techniques will likely recover a complex   model that combines them.  In this work, we introduce a way to identify a   maximal set of distinct but accurate models for a dataset.   We demonstrate   empirically that, in situations where the data supports multiple accurate   classifiers, we tend to recover simpler, more interpretable classifiers   rather than more complex ones. 	When building machine learning systems for high-risk situations with incomplete information (e.g. healthcare), explanation is an important safeguard against non-causal or otherwise nonsensical predictions \citep{caruana2015intelligible}.	   There has been growing interest in developing accurate models that can also   be explained to humans.  Unfortunately, if there exist multiple distinct but   accurate models for some dataset, current machine learning methods are   unlikely to find them: standard techniques will likely recover a complex   model that combines them.  In this work, we introduce a way to identify a   maximal set of distinct but accurate models for a dataset.   We demonstrate   empirically that, in situations where the data supports multiple accurate   classifiers, we tend to recover simpler, more interpretable classifiers   rather than more complex ones. 	When building machine learning systems for high-risk situations with incomplete information (e.g. healthcare), explanation is an important safeguard against non-causal or otherwise nonsensical predictions \citep{caruana2015intelligible}.	score:266
How should we format multi row data for machine learning model?	 Multi-context model learning is crucial for marine robotics where several factors can cause disturbances to the system's dynamics. This work addresses the problem of identifying multiple contexts of an AUV model. We build a simulation model of the robot from experimental data, and use it to fill in the missing data and generate different model contexts.  We implement an architecture based on long-short-term-memory (LSTM) networks to learn the different contexts directly from the data. We show that the LSTM network can achieve high classification accuracy compared to baseline methods, showing robustness against noise and scaling efficiently on large datasets. 	A robotic model is an essential tool for control, action and path planning, and several other  applications.	 Multi-context model learning is crucial for marine robotics where several factors can cause disturbances to the system's dynamics. This work addresses the problem of identifying multiple contexts of an AUV model. We build a simulation model of the robot from experimental data, and use it to fill in the missing data and generate different model contexts.  We implement an architecture based on long-short-term-memory (LSTM) networks to learn the different contexts directly from the data. We show that the LSTM network can achieve high classification accuracy compared to baseline methods, showing robustness against noise and scaling efficiently on large datasets. 	A robotic model is an essential tool for control, action and path planning, and several other  applications.	score:274
How should we format multi row data for machine learning model?	    Here we propose a novel model family with the objective of learning to   disentangle the factors of variation in data. Our approach is based on   the spike-and-slab restricted Boltzmann machine which we generalize to   include higher-order interactions among multiple latent variables. Seen   from a generative perspective, the multiplicative interactions emulates   the entangling of factors of variation.  Inference in the model can be   seen as disentangling these generative factors. Unlike previous attempts   at disentangling latent factors, the proposed model is trained using no   supervised information regarding the latent factors.  We apply our model   to the task of facial expression classification. 	In many machine learning tasks, data originates from a generative process involving complex interaction of multiple factors.  Alone each factor accounts for a source of variability in the data. Together their interaction gives rise to the rich structure characteristic of many of the most challenging domains of application.  Consider, for example, the task of facial expression recognition. Two images of different individuals with the same facial expression may result in images that are well separated in pixel space.	    Here we propose a novel model family with the objective of learning to   disentangle the factors of variation in data. Our approach is based on   the spike-and-slab restricted Boltzmann machine which we generalize to   include higher-order interactions among multiple latent variables. Seen   from a generative perspective, the multiplicative interactions emulates   the entangling of factors of variation.  Inference in the model can be   seen as disentangling these generative factors. Unlike previous attempts   at disentangling latent factors, the proposed model is trained using no   supervised information regarding the latent factors.  We apply our model   to the task of facial expression classification. 	In many machine learning tasks, data originates from a generative process involving complex interaction of multiple factors.  Alone each factor accounts for a source of variability in the data. Together their interaction gives rise to the rich structure characteristic of many of the most challenging domains of application.  Consider, for example, the task of facial expression recognition. Two images of different individuals with the same facial expression may result in images that are well separated in pixel space.	score:279
How should we format multi row data for machine learning model?	   There has been growing interest in developing accurate models that can also   be explained to humans.  Unfortunately, if there exist multiple distinct but   accurate models for some dataset, current machine learning methods are   unlikely to find them: standard techniques will likely recover a complex   model that combines them.  In this work, we introduce a way to identify a   maximal set of distinct but accurate models for a dataset.   We demonstrate   empirically that, in situations where the data supports multiple accurate   classifiers, we tend to recover simpler, more interpretable classifiers   rather than more complex ones. 	When building machine learning systems for high-risk situations with incomplete information (e.g. healthcare), explanation is an important safeguard against non-causal or otherwise nonsensical predictions \citep{caruana2015intelligible}.	   In this work, we focus on interpretability specifically in contexts where the data supports multiple functions of equal predictive accuracy for classification.   In such situations, we demonstrate empirically that standard machine learning techniques tend to recover combinations and conflations of the multiple functions, regardless of the choice of model class.  While the individual functions each may be more or less interpretable, one can certainly argue that combinations of functions---however complex they are individually---are likely to be harder for humans to understand than just one of the functions alone.  Providing these multiple options can help human experts choose one that is likely to generalize best.	score:279
How should we format multi row data for machine learning model?	 By examining properties of this structure we can determine the success of the learning approach.  Our proposed work makes several contributions. We create a reconstruction error based framework that can model and create classifiers for complex data sets that: contain many missing labels, and are multi-view and multi-task/label. Multi-view learning involves multiple feature sets for the same set of objects.  Previous work \cite{cotrain,Yarowsky95} shows that simply placing all the feature sets into one single view is suboptimal and raises difficult engineering issues if the views are fundamentally different (i.e. binary indicators in one view and real values in the other, or dimension and normalization issues).   Our reconstruction error framework makes the following contributions to the field:  \begin{itemize} \item Simultaneously perform dimension reduction, multi-task/label propagation in a multi-view and semi-supervised setting.	 By examining properties of this structure we can determine the success of the learning approach.  Our proposed work makes several contributions. We create a reconstruction error based framework that can model and create classifiers for complex data sets that: contain many missing labels, and are multi-view and multi-task/label. Multi-view learning involves multiple feature sets for the same set of objects.  Previous work \cite{cotrain,Yarowsky95} shows that simply placing all the feature sets into one single view is suboptimal and raises difficult engineering issues if the views are fundamentally different (i.e. binary indicators in one view and real values in the other, or dimension and normalization issues).   Our reconstruction error framework makes the following contributions to the field:  \begin{itemize} \item Simultaneously perform dimension reduction, multi-task/label propagation in a multi-view and semi-supervised setting.	score:284
How should we format multi row data for machine learning model?	 Machine learning models are vulnerable to adversarial examples. An adversary  modifies the input data such that humans still assign the same label, however,   machine learning models misclassify it. Previous approaches in the literature demonstrated that adversarial examples can even be generated for the remotely hosted model.  In this paper, we propose a Siamese network based approach to generate adversarial examples for a multiclass target CNN.  We assume that the adversary do not possess any knowledge of the target data distribution, and we use an unlabeled mismatched dataset to query the target, e.g., for the ResNet-50 target, we use the Food-101 dataset as the query. Initially, the target model assigns labels to the query dataset, and a Siamese network is trained on the image pairs derived from these multiclass labels.	 Machine learning models are vulnerable to adversarial examples. An adversary  modifies the input data such that humans still assign the same label, however,   machine learning models misclassify it. Previous approaches in the literature demonstrated that adversarial examples can even be generated for the remotely hosted model.  In this paper, we propose a Siamese network based approach to generate adversarial examples for a multiclass target CNN.  We assume that the adversary do not possess any knowledge of the target data distribution, and we use an unlabeled mismatched dataset to query the target, e.g., for the ResNet-50 target, we use the Food-101 dataset as the query. Initially, the target model assigns labels to the query dataset, and a Siamese network is trained on the image pairs derived from these multiclass labels.	score:286
What are your recommendations for self-studying machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:370
What are your recommendations for self-studying machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:396
What are your recommendations for self-studying machine learning?	 However, as machine learning systems are deployed in increasingly large-scale, autonomous, open-domain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.   There has been a great deal of public discussion around accidents.  To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents \cite{bostrom2014superintelligence}.  However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics \cite{davis2015ethical,lawrence2016discussion}.	 However, as machine learning systems are deployed in increasingly large-scale, autonomous, open-domain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.   There has been a great deal of public discussion around accidents.  To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents \cite{bostrom2014superintelligence}.  However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics \cite{davis2015ethical,lawrence2016discussion}.	score:398
What are your recommendations for self-studying machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:412
What are your recommendations for self-studying machine learning?	  One usually needs to try many times and adjust the learning rate manually to accumulate knowledge about the problem. However, human involvement often needs domain knowledge about the target problems, which is inefficient and difficult to scale up to different problems. Thus, a natural question arises: can we learn to adjust the learning rate? This is exactly the focus of this work and we aim to learn learning rates for SGD based machine learning (ML) algorithms without human-designed rules or hand-crafted features.    By examining the current practice of learning rate control/adjustment, we have two observations. First, learning rate control is a sequential decision process: We set an initial learning rate at the beginning, and then at each step we decide whether to change the learning rate and how to change it, based on the current model and loss, training data at hand, and maybe history of the training process.	  One usually needs to try many times and adjust the learning rate manually to accumulate knowledge about the problem. However, human involvement often needs domain knowledge about the target problems, which is inefficient and difficult to scale up to different problems. Thus, a natural question arises: can we learn to adjust the learning rate? This is exactly the focus of this work and we aim to learn learning rates for SGD based machine learning (ML) algorithms without human-designed rules or hand-crafted features.    By examining the current practice of learning rate control/adjustment, we have two observations. First, learning rate control is a sequential decision process: We set an initial learning rate at the beginning, and then at each step we decide whether to change the learning rate and how to change it, based on the current model and loss, training data at hand, and maybe history of the training process.	score:416
What are your views on PM Modi's new interview to CNN Network 18 News?	 In this paper, we lift this assumption and present two semi-supervised methods based on convolutional neural networks (CNNs) to learn discriminative hidden features. Our semi-supervised CNNs learn from both labeled and unlabeled data while also performing feature learning on raw sensor data. In experiments on three real world datasets, we show that our CNNs outperform supervised methods and traditional semi-supervised learning methods by up to $18\%$ in mean F1-score ($F_m$).  	Human activity recognition (HAR) is an important application area for mobile, on-body, and worn mobile technologies. Supervised learning for human activity recognition has shown great promise. Among supervised methods, deep neural networks (DNNs) have emerged as a method with much potential, in that they are less dependent on clever feature engineering and has strong generalization ability~\cite{zhang2016understanding} compared to other supervised methods~\cite{lane2015can, ordonez2016deep}.	 In this paper, we lift this assumption and present two semi-supervised methods based on convolutional neural networks (CNNs) to learn discriminative hidden features. Our semi-supervised CNNs learn from both labeled and unlabeled data while also performing feature learning on raw sensor data. In experiments on three real world datasets, we show that our CNNs outperform supervised methods and traditional semi-supervised learning methods by up to $18\%$ in mean F1-score ($F_m$).  	Human activity recognition (HAR) is an important application area for mobile, on-body, and worn mobile technologies. Supervised learning for human activity recognition has shown great promise. Among supervised methods, deep neural networks (DNNs) have emerged as a method with much potential, in that they are less dependent on clever feature engineering and has strong generalization ability~\cite{zhang2016understanding} compared to other supervised methods~\cite{lane2015can, ordonez2016deep}.	score:432
What are your views on PM Modi's new interview to CNN Network 18 News?	15 seconds. The proposed work, to the best of our knowledge, is the first deep learning based method for segmentation of both the lumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the best results without any manual intervention. Code is available at \url{https://github.com/Kulbear/ivus-segmentation-icsm2018}. 	Convolutional Neural Networks (CNNs) play an important role in visual image recognition.  In the past few years, CNNs have achieved promising results in image classification \cite{krizhevsky2012imagenet,simonyan2014very,srivastava2015highway,he2016deep,huang2017densely} and semantic segmentation \cite{ciresan2012deep,long2015fully,chen2016deeplab,rajpurkar2017chexnet,peng2017large}. Fully Convolutional Networks (FCNs) \cite{long2015fully} have become popular and used to solve the problem of making dense predictions at a pixel level.	15 seconds. The proposed work, to the best of our knowledge, is the first deep learning based method for segmentation of both the lumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the best results without any manual intervention. Code is available at \url{https://github.com/Kulbear/ivus-segmentation-icsm2018}. 	Convolutional Neural Networks (CNNs) play an important role in visual image recognition.  In the past few years, CNNs have achieved promising results in image classification \cite{krizhevsky2012imagenet,simonyan2014very,srivastava2015highway,he2016deep,huang2017densely} and semantic segmentation \cite{ciresan2012deep,long2015fully,chen2016deeplab,rajpurkar2017chexnet,peng2017large}. Fully Convolutional Networks (FCNs) \cite{long2015fully} have become popular and used to solve the problem of making dense predictions at a pixel level.	score:436
What are your views on PM Modi's new interview to CNN Network 18 News?	 Our $2\times$ speedup ResNet-50 only suffers $0.8\%$ loss of top-5 accuracy on ImageNet. We further show the effectiveness of SPP on transfer learning tasks. 	Convolutional Neural Network~(CNN) has obtained better performance in classification, detection and segmentation tasks than traditional methods in computer vision. However, CNN leads to massive computation and storage consumption, thus hindering its deployment on mobile and embedded devices.               \begin{figure}    \begin{minipage}{0.5\linewidth}       \centering       \includegraphics[width=1.0\textwidth]{SPP.pdf}       \caption{\textbf{left}: The main idea of probabilistic pruning. We assign different pruning probabilities $p$ to different neurons/weights based on some criterion. The dashed circle/line means the neuron/weight has not been totally pruned ($0<p<1$); while the blank circle means the neuron has been eliminated ($p=0$), and thus corresponding connections removed.	 Our $2\times$ speedup ResNet-50 only suffers $0.8\%$ loss of top-5 accuracy on ImageNet. We further show the effectiveness of SPP on transfer learning tasks. 	Convolutional Neural Network~(CNN) has obtained better performance in classification, detection and segmentation tasks than traditional methods in computer vision. However, CNN leads to massive computation and storage consumption, thus hindering its deployment on mobile and embedded devices.               \begin{figure}    \begin{minipage}{0.5\linewidth}       \centering       \includegraphics[width=1.0\textwidth]{SPP.pdf}       \caption{\textbf{left}: The main idea of probabilistic pruning. We assign different pruning probabilities $p$ to different neurons/weights based on some criterion. The dashed circle/line means the neuron/weight has not been totally pruned ($0<p<1$); while the blank circle means the neuron has been eliminated ($p=0$), and thus corresponding connections removed.	score:446
What are your views on PM Modi's new interview to CNN Network 18 News?	 In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss function of the original VQ-VAE \cite{oord:17:nips} can be derived from the variational deterministic information bottleneck (VDIB) principle \cite{Strouse:18}. On the other hand, the VQ-VAE trained by the Expectation Maximization (EM) algorithm \cite{roy:18} can be viewed as an approximation to the variational information bottleneck(VIB) principle \cite{alemi:17:iclr}.  	The recent advances of variational autoencoder(VAE) provide new unsupervised approaches to learn hidden structure of the data \cite{kingma:14:iclr}. The variational autoencoder is a powerful generative model which allows inference of the learned latent representation. However, the classic VAEs are prone to the \textquotedblleft posterior collapse \textquotedblright phenomenon that the latent representations are ignored due to the powerful decoder.	 In this paper, we provide an information-theoretic interpretation of the Vector Quantized-Variational Autoencoder (VQ-VAE). We show that the loss function of the original VQ-VAE \cite{oord:17:nips} can be derived from the variational deterministic information bottleneck (VDIB) principle \cite{Strouse:18}. On the other hand, the VQ-VAE trained by the Expectation Maximization (EM) algorithm \cite{roy:18} can be viewed as an approximation to the variational information bottleneck(VIB) principle \cite{alemi:17:iclr}.  	The recent advances of variational autoencoder(VAE) provide new unsupervised approaches to learn hidden structure of the data \cite{kingma:14:iclr}. The variational autoencoder is a powerful generative model which allows inference of the learned latent representation. However, the classic VAEs are prone to the \textquotedblleft posterior collapse \textquotedblright phenomenon that the latent representations are ignored due to the powerful decoder.	score:448
What are your views on PM Modi's new interview to CNN Network 18 News?	 This paper describes a novel text-to-speech (TTS) technique based on deep convolutional neural networks (CNN), without use of any recurrent units. Recurrent neural networks (RNN) have become a standard technique to model sequential data recently, and this technique has been used in some cutting-edge neural TTS techniques. However, training RNN components often requires a very powerful computer, or a very long time, typically several days or weeks.  Recent other studies, on the other hand, have shown that CNN-based sequence synthesis can be much faster than RNN-based techniques, because of high parallelizability. The objective of this paper is to show that an alternative neural TTS based only on CNN alleviate these economic costs of training. In our experiment, the proposed Deep Convolutional TTS was sufficiently trained overnight (15 hours), using an ordinary gaming PC equipped with two GPUs, while the quality of the synthesized speech was almost acceptable.  	Text-to-speech (TTS) is getting more and more common recently,     and is getting to be a basic user interface for many systems.     To further promote the use of TTS in various systems,     it is significant to develop a manageable, maintainable, and extensible TTS component that is accessible to speech non-specialists,     enterprising individuals and small teams.	 This paper describes a novel text-to-speech (TTS) technique based on deep convolutional neural networks (CNN), without use of any recurrent units. Recurrent neural networks (RNN) have become a standard technique to model sequential data recently, and this technique has been used in some cutting-edge neural TTS techniques. However, training RNN components often requires a very powerful computer, or a very long time, typically several days or weeks.  Recent other studies, on the other hand, have shown that CNN-based sequence synthesis can be much faster than RNN-based techniques, because of high parallelizability. The objective of this paper is to show that an alternative neural TTS based only on CNN alleviate these economic costs of training. In our experiment, the proposed Deep Convolutional TTS was sufficiently trained overnight (15 hours), using an ordinary gaming PC equipped with two GPUs, while the quality of the synthesized speech was almost acceptable.  	Text-to-speech (TTS) is getting more and more common recently,     and is getting to be a basic user interface for many systems.     To further promote the use of TTS in various systems,     it is significant to develop a manageable, maintainable, and extensible TTS component that is accessible to speech non-specialists,     enterprising individuals and small teams.	score:450
Is it currently possible to develop a software that can speak English like a native speaker with deep learning or any other machine learning method?	    Deep learning, in the form of artificial neural networks, has achieved remarkable practical success in recent years, for a variety of difficult machine learning applications. However, a theoretical explanation for this remains a major open problem, since training neural networks involves optimizing a highly non-convex objective function, and is known to be computationally hard in the worst case.  In this work, we study the \emph{geometric} structure of the associated non-convex objective function, in the context of ReLU networks and starting from a random initialization of the network parameters. We identify some conditions under which it becomes more favorable to optimization, in the sense of (i) High probability of initializing at a point from which there is a monotonically decreasing path to a global minimum; and (ii) High probability of initializing at a basin (suitably defined) with a small minimal objective value.	    Deep learning, in the form of artificial neural networks, has achieved remarkable practical success in recent years, for a variety of difficult machine learning applications. However, a theoretical explanation for this remains a major open problem, since training neural networks involves optimizing a highly non-convex objective function, and is known to be computationally hard in the worst case.  In this work, we study the \emph{geometric} structure of the associated non-convex objective function, in the context of ReLU networks and starting from a random initialization of the network parameters. We identify some conditions under which it becomes more favorable to optimization, in the sense of (i) High probability of initializing at a point from which there is a monotonically decreasing path to a global minimum; and (ii) High probability of initializing at a basin (suitably defined) with a small minimal objective value.	score:255
Is it currently possible to develop a software that can speak English like a native speaker with deep learning or any other machine learning method?	 In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input.  This paper reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.  	Reinforcement Learning (RL) \cite{kaelbling1996reinforcement,sutton1998introduction} is a branch of machine learning in which an agent learns from interacting with an environment.	 In recent years, a specific machine learning method called deep learning has gained huge attraction, as it has obtained astonishing results in broad applications such as pattern recognition, speech recognition, computer vision, and natural language processing. Recent research has also been shown that deep learning techniques can be combined with reinforcement learning methods to learn useful representations for the problems with high dimensional raw data input.  This paper reviews the recent advances in deep reinforcement learning with a focus on the most used deep architectures such as autoencoders, convolutional neural networks and recurrent neural networks which have successfully been come together with the reinforcement learning framework.  	Reinforcement Learning (RL) \cite{kaelbling1996reinforcement,sutton1998introduction} is a branch of machine learning in which an agent learns from interacting with an environment.	score:259
Is it currently possible to develop a software that can speak English like a native speaker with deep learning or any other machine learning method?	 Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation.  In this pioneering paper, we propose the ``coding criterion'' to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations.	 Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation.  In this pioneering paper, we propose the ``coding criterion'' to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations.	score:266
Is it currently possible to develop a software that can speak English like a native speaker with deep learning or any other machine learning method?	 Deep learning has become a powerful and popular tool for a variety of machine learning tasks. However, it is challenging to understand the mechanism of deep learning from a theoretical perspective. In this work, we propose a random active path model to study collective properties of  deep neural networks with binary synapses, under the removal perturbation of connections between layers.  In the model, the path from input to output is randomly activated, and the corresponding input unit constrains the weights along the path into the form of a $p$-weight interaction glass model. A critical value of the perturbation is observed to separate a spin glass regime from a paramagnetic regime, with the transition being of the first order. The paramagnetic phase is conjectured to have a poor  generalization performance.	 Deep learning has become a powerful and popular tool for a variety of machine learning tasks. However, it is challenging to understand the mechanism of deep learning from a theoretical perspective. In this work, we propose a random active path model to study collective properties of  deep neural networks with binary synapses, under the removal perturbation of connections between layers.  In the model, the path from input to output is randomly activated, and the corresponding input unit constrains the weights along the path into the form of a $p$-weight interaction glass model. A critical value of the perturbation is observed to separate a spin glass regime from a paramagnetic regime, with the transition being of the first order. The paramagnetic phase is conjectured to have a poor  generalization performance.	score:266
Is it currently possible to develop a software that can speak English like a native speaker with deep learning or any other machine learning method?	  Recently, Deep Learning has been showing promising results in various Artificial Intelligence applications like image recognition, natural language processing, language modeling, neural machine translation, etc. Although, in general, it is computationally more expensive as compared to classical machine learning techniques, their results are found to be more effective in some cases.  Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 \& 7 layers architectures with the four different feature sets, and found that  irrespective of the features inputs, the classical RF accuracy outperforms the DNN.	  Recently, Deep Learning has been showing promising results in various Artificial Intelligence applications like image recognition, natural language processing, language modeling, neural machine translation, etc. Although, in general, it is computationally more expensive as compared to classical machine learning techniques, their results are found to be more effective in some cases.  Therefore, in this paper, we investigated and compared one of the Deep Learning Architecture called Deep Neural Network (DNN) with the classical Random Forest (RF) machine learning algorithm for the malware classification. We studied the performance of the classical RF and DNN with 2, 4 \& 7 layers architectures with the four different feature sets, and found that  irrespective of the features inputs, the classical RF accuracy outperforms the DNN.	score:266
What is machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:485
What is machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:498
What is machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.	score:524
What is machine learning?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:526
What is machine learning?	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	  Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process.  In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks.	score:533
What is machine learning algorithm?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:425
What is machine learning algorithm?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:427
What is machine learning algorithm?	  Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM), and solved by optimization algorithms such as gradient descent (GD), stochastic gradient descent (SGD), and stochastic variance reduction (SVRG). Conventional analysis on these optimization algorithms focuses on their convergence rates during the training process, however, people in the machine learning community may care more about the generalization performance of the learned model on unseen test data.	  Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM), and solved by optimization algorithms such as gradient descent (GD), stochastic gradient descent (SGD), and stochastic variance reduction (SVRG). Conventional analysis on these optimization algorithms focuses on their convergence rates during the training process, however, people in the machine learning community may care more about the generalization performance of the learned model on unseen test data.	score:434
What is machine learning algorithm?	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	 Usually, expert knowledge is used for the definition of these high-level variables and the mapping from the sensorial data. After this preprocessing stage, machine learning algorithms can be used to automatically obtain the mapping from the high-level input variables to the robot control commands. This paper describes an algorithm that is able to perform the preprocessing stage embedded in the learning stage, thus avoiding the use of expert knowledge.  Therefore, the mapping between low-level and high-level input variables is done automatically during the learning phase of the controller.  The data provided by the sensors is of high dimensionality. For example, a robot equipped with two laser range finders can generate over 720 low-level variables. However, in mobile robotics it is more common to work with sets or groupings of these variables, (e.	score:436
What is machine learning algorithm?	  For short-term solar forecasting, statistical methods are popularly used due to their learning power and high computational efficiency. Based on the algorithm complexity, statistical methods can be categorized into traditional time series methods (e.g., autoregressive integrated moving average (ARIMA) method~\cite{david2016probabilistic}), machine learning methods (e. g., support vector machine (SVM)~\cite{deo2016wavelet}), and deep learning models (e.g., long short term memory neural network (LSTM-NN)~\cite{gensler2016deep}). A more comprehensive review of statistical methods for solar forecasting can be found in recent review papers~\cite{voyant2017machine, antonanzas2016review, raza2016recent}.  To improve forecasting accuracy, different techniques have been proposed in the literature, such as dividing forecasting into different subtasks.	  For short-term solar forecasting, statistical methods are popularly used due to their learning power and high computational efficiency. Based on the algorithm complexity, statistical methods can be categorized into traditional time series methods (e.g., autoregressive integrated moving average (ARIMA) method~\cite{david2016probabilistic}), machine learning methods (e. g., support vector machine (SVM)~\cite{deo2016wavelet}), and deep learning models (e.g., long short term memory neural network (LSTM-NN)~\cite{gensler2016deep}). A more comprehensive review of statistical methods for solar forecasting can be found in recent review papers~\cite{voyant2017machine, antonanzas2016review, raza2016recent}.  To improve forecasting accuracy, different techniques have been proposed in the literature, such as dividing forecasting into different subtasks.	score:437
Did the first batch of RGIPT graduates get their B.Tech degree from IIT Kanpur?	 In this setting, online techniques based on stochastic approximations are an attractive alternative to batch methods~(see, e.g., \citealp{bottou2,kushner,shwartz}).  For example, first-order stochastic gradient descent with projections on the constraint set \citep{kushner} is sometimes used for dictionary learning (see \citealp{aharon2,kavukcuoglu} for instance).  We show in this paper that it is possible to go further and exploit the specific structure of sparse coding in the design of an optimization procedure tuned to this problem, with low memory consumption and lower computational cost than classical batch algorithms.  As demonstrated by our experiments, it scales up gracefully to large data sets with millions of training samples, is easy to use, and is faster than competitive methods.	 In this setting, online techniques based on stochastic approximations are an attractive alternative to batch methods~(see, e.g., \citealp{bottou2,kushner,shwartz}).  For example, first-order stochastic gradient descent with projections on the constraint set \citep{kushner} is sometimes used for dictionary learning (see \citealp{aharon2,kavukcuoglu} for instance).  We show in this paper that it is possible to go further and exploit the specific structure of sparse coding in the design of an optimization procedure tuned to this problem, with low memory consumption and lower computational cost than classical batch algorithms.  As demonstrated by our experiments, it scales up gracefully to large data sets with millions of training samples, is easy to use, and is faster than competitive methods.	score:380
Did the first batch of RGIPT graduates get their B.Tech degree from IIT Kanpur?	 However, similar to the previously mentioned supervised learning algorithms, these algorithms myopically optimize the likelihood of next utterance and neglect the overarching goal of the dialog. They also tends to generate very generic responses~\cite{kandasamy2016batch,li2016deep}.  Inspired by recent RL-based algorithms for sequence prediction~\cite{Bahdanau2017an,ranzato2015sequence}, we propose a RL-based end-to-end dialog policy learning algorithm that can overcome these two drawbacks.  Specifically, we model agent response generation as a Markov decision process (MDP) in which each episode (from an initial state to a terminal state) corresponds to a sequence of words in an agent utterance. Note that this is different from the MDP defined in related literature~\cite{dhingra2017,su2017sample}, where each episode corresponds to a sequence of dialog acts.	 However, similar to the previously mentioned supervised learning algorithms, these algorithms myopically optimize the likelihood of next utterance and neglect the overarching goal of the dialog. They also tends to generate very generic responses~\cite{kandasamy2016batch,li2016deep}.  Inspired by recent RL-based algorithms for sequence prediction~\cite{Bahdanau2017an,ranzato2015sequence}, we propose a RL-based end-to-end dialog policy learning algorithm that can overcome these two drawbacks.  Specifically, we model agent response generation as a Markov decision process (MDP) in which each episode (from an initial state to a terminal state) corresponds to a sequence of words in an agent utterance. Note that this is different from the MDP defined in related literature~\cite{dhingra2017,su2017sample}, where each episode corresponds to a sequence of dialog acts.	score:405
Did the first batch of RGIPT graduates get their B.Tech degree from IIT Kanpur?	org/code/rca.r}}   \end{center}    \subsection{Related Work}   There has been a recent stream of research in kernel approximations via   randomized feature maps since the seminal work of \citet{Rahimi08}. For   instance, their extensions to dot-product kernels \citep{kar2012random} and   polynomial kernels \citep{hamid2013compact}; the development of advanced   sampling techniques using Quasi-Monte-Carlo methods \citep{yang14} or their   accelerated computation via fast Walsh-Hadamard transforms \citep{Le13}.     The use of randomized techniques for kernelized component analysis methods   dates back to \citep{achlio02}, where three kernel sub-sampling strategies   were suggested to speed up KPCA.  On the other hand, \citep{Avron13} made use   of randomized Walsh-Hadamard transforms to adapt linear CCA to large-scale   datasets. The use of non-linear random features is more scarce and has only   appeared twice in previous literature. First, \citet{Lopez-Paz13} defined the   dependence statistic RDC as the largest canonical correlation between two   sets of copula random projections.  Second, \citet{McWilliams13} used the   Nystr\"om method to define a randomized feature map and performed CCA to   achieve state-of-the-art semi-supervised learning.	    The use of randomized techniques for kernelized component analysis methods   dates back to \citep{achlio02}, where three kernel sub-sampling strategies   were suggested to speed up KPCA.  On the other hand, \citep{Avron13} made use   of randomized Walsh-Hadamard transforms to adapt linear CCA to large-scale   datasets. The use of non-linear random features is more scarce and has only   appeared twice in previous literature. First, \citet{Lopez-Paz13} defined the   dependence statistic RDC as the largest canonical correlation between two   sets of copula random projections.  Second, \citet{McWilliams13} used the   Nystr\"om method to define a randomized feature map and performed CCA to   achieve state-of-the-art semi-supervised learning.	score:410
Did the first batch of RGIPT graduates get their B.Tech degree from IIT Kanpur?	 Advanced gradient based methods (e.g., Adam \cite{Kingma17}, AdaGrad \cite{Duchi11AdaGrad},  RMSProp \cite{Tieleman12RMSProp}) have thus been proposed to enable both fast training speed and adaptive learning rates.     In recent years, a family of normalization techniques have been proposed to accelerate the training process. The motivation behind these techniques is to make proper adjustment at each individual layer so that either the input or output statistics of the activation functions of the layer are unified in terms of the first and/or second moments.	 Advanced gradient based methods (e.g., Adam \cite{Kingma17}, AdaGrad \cite{Duchi11AdaGrad},  RMSProp \cite{Tieleman12RMSProp}) have thus been proposed to enable both fast training speed and adaptive learning rates.     In recent years, a family of normalization techniques have been proposed to accelerate the training process. The motivation behind these techniques is to make proper adjustment at each individual layer so that either the input or output statistics of the activation functions of the layer are unified in terms of the first and/or second moments.	score:418
Did the first batch of RGIPT graduates get their B.Tech degree from IIT Kanpur?	 Grohe and Ritzert~\cite{DBLP:conf/lics/GroheR17}, for instance, considered learning of first-order definable concepts over structures of small degree. Subsequently, Grohe, Löding, and Ritzert~\cite{DBLP:conf/alt/GroheLR17} studied the learning of hypotheses definable using monadic second order logic on strings. Due to the fundamental differences between PAC learning and the learning model considered here (one being approximate and the other being exact), their techniques cannot easily be applied.	 Grohe and Ritzert~\cite{DBLP:conf/lics/GroheR17}, for instance, considered learning of first-order definable concepts over structures of small degree. Subsequently, Grohe, Löding, and Ritzert~\cite{DBLP:conf/alt/GroheLR17} studied the learning of hypotheses definable using monadic second order logic on strings. Due to the fundamental differences between PAC learning and the learning model considered here (one being approximate and the other being exact), their techniques cannot easily be applied.	score:422
How do I get into top MNCs if I am a 2015 batch graduate?	 Suppose that the outcome statistics of the set of quantum states are known. Can we infer the unknown quantum measurement from the quantum states at hand? How many samples of quantum states are needed for the learning machine to decide an optimal quantum measurement from the hypothesis set? Can the chosen candidate approximate the target measurement with the desired accuracy?  These questions are typical sample complexity problems in statistical learning theory, and the answer lies in a proper quantification of the ``effective size" of the hypothesis set. In this paper, we propose a framework (see Section \ref{Framework}) to connect the problems of learning two-outcome measurements with the tasks of learning real-valued linear functional on quantum states.	 Suppose that the outcome statistics of the set of quantum states are known. Can we infer the unknown quantum measurement from the quantum states at hand? How many samples of quantum states are needed for the learning machine to decide an optimal quantum measurement from the hypothesis set? Can the chosen candidate approximate the target measurement with the desired accuracy?  These questions are typical sample complexity problems in statistical learning theory, and the answer lies in a proper quantification of the ``effective size" of the hypothesis set. In this paper, we propose a framework (see Section \ref{Framework}) to connect the problems of learning two-outcome measurements with the tasks of learning real-valued linear functional on quantum states.	score:475
How do I get into top MNCs if I am a 2015 batch graduate?	   Prior to our work the only theoretical work related to  such learning dynamics has been  either in  deterministic special cases or in the asymptotic setting.  Finally, we observe that our infinite population dynamics is a stochastic variant of the classic multiplicative weights update (MWU) method. Consequently, we arrive at the following interesting converse:  the learning dynamics on a finite population considered here can be viewed as a novel distributed and low-memory implementation of the classic MWU method.	   Prior to our work the only theoretical work related to  such learning dynamics has been  either in  deterministic special cases or in the asymptotic setting.  Finally, we observe that our infinite population dynamics is a stochastic variant of the classic multiplicative weights update (MWU) method. Consequently, we arrive at the following interesting converse:  the learning dynamics on a finite population considered here can be viewed as a novel distributed and low-memory implementation of the classic MWU method.	score:478
How do I get into top MNCs if I am a 2015 batch graduate?	 Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example~\cite{Sohn_ICML2012, Zou_NIPS2012, Hui_ICML2013}.    Our proposed method can be related to work on metric learning, for example~\cite{Goldberger_NIPS2004, Hadsell_CVPR2006}. However, instead of enforcing a metric on the feature representation directly, as in~\cite{Hadsell_CVPR2006}, we only implicitly force the representation of transformed images to be mapped close together through the introduced surrogate labels.	 Among these unsupervised methods, most similar to our approach are several studies on learning invariant representations from transformed input samples, for example~\cite{Sohn_ICML2012, Zou_NIPS2012, Hui_ICML2013}.    Our proposed method can be related to work on metric learning, for example~\cite{Goldberger_NIPS2004, Hadsell_CVPR2006}. However, instead of enforcing a metric on the feature representation directly, as in~\cite{Hadsell_CVPR2006}, we only implicitly force the representation of transformed images to be mapped close together through the introduced surrogate labels.	score:480
How do I get into top MNCs if I am a 2015 batch graduate?	 Random reshuffling has recently been shown to have better convergence rates than stochastic gradient descent \cite{RR-arxiv-2015,IoSz:2015}.   Implementing random reshuffling in a distributed setting comes at the cost of an extra communication overhead, since at each iteration random data assignment is done for the distributed workers, and these data points need to be communicated to the distributed workers.  This leads to a fundamental trade-off between the communication overhead, and storage at each worker. On one extreme case when each worker can store the whole data-set, no communication is necessary for any shuffle. On the other extreme, when the workers are just able to store the batches under processing, which is refereed to as the \textit{no-excess storage} case, the communication overhead is expected to be maximum.	 Random reshuffling has recently been shown to have better convergence rates than stochastic gradient descent \cite{RR-arxiv-2015,IoSz:2015}.   Implementing random reshuffling in a distributed setting comes at the cost of an extra communication overhead, since at each iteration random data assignment is done for the distributed workers, and these data points need to be communicated to the distributed workers.  This leads to a fundamental trade-off between the communication overhead, and storage at each worker. On one extreme case when each worker can store the whole data-set, no communication is necessary for any shuffle. On the other extreme, when the workers are just able to store the batches under processing, which is refereed to as the \textit{no-excess storage} case, the communication overhead is expected to be maximum.	score:489
How do I get into top MNCs if I am a 2015 batch graduate?	 However, the CGM literature has focused primarily on inference, and uses expectation maximization for learning, which is effective in certain cases but provides no guarantees~\cite{Sheldon:2011aa,Sheldon:2013aa,Liu:2014aa,sun2015message}. Our work contributes the first learning method with guarantees of any kind for a subclass of CGMs.  In this paper, we initiate the study of method of moments estimators for aggregate Markov chains to explicitly deal with imperfect observations.  This is an important practical issue: aggregate data are rarely complete surveys of a population, so they should be considered \emph{noisy} counts of the number of individuals in each state. The method of moments viewpoint yields a number of useful observations. First, we show that CLS can be interpreted as a simple ``plug-in'' method of moments estimator.	 However, the CGM literature has focused primarily on inference, and uses expectation maximization for learning, which is effective in certain cases but provides no guarantees~\cite{Sheldon:2011aa,Sheldon:2013aa,Liu:2014aa,sun2015message}. Our work contributes the first learning method with guarantees of any kind for a subclass of CGMs.  In this paper, we initiate the study of method of moments estimators for aggregate Markov chains to explicitly deal with imperfect observations.  This is an important practical issue: aggregate data are rarely complete surveys of a population, so they should be considered \emph{noisy} counts of the number of individuals in each state. The method of moments viewpoint yields a number of useful observations. First, we show that CLS can be interpreted as a simple ``plug-in'' method of moments estimator.	score:490
Kevin Murphy: How do I learn machine learning from scratch?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:320
Kevin Murphy: How do I learn machine learning from scratch?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:320
Kevin Murphy: How do I learn machine learning from scratch?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:361
Kevin Murphy: How do I learn machine learning from scratch?	  As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?  random forest (RF)? etc.), what parameters to use (e.g., how many decision trees in a RF? how many hidden layers in an ANN? etc.), and so on. Experienced machine learning practitioners often have good intuitions on what choices are appropriate for the problem domain, but some practitioners can easily spend several weeks tinkering with model parameters and data transformations until the pipeline achieves an acceptable level of performance.	  As an example, a typical machine learning practitioner may build a pipeline as shown in Figure~\ref{fig:tpot-ml-pipeline-diagram}. At each step, there are dozens of possible choices to make: How to preprocess the data (e.g., what feature selector? what feature constructor? etc.), what model to use (e.g., support vector machine (SVM)? artificial neural network (ANN)?  random forest (RF)? etc.), what parameters to use (e.g., how many decision trees in a RF? how many hidden layers in an ANN? etc.), and so on. Experienced machine learning practitioners often have good intuitions on what choices are appropriate for the problem domain, but some practitioners can easily spend several weeks tinkering with model parameters and data transformations until the pipeline achieves an acceptable level of performance.	score:368
Kevin Murphy: How do I learn machine learning from scratch?	    	Building machines that are able to recognize speech represents a fundamental step towards flexible and natural human-machine interfaces. A primary role for improving such a technology is played by deep learning \cite{Goodfellow-et-al-2016-Book}, which has recently contributed to significantly outperform previous GMM/HMM speech recognizers \cite{lideng}.   During the last years, deep learning has been rapidly evolving, progressively offering more powerful and robust techniques, including effective regularization methods \cite{dropout,batchnorm}, improved optimization algorithms \cite{adam}, as well as better network architectures. The early deep learning works in speech recognition were mainly based on standard multilayer perceptrons (MLPs) \cite{IEEEexample:intro7,IEEEexample:intro1}, while recent systems benefit from more advanced architectures, such as Convolutional Neural Networks (CNNs) \cite{cnn1} and Time Delay Neural Network (TDNN) \cite{tdnn,tdnn2}.	    	Building machines that are able to recognize speech represents a fundamental step towards flexible and natural human-machine interfaces. A primary role for improving such a technology is played by deep learning \cite{Goodfellow-et-al-2016-Book}, which has recently contributed to significantly outperform previous GMM/HMM speech recognizers \cite{lideng}.   During the last years, deep learning has been rapidly evolving, progressively offering more powerful and robust techniques, including effective regularization methods \cite{dropout,batchnorm}, improved optimization algorithms \cite{adam}, as well as better network architectures. The early deep learning works in speech recognition were mainly based on standard multilayer perceptrons (MLPs) \cite{IEEEexample:intro7,IEEEexample:intro1}, while recent systems benefit from more advanced architectures, such as Convolutional Neural Networks (CNNs) \cite{cnn1} and Time Delay Neural Network (TDNN) \cite{tdnn,tdnn2}.	score:373
What should I read or do to understand machine learning from scratch?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:388
What should I read or do to understand machine learning from scratch?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:389
What should I read or do to understand machine learning from scratch?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:389
What should I read or do to understand machine learning from scratch?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:394
What should I read or do to understand machine learning from scratch?	  Such a machine must necessarily learn and adapt to a changing environment and, crucially, manage memories of what it has learned for the future tasks it will encounter. A spectrum of learning scenarios are available depending on problem requirements.  In lifelong learning \citep{LML} the machine is presented a sequence of tasks and must use knowledge learned from the previous tasks to perform better on the next.  In the resource-constrained lifelong learning setting the machine is constrained to a small buffer of previous experiences.  Some approaches to lifelong learning assume that a task is a set of examples chosen from the same distribution \citep{PNN,Pathnet,DGR,LLGen,CMAML,DEN}. If instead the machine is given a sequence of examples without any batching, then this is called continual learning.	  Such a machine must necessarily learn and adapt to a changing environment and, crucially, manage memories of what it has learned for the future tasks it will encounter. A spectrum of learning scenarios are available depending on problem requirements.  In lifelong learning \citep{LML} the machine is presented a sequence of tasks and must use knowledge learned from the previous tasks to perform better on the next.  In the resource-constrained lifelong learning setting the machine is constrained to a small buffer of previous experiences.  Some approaches to lifelong learning assume that a task is a set of examples chosen from the same distribution \citep{PNN,Pathnet,DGR,LLGen,CMAML,DEN}. If instead the machine is given a sequence of examples without any batching, then this is called continual learning.	score:409
Are hyenas closer to dogs or to cats within the animal classification hierarchy?	  Fortunately, such specialised disciplines often create expressive \emph{ontologies}, in the form of annotated relationships between terms (denoted by underlines). These may be semantic, such as \underline{dog} is a \underline{type of} \underline{animal}, or derived from domain-specific knowledge, such as \underline{anemia} is an \underline{associated disease of} \underline{leukemia}.  (This is a relationship found in the medical ontology system UMLS; see \citeauthor{bodenreider2004unified}, \citeyear{bodenreider2004unified}). We observe that these relationships can be thought of as additional \emph{contexts} from which co-occurrence statistics can be drawn; the set of diseases associated with leukemia arguably share a common context, even if they may not co-occur in a sentence (see \textbf{Figure~\ref{fig:illustration}}).	  Fortunately, such specialised disciplines often create expressive \emph{ontologies}, in the form of annotated relationships between terms (denoted by underlines). These may be semantic, such as \underline{dog} is a \underline{type of} \underline{animal}, or derived from domain-specific knowledge, such as \underline{anemia} is an \underline{associated disease of} \underline{leukemia}.  (This is a relationship found in the medical ontology system UMLS; see \citeauthor{bodenreider2004unified}, \citeyear{bodenreider2004unified}). We observe that these relationships can be thought of as additional \emph{contexts} from which co-occurrence statistics can be drawn; the set of diseases associated with leukemia arguably share a common context, even if they may not co-occur in a sentence (see \textbf{Figure~\ref{fig:illustration}}).	score:412
Are hyenas closer to dogs or to cats within the animal classification hierarchy?	 The two networks share the latent variables. Intuitively speaking one might think of these variables as specifying, for a given image, at different levels of abstraction, whether a particular object such as a cat or a dog is present in the input, or perhaps what the exact position and intensity of an edge at a given location might be.  During the recognition phase the network acquires information about the input and stores it in the latent variables, reducing their uncertainty.  For example, at first not knowing whether a cat or a dog is present in the image, the network observes the input and becomes nearly certain that it is a cat. The reduction in uncertainty is quantitatively equal to the amount of information the network acquired about the input. During generation the network starts with uncertain latent variables and selects their values from a prior distribution that specifies this uncertainty (e.	 How would we then get an image out of this single bit? If we have a good generative model, we can simply generate the entire image from this one latent variable, an image of a cat if the bit corresponds to `cat', and an image of a dog otherwise. Now let us imagine that instead of compressing to one bit we wanted to compress down to ten bits. Now we can store the most important properties of the animal as well -- e. g. its type, color, and basic pose. The rest would be `filled in' by the generative model that is conditioned on this information. If we increase the number of bits further we can preserve more and more about the image, while generating the fine details such as hair, or the exact pattern of the floor, etc. Most bits are in fact about such low level details.	score:420
Are hyenas closer to dogs or to cats within the animal classification hierarchy?	 We further show that, without specific supervision, ConvNet-AIG discovers parts of the class hierarchy and learns specialized layers focusing on subsets of categories such as animals and man-made objects. It even learns distinct inference graphs for some mid-level categories such as birds, dogs and reptiles. By grouping parameters for related classes and only executing relevant layers, ConvNet-AIG both improves efficiency and overall classification quality.  Lastly, we also study the effect of adaptive inference graphs on susceptibility towards adversarial examples. We show that ConvNet-AIG is consistently more robust than ResNets, independent of adversary strength and that the additional robustness persists even when applying additional defense mechanisms.  \begin{figure}[t]  \begin{center}   \includegraphics[width=1.	 We further show that, without specific supervision, ConvNet-AIG discovers parts of the class hierarchy and learns specialized layers focusing on subsets of categories such as animals and man-made objects. It even learns distinct inference graphs for some mid-level categories such as birds, dogs and reptiles. By grouping parameters for related classes and only executing relevant layers, ConvNet-AIG both improves efficiency and overall classification quality.  Lastly, we also study the effect of adaptive inference graphs on susceptibility towards adversarial examples. We show that ConvNet-AIG is consistently more robust than ResNets, independent of adversary strength and that the additional robustness persists even when applying additional defense mechanisms.  \begin{figure}[t]  \begin{center}   \includegraphics[width=1.	score:421
Are hyenas closer to dogs or to cats within the animal classification hierarchy?	  	Consider the illustrative task of grouping images of cats and dogs by \emph{perceived} similarity: depending on the intention of the user behind the task, the similarity could be defined by animal type (foreground object), environmental nativeness (background landscape, cp. Fig. \ref{fig:cats_dogs}) etc. This is characteristic of clustering perceptual, high-dimensional data like images \cite{kampffmeyer2017} or sound \cite{lukic2017speaker}: a user typically has some similarity criterion in mind when thinking about naturally arising groups (e. g., pictures by holiday destination, or persons appearing; songs by mood, or use of solo instrument). As defining such a similarity for every case is difficult, it is desirable to learn it. At the same time, the learned model will in many cases not be a classifier---the task will not be solved by classification---since the number and specific type of groups present at application time are not known in advance (e.	  	Consider the illustrative task of grouping images of cats and dogs by \emph{perceived} similarity: depending on the intention of the user behind the task, the similarity could be defined by animal type (foreground object), environmental nativeness (background landscape, cp. Fig. \ref{fig:cats_dogs}) etc. This is characteristic of clustering perceptual, high-dimensional data like images \cite{kampffmeyer2017} or sound \cite{lukic2017speaker}: a user typically has some similarity criterion in mind when thinking about naturally arising groups (e. g., pictures by holiday destination, or persons appearing; songs by mood, or use of solo instrument). As defining such a similarity for every case is difficult, it is desirable to learn it. At the same time, the learned model will in many cases not be a classifier---the task will not be solved by classification---since the number and specific type of groups present at application time are not known in advance (e.	score:437
Are hyenas closer to dogs or to cats within the animal classification hierarchy?	 Likewise, with representations of {\em dog}, {\em house}, and {\em kennel}, an algorithm should be able to recognize that the semantic composition of {\em dog} and {\em house}, {\em dog house}, is highly similar to {\em kennel} ({\em dog house} and {\em kennel} are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected.  However, up to now, the best models for relations are significantly different from the best models for compositions. In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity.	 For example, we know how to represent {\em dog} and {\em house} with vectors, but how should we represent {\em dog house}$\,$?  One approach to representing {\em dog house} is to treat it as a unit, the same way we handle individual words. We call this the {\em holistic} or {\em noncompositional} approach to representing phrases. The holistic approach may be suitable for some phrases, but it does not scale up.  With a vocabulary of $N$ individual words, we can have $N^2$ two-word phrases, $N^3$ three-word phrases, and so on. Even with a very large corpus of text, most of these possible phrases will never appear in the corpus. People are continually inventing new phrases, and we are able to understand these new phrases although we have never heard them before; we are able to infer the meaning of a new phrase by {\em composition} of the meanings of the component words.	score:440
Where can I find a mobile app testing matrix?	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	score:465
Where can I find a mobile app testing matrix?	com/verml/VerifyML}}.        We anticipate our approach to be used by a tester as follows: when a ML based application is submitted to his/her desk for testing, the first step of verification can be done in an automated fashion through our approach. The tester would select relevant metamorphic relations and a tool based on our approach would automatically create test cases to check for the properties.  If any of the properties do not hold, the verification has identified an implementation bug in the ML application.  This paper is structured as follows. We present the related work in Section \ref{relatedWork}. Section \ref{ourApproach} presents our approach for two ML based image classifiers. Section \ref{results} details the experimental results and we conclude in Section \ref{conclusion}.	com/verml/VerifyML}}.        We anticipate our approach to be used by a tester as follows: when a ML based application is submitted to his/her desk for testing, the first step of verification can be done in an automated fashion through our approach. The tester would select relevant metamorphic relations and a tool based on our approach would automatically create test cases to check for the properties.  If any of the properties do not hold, the verification has identified an implementation bug in the ML application.  This paper is structured as follows. We present the related work in Section \ref{relatedWork}. Section \ref{ourApproach} presents our approach for two ML based image classifiers. Section \ref{results} details the experimental results and we conclude in Section \ref{conclusion}.	score:475
Where can I find a mobile app testing matrix?	 \Latitude can be used to decompose relatively large data sets, as it scales linearly with the input data. \paragraph{Main contributions.} In this paper we present a novel matrix factorization model, called mixed linear--tropical model (Section~\ref{sec:model}) and a scalable algorithm for finding a decomposition in this model (Section~\ref{sec:algorithms}). Our experiments (Section~\ref{sec:experiments}) show that our algorithm finds decompositions that have smaller reconstruction error than what NMF or SMF methods -- or even SVD -- can find, and that the results are also intuitive and reveal interesting structures from the data sets.	 \Latitude can be used to decompose relatively large data sets, as it scales linearly with the input data. \paragraph{Main contributions.} In this paper we present a novel matrix factorization model, called mixed linear--tropical model (Section~\ref{sec:model}) and a scalable algorithm for finding a decomposition in this model (Section~\ref{sec:algorithms}). Our experiments (Section~\ref{sec:experiments}) show that our algorithm finds decompositions that have smaller reconstruction error than what NMF or SMF methods -- or even SVD -- can find, and that the results are also intuitive and reveal interesting structures from the data sets.	score:475
Where can I find a mobile app testing matrix?	 In the second step we find the values of these non-zero entries. This can be done by solving a least squares problem that finds $\mathbf{s}_i$ given $\mathbf{y}_i$ and $\mathbf{A}_{\Omega_i}$. $\mathbf{A}_{\Omega_i}$ is a matrix that includes only those atoms (columns) of $\mathbf{A}$ that are members of the support of $\mathbf{s}_i$.   To find the conditional probabilities at each iteration, we propose the use of a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells and a softmax layer on top of it.  To find the model parameters, we minimize a cross entropy cost function between the conditional probabilities given by the model and the known probabilities in the training data. The details on how to generate the training data and the training data probabilities are explained in subsequent sections. Please note that this training is done only once.	 In the second step we find the values of these non-zero entries. This can be done by solving a least squares problem that finds $\mathbf{s}_i$ given $\mathbf{y}_i$ and $\mathbf{A}_{\Omega_i}$. $\mathbf{A}_{\Omega_i}$ is a matrix that includes only those atoms (columns) of $\mathbf{A}$ that are members of the support of $\mathbf{s}_i$.   To find the conditional probabilities at each iteration, we propose the use of a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells and a softmax layer on top of it.  To find the model parameters, we minimize a cross entropy cost function between the conditional probabilities given by the model and the known probabilities in the training data. The details on how to generate the training data and the training data probabilities are explained in subsequent sections. Please note that this training is done only once.	score:481
Where can I find a mobile app testing matrix?	 At first glance, a direct application of this reduction seems impossible, since $\lambda$ is an infinite-dimensional vector, and at each iteration of the greedy algorithm one needs to search over the infinite set of the coordinates of $\lambda$. However, we show that this search problem can be cast as the problem of finding the first leading right and left singular vectors of a certain matrix.   After describing and analyzing the general algorithm, we show how to apply it  to the problems of matrix completion and robust low-rank matrix approximation. As a side benefit, our general analysis yields a new sample complexity bound for matrix completion. We demonstrate the efficacy of our algorithm by conducting experiments on large-scale movie recommendation data sets.	 This leads to an algorithm which can scale to large matrices arising in several applications such as matrix completion for collaborative filtering and robust low rank matrix approximation. 	Our goal is to approximately solve an optimization problem of the form: \begin{equation} \label{eqn:rankMin} \min_{A : \rank(A) \le r} R(A) ~, \end{equation} where $R: \reals^{m \times n} \to \reals$ is a convex and smooth function.  This problem arises in many machine learning applications such as collaborating filtering \cite{korenBeVo09},  robust low rank matrix approximation \cite{KeKan05,CrFil98,BacBeFal96}, and multiclass classification \cite{AmitFiSrUl07}. The rank constraint on $A$ is non-convex and therefore it is generally NP-hard to solve \eqref{eqn:rankMin} (this follows from \cite{Natarajan95,DavisMaAv97}).	score:481
Which is the best university for online machine learning course?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:437
Which is the best university for online machine learning course?	  Arguments in recent debates have claimed that it is only the accuracy of machine learning models that matters, not their interpretability.  However, taking this view ignores the fact that the overall system in high-stakes settings is a machine learning model communicating with a human who makes the final decision, and thus it is the accuracy of the overall system that is of relevance.   Interpretable machine learning models are an appropriate means for communication between AI and human \cite{DhurandharILS2017}; the contribution of this paper is to abstractly model the overall system and theoretically show the system performance advantage of interpretable machine learning models over black box machine learning models.  In this paper, we consider the population setting (the limit as the number of samples goes to infinity, allowing access to the probability distributions of the data) and appeal to the theory of distributed detection and data fusion \cite{Varshney1997}.	  Arguments in recent debates have claimed that it is only the accuracy of machine learning models that matters, not their interpretability.  However, taking this view ignores the fact that the overall system in high-stakes settings is a machine learning model communicating with a human who makes the final decision, and thus it is the accuracy of the overall system that is of relevance.   Interpretable machine learning models are an appropriate means for communication between AI and human \cite{DhurandharILS2017}; the contribution of this paper is to abstractly model the overall system and theoretically show the system performance advantage of interpretable machine learning models over black box machine learning models.  In this paper, we consider the population setting (the limit as the number of samples goes to infinity, allowing access to the probability distributions of the data) and appeal to the theory of distributed detection and data fusion \cite{Varshney1997}.	score:442
Which is the best university for online machine learning course?	 	Machine learning has a long history of being applied to networks for multifarious tasks, such as network classification~\cite{snbgge08}, prediction of protein binding~\cite{adwf15}, etc. Thanks to the advancement of technologies such as the Internet and database management systems, the amount of data that are available for machine learning algorithms have been growing tremendously over the past decade.  Among these datasets, a huge fraction can be modeled as networks, such as web networks, brain networks, citation networks, street networks, etc.~\cite{xskk18}. Therefore, improving machine learning algorithms on networks has become even more important.  However, due to the discrete and sparse nature of networks, it is often difficult to apply machine learning directly to them.  To resolve this issue, one major school of thought to approach networks using machine learning is via network embeddings~\cite{gf18}. A network embedding consists of a real number-valued Euclidean vector representation for each node in the network. These vectors can then be fed into machine learning algorithms for various classification and regression tasks.   In recent years, there have been dramatic advancements in learning network embeddings, such as DeepWalk~\cite{pas14} (and its variants using personalized graph or node context distributions~\cite{apaa2018,hhhs2020}), LINE~\cite{tqwzym15}, node2vec~\cite{gl16}, DNGR~\cite{clx16},  metapath2vec~\cite{dcs17},  M-NMF~\cite{wcwpzy17}, PRUNE~\cite{lhcyl17},  GraphSAGE~\cite{hyl17}, LANE~\cite{hlh17}, RSDNE~\cite{wywwwl18}, ANE~\cite{dai18}, ATP~\cite{jba19} and SIDE~\cite{kplk18}.	 	Machine learning has a long history of being applied to networks for multifarious tasks, such as network classification~\cite{snbgge08}, prediction of protein binding~\cite{adwf15}, etc. Thanks to the advancement of technologies such as the Internet and database management systems, the amount of data that are available for machine learning algorithms have been growing tremendously over the past decade.  Among these datasets, a huge fraction can be modeled as networks, such as web networks, brain networks, citation networks, street networks, etc.~\cite{xskk18}. Therefore, improving machine learning algorithms on networks has become even more important.  However, due to the discrete and sparse nature of networks, it is often difficult to apply machine learning directly to them.  To resolve this issue, one major school of thought to approach networks using machine learning is via network embeddings~\cite{gf18}. A network embedding consists of a real number-valued Euclidean vector representation for each node in the network. These vectors can then be fed into machine learning algorithms for various classification and regression tasks.   In recent years, there have been dramatic advancements in learning network embeddings, such as DeepWalk~\cite{pas14} (and its variants using personalized graph or node context distributions~\cite{apaa2018,hhhs2020}), LINE~\cite{tqwzym15}, node2vec~\cite{gl16}, DNGR~\cite{clx16},  metapath2vec~\cite{dcs17},  M-NMF~\cite{wcwpzy17}, PRUNE~\cite{lhcyl17},  GraphSAGE~\cite{hyl17}, LANE~\cite{hlh17}, RSDNE~\cite{wywwwl18}, ANE~\cite{dai18}, ATP~\cite{jba19} and SIDE~\cite{kplk18}.	score:443
Which is the best university for online machine learning course?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:446
Which is the best university for online machine learning course?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:453
What is matrix learning?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:460
What is matrix learning?	 Significant speedup  over the state-of-the-art is observed.  	\IEEEPARstart{L}{ow}-rank matrix  learning  is a central issue in many machine learning and computer vision problems. For example,  matrix completion \cite{candes2009exact}, which is one of the most successful approaches in collaborative filtering, assumes that the target rating matrix is low-rank.   Besides  collaborative filtering, matrix completion has also been used on tasks such as video and image processing \cite{hu2013fast,lu2016nonconvex,gu2016weighted}. Another important use of low-rank matrix learning is robust principal component analysis (RPCA) \cite{candes2011robust},  which assumes that the target  matrix  is  low-rank  and also corrupted by  sparse noise.	 Significant speedup  over the state-of-the-art is observed.  	\IEEEPARstart{L}{ow}-rank matrix  learning  is a central issue in many machine learning and computer vision problems. For example,  matrix completion \cite{candes2009exact}, which is one of the most successful approaches in collaborative filtering, assumes that the target rating matrix is low-rank.   Besides  collaborative filtering, matrix completion has also been used on tasks such as video and image processing \cite{hu2013fast,lu2016nonconvex,gu2016weighted}. Another important use of low-rank matrix learning is robust principal component analysis (RPCA) \cite{candes2011robust},  which assumes that the target  matrix  is  low-rank  and also corrupted by  sparse noise.	score:483
What is matrix learning?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	score:500
What is matrix learning?	 The focus of this work is to apply this idea of life-long learning to the matrix completion problem. That is, given columns of a matrix that arrive online over time with missing entries, how to approximately/exactly recover the underlying matrix by exploiting the low-rank commonality across each column.  Our study is motivated by several promising applications where life-long matrix completion is applicable.  In recommendation systems, the column of the hidden matrix consists of ratings by multiple users to a specific movie/news; The news or movies are updated online over time but usually only a few ratings are submitted by those users. In computer vision, inferring camera motion from a sequence of online arriving images with missing pixels has received significant attention in recent years, known as the structure-from-motion problem; Recovering those missing pixels from those partial measurements is an important preprocessing step.	 The focus of this work is to apply this idea of life-long learning to the matrix completion problem. That is, given columns of a matrix that arrive online over time with missing entries, how to approximately/exactly recover the underlying matrix by exploiting the low-rank commonality across each column.  Our study is motivated by several promising applications where life-long matrix completion is applicable.  In recommendation systems, the column of the hidden matrix consists of ratings by multiple users to a specific movie/news; The news or movies are updated online over time but usually only a few ratings are submitted by those users. In computer vision, inferring camera motion from a sequence of online arriving images with missing pixels has received significant attention in recent years, known as the structure-from-motion problem; Recovering those missing pixels from those partial measurements is an important preprocessing step.	score:505
What is matrix learning?	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	score:516
How do I use Latent Semantic Analysis(LSA) for document classification, preferably in java?	}   \label{fig:topology} \end{figure*}  We describe an entity linking method in order to map a given text entity to an underlying class type implementation from the Java standard libraries. Next, we describe corpus and code based information that we use for the relation discovery task. Corpus based methods include distributional similarity and string matching similarity.  Additionally, we use two sources of code based information: (1) we define the \emph{class-context} of a Java class in a given code repository, and are therefore able to calculate a code-based distributional similarity measure for classes, and (2) we consider the hierarchical organization of classes, described by the Java class type and namespace hierarchies.  We demonstrate that using our approach, cross-validation accuracy on this dataset is improved from 60.9\% to 88\%. According to human labeling, our classifier has an F1-score of 86\% over the highest-ranking 1000 predicted pairs.  We see this work as a first step towards building a knowledge representation system for the software domain, in which text entities refer to elements from a software code base, for example classes, methods, applications and programming languages.  Understanding software entity relations will allow the construction of a domain specific taxonomy and knowledge base, which can enable higher reasoning capabilities in NLP applications for the software domain \cite{weimer2007automatically,wang2009extracting,branavan2010reading,movshovitzattias-wcohen:2013:ACL} and improve a variety of code assisting applications, including code refactoring and token completion \cite{han2009code,jacob2010code,binkley2011improving,schulam:2013:DAPSE13p1}.  Figure~\ref{fig:topology} shows a visualization based on coordinate term pairs predicted using the proposed method. Java classes with similar functionality are highly connected in this graph, indicating that our method can be used to construct a code taxonomy.	 Additionally, we use two sources of code based information: (1) we define the \emph{class-context} of a Java class in a given code repository, and are therefore able to calculate a code-based distributional similarity measure for classes, and (2) we consider the hierarchical organization of classes, described by the Java class type and namespace hierarchies.	score:280
How do I use Latent Semantic Analysis(LSA) for document classification, preferably in java?	   This paper introduces \pydci, a new implementation of   \emph{Distributional Correspondence Indexing} (DCI) written in   Python. DCI is a transfer learning method for cross-domain and   cross-lingual text classification for which we had provided an   implementation (here called \jadci) built on top of \texttt{JaTeCS},   a Java framework for text classification.  \pydci\ is a stand-alone   version of DCI that exploits \texttt{scikit-learn} and the   \texttt{SciPy} stack.      We here report on new experiments that we have carried out in order   to test \pydci, and in which we use as baselines new high-performing   methods that have appeared after DCI was originally proposed.  These   experiments show that, thanks to a few subtle ways in which we have   improved DCI, \pydci\ outperforms both \jadci\ and the   above-mentioned high-performing methods, and delivers the best known   results on the two popular benchmarks on which we had tested DCI,   i.	   This paper introduces \pydci, a new implementation of   \emph{Distributional Correspondence Indexing} (DCI) written in   Python. DCI is a transfer learning method for cross-domain and   cross-lingual text classification for which we had provided an   implementation (here called \jadci) built on top of \texttt{JaTeCS},   a Java framework for text classification.  \pydci\ is a stand-alone   version of DCI that exploits \texttt{scikit-learn} and the   \texttt{SciPy} stack.      We here report on new experiments that we have carried out in order   to test \pydci, and in which we use as baselines new high-performing   methods that have appeared after DCI was originally proposed.  These   experiments show that, thanks to a few subtle ways in which we have   improved DCI, \pydci\ outperforms both \jadci\ and the   above-mentioned high-performing methods, and delivers the best known   results on the two popular benchmarks on which we had tested DCI,   i.	score:295
How do I use Latent Semantic Analysis(LSA) for document classification, preferably in java?	  Computational tractability is a critical concern, because most known algorithms for this setting~\citep[e.g.,][]{balcan2006agnostic,koltchinskii2010rademacher,zhang2014beyond} require explicit enumeration of classifiers, implying exponentially-worse computational complexity compared to typical supervised learning algorithms.  Active learning algorithms based on empirical risk minimization (ERM) oracles~\citep{BeygelDL09,BeygelHLZ10,hsu2010algorithms} can overcome this intractability by using passive classification algorithms as the oracle to achieve a computationally acceptable solution.	  Computational tractability is a critical concern, because most known algorithms for this setting~\citep[e.g.,][]{balcan2006agnostic,koltchinskii2010rademacher,zhang2014beyond} require explicit enumeration of classifiers, implying exponentially-worse computational complexity compared to typical supervised learning algorithms.  Active learning algorithms based on empirical risk minimization (ERM) oracles~\citep{BeygelDL09,BeygelHLZ10,hsu2010algorithms} can overcome this intractability by using passive classification algorithms as the oracle to achieve a computationally acceptable solution.	score:304
How do I use Latent Semantic Analysis(LSA) for document classification, preferably in java?	  Classifiers and rating scores are prone to implicitly codifying biases, which may be present in the training data, against protected classes (i.e., age, gender, or race). So it is important to understand how to design classifiers and scores that prevent discrimination in predictions.  This paper develops computationally tractable algorithms for designing accurate but fair support vector machines (SVM's).   Our approach imposes a constraint on the covariance matrices conditioned on each protected class, which leads to a nonconvex quadratic constraint in the SVM formulation.  We develop iterative algorithms to compute fair linear and kernel SVM's, which solve a sequence of relaxations constructed using a spectral decomposition of the nonconvex constraint.	  Classifiers and rating scores are prone to implicitly codifying biases, which may be present in the training data, against protected classes (i.e., age, gender, or race). So it is important to understand how to design classifiers and scores that prevent discrimination in predictions.  This paper develops computationally tractable algorithms for designing accurate but fair support vector machines (SVM's).   Our approach imposes a constraint on the covariance matrices conditioned on each protected class, which leads to a nonconvex quadratic constraint in the SVM formulation.  We develop iterative algorithms to compute fair linear and kernel SVM's, which solve a sequence of relaxations constructed using a spectral decomposition of the nonconvex constraint.	score:308
How do I use Latent Semantic Analysis(LSA) for document classification, preferably in java?	 But a large-margin model rarely has the flexibility of nonparametric Bayesian models to automatically handle  model complexity from data, especially when latent variables are present~\citep{Jebara:thesis,Zhu:MedLDA09}. In this paper, we intend to bridge this gap using the RegBayes principle.  Specifically, we develop the {\it infinite latent support vector machines} (iLSVM) and {\it multi-task infinite latent support vector machines} (MT-iLSVM), which explore the discriminative large-margin idea to learn infinite latent feature models for classification and multi-task learning~\citep{Argyriou:nips07,Bakker:JMLR03}, respectively.  We show that both models can be readily instantiated from the RegBayes master equation~(\ref{eq:RegBayes-Generic}) by defining appropriate posterior regularization using the large-margin principle, and by employing an appropriate prior. For iLSVM, we use the IBP prior to allow the model to have an unbounded number of latent features {\it a priori}.	 But a large-margin model rarely has the flexibility of nonparametric Bayesian models to automatically handle  model complexity from data, especially when latent variables are present~\citep{Jebara:thesis,Zhu:MedLDA09}. In this paper, we intend to bridge this gap using the RegBayes principle.  Specifically, we develop the {\it infinite latent support vector machines} (iLSVM) and {\it multi-task infinite latent support vector machines} (MT-iLSVM), which explore the discriminative large-margin idea to learn infinite latent feature models for classification and multi-task learning~\citep{Argyriou:nips07,Bakker:JMLR03}, respectively.  We show that both models can be readily instantiated from the RegBayes master equation~(\ref{eq:RegBayes-Generic}) by defining appropriate posterior regularization using the large-margin principle, and by employing an appropriate prior. For iLSVM, we use the IBP prior to allow the model to have an unbounded number of latent features {\it a priori}.	score:308
How much did CNN acquire Zite for?	 \citet{invert-cnn} used upsampling-deconvolutional architectures to invert the hidden activations of feedforward CNNs to the input domain.  In another related work, \citet{what-where} proposed a stacked what-where autoencoder network and demonstrated its promise in unsupervised and semi-supervised settings.  \citet{deconv-recon} showed that CNNs discriminately trained for image classification (e. g., VGGNet~\citep{vggnet}) are almost fully invertible using pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why CNNs are invertible yet.  \textgray{ Our setup is also similar to recent work of Zhang et al. (http://bit.ly/2a6sR) which empirically shows that CNNs are almost fully invertible using pooling switches (it also shows that encouraging this property improves large-scale image classification).	 \citet{invert-cnn} used upsampling-deconvolutional architectures to invert the hidden activations of feedforward CNNs to the input domain.  In another related work, \citet{what-where} proposed a stacked what-where autoencoder network and demonstrated its promise in unsupervised and semi-supervised settings.  \citet{deconv-recon} showed that CNNs discriminately trained for image classification (e. g., VGGNet~\citep{vggnet}) are almost fully invertible using pooling switches. Despite these interesting results, there is no clear theoretical explanation as to why CNNs are invertible yet.  \textgray{ Our setup is also similar to recent work of Zhang et al. (http://bit.ly/2a6sR) which empirically shows that CNNs are almost fully invertible using pooling switches (it also shows that encouraging this property improves large-scale image classification).	score:324
How much did CNN acquire Zite for?	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	score:330
How much did CNN acquire Zite for?	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	score:330
How much did CNN acquire Zite for?	 Most representative progress was made by \newcite{zeng14}, who proposed a CNN-based approach that can deliver quite competitive results without any extra knowledge resource and NLP modules. Following the success of CNN, there are some valuable models  such as multi-window CNN\cite{nguyenrelation}, CR-CNN\cite{dos2015classifying} and NS-depLCNN\cite{xu2015semantic} been proposed recently,  which are all based on CNN structure.  Though some models also based on other structures like MV-RNN\cite{SocherEtAl2012:MVRNN}, FCM\cite{yufactor14} and SDP-LSTM\cite{xu2015classifying}, CNN occupies a leading position.  Despite the success obtained so far, most of the current CNN-based learning approaches to relation learning and classification are weak in modeling temporal patterns. Though SDP-LSTM algorithm utilize recurrent structure on dependency parsing, no further analysis has been shown to compare RNN with CNN models.	 Most representative progress was made by \newcite{zeng14}, who proposed a CNN-based approach that can deliver quite competitive results without any extra knowledge resource and NLP modules. Following the success of CNN, there are some valuable models  such as multi-window CNN\cite{nguyenrelation}, CR-CNN\cite{dos2015classifying} and NS-depLCNN\cite{xu2015semantic} been proposed recently,  which are all based on CNN structure.  Though some models also based on other structures like MV-RNN\cite{SocherEtAl2012:MVRNN}, FCM\cite{yufactor14} and SDP-LSTM\cite{xu2015classifying}, CNN occupies a leading position.  Despite the success obtained so far, most of the current CNN-based learning approaches to relation learning and classification are weak in modeling temporal patterns. Though SDP-LSTM algorithm utilize recurrent structure on dependency parsing, no further analysis has been shown to compare RNN with CNN models.	score:331
How much did CNN acquire Zite for?	 More specifically, we replace the classification stage of the CNN based system, which was a non-linear multi-layer perceptron, by a linear single layer perceptron. Thus, the features learned by the CNNs are trained to be linearly separable. We evaluate the proposed approach on phoneme recognition task on the TIMIT corpus and on large vocabulary continuous speech recognition on the WSJ corpus.	 More specifically, we replace the classification stage of the CNN based system, which was a non-linear multi-layer perceptron, by a linear single layer perceptron. Thus, the features learned by the CNNs are trained to be linearly separable. We evaluate the proposed approach on phoneme recognition task on the TIMIT corpus and on large vocabulary continuous speech recognition on the WSJ corpus.	score:341
Is it true that Fox News has a conservative bias and CNN has a liberal bias?	 However, previous works on binarizing CNNs usually result in severe prediction accuracy degradation. In this paper, we address this issue with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The implementation of the resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations.	 However, previous works on binarizing CNNs usually result in severe prediction accuracy degradation. In this paper, we address this issue with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The implementation of the resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations.	score:373
Is it true that Fox News has a conservative bias and CNN has a liberal bias?	 Applications of CNN may include both non-biological \cite{david2016deeppainter} and biological images \cite{esteva2017dermatologist}.  The aim of this paper is to describe the novel application of CNN to QLF-images obtained during clinical intervention study \cite{van2016dynamics}. Furthermore, we compare the performances of the CNN and several state of the art classification models.  We tested all of these models on three existing plaque assessment scoring systems. We also checked the influence of adding various colour channels on the model performance. Possible differences were explained based on the biological nature of the problem and based on the properties of these models. Previous studies on this topic either focused on only a single plaque scoring system without providing detailed analysis of results \cite{imangaliyev2016deep} or used small dataset of different images and different network architecture \cite{kang2006dental}.	 Applications of CNN may include both non-biological \cite{david2016deeppainter} and biological images \cite{esteva2017dermatologist}.  The aim of this paper is to describe the novel application of CNN to QLF-images obtained during clinical intervention study \cite{van2016dynamics}. Furthermore, we compare the performances of the CNN and several state of the art classification models.  We tested all of these models on three existing plaque assessment scoring systems. We also checked the influence of adding various colour channels on the model performance. Possible differences were explained based on the biological nature of the problem and based on the properties of these models. Previous studies on this topic either focused on only a single plaque scoring system without providing detailed analysis of results \cite{imangaliyev2016deep} or used small dataset of different images and different network architecture \cite{kang2006dental}.	score:382
Is it true that Fox News has a conservative bias and CNN has a liberal bias?	 Furthermore, the content information can also be leveraged by setting the mean and precision of the Gaussian function according to the content. Experiments on some variant of MNIST clearly show our advantages over CNN and locally connected layer. 	Recently, Convolutional Neural Networks (CNN)~\cite{lecun1998gradient,krizhevsky2012imagenet} have attracted great attentions in the communities of computer vision, speech recognition, and natural language processing.  One advantage of CNN is that it has the ability to exploit the local translational invariance, through adopting the local connectivity and weight sharing strategies. For example, in the traditional image recognition problem, since the precise location of a feature is independent of the class labels, the weight sharing strategy can benefit from the location invariant features~\cite{lecun1989generalization}.	 Furthermore, the content information can also be leveraged by setting the mean and precision of the Gaussian function according to the content. Experiments on some variant of MNIST clearly show our advantages over CNN and locally connected layer. 	Recently, Convolutional Neural Networks (CNN)~\cite{lecun1998gradient,krizhevsky2012imagenet} have attracted great attentions in the communities of computer vision, speech recognition, and natural language processing.  One advantage of CNN is that it has the ability to exploit the local translational invariance, through adopting the local connectivity and weight sharing strategies. For example, in the traditional image recognition problem, since the precise location of a feature is independent of the class labels, the weight sharing strategy can benefit from the location invariant features~\cite{lecun1989generalization}.	score:402
Is it true that Fox News has a conservative bias and CNN has a liberal bias?	 The method is integrated into an innovative sparse non-homogeneous pooling layer. A fusion-based detection network is constructed accordingly. The single-sensor data processing units are modularized and are quite flexible. Therefore, the CNN backbones can be directly adapted from the state-of-the-art networks developed for single sensor. In this paper, we are able to switch among VGGnet used by \cite{chen2016multi}, multi-scale network by \cite{cai2016unified} and 3D-CNN by \cite{Apple_VoxelNet}, and benefit from the best single-sensor network.	 The method is integrated into an innovative sparse non-homogeneous pooling layer. A fusion-based detection network is constructed accordingly. The single-sensor data processing units are modularized and are quite flexible. Therefore, the CNN backbones can be directly adapted from the state-of-the-art networks developed for single sensor. In this paper, we are able to switch among VGGnet used by \cite{chen2016multi}, multi-scale network by \cite{cai2016unified} and 3D-CNN by \cite{Apple_VoxelNet}, and benefit from the best single-sensor network.	score:403
Is it true that Fox News has a conservative bias and CNN has a liberal bias?	 In addition, in the CNN-PCA formulation presented here, a convolutional neural network is trained as an explicit transform function that can post-process PCA models quickly. CNN-PCA is shown to provide both unconditional and conditional realizations that honor the geological features present in reference SGeMS geostatistical realizations for a binary channelized system.  Flow statistics obtained through simulation of random CNN-PCA models closely match results for random SGeMS models for a demanding case in which O-PCA models lead to significant discrepancies. Results for history matching are also presented. In this assessment CNN-PCA is applied with derivative-free optimization, and a subspace randomized maximum likelihood method is used to provide multiple posterior models.	 In addition, in the CNN-PCA formulation presented here, a convolutional neural network is trained as an explicit transform function that can post-process PCA models quickly. CNN-PCA is shown to provide both unconditional and conditional realizations that honor the geological features present in reference SGeMS geostatistical realizations for a binary channelized system.  Flow statistics obtained through simulation of random CNN-PCA models closely match results for random SGeMS models for a demanding case in which O-PCA models lead to significant discrepancies. Results for history matching are also presented. In this assessment CNN-PCA is applied with derivative-free optimization, and a subspace randomized maximum likelihood method is used to provide multiple posterior models.	score:403
Is there any research in one-shot fine grained classification?	   As a counterpart of conventional supervised learning,  few-shot learning aims to deal with the situation where only a few training instances are available from each class, while its extreme version, `one-shot learning', tackles the more challenging scenario where only one instance is available for each class.   Typically a few-shot learning setting with $N$ novel classes and $k$ instances from each class is referred to as `N-way, k-shot' learning.    As learning classifiers solely on one or a few examples from each novel class is extremely difficult,  studies on one-/few-shot learning have focused on exploiting data from a set of well labeled available base classes.  Figure~\ref{fig:one-shot} presents an example of one-shot learning task in such a setting.    For one-shot learning, due to the extreme sparseness of the training instances in the novel target classes, a natural learning scheme adopted in the literature is to learn image representations under metric oriented  learning frameworks that categorize instances based on the similarities between pairs of  instances~\cite{koch2015siamese}.	   As learning classifiers solely on one or a few examples from each novel class is extremely difficult,  studies on one-/few-shot learning have focused on exploiting data from a set of well labeled available base classes.  Figure~\ref{fig:one-shot} presents an example of one-shot learning task in such a setting.    For one-shot learning, due to the extreme sparseness of the training instances in the novel target classes, a natural learning scheme adopted in the literature is to learn image representations under metric oriented  learning frameworks that categorize instances based on the similarities between pairs of  instances~\cite{koch2015siamese}.	score:350
Is there any research in one-shot fine grained classification?	 Each class is represented by a small set of training examples. In testing, the meta-classifier only uses the examples of the maintained seen classes (including the newly added classes) on-the-fly for classification and rejection. That is, the learned meta-classifier classifies or rejects a test example by comparing it with its nearest examples from each seen class in $S$.  Based on the comparison results, it determines whether the test example belongs to a seen class or not. If the test example is not classified as any seen class in $S$, it is rejected as unseen. Unlike existing OWL models,  the parameters of the meta-classifier are not trained on the set of seen classes but on a large number of other classes which can share a large number of features with seen and unseen classes, and thus can work with any seen classification and unseen class rejection without re-training.     We can see that the proposed method works like a nearest neighbor classifier (e.g., $k$NN). However, the key difference is that we train a meta-classifier to perform both classification and rejection based on a learned metric and a learned voting mechanism. Also,  $k$NN cannot do rejection on unseen classes.    The main contributions of this paper are as follows.	    We can see that the proposed method works like a nearest neighbor classifier (e.g., $k$NN). However, the key difference is that we train a meta-classifier to perform both classification and rejection based on a learned metric and a learned voting mechanism. Also,  $k$NN cannot do rejection on unseen classes.    The main contributions of this paper are as follows.	score:363
Is there any research in one-shot fine grained classification?	 Classification is performed, as in the few-shot scenario, by finding the nearest class prototype for an embedded query point.  In this paper, we formulate prototypical networks for both the few-shot and zero-shot settings. We draw connections to matching networks in the one-shot setting, and analyze the underlying distance function used in the model.  In particular, we relate prototypical networks to clustering \cite{banerjee2005clustering} in order to justify the use of class means as prototypes when distances are computed with a Bregman divergence, such as squared Euclidean distance. We find empirically that the choice of distance is vital, as Euclidean distance greatly outperforms the more commonly used cosine similarity.	 A naive approach, such as re-training the model on the new data, would severely overfit. While the problem is quite difficult, it has been demonstrated that humans have the ability to perform even one-shot classification, where only a single example of each new class is given, with a high degree of accuracy~\citep{lake2011one}.   Two recent approaches have made significant progress in few-shot learning.  \citet{vinyals2016matching} proposed \emph{matching networks}, which uses an attention mechanism over a learned embedding of the labeled set of examples (the \emph{support set}) to predict classes for the unlabeled points (the \emph{query set}). Matching networks can be interpreted as a weighted nearest-neighbor classifier applied within an embedding space.	score:365
Is there any research in one-shot fine grained classification?	  We use the following formulation of the few-shot classification problem. There is a training set which consists of a large set of classes and we have access to (potentially many) labeled samples from each class in the training set. At test time, the task is to separate $N$ previously unseen classes using a small number $k$ of labeled samples and (potentially many) unlabeled samples from the same $N$ classes. This is called an $N$-way $k$-shot classification task. We follow the recent literature and use the episodic regime of training and evaluation, as we explain in the following section.	  We use the following formulation of the few-shot classification problem. There is a training set which consists of a large set of classes and we have access to (potentially many) labeled samples from each class in the training set. At test time, the task is to separate $N$ previously unseen classes using a small number $k$ of labeled samples and (potentially many) unlabeled samples from the same $N$ classes. This is called an $N$-way $k$-shot classification task. We follow the recent literature and use the episodic regime of training and evaluation, as we explain in the following section.	score:369
Is there any research in one-shot fine grained classification?	g., for a classification task, the feature extractor and classifier are learned and trained at the same time), and thus the learned features can be considered to be optimal in an error minimization sense for the inference task at hand.  One of the most successful strategies in recent years for feature learning is deep learning~\cite{ramos2012texture}, where the feature extraction and inference method for a particular task can be learned and performed within an end-to-end learning process.	g., for a classification task, the feature extractor and classifier are learned and trained at the same time), and thus the learned features can be considered to be optimal in an error minimization sense for the inference task at hand.  One of the most successful strategies in recent years for feature learning is deep learning~\cite{ramos2012texture}, where the feature extraction and inference method for a particular task can be learned and performed within an end-to-end learning process.	score:374
Prove that the eigenvalues of an upper triangular matrix M are the diagonal entries of M?	  Predicting unobserved entries of a partially observed matrix has found wide applicability in several areas, such as recommender systems, computational biology, and computer vision. Many scalable methods with rigorous theoretical guarantees have been developed for algorithms where the matrix is factored into low-rank components, and embeddings are learned for the row and column entities.  While there has been recent research on incorporating explicit side information in the low-rank matrix factorization setting, often implicit information can be gleaned from the data, via higher order interactions among entities. Such implicit information is especially useful in cases where the data is very sparse, as is often the case in real world datasets.	  Predicting unobserved entries of a partially observed matrix has found wide applicability in several areas, such as recommender systems, computational biology, and computer vision. Many scalable methods with rigorous theoretical guarantees have been developed for algorithms where the matrix is factored into low-rank components, and embeddings are learned for the row and column entities.  While there has been recent research on incorporating explicit side information in the low-rank matrix factorization setting, often implicit information can be gleaned from the data, via higher order interactions among entities. Such implicit information is especially useful in cases where the data is very sparse, as is often the case in real world datasets.	score:252
Prove that the eigenvalues of an upper triangular matrix M are the diagonal entries of M?	 This algorithm is motivated by the observation that the Jacobian matrix (of derivatives) of the encoding function provides an estimator of a local Gaussian approximation of the density, i.e., the leading singular vectors of that matrix span the tangent plane of the manifold near which the data density concentrates.  However, a formal justification for this algorithm remains an open problem.   The last step in this development~\citep{Alain+Bengio-ICLR2013} generalized the result from~\citet{Vincent-NC-2011-small} by showing that when a DAE (or a contractive auto-encoder with the contraction on the whole encode/decode reconstruction function) is trained with small Gaussian corruption and squared error loss, it estimates the score (derivative of the log-density) of the underlying data-generating distribution, which is proportional to the difference between reconstruction and input.	 This algorithm is motivated by the observation that the Jacobian matrix (of derivatives) of the encoding function provides an estimator of a local Gaussian approximation of the density, i.e., the leading singular vectors of that matrix span the tangent plane of the manifold near which the data density concentrates.  However, a formal justification for this algorithm remains an open problem.   The last step in this development~\citep{Alain+Bengio-ICLR2013} generalized the result from~\citet{Vincent-NC-2011-small} by showing that when a DAE (or a contractive auto-encoder with the contraction on the whole encode/decode reconstruction function) is trained with small Gaussian corruption and squared error loss, it estimates the score (derivative of the log-density) of the underlying data-generating distribution, which is proportional to the difference between reconstruction and input.	score:273
Prove that the eigenvalues of an upper triangular matrix M are the diagonal entries of M?	 Using matrix completion theory, it can be surmised that relatively less number of samples are required for the Gram matrix completion. Numerical experiments in \cite{lai2017solve} confirm this observation. In \cite{javanmard2013localization}, the authors consider a theoretical analysis of a specific instance of localization problem and propose an algorithm similar to  \eqref{eq:nnm_EDG}.  The paper considers a random geometric model and derives  interesting results of bound of errors in reconstructing point coordinates. While the localization problem and EDG problem share a common theme, we remark that the EDG problem is different and our analysis adopts the matrix completion framework.  The main task of this paper is a theoretical analysis of the above minimization problem.	 In particular, the measurements in \eqref{eq:nnm_EDG} are with respect to the Gram matrix while the measurements in the work of \cite{candes2009exact,recht2010guaranteed} are entry wise sampling of the distance matrix.  The emphasis in this paper is on theoretical understanding of the nuclear norm minimization formulation for the EDG problem as stated in \eqref{eq:nnm_EDG}.  In particular, the goal is to provide rigorous probabilistic guarantees that give precise estimates for the minimum number of measurements needed for a certain success probability of the recovery algorithm.    \paragraph{Main Challenges} The random linear constraints in \eqref{eq:nnm_EDG} can be equivalently written as a linear map $\mathcal{L}$ acting on $\X$.	score:275
Prove that the eigenvalues of an upper triangular matrix M are the diagonal entries of M?	 \begin{normalsize} We present a novel algebraic combinatorial view on low-rank matrix completion based on studying relations between a few entries with tools from algebraic geometry and matroid theory. The intrinsic locality of the approach allows for the treatment of single entries in a closed theoretical and practical framework. More specifically, apart from introducing an algebraic combinatorial theory of low-rank matrix completion, we present probability-one algorithms to decide whether a particular entry of the matrix can be completed.  We also describe methods to complete that entry from a few others, and to estimate the error which is incurred by any method completing that entry. Furthermore, we show how known results on matrix completion and their sampling assumptions can be related to our new perspective and interpreted in terms of a completability phase transition. \end{normalsize} 		 \begin{normalsize} We present a novel algebraic combinatorial view on low-rank matrix completion based on studying relations between a few entries with tools from algebraic geometry and matroid theory. The intrinsic locality of the approach allows for the treatment of single entries in a closed theoretical and practical framework. More specifically, apart from introducing an algebraic combinatorial theory of low-rank matrix completion, we present probability-one algorithms to decide whether a particular entry of the matrix can be completed.  We also describe methods to complete that entry from a few others, and to estimate the error which is incurred by any method completing that entry. Furthermore, we show how known results on matrix completion and their sampling assumptions can be related to our new perspective and interpreted in terms of a completability phase transition. \end{normalsize} 		score:279
Prove that the eigenvalues of an upper triangular matrix M are the diagonal entries of M?	 Suppose a given observation matrix can be decomposed as the sum of a low-rank matrix and a sparse matrix (outliers),  and the goal is to recover these individual components from the observed sum. Such additive decompositions have applications in a variety of numerical problems including system identification, latent variable graphical modeling, and principal components analysis.  We study conditions under which recovering such a decomposition is possible via a combination of $\ell_1$ norm and trace norm minimization. We are specifically interested in the question of how many outliers are allowed so that convex programming can still achieve accurate recovery, and we obtain stronger recovery guarantees than previous studies. Moreover, we do not assume that the spatial pattern of outliers is random, which stands in contrast to related analyses under such assumptions via matrix completion.  	This work studies additive decompositions of matrices into sparse (outliers) and low-rank components. Such decompositions have found applications in a variety of numerical problems, including system identification~\citep{CSPW09}, latent variable graphical modeling~\citep{CPW10}, and principal component analysis~\citep{CLMW09}. In these settings, the user has an input matrix $Y \in \R^{m \times n}$ which is believed to be the sum of a sparse matrix $\XS$ and a low-rank matrix $\XL$.  For instance, in the application to principal component analysis, $\XL$ represents a matrix of $m$ data points from a low-dimensional subspace of $\R^n$, and is corrupted by a sparse matrix $\XS$ of errors before being observed as \[ \begin{array}{ccccc} Y & = & \XS & + & \XL . \\ & & \text{\scriptsize{(sparse)}} & & \text{\scriptsize{(low-rank)}} \\ \end{array} \] The goal is to recover the original data matrix $\XL$ (and the error components $\XS$) from the corrupted observations $Y$.	 	This work studies additive decompositions of matrices into sparse (outliers) and low-rank components. Such decompositions have found applications in a variety of numerical problems, including system identification~\citep{CSPW09}, latent variable graphical modeling~\citep{CPW10}, and principal component analysis~\citep{CLMW09}. In these settings, the user has an input matrix $Y \in \R^{m \times n}$ which is believed to be the sum of a sparse matrix $\XS$ and a low-rank matrix $\XL$.	score:283
What is sparse matrix?	   \item \textit{Joint Sparsity and Rank-One:} The covariance matrix can be approximated by a jointly sparse and rank-one matrix. This has received much attention in recent development of sparse PCA, and is closely related to sparse signal recovery from magnitude measurements (called \emph{sparse phase retrieval}).  \end{itemize}    In this paper, we wish to reconstruct an unknown covariance matrix $\bSigma\in\mathbb{R}^{n\times n}$ with the above structure from a small number of rank-one measurements.  In particular, we explore  sampling methods of the form  \begin{align} y_{i} & =\ba_{i}^{\top}\bSigma\ba_{i}+\eta_{i},\quad i=1,\ldots,m,\label{measurements} \end{align} where $\boldsymbol{y}:=\left\{ y_{i}\right\} _{i=1}^{m}$ denotes the measurements, $\ba_{i}\in\mathbb{R}^{n}$ represents the sensing vector, $\bfeta:=\left\{ \eta_{i}\right\} _{i=1}^{m}$ stands for the noise term, and $m$ is the number of measurements.	   \item \textit{Joint Sparsity and Rank-One:} The covariance matrix can be approximated by a jointly sparse and rank-one matrix. This has received much attention in recent development of sparse PCA, and is closely related to sparse signal recovery from magnitude measurements (called \emph{sparse phase retrieval}).  \end{itemize}    In this paper, we wish to reconstruct an unknown covariance matrix $\bSigma\in\mathbb{R}^{n\times n}$ with the above structure from a small number of rank-one measurements.  In particular, we explore  sampling methods of the form  \begin{align} y_{i} & =\ba_{i}^{\top}\bSigma\ba_{i}+\eta_{i},\quad i=1,\ldots,m,\label{measurements} \end{align} where $\boldsymbol{y}:=\left\{ y_{i}\right\} _{i=1}^{m}$ denotes the measurements, $\ba_{i}\in\mathbb{R}^{n}$ represents the sensing vector, $\bfeta:=\left\{ \eta_{i}\right\} _{i=1}^{m}$ stands for the noise term, and $m$ is the number of measurements.	score:454
What is sparse matrix?	   The sparse low-rank (SLR) formulation in \eqref{eq::SLR} is different from the low-rank + sparse decomposition \cite{Candes2011}, also known as the robust principal component analysis (RPCA). Both the SLR and the RPCA formulations utilize the nuclear norm and the $\ell_1$ norm as sparsity-inducing regularizers \cite{Zhou2014,Zhou2011}. The RPCA formulation aims to estimate the matrix, which is the sum of a low-rank and a sparse matrix.  Note that, in the case of RPCA, the matrix to be estimated is itself neither sparse or low-rank \cite{Chandrasekaran2009,Chandrasekaran2009J}. In contrast, the SLR problem \eqref{eq::SLR}, and the one proposed in this paper, considers the case wherein the matrix to be estimated is simultaneously sparse and low-rank (similar to \cite{Giampouras2016}).	   The sparse low-rank (SLR) formulation in \eqref{eq::SLR} is different from the low-rank + sparse decomposition \cite{Candes2011}, also known as the robust principal component analysis (RPCA). Both the SLR and the RPCA formulations utilize the nuclear norm and the $\ell_1$ norm as sparsity-inducing regularizers \cite{Zhou2014,Zhou2011}. The RPCA formulation aims to estimate the matrix, which is the sum of a low-rank and a sparse matrix.  Note that, in the case of RPCA, the matrix to be estimated is itself neither sparse or low-rank \cite{Chandrasekaran2009,Chandrasekaran2009J}. In contrast, the SLR problem \eqref{eq::SLR}, and the one proposed in this paper, considers the case wherein the matrix to be estimated is simultaneously sparse and low-rank (similar to \cite{Giampouras2016}).	score:474
What is sparse matrix?	   One can attempt to solve  the robust tensor problem  in \eqref{eqn:robust-def}  using matrix methods. In other words, robust matrix PCA can be applied either to  each matrix slice of the tensor, or to the matrix obtained by flattening the tensor. However, such matrix methods ignore the  tensor algebraic constraints or the CP  rank constraints, which differ from the matrix rank constraints.  There are however a number of challenges to incorporating the tensor CP rank   constraints. Enforcing a given  tensor rank   is NP-hard~\cite{hillar2013most}, unlike the matrix case, where low rank projections can be computed efficiently. Moreover, finding the best convex relaxation of the tensor CP rank is also NP-hard~\cite{hillar2013most}, unlike   the matrix case, where the convex relaxation of the rank, \viz the nuclear norm,  can be computed efficiently.	   One can attempt to solve  the robust tensor problem  in \eqref{eqn:robust-def}  using matrix methods. In other words, robust matrix PCA can be applied either to  each matrix slice of the tensor, or to the matrix obtained by flattening the tensor. However, such matrix methods ignore the  tensor algebraic constraints or the CP  rank constraints, which differ from the matrix rank constraints.  There are however a number of challenges to incorporating the tensor CP rank   constraints. Enforcing a given  tensor rank   is NP-hard~\cite{hillar2013most}, unlike the matrix case, where low rank projections can be computed efficiently. Moreover, finding the best convex relaxation of the tensor CP rank is also NP-hard~\cite{hillar2013most}, unlike   the matrix case, where the convex relaxation of the rank, \viz the nuclear norm,  can be computed efficiently.	score:479
What is sparse matrix?	 Basically, robust PCA is a joint sparse and low-rank recovery problem, which seeks to identify a low-dimensional structure from grossly corrupted observations. Existing works using different combinations of nonconvex sparse and low-rank penalties for robust PCA include [9], [193]--[197].  Among these topics, CS, sparse regression, sparse signals separation, and sparse PCA are sparse vector recovery problems, large sparse covariance and inverse covariance matrices estimation are sparse matrix recovery problems, whilst matrix completion and robust PCA are low-rank recovery problems.  To be more precise, sparse PCA is not a vector recovery problem, but in many popular greedy methods, the PCs are estimated in a vector-by-vector manner. Meanwhile, robust PCA is a joint sparse and low-rank recovery problem.  In this paper, we also provide some critical perspectives. As it is often the case that, when new techniques are introduced, there may also be some overexcitement and abuse.	 In recent, nonconvex regularization based sparse and low-rank recovery is of considerable interest and it in fact is a main driver of the recent progress in nonconvex and nonsmooth optimization. This paper gives an overview of this topic in various fields in signal processing, statistics and machine learning, including compressive sensing (CS), sparse regression and variable selection, sparse signals separation, sparse principal component analysis (PCA), large covariance and inverse covariance matrices estimation, matrix completion, and robust PCA.	score:480
What is sparse matrix?	} over the probability mass distribution of the elements of the matrix. If the actual distribution of the elements greatly differs from this assumption, then the data structures devised for sparse matrices become inefficient. Hence, sparsity can be a too constrained assumption for some applications of current interest, e.g., representation of quantized neural networks.     In this work, we alleviate the shortcomings of sparse representations by considering a more relaxed prior over the distribution of the matrix elements. More precisely, we assume that the empirical probability mass distribution of the matrix elements has a low entropy value as defined by Shannon \cite{Shannon}.  Mathematically, sparsity can be considered a subclass of the general family of low entropic distributions.	} over the probability mass distribution of the elements of the matrix. If the actual distribution of the elements greatly differs from this assumption, then the data structures devised for sparse matrices become inefficient. Hence, sparsity can be a too constrained assumption for some applications of current interest, e.g., representation of quantized neural networks.     In this work, we alleviate the shortcomings of sparse representations by considering a more relaxed prior over the distribution of the matrix elements. More precisely, we assume that the empirical probability mass distribution of the matrix elements has a low entropy value as defined by Shannon \cite{Shannon}.  Mathematically, sparsity can be considered a subclass of the general family of low entropic distributions.	score:480
What is a sparse matrix?	   \item \textit{Joint Sparsity and Rank-One:} The covariance matrix can be approximated by a jointly sparse and rank-one matrix. This has received much attention in recent development of sparse PCA, and is closely related to sparse signal recovery from magnitude measurements (called \emph{sparse phase retrieval}).  \end{itemize}    In this paper, we wish to reconstruct an unknown covariance matrix $\bSigma\in\mathbb{R}^{n\times n}$ with the above structure from a small number of rank-one measurements.  In particular, we explore  sampling methods of the form  \begin{align} y_{i} & =\ba_{i}^{\top}\bSigma\ba_{i}+\eta_{i},\quad i=1,\ldots,m,\label{measurements} \end{align} where $\boldsymbol{y}:=\left\{ y_{i}\right\} _{i=1}^{m}$ denotes the measurements, $\ba_{i}\in\mathbb{R}^{n}$ represents the sensing vector, $\bfeta:=\left\{ \eta_{i}\right\} _{i=1}^{m}$ stands for the noise term, and $m$ is the number of measurements.	 As we will show in this paper, covariance estimation from compressive measurements can be highly robust.  When the covariance matrix is assumed to be approximately sparse, recent work \cite{tian2012cyclic,Leus2011} explored reconstruction of second-order statistics of a  cyclostationary signal from random linear measurements, by $\ell_{1}$-minimization without performance guarantees.  Other spectral prior information has been considered as well in \cite{romero2013wideband} for stationary processes. These problem setups are quite different from \eqref{measurements} in the current work. Another work by Dasarathy et al. \cite{dasarathy2013sketching} proposed estimating an approximately sparse covariance matrix from measurements of the form $\boldsymbol{Y}=\boldsymbol{A}\bSigma\boldsymbol{A}^{\top}$, where $\boldsymbol{A}\in\mathbb{R}^{m\times n}$ denotes the sketching matrix constructed from expander graphs.	score:439
What is a sparse matrix?	  For a dataset $\X \in \R^{m \times N}$, containing $N$ data vectors of $m$ dimensions, one computes a representation $\X \approx \X \V$, where $\V \in \R^{N \times N}$ is a sparse matrix with zeros along the diagonal. The resulting sparse matrix $\V$ can be interpreted as an affinity matrix, where data vectors that lie in the same subspace (or cluster) are presumed to use one another in their sparse representations.  The dataset is then clustered by applying spectral clustering methods \cite{graphcuts} to the graph Laplacian of $\A = | \V | + |\V^T|$.  While self-expressive methods like SSC and LRR provide a principled segmentation of $\X$ into low-dimensional subspaces (subspace clustering), applying these methods to big datasets is challenging. Both SSC and LRR require the construction and  storage of an $N \times N$ affinity matrix for a dataset of size $N$.	  For a dataset $\X \in \R^{m \times N}$, containing $N$ data vectors of $m$ dimensions, one computes a representation $\X \approx \X \V$, where $\V \in \R^{N \times N}$ is a sparse matrix with zeros along the diagonal. The resulting sparse matrix $\V$ can be interpreted as an affinity matrix, where data vectors that lie in the same subspace (or cluster) are presumed to use one another in their sparse representations.  The dataset is then clustered by applying spectral clustering methods \cite{graphcuts} to the graph Laplacian of $\A = | \V | + |\V^T|$.  While self-expressive methods like SSC and LRR provide a principled segmentation of $\X$ into low-dimensional subspaces (subspace clustering), applying these methods to big datasets is challenging. Both SSC and LRR require the construction and  storage of an $N \times N$ affinity matrix for a dataset of size $N$.	score:451
What is a sparse matrix?	} over the probability mass distribution of the elements of the matrix. If the actual distribution of the elements greatly differs from this assumption, then the data structures devised for sparse matrices become inefficient. Hence, sparsity can be a too constrained assumption for some applications of current interest, e.g., representation of quantized neural networks.     In this work, we alleviate the shortcomings of sparse representations by considering a more relaxed prior over the distribution of the matrix elements. More precisely, we assume that the empirical probability mass distribution of the matrix elements has a low entropy value as defined by Shannon \cite{Shannon}.  Mathematically, sparsity can be considered a subclass of the general family of low entropic distributions.	} over the probability mass distribution of the elements of the matrix. If the actual distribution of the elements greatly differs from this assumption, then the data structures devised for sparse matrices become inefficient. Hence, sparsity can be a too constrained assumption for some applications of current interest, e.g., representation of quantized neural networks.     In this work, we alleviate the shortcomings of sparse representations by considering a more relaxed prior over the distribution of the matrix elements. More precisely, we assume that the empirical probability mass distribution of the matrix elements has a low entropy value as defined by Shannon \cite{Shannon}.  Mathematically, sparsity can be considered a subclass of the general family of low entropic distributions.	score:452
What is a sparse matrix?	   The sparse low-rank (SLR) formulation in \eqref{eq::SLR} is different from the low-rank + sparse decomposition \cite{Candes2011}, also known as the robust principal component analysis (RPCA). Both the SLR and the RPCA formulations utilize the nuclear norm and the $\ell_1$ norm as sparsity-inducing regularizers \cite{Zhou2014,Zhou2011}. The RPCA formulation aims to estimate the matrix, which is the sum of a low-rank and a sparse matrix.  Note that, in the case of RPCA, the matrix to be estimated is itself neither sparse or low-rank \cite{Chandrasekaran2009,Chandrasekaran2009J}. In contrast, the SLR problem \eqref{eq::SLR}, and the one proposed in this paper, considers the case wherein the matrix to be estimated is simultaneously sparse and low-rank (similar to \cite{Giampouras2016}).	   The sparse low-rank (SLR) formulation in \eqref{eq::SLR} is different from the low-rank + sparse decomposition \cite{Candes2011}, also known as the robust principal component analysis (RPCA). Both the SLR and the RPCA formulations utilize the nuclear norm and the $\ell_1$ norm as sparsity-inducing regularizers \cite{Zhou2014,Zhou2011}. The RPCA formulation aims to estimate the matrix, which is the sum of a low-rank and a sparse matrix.  Note that, in the case of RPCA, the matrix to be estimated is itself neither sparse or low-rank \cite{Chandrasekaran2009,Chandrasekaran2009J}. In contrast, the SLR problem \eqref{eq::SLR}, and the one proposed in this paper, considers the case wherein the matrix to be estimated is simultaneously sparse and low-rank (similar to \cite{Giampouras2016}).	score:462
What is a sparse matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.	}   In this paper, we extend the theory of low-rank matrix completion to a collection of multiple and heterogeneous matrices. We first consider general matrix completion setting where we assume that for each matrix its  entries are sampled from natural exponential distributions~\citep{lehmCase98}. In this setting, we may have Gaussian distribution for continuous data; Bernoulli for binary data; Poisson for count-data, etc.  In a second part, we relax the assumption of exponential family distribution for the noise and we do not assume any specific model for the observations.  This approach is more popular and widely used in machine learning.  The proposed estimation procedure is based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix.	score:468
Is nuclear energy considered a renewable or nonrenewable resource? What are the reasons behind this classification?	  These directions span the Fisher subspace of the mixture, a classical concept in Pattern Recognition.  Once these directions are identified, points can be classified according to which component of the distribution generated them, and hence all parameters of the mixture can be learned.  What separates this paper from previous work on learning mixtures is that our algorithm is affine-invariant.  Indeed, for every mixture distribution that can be learned using a previously known algorithm, there is a linear transformation of bounded condition number that causes the algorithm to fail.  For $k=2$ components our algorithm has nearly the best possible guarantees (and subsumes all previous results) for clustering Gaussian mixtures.  For $k > 2$, it requires that there be a $(k-1)$-dimensional subspace where the \emph{overlap} of the components is small in every direction (See section \ref{sec:results}).	  These directions span the Fisher subspace of the mixture, a classical concept in Pattern Recognition.  Once these directions are identified, points can be classified according to which component of the distribution generated them, and hence all parameters of the mixture can be learned.  What separates this paper from previous work on learning mixtures is that our algorithm is affine-invariant.  Indeed, for every mixture distribution that can be learned using a previously known algorithm, there is a linear transformation of bounded condition number that causes the algorithm to fail.  For $k=2$ components our algorithm has nearly the best possible guarantees (and subsumes all previous results) for clustering Gaussian mixtures.  For $k > 2$, it requires that there be a $(k-1)$-dimensional subspace where the \emph{overlap} of the components is small in every direction (See section \ref{sec:results}).	score:288
Is nuclear energy considered a renewable or nonrenewable resource? What are the reasons behind this classification?	 This shared knowledge can be regarded as meta-knowledge. Our experiments show a clear advantage of class-level support embedding over previous approaches.  We also provide a statistical view of the proposed class-level embedding. A support set is sampled at random, thus the elements of the support set, regardless of the name, do not provide the optimal (or even good) support of the classes.   Our proposed class support embedding is aimed at correcting this problem. Specifically, during episode training, the class-level embedding sees many sets of points from different classes, which allows it to learn the distribution of the entire space. Thus, given an approximate location of the classes via a random support set, the network produces a corrected set of class representatives, which takes into account the knowledge of the distribution that the network learned from many samples of support sets.	 This shared knowledge can be regarded as meta-knowledge. Our experiments show a clear advantage of class-level support embedding over previous approaches.  We also provide a statistical view of the proposed class-level embedding. A support set is sampled at random, thus the elements of the support set, regardless of the name, do not provide the optimal (or even good) support of the classes.   Our proposed class support embedding is aimed at correcting this problem. Specifically, during episode training, the class-level embedding sees many sets of points from different classes, which allows it to learn the distribution of the entire space. Thus, given an approximate location of the classes via a random support set, the network produces a corrected set of class representatives, which takes into account the knowledge of the distribution that the network learned from many samples of support sets.	score:300
Is nuclear energy considered a renewable or nonrenewable resource? What are the reasons behind this classification?	 A common approach in positive-unlabeled learning is to train a classification model between labeled and unlabeled data. This strategy is in fact known to give an optimal classifier under mild conditions; however, it results in biased empirical estimates of the classifier performance. In this work, we show that the typically used performance measures such as the receiver operating characteristic curve, or the precision-recall curve obtained on such data can be corrected with the knowledge of class priors; i.	 A common approach in positive-unlabeled learning is to train a classification model between labeled and unlabeled data. This strategy is in fact known to give an optimal classifier under mild conditions; however, it results in biased empirical estimates of the classifier performance. In this work, we show that the typically used performance measures such as the receiver operating characteristic curve, or the precision-recall curve obtained on such data can be corrected with the knowledge of class priors; i.	score:308
Is nuclear energy considered a renewable or nonrenewable resource? What are the reasons behind this classification?	  In machine learning, this problem is referred as the two-class classification problem: we need to create a predictive classification model (a classifier) that can identify whether the bit at the current moment is in the shale-rich part of the formation (the first class) or not (the second class). In addition to labeling objects, the classifier can output the probability of the object to belong to a specific class, thus allowing to introduce confidence of predictions.    From the machine learning perspective, the main peculiarities of the problem are missing values in measurements and a relatively high imbalance of classes: there are only 13.5\% of shales and hard-rocks in the available data, where "hard" refers to a measure of the resistance to localized plastic deformation induced by either mechanical indentation or abrasion, and 86.	  In machine learning, this problem is referred as the two-class classification problem: we need to create a predictive classification model (a classifier) that can identify whether the bit at the current moment is in the shale-rich part of the formation (the first class) or not (the second class). In addition to labeling objects, the classifier can output the probability of the object to belong to a specific class, thus allowing to introduce confidence of predictions.    From the machine learning perspective, the main peculiarities of the problem are missing values in measurements and a relatively high imbalance of classes: there are only 13.5\% of shales and hard-rocks in the available data, where "hard" refers to a measure of the resistance to localized plastic deformation induced by either mechanical indentation or abrasion, and 86.	score:309
Is nuclear energy considered a renewable or nonrenewable resource? What are the reasons behind this classification?	  The previous deep domain adaptation methods work under the assumption that the source classifier can be directly transferred to the target domain upon the learned domain-invariant feature representations. This assumption is rather strong as in practical applications, it is often infeasible to check whether the source classifier and target classifier can be shared or not.  Hence we focus in this paper on a more general, and safe, domain adaptation scenario in which the source classifier and target classifier differ by a small perturbation function. The goal of this paper is to simultaneously learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain by embedding the adaptations of both classifiers and features in a unified deep architecture.	  The previous deep domain adaptation methods work under the assumption that the source classifier can be directly transferred to the target domain upon the learned domain-invariant feature representations. This assumption is rather strong as in practical applications, it is often infeasible to check whether the source classifier and target classifier can be shared or not.  Hence we focus in this paper on a more general, and safe, domain adaptation scenario in which the source classifier and target classifier differ by a small perturbation function. The goal of this paper is to simultaneously learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain by embedding the adaptations of both classifiers and features in a unified deep architecture.	score:310
What is the physical significance of rank of matrix?	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	score:399
What is the physical significance of rank of matrix?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:408
What is the physical significance of rank of matrix?	 Low-rank matrix approximations are often used to help scale standard machine learning algorithms to large-scale problems.  Recently, matrix coherence has been used to characterize the ability to extract global information from a subset of matrix entries in the context of these low-rank approximations and other sampling-based algorithms, e.g., matrix completion, robust PCA.   Since coherence is defined in terms of the singular vectors of a matrix and is expensive to compute, the practical significance of these results largely hinges on the following question: \emph{Can we efficiently and accurately estimate the coherence of a matrix?} In this paper we address this question. We propose a novel algorithm for estimating coherence from a small number of columns, formally analyze its behavior, and derive a new coherence-based matrix approximation bound based on this analysis.  We then present extensive experimental results on synthetic and real datasets that corroborate our worst-case theoretical analysis, yet provide strong support for the use of our proposed algorithm whenever low-rank approximation is being considered.  Our algorithm efficiently and accurately estimates matrix coherence across a wide range of datasets, and these coherence estimates are excellent predictors of the effectiveness of sampling-based matrix approximation on a case-by-case basis.	  Since coherence is defined in terms of the singular vectors of a matrix and is expensive to compute, the practical significance of these results largely hinges on the following question: \emph{Can we efficiently and accurately estimate the coherence of a matrix?} In this paper we address this question. We propose a novel algorithm for estimating coherence from a small number of columns, formally analyze its behavior, and derive a new coherence-based matrix approximation bound based on this analysis.	score:409
What is the physical significance of rank of matrix?	 The basic principle of matrix completion  consists in recovering all the entries of an unknown data matrix from incomplete and noisy observations of its entries.  To address the high-dimensionality in matrix completion problem, statistical inference based on low-rank constraint is now an ubiquitous technique for recovering the underlying data matrix.  Thus, matrix completion can be formulated as minimizing the rank of the matrix given a random sample of its entries. However, this rank minimization problem is in general NP-hard due to the combinatorial nature of the rank function~\citep{fazel2001,fazelPhD-2000}. To alleviate this problem and make it tractable, convex relaxation strategies were proposed, e.	  Previous works on collective matrix completion are mainly based on matrix factorization~\citep{srebo2005}. In a nutshell, this approach fits the target matrix as the product of two low-rank matrices. Matrix factorization gives rise to non-convex optimization problems and   its theoretical understanding is quite limited.   For example, \cite{singh2008} proposed the collective matrix factorization that jointly factorizes multiple matrices sharing latent factors.  As in our setting, each matrix can have a different value type and error distribution. In \cite{singh2008}, the authors use Bregman divergences to measure the error and extend standard alternating projection algorithms to this setting. They consider a quite general setting which includes as a particular case the nuclear norm penalization approach that we study in the present paper.	score:414
What is the physical significance of rank of matrix?	 Significant speedup  over the state-of-the-art is observed.  	\IEEEPARstart{L}{ow}-rank matrix  learning  is a central issue in many machine learning and computer vision problems. For example,  matrix completion \cite{candes2009exact}, which is one of the most successful approaches in collaborative filtering, assumes that the target rating matrix is low-rank.   Besides  collaborative filtering, matrix completion has also been used on tasks such as video and image processing \cite{hu2013fast,lu2016nonconvex,gu2016weighted}. Another important use of low-rank matrix learning is robust principal component analysis (RPCA) \cite{candes2011robust},  which assumes that the target  matrix  is  low-rank  and also corrupted by  sparse noise.	 Significant speedup  over the state-of-the-art is observed.  	\IEEEPARstart{L}{ow}-rank matrix  learning  is a central issue in many machine learning and computer vision problems. For example,  matrix completion \cite{candes2009exact}, which is one of the most successful approaches in collaborative filtering, assumes that the target rating matrix is low-rank.   Besides  collaborative filtering, matrix completion has also been used on tasks such as video and image processing \cite{hu2013fast,lu2016nonconvex,gu2016weighted}. Another important use of low-rank matrix learning is robust principal component analysis (RPCA) \cite{candes2011robust},  which assumes that the target  matrix  is  low-rank  and also corrupted by  sparse noise.	score:415
What is an intuitive explanation of the rank of a matrix?	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	  Given a sampled matrix and a rank constraint, any matrix that agrees with the sampled entries and rank constraint is called a completion. A sampled matrix with a rank constraint is finitely completable if and only if there exist only finitely many completions of it. Most literature on matrix completion focus on developing optimization methods to obtain a completion.  For single-view learning, methods including alternating minimization \cite{jain2013low,hardt2014understanding}, convex relaxation of rank \cite{candes,candes2,ashraphijuoc,cai}, etc., have been proposed. One generalization of the matrix completion problem is tensor completion where the number of orders can be more than two and alternating minimization methods \cite{liulow2,wang2016tensor} and other optimization-based methods \cite{liu2016tensor,7347424}, etc.	score:349
What is an intuitive explanation of the rank of a matrix?	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	 A typical instance is the matrix factorisation technique used in recommender systems.    In a recommender system, the factorising matrix is usually the rating matrix whose entries denote the ratings given by the corresponding user (row) to the corresponding item (column), and the factorised matrices are usually regarded as the user feature matrix (the left factorised matrix) and the item feature matrix (the right factorised matrix).  Fig. \ref{movie} shows an example of a simple movie recommender system. In this example, after factorisation by NMF, the distance between ``Star Wars'' and ``Titanic'' becomes less than the distance between ``Star Wars'' and ``Star Trek''. However, as we all know, the movie ``Star Wars'' should be closer to the movie ``Star Trek'' -- both are within the scientific fiction genre, unlike ``Titanic'', which is a love story.	score:365
What is an intuitive explanation of the rank of a matrix?	     The constraints should be reflected in the explanations of instances.      The main contribution in this paper is that we propose an analytic method of a model explanation that is applicable to general classification models.     We introduce the concept of a contribution matrix and an explanation embedding in a constraint space by using the matrix factorization.      We experimentally show that the embedded explanations are well clustered according to the classification category of instances.      By using the explanation embedding, we extract a model explanation of the rule-like form.     We also perform experiments with open datasets as well as an industrial dataset to show the validity of our approach in practice.     In particular, many of industrial datasets, especially defect detection or diagnosis systems, consist of numerical attributes each of them has its own meaning.     Our method therefore will focus on dataset having numeric attributes.	     Our goal in this paper is to propose an analytic method of a model explanation that is applicable to general classification models.     To this end, we introduce the concept of a contribution matrix and an explanation embedding in a constraint space by using a matrix factorization.     We extract a rule-like model explanation from the contribution matrix with the help of the nonnegative matrix factorization.      To validate our method, the experiment results provide with open datasets as well as an industry dataset of a LTE network diagnosis and the results show our method extracts reasonable explanations.  	In recent years, a number of artificial intelligent services have been developed such as defect detection system or diagnosis system for customer services.	score:367
What is an intuitive explanation of the rank of a matrix?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	score:369
What is an intuitive explanation of the rank of a matrix?	 This involves computing the average of the trace norm of each matricization of the tensor \cite{Kolda2009}.   A key insight behind using trace norm regularization for matrix completion is that this norm provides a tight convex relaxation of the rank of a matrix defined on the spectral unit ball \cite{Fazel}. Unfortunately, the extension of this methodology to the more general tensor setting presents some difficulties.  In particular, we shall prove in this paper that the tensor trace norm is not a tight convex relaxation of the tensor rank.    The above negative result stems from the fact that the spectral norm, used to compute the convex relaxation for the trace norm, is not an invariant property of the matricization of a tensor.     This observation leads us to take a different route and study afresh the convex relaxation of tensor rank on the Euclidean ball.	 This involves computing the average of the trace norm of each matricization of the tensor \cite{Kolda2009}.   A key insight behind using trace norm regularization for matrix completion is that this norm provides a tight convex relaxation of the rank of a matrix defined on the spectral unit ball \cite{Fazel}. Unfortunately, the extension of this methodology to the more general tensor setting presents some difficulties.  In particular, we shall prove in this paper that the tensor trace norm is not a tight convex relaxation of the tensor rank.    The above negative result stems from the fact that the spectral norm, used to compute the convex relaxation for the trace norm, is not an invariant property of the matricization of a tensor.     This observation leads us to take a different route and study afresh the convex relaxation of tensor rank on the Euclidean ball.	score:372
Can anyone give me good Google maps apps ideas to implement as a graduation project, I don't mind android ideas or just a regular Web app, and if it relates to machine learning that would be better. Thanks	g., think about a process that harvests text and images from the web and feeds it to a machine learning algorithm). In that context, it is most efficient to simply update the parameters of the model after each example or few examples, as they arrive. This is the ideal {\em online learning} scenario, and in a simplified setting, we can even consider each new example $z$ as being sampled i. i.d. from an unknown generating distribution with probability density $p(z)$.  More realistically, examples in online learning do not arrive i.i.d. but instead from an unknown stochastic process which exhibits serial correlation and other temporal dependencies.  Many learning algorithms rely on gradient-based numerical optimization of a training criterion.	g., think about a process that harvests text and images from the web and feeds it to a machine learning algorithm). In that context, it is most efficient to simply update the parameters of the model after each example or few examples, as they arrive. This is the ideal {\em online learning} scenario, and in a simplified setting, we can even consider each new example $z$ as being sampled i. i.d. from an unknown generating distribution with probability density $p(z)$.  More realistically, examples in online learning do not arrive i.i.d. but instead from an unknown stochastic process which exhibits serial correlation and other temporal dependencies.  Many learning algorithms rely on gradient-based numerical optimization of a training criterion.	score:270
Can anyone give me good Google maps apps ideas to implement as a graduation project, I don't mind android ideas or just a regular Web app, and if it relates to machine learning that would be better. Thanks	 These classifiers are then evaluated using various evaluation methods and their results compared against each other.  The results show that the Naïve Bayes algorithm performs well for our classification problem and can potentially automate app categorization for Android app publishers on Google Play Store. 	Machine Learning has been widely used in various domains to study and learn from patterns in data to make accurate predictions.  The use of machine learning can be seen in our daily lives, especially in email providers for detecting spam messages.  With the increase in data in recent years, understanding and making appropriate decisions can be challenging due to the vast amount of data to be analyzed. Furthermore, because the data is in different forms, making accurate decisions can be overwhelming.	  \subsection{Motivation} Making accurate predictions for developers about what category an app should be uploaded to on Google Play Store will potentially improve the discovery of their applications and revenue at the long run. Classification of apps on the store is a useful application of machine learning. There is an increasing number of research using machine learning to classify text, sentiment analysis and documents with Naïve Bayes such as \cite{Eyheramendy2003,Frank2006,Howedi2014,Ting2011}.  With the increasing amount of data available online, providing useful information from this data creates new knowledge and improves the overall success of businesses \cite{Ting2011}. Since most machine learning classification algorithms are time-consuming and complicated, using Naïve Bayes classification provides a fast and simple way to classify data \cite{Narayanan2013}.	score:270
Can anyone give me good Google maps apps ideas to implement as a graduation project, I don't mind android ideas or just a regular Web app, and if it relates to machine learning that would be better. Thanks	 This module focuses on bringing machine learning to  non-specialists using a general-purpose high-level language  as well as researchers who want to  implement, benchmark, and compare their new methods in a structured environment. Emphasis is put on ease of use,  performance, documentation, and API consistency. 	TensorFlow, a general-purpose numerical computation library open-sourced by Google in November 2015, has flexible implementation and architecture enables users to focus on building the computation graph and deploy the model with little efforts on heterogeous platforms such as mobile devices, hundreds of machines, or thousands of computational devices.	 This module focuses on bringing machine learning to  non-specialists using a general-purpose high-level language  as well as researchers who want to  implement, benchmark, and compare their new methods in a structured environment. Emphasis is put on ease of use,  performance, documentation, and API consistency. 	TensorFlow, a general-purpose numerical computation library open-sourced by Google in November 2015, has flexible implementation and architecture enables users to focus on building the computation graph and deploy the model with little efforts on heterogeous platforms such as mobile devices, hundreds of machines, or thousands of computational devices.	score:272
Can anyone give me good Google maps apps ideas to implement as a graduation project, I don't mind android ideas or just a regular Web app, and if it relates to machine learning that would be better. Thanks	  If we compare this to the way in which many problems in machine learning are phrased, there is a novel element here. Usually, a loss function is designed with a specific problem in mind, and so errors in the performance of that problem are de-facto important. But if we wish to construct an unsupervised technique, it should somehow decide on its own in a way inspired on the dependencies within the data itself what is asymptotically important and what errors are irrelevant.  For example, recent advances in image synthesis such as style transfer use error functions constructed out of intermediate layer activations of an object classifier network rather than working at the pixel level, with the result of minimizing perceptually meaningful inconsistencies rather than errors in the raw pixel values \cite{gatys2015neural,johnson2016perceptual}.	  If we compare this to the way in which many problems in machine learning are phrased, there is a novel element here. Usually, a loss function is designed with a specific problem in mind, and so errors in the performance of that problem are de-facto important. But if we wish to construct an unsupervised technique, it should somehow decide on its own in a way inspired on the dependencies within the data itself what is asymptotically important and what errors are irrelevant.  For example, recent advances in image synthesis such as style transfer use error functions constructed out of intermediate layer activations of an object classifier network rather than working at the pixel level, with the result of minimizing perceptually meaningful inconsistencies rather than errors in the raw pixel values \cite{gatys2015neural,johnson2016perceptual}.	score:275
Can anyone give me good Google maps apps ideas to implement as a graduation project, I don't mind android ideas or just a regular Web app, and if it relates to machine learning that would be better. Thanks	 Training deep neural networks requires access to large amounts of  data and computing powers. As a result, these neural networks are often trained by leveraging cheaper, yet more powerful cloud GPU clusters. Once trained, the inference phase can be completed in a reasonable amount of time, e.g., less than one second, using a single machine. Pre-trained models can be hosted for private use or offered as public cloud deep learning services~\cite{ws:googleCloudVision,ws:clarifai}.	 Training deep neural networks requires access to large amounts of  data and computing powers. As a result, these neural networks are often trained by leveraging cheaper, yet more powerful cloud GPU clusters. Once trained, the inference phase can be completed in a reasonable amount of time, e.g., less than one second, using a single machine. Pre-trained models can be hosted for private use or offered as public cloud deep learning services~\cite{ws:googleCloudVision,ws:clarifai}.	score:276
What are some machine learning algorithms I can learn without calculus?	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	 In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner.  We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers.	score:355
What are some machine learning algorithms I can learn without calculus?	  Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted much attention due to their low computational cost in each iteration. However, these algorithms might perform poorly especially if it is hard to approximate the Hessian well and efficiently. As far as we know, there is no effective way to handle this problem.  In this paper, we resort to Nesterov's acceleration technique to improve the convergence performance of a class of second-order methods called approximate Newton. We give a theoretical analysis that Nesterov's acceleration technique can improve the convergence performance for approximate Newton just like for first-order methods. We accordingly propose an accelerated regularized sub-sampled Newton.	  Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted much attention due to their low computational cost in each iteration. However, these algorithms might perform poorly especially if it is hard to approximate the Hessian well and efficiently. As far as we know, there is no effective way to handle this problem.  In this paper, we resort to Nesterov's acceleration technique to improve the convergence performance of a class of second-order methods called approximate Newton. We give a theoretical analysis that Nesterov's acceleration technique can improve the convergence performance for approximate Newton just like for first-order methods. We accordingly propose an accelerated regularized sub-sampled Newton.	score:372
What are some machine learning algorithms I can learn without calculus?	  It is desirable to understand how to include a concept of cost into the optimization procedure.  Second, machine learning experiments are often run in parallel, on multiple cores or machines.  We would like to build Bayesian optimization procedures that can take advantage of this parallelism to reach better solutions more quickly.  In this work, our first contribution is the identification of good practices for Bayesian optimization of machine learning algorithms.	   Machine learning algorithms frequently require careful tuning of   model hyperparameters, regularization terms, and optimization   parameters.  Unfortunately, this tuning is often a ``black art''   that requires expert experience, unwritten rules of thumb, or   sometimes brute-force search. Much more appealing is the idea of   developing automatic approaches which can optimize the performance   of a given learning algorithm to the task at hand.   In this work, we   consider the automatic tuning problem within the framework of   Bayesian optimization, in which a learning algorithm's   generalization performance is modeled as a sample from a Gaussian   process (GP).  The tractable posterior distribution induced by the   GP leads to efficient use of the information gathered by previous   experiments, enabling optimal choices about what parameters to try   next.   Here we show how the effects of the Gaussian process prior   and the associated inference procedure can have a large impact on   the success or failure of Bayesian optimization.  We show that   thoughtful choices can lead to results that exceed expert-level   performance in tuning machine learning algorithms.  We also describe   new algorithms that take into account the variable cost (duration)   of learning experiments and that can leverage the presence of   multiple cores for parallel experimentation.	score:378
What are some machine learning algorithms I can learn without calculus?	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	score:381
What are some machine learning algorithms I can learn without calculus?	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	score:383
What are some books which will help me in understanding machine learning mathematical concepts?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:346
What are some books which will help me in understanding machine learning mathematical concepts?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:349
What are some books which will help me in understanding machine learning mathematical concepts?	  A number of heuristics have been developed to help with such choices, but the complicated nature of the geometric relationships involved means these are imperfect and can sometimes make poor choices.   We investigate the use of machine learning (specifically support vector machines) to make such choices instead.    Machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data.  In this paper we apply it in two case studies: the first to select between heuristics for choosing a CAD variable ordering; the second to identify when a CAD problem instance would benefit from Gr\"obner Basis preconditioning.  These appear to be the first such applications of machine learning to Symbolic Computation.  We demonstrate in both cases that the machine learned choice outperforms human developed heuristics.	  A number of heuristics have been developed to help with such choices, but the complicated nature of the geometric relationships involved means these are imperfect and can sometimes make poor choices.   We investigate the use of machine learning (specifically support vector machines) to make such choices instead.    Machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data.  In this paper we apply it in two case studies: the first to select between heuristics for choosing a CAD variable ordering; the second to identify when a CAD problem instance would benefit from Gr\"obner Basis preconditioning.  These appear to be the first such applications of machine learning to Symbolic Computation.  We demonstrate in both cases that the machine learned choice outperforms human developed heuristics.	score:353
What are some books which will help me in understanding machine learning mathematical concepts?	  We compare the performance of our proposed model with four widely adopted machine learning algorithms, and illustrate the gain offered by our model.  The remainder of this paper is organized as follows.  In Section~\ref{sec:machine_learning_based_malware_detection}, we review the existing efforts on machine-learning-based malicious application detection. In Section~\ref{sec:framework}, we describe the proposed deep-learning-based model.  In Section~\ref{sec:performance_comparison}, we compare the performance of four widely used machine learning algorithms with the proposed model.  Section~\ref{sec:conclusion} concludes the paper.	  We compare the performance of our proposed model with four widely adopted machine learning algorithms, and illustrate the gain offered by our model.  The remainder of this paper is organized as follows.  In Section~\ref{sec:machine_learning_based_malware_detection}, we review the existing efforts on machine-learning-based malicious application detection. In Section~\ref{sec:framework}, we describe the proposed deep-learning-based model.  In Section~\ref{sec:performance_comparison}, we compare the performance of four widely used machine learning algorithms with the proposed model.  Section~\ref{sec:conclusion} concludes the paper.	score:356
What are some books which will help me in understanding machine learning mathematical concepts?	    Our thesis is that the best heuristic to use is dependent upon the problem considered.  However, the relationship between the problems and heuristics is far from obvious   and so we investigate whether machine learning can help with these choices.  Machine learning is a branch of artificial intelligence. It uses statistical methods to infer information from supplied data which is then used to make predictions for previously unseen data \cite{alpaydin2004introduction}.   We have applied machine learning (specifically a support vector machine) to the problem of selecting a variable ordering for both CAD itself and quantifier elimination by CAD, using the nlsat dataset \cite{nlsat} of fully existentially quantified problems.  Our results show that the choices made by machine learning are on average superior to both any individual heuristic and to picking a heuristic at random.	    Our thesis is that the best heuristic to use is dependent upon the problem considered.  However, the relationship between the problems and heuristics is far from obvious   and so we investigate whether machine learning can help with these choices.  Machine learning is a branch of artificial intelligence. It uses statistical methods to infer information from supplied data which is then used to make predictions for previously unseen data \cite{alpaydin2004introduction}.   We have applied machine learning (specifically a support vector machine) to the problem of selecting a variable ordering for both CAD itself and quantifier elimination by CAD, using the nlsat dataset \cite{nlsat} of fully existentially quantified problems.  Our results show that the choices made by machine learning are on average superior to both any individual heuristic and to picking a heuristic at random.	score:363
Should economists learn machine learning?	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.    \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	  In this position paper, I first describe a new perspective on machine learning (\textbf{ML}) by  four basic problems (or levels), namely, {\it ``What to learn?''}, {\it ``How to learn?''}, {\it ``What to evaluate?''},  and {\it ``What to adjust?''}. The paper stresses more on the first level of {\it ``What to learn?''},  or {\it ``Learning Target Selection''}.  Towards this primary problem within the four levels,  I briefly review the existing studies about the connection between information theoretical learning  (\textbf{ITL} \cite{Principe}) and machine learning.  A theorem is given on the relation between the empirically-defined similarity measure and  information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.    \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.	score:398
Should economists learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:418
Should economists learn machine learning?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:418
Should economists learn machine learning?	 Meta-learning, or \emph{learning to learn}, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or \emph{meta-data}, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way.  In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field. 	When we learn new skills, we rarely - if ever - start from scratch. We start from skills learned earlier in related tasks, reuse approaches that worked well before, and focus on what is likely worth trying based on experience \citep{lake2017building}.	 Meta-learning, or \emph{learning to learn}, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or \emph{meta-data}, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way.  In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field. 	When we learn new skills, we rarely - if ever - start from scratch. We start from skills learned earlier in related tasks, reuse approaches that worked well before, and focus on what is likely worth trying based on experience \citep{lake2017building}.	score:426
Should economists learn machine learning?	 It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, ``learning to learn'' as they do so.  In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal.   Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm.  Our aim is to learn new internal representations as the algorithm learns new target functions,  that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data.	 	Machine learning has developed a deep mathematical understanding as well as powerful practical methods for the problem of learning a single target function from large amounts of labeled data. Yet if we wish to produce machine learning systems that persist in the world, we need methods for continually learning many tasks over time and that,  like humans \cite{Gop01}, improve their ability to learn as they do so, needing less data (per task) as they learn more.   A natural approach for tackling this goal (called ``life-long learning''~\cite{Thrun96b,ThrunM95}  or ``transfer learning''~\cite{aep08,pm:13} or ``learning to learn''~\cite{Baxter97,TP97}) is to use information from previously-learned tasks to {\em  improve the underlying representation} used by the learning algorithm, under the hope or belief that some kinds of commonalities across tasks exist.	score:432
What is your opinion about Donald Trump refusing to answer a CNN reporter, calling the network "fake news"?	 Deep learning\blfootnote{$^*$ These authors contributed equally to this work}, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a `good' architecture. The existing works tend to focus on reporting CNN architectures that work well  for face recognition rather than investigate the reason.   In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible.  Specifically, we  use public database LFW (Labeled Faces in the Wild) to train CNNs,  unlike most existing CNNs trained on private databases.  We propose three CNN architectures which are the first reported architectures trained using LFW data.	 Deep learning\blfootnote{$^*$ These authors contributed equally to this work}, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a `good' architecture. The existing works tend to focus on reporting CNN architectures that work well  for face recognition rather than investigate the reason.   In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible.  Specifically, we  use public database LFW (Labeled Faces in the Wild) to train CNNs,  unlike most existing CNNs trained on private databases.  We propose three CNN architectures which are the first reported architectures trained using LFW data.	score:403
What is your opinion about Donald Trump refusing to answer a CNN reporter, calling the network "fake news"?	  For example, a tweet saying ``\textit{Vote Trump!}'' shows a positive sentiment link from the poster to Donald Trump, and ``\textit{Trump is mad...}'' indicates the opposite case.    For a given sentiment link, we define its \textit{sign} to be positive or negative depending on whether its related content expresses a positive or negative attitude from the generator of the link to the recipient \cite{leskovec2010predicting}, and all such sentiment links form a new network topology called \textit{sentiment network}.	  For example, a tweet saying ``\textit{Vote Trump!}'' shows a positive sentiment link from the poster to Donald Trump, and ``\textit{Trump is mad...}'' indicates the opposite case.    For a given sentiment link, we define its \textit{sign} to be positive or negative depending on whether its related content expresses a positive or negative attitude from the generator of the link to the recipient \cite{leskovec2010predicting}, and all such sentiment links form a new network topology called \textit{sentiment network}.	score:415
What is your opinion about Donald Trump refusing to answer a CNN reporter, calling the network "fake news"?	 Also, some representation that does not make any sense for human may work for CNN~\cite{DNN_fool}. One thing obvious is that CNN learns all the things from the data fed into it. This implies what CNN recognizes as a noise comes from the data set. In this paper, we mainly focus on how to add the effective noise that disrupts CNN's recognition for the purpose of privacy protection.   Differential privacy~\cite{cynthia} is the mathematical definition that has been introduced to measure the privacy loss for the domains that handle massive user data such as data mining applications.  Google~\cite{rappor} and Apple~\cite{apple} have adopted the differential privacy techniques into their crowdsourced applications with some data sketching technique to approximate users' private data.	 Also, some representation that does not make any sense for human may work for CNN~\cite{DNN_fool}. One thing obvious is that CNN learns all the things from the data fed into it. This implies what CNN recognizes as a noise comes from the data set. In this paper, we mainly focus on how to add the effective noise that disrupts CNN's recognition for the purpose of privacy protection.   Differential privacy~\cite{cynthia} is the mathematical definition that has been introduced to measure the privacy loss for the domains that handle massive user data such as data mining applications.  Google~\cite{rappor} and Apple~\cite{apple} have adopted the differential privacy techniques into their crowdsourced applications with some data sketching technique to approximate users' private data.	score:421
What is your opinion about Donald Trump refusing to answer a CNN reporter, calling the network "fake news"?	  Second, people are topic-sensitive in news reading as they are usually interested in multiple specific news categories (see Section \ref{sec:cs}).  How to dynamically measure a user's interest based on his diversified reading history for current candidate news is key to news recommender systems.  Third, news language is usually highly condensed and comprised of a large amount of knowledge entities and common sense.   For example, as shown in Figure \ref{fig:buzzfeed}, a user clicks a piece of news with title ``\textsf{Boris Johnson Has Warned Donald Trump To Stick To The Iran Nuclear Deal}" that contains four knowledge entities: ``\textsf{Boris Johnson}'', ``\textsf{Donald Trump}'', ``\textsf{Iran}'' and ``\textsf{Nuclear}''.  In fact, the user may also be interested in another piece of news with title ``\textsf{North Korean EMP Attack Would Cause Mass U.	  Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users.  In general, news language is highly condensed, full of knowledge entities and common sense.  However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news.   The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably.    To solve the above problem, in this paper, we propose a \textit{deep knowledge-aware network} (DKN) that incorporates knowledge graph representation into news recommendation.  DKN is a content-based deep recommendation framework for click-through rate prediction.	score:427
What is your opinion about Donald Trump refusing to answer a CNN reporter, calling the network "fake news"?	   A Convolutional Neural Network (CNN) is a variant of a Deep Neural Network (DNN) \cite{110_deeplearning_lenet}. Inspired by the visual cortex of animals, CNNs are applied to state-of-the-art applications, including computer vision and speech recognition \cite{deng2014deep}. However, supervised training of CNNs is computationally demanding and time consuming, and in many cases, several weeks are required to complete a training session.  Often applications are tested with different parameters, and each test requires a full session of training.   Multi-core processors~\cite{Williams:2009} and in particular many-core~\cite{benkner11} processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) \cite{nvidia_gpu} or the Intel Xeon Phi \cite{chrysos2012intel} co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs.	   A Convolutional Neural Network (CNN) is a variant of a Deep Neural Network (DNN) \cite{110_deeplearning_lenet}. Inspired by the visual cortex of animals, CNNs are applied to state-of-the-art applications, including computer vision and speech recognition \cite{deng2014deep}. However, supervised training of CNNs is computationally demanding and time consuming, and in many cases, several weeks are required to complete a training session.  Often applications are tested with different parameters, and each test requires a full session of training.   Multi-core processors~\cite{Williams:2009} and in particular many-core~\cite{benkner11} processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) \cite{nvidia_gpu} or the Intel Xeon Phi \cite{chrysos2012intel} co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs.	score:431
Is there any machine learning algorithm that can learn how to add any two numbers?	  	The statistical comparison of competing algorithms is fundamental in machine learning; it is typically carried through hypothesis testing. As an example in this paper we assume that one is interested in comparing the accuracy of two competing classifiers. However our discussion readily applies to any other measure of performance.  Assume that two classifiers have been assessed via cross-validation on a single data set.  The recommended approach for comparing them is the correlated t-test \citep{nadeau2003inference}. If instead one aims at comparing two classifiers on multiple data sets the recommended test is the signed-rank test \citep{demvsar2006statistical}. Both tests are based on the frequentist framework of the null-hypothesis significance tests (nhst), which has severe drawbacks.	  	The statistical comparison of competing algorithms is fundamental in machine learning; it is typically carried through hypothesis testing. As an example in this paper we assume that one is interested in comparing the accuracy of two competing classifiers. However our discussion readily applies to any other measure of performance.  Assume that two classifiers have been assessed via cross-validation on a single data set.  The recommended approach for comparing them is the correlated t-test \citep{nadeau2003inference}. If instead one aims at comparing two classifiers on multiple data sets the recommended test is the signed-rank test \citep{demvsar2006statistical}. Both tests are based on the frequentist framework of the null-hypothesis significance tests (nhst), which has severe drawbacks.	score:331
Is there any machine learning algorithm that can learn how to add any two numbers?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:345
Is there any machine learning algorithm that can learn how to add any two numbers?	 Meanwhile, the two techniques can be used only with a specific learning algorithm, for example, kernel-based methods is implemented with support vector machines and some cost sensitive methods with adaptive boosting~\citep{he-2009}.   There are many machine learning algorithms developed already for the classification task but each algorithm typically suits some classification problems better than others; and this is why there is no universal data mining method for all problem types~\citep{fayyad-1996}.	 Meanwhile, the two techniques can be used only with a specific learning algorithm, for example, kernel-based methods is implemented with support vector machines and some cost sensitive methods with adaptive boosting~\citep{he-2009}.   There are many machine learning algorithms developed already for the classification task but each algorithm typically suits some classification problems better than others; and this is why there is no universal data mining method for all problem types~\citep{fayyad-1996}.	score:354
Is there any machine learning algorithm that can learn how to add any two numbers?	 One would hope for much stronger theoretical guarantees of success, but no widely used machine learning methods provide them, so it is difficult to know when they will work.  One key question faced by any machine learning system is: what kind of models will it generate? In neural networks one asks whether the architecture is feed-forward or recurrent and how many layers it has.  In genetic programming one asks for the program representation and which functions are built-in. In general, each machine learning system must make this choice. If the class of generated models or programs is too broad, it might be impossible to learn them efficiently. If it is too narrow, it might not suffice for the task at hand. To solve a different task, one might need a different kind of model.	 Then, we show how to iterate these solutions to get reasonably efficient semi-decision procedures with strong theoretical guarantees.  One advantage of a machine learning approach with such strong guarantees is that it can give negative answers to certain questions. For example, our approach can sometimes prove that there is no model in a certain class that is sufficient for the given machine learning task.  A main disadvantage is that this does not scale to large models. Still, it can be used to enrich our understanding of complexity theory and model classes, even when only applied on a small scale. For example, in Section~\ref{subsec:reduc} we show how our approach can be used to prove that certain reductions between complexity classes do not exist.  The rest of the paper is organized as follows.	score:365
Is there any machine learning algorithm that can learn how to add any two numbers?	 Since it is closely connected to the moment problem, Section \ref{sec_log} provides an algorithm for approximating the logarithmic norm, as well as for the logarithmic distance; the latter can be quite useful in machine learning practice with massive heavy-tailed data (either dynamic or static) in lieu of the usual $l_2$ distance.  Entropy is also an important summary statistic.  Recently \cite{Proc:Zhao_IMC07} proposed to approximate the entropy moment $\sum_{i=1}^D x_i\log x_i$ using the $\alpha$th moments with $\alpha = 1\pm\Delta$ and very small $\Delta$.  \vspace{-0.05in} \subsection{Comparisons with Previous Studies}  Pioneered by\cite{Proc:Alon_STOC96}, there have been many studies on approximating the $\alpha$th frequency moment $F_{(\alpha)}$.	   Finally, our another contribution is an algorithm for approximating the logarithmic norm, {\small$\sum_{i=1}^D\log A_t[i]$}, and the logarithmic distance, {\small$\sum_{i=1}^D\log\left|A_t[i] - B_t[i]\right|$}. The logarithmic norm arises in  statistical estimations. The logarithmic distance is  useful in machine learning practice with heavy-tailed data.   	\vspace{-0.05in}  This paper  focuses on {\em counting}, which is among the most fundamental operations in almost every field of science and engineering. Computing the sum {\small$\sum_{i=1}^DA_t[i]$} is the simplest counting ($t$ denotes time). Counting the {\small$\alpha$th moment $\sum_{i=1}^DA_t[i]^\alpha$} is more general. When {\small$\alpha\rightarrow 0+$, $\sum_{i=1}^DA_i[i]^\alpha$} counts the total number of non-zeros in $A_t$.	score:365
What machine learning algorithms does FiscalNote use?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:282
What machine learning algorithms does FiscalNote use?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:316
What machine learning algorithms does FiscalNote use?	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	 Learning from very few training samples or $k$-shot learning, is an important learning paradigm that is widely believed to be how humans learn new concepts as discussed in \cite{thorpe1996speed} and \cite{li2002rapid}. However, $k$-shot learning still remains a key-challenge in machine learning.   Fine-tuning methods seek to overcome this limitation by leveraging networks that have been pre-trained on large scale data.	score:316
What machine learning algorithms does FiscalNote use?	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	score:317
What machine learning algorithms does FiscalNote use?	  In essence, learning the relationship between an entity and its opposite entity for a given problem is a special case of \textit{a-priori knowledge}, which can be beneficial for computationally intelligent algorithms in stochastic setups. In context of machine learning algorithm, one may ask, why should effort be spent on extraction of the opposite relations when input-output relationship itself is not well defined?  However, various research on this topic has shown that simultaneous analysis of entities and their opposites can accelerate the task in focus -- since it allows the algorithm to harness the knowledge about symmetry in the solution domain thus allowing a better exploration of the solutions. \textit{Opposition-based Differential Evolution (ODE)}, however, seems to be the most successful oppositional inspired algorithm so far \cite{rahnamayan2008opposition}.	  In essence, learning the relationship between an entity and its opposite entity for a given problem is a special case of \textit{a-priori knowledge}, which can be beneficial for computationally intelligent algorithms in stochastic setups. In context of machine learning algorithm, one may ask, why should effort be spent on extraction of the opposite relations when input-output relationship itself is not well defined?  However, various research on this topic has shown that simultaneous analysis of entities and their opposites can accelerate the task in focus -- since it allows the algorithm to harness the knowledge about symmetry in the solution domain thus allowing a better exploration of the solutions. \textit{Opposition-based Differential Evolution (ODE)}, however, seems to be the most successful oppositional inspired algorithm so far \cite{rahnamayan2008opposition}.	score:325
Has machine learning made tangible improvements in drug discovery efficiency?	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	score:263
Has machine learning made tangible improvements in drug discovery efficiency?	 Many machine learning models, such as logistic regression~(LR) and support vector machine~(SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization~(DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods.  However, most of these DSO methods might not be scalable enough. In this paper, we propose a novel DSO method, called \underline{s}calable \underline{c}omposite \underline{op}timization for l\underline{e}arning~({SCOPE}), and implement it on the fault-tolerant distributed platform \mbox{Spark}. SCOPE is both computation-efficient and communication-efficient.	 Many machine learning models, such as logistic regression~(LR) and support vector machine~(SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization~(DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods.  However, most of these DSO methods might not be scalable enough. In this paper, we propose a novel DSO method, called \underline{s}calable \underline{c}omposite \underline{op}timization for l\underline{e}arning~({SCOPE}), and implement it on the fault-tolerant distributed platform \mbox{Spark}. SCOPE is both computation-efficient and communication-efficient.	score:269
Has machine learning made tangible improvements in drug discovery efficiency?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:269
Has machine learning made tangible improvements in drug discovery efficiency?	 While existing machine learning methods such as Recurrent Neural Networks (RNNs) can provide exceptional results, it is challenging to discover hidden patterns of the sequential data due to the irregular observation time instances. Meanwhile, the lack of understanding of why those learning models are effective also limits further improvements on their architectures.  Thus, in this work, we develop a RNN based time-aware architecture to tackle the challenging problem of handling irregular observation times and relevant feature extractions from longitudinal patient records for obesity status improvement prediction. To improve the prediction performance, we train our model using two data sources: (i) electronic medical records containing information regarding lab tests, diagnoses, and demographics; (ii) continuous activity data collected from popular wearables.	 While existing machine learning methods such as Recurrent Neural Networks (RNNs) can provide exceptional results, it is challenging to discover hidden patterns of the sequential data due to the irregular observation time instances. Meanwhile, the lack of understanding of why those learning models are effective also limits further improvements on their architectures.  Thus, in this work, we develop a RNN based time-aware architecture to tackle the challenging problem of handling irregular observation times and relevant feature extractions from longitudinal patient records for obesity status improvement prediction. To improve the prediction performance, we train our model using two data sources: (i) electronic medical records containing information regarding lab tests, diagnoses, and demographics; (ii) continuous activity data collected from popular wearables.	score:271
Has machine learning made tangible improvements in drug discovery efficiency?	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	 To the best of our knowledge, this is the first work that establishes the linear convergence rate for the variance-reduced stochastic gradient algorithms on solving both constrained and regularized problems without strong convexity. 	Convex optimization has played an important role in machine learning as many machine learning problems can be cast into a convex optimization problem.  Nowadays the emergence of big data makes the optimization problem challenging to solve and first-order stochastic gradient algorithms are often preferred due to their simplicity and low per-iteration cost. The stochastic gradient algorithms estimate the gradient based on only one or a few samples, and have been extensively studied in large-scale optimization problems \cite{zhang2004solving,duchi2009efficient,hu2009accelerated,xiao2010dual,hazan2011beyond,rakhlin2011making,lan2012optimal,shamir2013stochastic}.	score:273
What is the difference between classification and clustering?	 Inspired by the adequacy of CNN in visual feature extraction and the efficiency of Long Short-Term Memory (LSTM) Recurrent Neural Networks in dealing with long-range temporal dependencies our approach allows to model dynamic temporal dependences in the context of steering angle estimation based on camera input.   Posing regression problems as deep classification problems often shows improvements over direct regression training of CNN’s \cite{rothe2015dex}.  We argue that it can still be further improved. Classification tasks assume independence between the output neurons that encode the different classes. However, this assumption loses validity if the classification is used to model a regression. Because, in such case, classes neurons that are spatially close to each other should infer convergent decisions. Here, we propose a method to introduce correlation between the class neurons and thus bridge the gap between full classification problems and regression problems.	 We argue that it can still be further improved. Classification tasks assume independence between the output neurons that encode the different classes. However, this assumption loses validity if the classification is used to model a regression. Because, in such case, classes neurons that are spatially close to each other should infer convergent decisions. Here, we propose a method to introduce correlation between the class neurons and thus bridge the gap between full classification problems and regression problems.	score:341
What is the difference between classification and clustering?	      Efficient label acquisition processes are key to obtaining robust classifiers. However, data labeling is often challenging and subject to high levels of label noise. This can arise even when classification targets are well defined, if instances to be labeled are more difficult than the prototypes used to define the class, leading to disagreements among the expert community.    Here, we enable efficient training of deep neural networks. From low-confidence labels, we iteratively improve their quality by simultaneous learning of machines and experts. We call it Human And Machine co-LEarning Technique (\mname). Throughout the process, experts become more consistent, while the algorithm provides them with explainable feedback for confirmation.	      Efficient label acquisition processes are key to obtaining robust classifiers. However, data labeling is often challenging and subject to high levels of label noise. This can arise even when classification targets are well defined, if instances to be labeled are more difficult than the prototypes used to define the class, leading to disagreements among the expert community.    Here, we enable efficient training of deep neural networks. From low-confidence labels, we iteratively improve their quality by simultaneous learning of machines and experts. We call it Human And Machine co-LEarning Technique (\mname). Throughout the process, experts become more consistent, while the algorithm provides them with explainable feedback for confirmation.	score:348
What is the difference between classification and clustering?	  Note that even if we knew that the negative distribution lies in the lower right of the positive distribution, it is still impossible to find the decision boundary, because we still need to know the degree of overlap between the two distributions and the class prior. One-class methods are designed for and work well for anomaly detection, but have critical limitations if the problem of interest is ``classification''.   On the other hand, Pconf classification is aimed at constructing a discriminative classifier and thus hyper-parameters can be objectively chosen to discriminate between positive and negative data. We want to emphasize that the key contribution of our paper is to propose a method that is purely based on empirical risk minimization (ERM) \cite{Vapnik}, which makes it suitable for binary classification.	  Note that even if we knew that the negative distribution lies in the lower right of the positive distribution, it is still impossible to find the decision boundary, because we still need to know the degree of overlap between the two distributions and the class prior. One-class methods are designed for and work well for anomaly detection, but have critical limitations if the problem of interest is ``classification''.   On the other hand, Pconf classification is aimed at constructing a discriminative classifier and thus hyper-parameters can be objectively chosen to discriminate between positive and negative data. We want to emphasize that the key contribution of our paper is to propose a method that is purely based on empirical risk minimization (ERM) \cite{Vapnik}, which makes it suitable for binary classification.	score:352
What is the difference between classification and clustering?	  Similarly, the out-of-domain data is treated as if drawn from a mixture of a ``truly out-of-domain'' distribution and a ``general domain'' distribution.  We apply this framework in the context of conditional classification models and conditional linear-chain sequence labeling models, for which inference may be efficiently solved using the technique of conditional expectation maximization.   We apply our model to four data sets with varying degrees of divergence between the ``in-domain'' and ``out-of-domain'' data and obtain predictive accuracies higher than any of a large number of baseline systems and a second model proposed in the literature for this problem.  The domain adaptation problem arises very frequently in the natural language processing domain, in which millions of dollars have been spent annotating text resources for morphological, syntactic and semantic information.	  Similarly, the out-of-domain data is treated as if drawn from a mixture of a ``truly out-of-domain'' distribution and a ``general domain'' distribution.  We apply this framework in the context of conditional classification models and conditional linear-chain sequence labeling models, for which inference may be efficiently solved using the technique of conditional expectation maximization.   We apply our model to four data sets with varying degrees of divergence between the ``in-domain'' and ``out-of-domain'' data and obtain predictive accuracies higher than any of a large number of baseline systems and a second model proposed in the literature for this problem.  The domain adaptation problem arises very frequently in the natural language processing domain, in which millions of dollars have been spent annotating text resources for morphological, syntactic and semantic information.	score:355
What is the difference between classification and clustering?	 This method can be formulated as a quadratic programming problem and its solution can be found using a simple gradient descent procedure.  We prove that, in a limited 1-dimensional setting, this approach never leads to performance worse than the supervised classifier. Experimental results show that also in the general multidimensional case performance improvements can be expected, both in terms of the squared loss that is intrinsic to the classifier, as well as in terms of the expected classification error.  	We consider the problem of semi-supervised learning of binary classification functions. As in the supervised paradigm, the goal in semi-supervised learning is to construct a classification rule that maps objects in some input space to a target outcome, such that future objects map to correct target outcomes as well as possible. In the supervised paradigm this mapping is learned using a set of $\Nlab$ training objects and their corresponding outputs.	 This method can be formulated as a quadratic programming problem and its solution can be found using a simple gradient descent procedure.  We prove that, in a limited 1-dimensional setting, this approach never leads to performance worse than the supervised classifier. Experimental results show that also in the general multidimensional case performance improvements can be expected, both in terms of the squared loss that is intrinsic to the classifier, as well as in terms of the expected classification error.  	We consider the problem of semi-supervised learning of binary classification functions. As in the supervised paradigm, the goal in semi-supervised learning is to construct a classification rule that maps objects in some input space to a target outcome, such that future objects map to correct target outcomes as well as possible. In the supervised paradigm this mapping is learned using a set of $\Nlab$ training objects and their corresponding outputs.	score:356
Was CNN wrong when it reported that the Secret Service met with Donald Trump?	  Thus, as presented in~\cite{sexual_orientation}, if CNN is used to leak a very private matter of individuals, it is very hard to prevent CNN from exposing the privacy because we do not know what noise can hamper the recognition of a CNN. Actually, some noise that does not affect humans' recognition does make a huge difference for CNN~\cite{intriguing}.  Also, some representation that does not make any sense for human may work for CNN~\cite{DNN_fool}. One thing obvious is that CNN learns all the things from the data fed into it. This implies what CNN recognizes as a noise comes from the data set. In this paper, we mainly focus on how to add the effective noise that disrupts CNN's recognition for the purpose of privacy protection.   Differential privacy~\cite{cynthia} is the mathematical definition that has been introduced to measure the privacy loss for the domains that handle massive user data such as data mining applications.  Google~\cite{rappor} and Apple~\cite{apple} have adopted the differential privacy techniques into their crowdsourced applications with some data sketching technique to approximate users' private data.	  Thus, as presented in~\cite{sexual_orientation}, if CNN is used to leak a very private matter of individuals, it is very hard to prevent CNN from exposing the privacy because we do not know what noise can hamper the recognition of a CNN. Actually, some noise that does not affect humans' recognition does make a huge difference for CNN~\cite{intriguing}.  Also, some representation that does not make any sense for human may work for CNN~\cite{DNN_fool}. One thing obvious is that CNN learns all the things from the data fed into it. This implies what CNN recognizes as a noise comes from the data set. In this paper, we mainly focus on how to add the effective noise that disrupts CNN's recognition for the purpose of privacy protection.   Differential privacy~\cite{cynthia} is the mathematical definition that has been introduced to measure the privacy loss for the domains that handle massive user data such as data mining applications.  Google~\cite{rappor} and Apple~\cite{apple} have adopted the differential privacy techniques into their crowdsourced applications with some data sketching technique to approximate users' private data.	score:509
Was CNN wrong when it reported that the Secret Service met with Donald Trump?	 (\expandafter{\romannumeral2}) CNN has the ability to capture the frequency shift in human speech signal.  Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks \cite{cnn7,cnn8,cnn9,cnn10}. Most of previous application of CNNs in speech recognition only used fewer convolutional layers.  For example, Abdel-Hamid~et~al. \cite{cnn1} used one convolutional layer, one pooling layer and a few full-connected layers. Amodei~et~al. \cite{rnn2} also used only three convolutional layers as the feature preprocessing layers. Some researcher has shown that CNN-based speech recognition which uses raw speech as input can be more robust \cite{cnn11}, and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks \cite{cnn2,cnn3,cnn9,cnn14}.	 (\expandafter{\romannumeral2}) CNN has the ability to capture the frequency shift in human speech signal.  Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks \cite{cnn7,cnn8,cnn9,cnn10}. Most of previous application of CNNs in speech recognition only used fewer convolutional layers.  For example, Abdel-Hamid~et~al. \cite{cnn1} used one convolutional layer, one pooling layer and a few full-connected layers. Amodei~et~al. \cite{rnn2} also used only three convolutional layers as the feature preprocessing layers. Some researcher has shown that CNN-based speech recognition which uses raw speech as input can be more robust \cite{cnn11}, and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks \cite{cnn2,cnn3,cnn9,cnn14}.	score:534
Was CNN wrong when it reported that the Secret Service met with Donald Trump?	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	score:535
Was CNN wrong when it reported that the Secret Service met with Donald Trump?	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	 Here, we employ a CNN as the sentence (doctor's note) encoder, due to its excellent performance at identifying sentence structure, especially in long noisy sentences. A deep residual network \cite{he2016deep} is added on top of the CNN encoder, to capture label correlations and incorporate information from the encoded sentence vector (CNN's output).  This is achieved using shortcut connections between layers \cite{bishop1995neural,venables2013modern}. A common problem in deep networks is that performance saturates, then degrades rapidly as the network grows deeper. Deep residual networks mitigate this problem by having stacked layers that fit a residual mapping.  \paragraph{Clinical Relevance} This work focuses on predicting diagnoses given a plain doctor's note on a patient's presentation, symptoms, medical history, etc.	score:535
Was CNN wrong when it reported that the Secret Service met with Donald Trump?	   A Convolutional Neural Network (CNN) is a variant of a Deep Neural Network (DNN) \cite{110_deeplearning_lenet}. Inspired by the visual cortex of animals, CNNs are applied to state-of-the-art applications, including computer vision and speech recognition \cite{deng2014deep}. However, supervised training of CNNs is computationally demanding and time consuming, and in many cases, several weeks are required to complete a training session.  Often applications are tested with different parameters, and each test requires a full session of training.   Multi-core processors~\cite{Williams:2009} and in particular many-core~\cite{benkner11} processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) \cite{nvidia_gpu} or the Intel Xeon Phi \cite{chrysos2012intel} co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs.	   A Convolutional Neural Network (CNN) is a variant of a Deep Neural Network (DNN) \cite{110_deeplearning_lenet}. Inspired by the visual cortex of animals, CNNs are applied to state-of-the-art applications, including computer vision and speech recognition \cite{deng2014deep}. However, supervised training of CNNs is computationally demanding and time consuming, and in many cases, several weeks are required to complete a training session.  Often applications are tested with different parameters, and each test requires a full session of training.   Multi-core processors~\cite{Williams:2009} and in particular many-core~\cite{benkner11} processing architectures, such as the NVIDIA Graphical Processing Unit (GPU) \cite{nvidia_gpu} or the Intel Xeon Phi \cite{chrysos2012intel} co-processor, provide processing capabilities that may be used to significantly speed-up the training of CNNs.	score:537
How do i solve [math] \begin{vmatrix} 1+a &1  &1 \\  1&1+b  &1 \\  1&1  &1+c  \end{vmatrix} [/math]?	  A preliminary version of this paper appeared as a poster in~\cite{NIPS-workshop}.  \subsection{Classification Using Support Vector Machines} We begin by formulating the SVM problem. Consider a training set:  \begin{equation} \begin{array}{l} D=\left\{\left(\mathbf{x}_i, y_i\right),\;\;\; i=1,\ldots,N,\;\;\;   \mathbf{x}_i\in \Re^m, \;\;\;y_i\in \left\{-1, 1\right\}\right\}.  \end{array} \end{equation} The goal of the SVM is to learn a mapping from $\mathbf{x}_i$ to $y_i$ such that the error in mapping, as measured on a new dataset, would be minimal. SVMs learn to find the linear weight vector that separates the two classes so that \begin{equation} \begin{array}{l} y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) \geq 1 \;\; for \;\; i = 1,\ldots,N.  \end{array} \end{equation}  There may exist many hyperplanes that achieve such separation, but SVMs find a weight vector $\mathbf{w}$ and a bias term $b$ that maximize the margin $2 / \left\| \mathbf{w} \right\| $. Therefore, the optimization problem that needs to be solved is \begin{equation} \min J_D(\mathbf{w}) = \frac{1}{2}\left\| \mathbf{w} \right\|, \end{equation} \begin{equation} Subject \; to \; y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) \geq 1 \;\; for \;\; i = 1,\ldots,N.  \end{equation}  Any points lying on the hyperplane $y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) = 1 $ are called support vectors.  If the data cannot be separated using a linear separator, a slack variable $\xi \geq 0$ is introduced and the constraint is relaxed to: \begin{equation} y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) \geq 1 - \xi_i \;\; for \;\; i = 1,\ldots,N.  \end{equation}  The optimization problem then becomes: \begin{equation} \min J_D(\mathbf{w}) = \frac{1}{2}\left\| \mathbf{w} \right\| + C \sum_{i=1}^N \xi_i, \end{equation} \begin{equation} subject \; to \; y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) \geq 1 \;\; for \;\; i = 1,\ldots,N, \newline \end{equation} \begin{equation} \xi_i \geq 0  \;\; for \;\; i = 1,\ldots,N.  \end{equation}  The weights of the linear function can be found directly or by converting the problem into its dual optimization problem, which is usually easier to solve.  Using the notation of Vijayakumar and Wu~\cite{SVMSeq}, the dual problem is thus: \begin{equation} \label{dual}     \max \;\; L_D(h)=\sum_{i}h_i-\frac{1}{2}h'\cdot D\cdot h, \\ \end{equation} \begin{equation} \label{cons1}     subject \; to \;\; 0\leq h_i \leq C, \;\; i=1,.	  A preliminary version of this paper appeared as a poster in~\cite{NIPS-workshop}.  \subsection{Classification Using Support Vector Machines} We begin by formulating the SVM problem. Consider a training set:  \begin{equation} \begin{array}{l} D=\left\{\left(\mathbf{x}_i, y_i\right),\;\;\; i=1,\ldots,N,\;\;\;   \mathbf{x}_i\in \Re^m, \;\;\;y_i\in \left\{-1, 1\right\}\right\}.  \end{array} \end{equation} The goal of the SVM is to learn a mapping from $\mathbf{x}_i$ to $y_i$ such that the error in mapping, as measured on a new dataset, would be minimal. SVMs learn to find the linear weight vector that separates the two classes so that \begin{equation} \begin{array}{l} y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) \geq 1 \;\; for \;\; i = 1,\ldots,N.  \end{array} \end{equation}  There may exist many hyperplanes that achieve such separation, but SVMs find a weight vector $\mathbf{w}$ and a bias term $b$ that maximize the margin $2 / \left\| \mathbf{w} \right\| $. Therefore, the optimization problem that needs to be solved is \begin{equation} \min J_D(\mathbf{w}) = \frac{1}{2}\left\| \mathbf{w} \right\|, \end{equation} \begin{equation} Subject \; to \; y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) \geq 1 \;\; for \;\; i = 1,\ldots,N.  \end{equation}  Any points lying on the hyperplane $y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) = 1 $ are called support vectors.  If the data cannot be separated using a linear separator, a slack variable $\xi \geq 0$ is introduced and the constraint is relaxed to: \begin{equation} y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) \geq 1 - \xi_i \;\; for \;\; i = 1,\ldots,N.  \end{equation}  The optimization problem then becomes: \begin{equation} \min J_D(\mathbf{w}) = \frac{1}{2}\left\| \mathbf{w} \right\| + C \sum_{i=1}^N \xi_i, \end{equation} \begin{equation} subject \; to \; y_i \left( \mathbf{x_i} \cdot \mathbf{w} + b \right) \geq 1 \;\; for \;\; i = 1,\ldots,N, \newline \end{equation} \begin{equation} \xi_i \geq 0  \;\; for \;\; i = 1,\ldots,N.  \end{equation}  The weights of the linear function can be found directly or by converting the problem into its dual optimization problem, which is usually easier to solve.  Using the notation of Vijayakumar and Wu~\cite{SVMSeq}, the dual problem is thus: \begin{equation} \label{dual}     \max \;\; L_D(h)=\sum_{i}h_i-\frac{1}{2}h'\cdot D\cdot h, \\ \end{equation} \begin{equation} \label{cons1}     subject \; to \;\; 0\leq h_i \leq C, \;\; i=1,.	score:289
How do i solve [math] \begin{vmatrix} 1+a &1  &1 \\  1&1+b  &1 \\  1&1  &1+c  \end{vmatrix} [/math]?	       To compare and contrast our loss function with other common loss functions such as those in equations (\ref{eqn:logloss}-\ref{eqn:01loss}) and others reviewed below,   we express our loss here using $z_i$ and $\gamma$ as arguments. For $t=1$, the $\mathcal{B} B\gamma$ loss can be expressed as  \begin{equation}  L_{\mathcal{B} B\gamma}(z_i,\gamma)= -\log \big( a + b [1+\exp(-\gamma z_i)]^{-1} \big), \label{eqn:BBg_pos} \end{equation}  while for $t=-1$ it can be expressed as  \begin{equation}  L_{\mathcal{B} B\gamma}(z_i,\gamma)= -\log \big[ 1 - \big(a + b [1+\exp(\gamma z_i)]^{-1} \big) \big].  \label{eqn:BBg_neg} \end{equation}                  We show in section \ref{sec:theory} that the constants $a$ and $b$ have well defined interpretations in terms of the standard $\alpha$, $\beta$, and $n$ parameters of the Beta distribution. Their impact on our proposed generalized Beta-Bernoulli loss arise from applying a fuller Bayesian analysis to the formulation of a logistic function.	       To compare and contrast our loss function with other common loss functions such as those in equations (\ref{eqn:logloss}-\ref{eqn:01loss}) and others reviewed below,   we express our loss here using $z_i$ and $\gamma$ as arguments. For $t=1$, the $\mathcal{B} B\gamma$ loss can be expressed as  \begin{equation}  L_{\mathcal{B} B\gamma}(z_i,\gamma)= -\log \big( a + b [1+\exp(-\gamma z_i)]^{-1} \big), \label{eqn:BBg_pos} \end{equation}  while for $t=-1$ it can be expressed as  \begin{equation}  L_{\mathcal{B} B\gamma}(z_i,\gamma)= -\log \big[ 1 - \big(a + b [1+\exp(\gamma z_i)]^{-1} \big) \big].  \label{eqn:BBg_neg} \end{equation}                  We show in section \ref{sec:theory} that the constants $a$ and $b$ have well defined interpretations in terms of the standard $\alpha$, $\beta$, and $n$ parameters of the Beta distribution. Their impact on our proposed generalized Beta-Bernoulli loss arise from applying a fuller Bayesian analysis to the formulation of a logistic function.	score:309
How do i solve [math] \begin{vmatrix} 1+a &1  &1 \\  1&1+b  &1 \\  1&1  &1+c  \end{vmatrix} [/math]?	 Therefore the joint matrix $\XS$ can be recovered with the semi-definite program \begin{align} \nonumber {\PhaseCalJ}\textbf{:} \quad\qquad &\\ \label{eq:phasecal} \Xh = \argmin_{\Z}\quad &f_{\lambda}(\Z) && \\ \text{subject to}\quad \nonumber & \Z \succcurlyeq 0& &\\ \nonumber & g_{i,k,\l} = \m_i'\Z_{k,\l}\m_i& i&=1,\dots, M\\ \nonumber & & k,\l&=1,\dots, L \end{align} where \begin{align} \label{eq:flambda} f_{\lambda}(\Z) \triangleq  \Tr(\Z) + \lambda \Vert\Z\Vert_1 \end{align} It can be noted that when $L=1$ the optimization problem in \eqref{eq:phasecal} becomes identical to \eqref{eq:cprl} even though the originating problems are completely different.	 Therefore the joint matrix $\XS$ can be recovered with the semi-definite program \begin{align} \nonumber {\PhaseCalJ}\textbf{:} \quad\qquad &\\ \label{eq:phasecal} \Xh = \argmin_{\Z}\quad &f_{\lambda}(\Z) && \\ \text{subject to}\quad \nonumber & \Z \succcurlyeq 0& &\\ \nonumber & g_{i,k,\l} = \m_i'\Z_{k,\l}\m_i& i&=1,\dots, M\\ \nonumber & & k,\l&=1,\dots, L \end{align} where \begin{align} \label{eq:flambda} f_{\lambda}(\Z) \triangleq  \Tr(\Z) + \lambda \Vert\Z\Vert_1 \end{align} It can be noted that when $L=1$ the optimization problem in \eqref{eq:phasecal} becomes identical to \eqref{eq:cprl} even though the originating problems are completely different.	score:313
How do i solve [math] \begin{vmatrix} 1+a &1  &1 \\  1&1+b  &1 \\  1&1  &1+c  \end{vmatrix} [/math]?	 $\bm{U}_i$ ($\bm{V}_i$) corresponds to the $i$-th column of $\bm{U}$ ($\bm{V}$), which is a column orthogonal matrix. To solve the above optimization problem, we introduce a general regularized rank-one SVD model: \begin{equation} \begin{aligned}\label{equ:01} & \underset{\bm{u},\bm{v},d}{\text{minimize}} && \|\bm{X} - d\bm{uv}^T\|_F^2\\ & \text{subject to}                  && \|\bm{u}\|^2 \leq 1, \Omega_1(\bm{u}) \leq c_1,\\ &                                    && \|\bm{v}\|^2 \leq 1, \Omega_2(\bm{v}) \leq c_2, \end{aligned} \end{equation} where $d$ is a positive singular value, $\bm{u}$ is a $p$-dimensional column vector, and $\bm{v}$ is a $n$-dimensional column vector.	 $\bm{U}_i$ ($\bm{V}_i$) corresponds to the $i$-th column of $\bm{U}$ ($\bm{V}$), which is a column orthogonal matrix. To solve the above optimization problem, we introduce a general regularized rank-one SVD model: \begin{equation} \begin{aligned}\label{equ:01} & \underset{\bm{u},\bm{v},d}{\text{minimize}} && \|\bm{X} - d\bm{uv}^T\|_F^2\\ & \text{subject to}                  && \|\bm{u}\|^2 \leq 1, \Omega_1(\bm{u}) \leq c_1,\\ &                                    && \|\bm{v}\|^2 \leq 1, \Omega_2(\bm{v}) \leq c_2, \end{aligned} \end{equation} where $d$ is a positive singular value, $\bm{u}$ is a $p$-dimensional column vector, and $\bm{v}$ is a $n$-dimensional column vector.	score:315
How do i solve [math] \begin{vmatrix} 1+a &1  &1 \\  1&1+b  &1 \\  1&1  &1+c  \end{vmatrix} [/math]?	 \] An optimal solution to ($L'$) is $x'=(0,\frac{1}{2})^\tr$. To map $x'$ to a solution of the original LP ($L$), we multiply it with the following matrix.  \begin{equation}\label{eq:exa-intro} D:= \begin{pmatrix}   1&0&0&0\\   1&0&0&0\\   1&0&0&0\\   0&1&0&0\\   0&1&0&0\\   0&1&0&0\\   0&1&0&0\\   0&0&1&0\\   0&0&1&0\\   0&0&0&1\\   0&0&0&1\\   0&0&0&1\\   0&0&0&1 \end{pmatrix} \begin{pmatrix}   1&0\\   1&0\\   0&1\\   0&1 \end{pmatrix} = \begin{pmatrix}   1&0\\   1&0\\   1&0\\   1&0\\   1&0\\   1&0\\   1&0\\   0&1\\   0&1\\   0&1\\   0&1\\   0&1\\   0&1 \end{pmatrix} \end{equation} We will see later where this matrix comes from.	 \] An optimal solution to ($L'$) is $x'=(0,\frac{1}{2})^\tr$. To map $x'$ to a solution of the original LP ($L$), we multiply it with the following matrix.  \begin{equation}\label{eq:exa-intro} D:= \begin{pmatrix}   1&0&0&0\\   1&0&0&0\\   1&0&0&0\\   0&1&0&0\\   0&1&0&0\\   0&1&0&0\\   0&1&0&0\\   0&0&1&0\\   0&0&1&0\\   0&0&0&1\\   0&0&0&1\\   0&0&0&1\\   0&0&0&1 \end{pmatrix} \begin{pmatrix}   1&0\\   1&0\\   0&1\\   0&1 \end{pmatrix} = \begin{pmatrix}   1&0\\   1&0\\   1&0\\   1&0\\   1&0\\   1&0\\   1&0\\   0&1\\   0&1\\   0&1\\   0&1\\   0&1\\   0&1 \end{pmatrix} \end{equation} We will see later where this matrix comes from.	score:316
What is a good way to create a tridiagonal matrix in matlab?	   Since LightNet is implemented \textbf{solely} with Matlab, the major computations are vectorized and implemented in \textbf{hundreds} of lines of code, orders of magnitude more succinct than existing pipelines. All fundamental operations can be easily customized, only basic knowledge of Matlab programming is required. Mathematically oriented researchers can focus on the mathematical modeling part rather than the engineering part.  Application oriented users can easily understand and modify any part of the framework to develop new network architectures and adapt them to new applications. Aside from its simplicity, LightNet has the following features:  1. LightNet contains the most modern network architectures. 2. Applications in computer vision, natural language processing and reinforcement learning are demonstrated.	   Since LightNet is implemented \textbf{solely} with Matlab, the major computations are vectorized and implemented in \textbf{hundreds} of lines of code, orders of magnitude more succinct than existing pipelines. All fundamental operations can be easily customized, only basic knowledge of Matlab programming is required. Mathematically oriented researchers can focus on the mathematical modeling part rather than the engineering part.  Application oriented users can easily understand and modify any part of the framework to develop new network architectures and adapt them to new applications. Aside from its simplicity, LightNet has the following features:  1. LightNet contains the most modern network architectures. 2. Applications in computer vision, natural language processing and reinforcement learning are demonstrated.	score:360
What is a good way to create a tridiagonal matrix in matlab?	 Diagonal    and   full    covariance   matrices    were   investigated in~\cite{abe:training}   for  the   purpose   of  classification   and in~\cite{abe:regression} for the purpose  of regression. However, in a similar way, the  covariance matrix was computed for  all the training samples. Computing the covariance  matrix for the Mahalanobis distance with all the training samples is equivalent to project the data on all the principal components, scale the variance to one, and then applying the Euclidean distance.   By doing  so, classes could overlap more than in the original input space  and the discrimination between them would be decreased.  In this  work, the HDDA  model is used for  the definition of  a class specific covariance  matrix adapted for  HD data. The  specific signal and noise subspaces are estimated  for each considered class, ensuring a  parsimonious characterization  of the  classes.	 Diagonal    and   full    covariance   matrices    were   investigated in~\cite{abe:training}   for  the   purpose   of  classification   and in~\cite{abe:regression} for the purpose  of regression. However, in a similar way, the  covariance matrix was computed for  all the training samples. Computing the covariance  matrix for the Mahalanobis distance with all the training samples is equivalent to project the data on all the principal components, scale the variance to one, and then applying the Euclidean distance.   By doing  so, classes could overlap more than in the original input space  and the discrimination between them would be decreased.  In this  work, the HDDA  model is used for  the definition of  a class specific covariance  matrix adapted for  HD data. The  specific signal and noise subspaces are estimated  for each considered class, ensuring a  parsimonious characterization  of the  classes.	score:369
What is a good way to create a tridiagonal matrix in matlab?	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	score:375
What is a good way to create a tridiagonal matrix in matlab?	 \cite{moczulski2016ACDC} defines a structured matrix via discrete cosine transform (DCT) for enabling fast matrix multiplication via a fast Fourier transform algorithm. These methods successfully reduce the computational cost for neural network prediction; however, since those methods restrict the parameter space prior to training, these method may restrict the flexibility of neural networks.  Our proposed method attempts to jointly optimize the structure of the neural network and its parameters. This way, the optimization algorithm can determine an effective low-dimensional representation of the hidden layers.  Several approaches for reducing numerical precision to speed up training and evaluation have been proposed~\cite{gupta2015deep,courbariaux2014low,courbariaux2015binaryconnect}.	 \cite{moczulski2016ACDC} defines a structured matrix via discrete cosine transform (DCT) for enabling fast matrix multiplication via a fast Fourier transform algorithm. These methods successfully reduce the computational cost for neural network prediction; however, since those methods restrict the parameter space prior to training, these method may restrict the flexibility of neural networks.  Our proposed method attempts to jointly optimize the structure of the neural network and its parameters. This way, the optimization algorithm can determine an effective low-dimensional representation of the hidden layers.  Several approaches for reducing numerical precision to speed up training and evaluation have been proposed~\cite{gupta2015deep,courbariaux2014low,courbariaux2015binaryconnect}.	score:388
What is a good way to create a tridiagonal matrix in matlab?	 This paper describes a new approach, based on linear programming, for computing nonnegative matrix factorizations (NMFs).  The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features.  More precisely, given a data matrix $\mtx{X}$, the algorithm identifies a matrix $\mtx{C}$ that satisfies $\mtx{X} \approx \mtx{CX}$ and some linear constraints.   The constraints are chosen to ensure that the matrix $\mtx{C}$ selects features; these features can then be used to find a low-rank NMF of $\mtx{X}$.  A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al.~(2012).  In contrast with this earlier work, the proposed method extends to more general noise models and leads to efficient, scalable algorithms.   Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation can factor a multigigabyte matrix in a matter of minutes. 	Nonnegative matrix factorization (NMF) is a popular approach for selecting features in data~\cite{LeeSeungNMFNature,LeeNMFAlgorithms,Hofmann99,Smaragdis03}.  \eat{\bf [xxx add cites! xxx]} Many machine-learning and data-mining software packages (including Matlab~\cite{MatlabNMFurl}, R~\cite{Gaujoux10}, and Oracle Data Mining~\cite{OracleDataMiningNMFurl}) now include heuristic computational methods for NMF. Nevertheless, we still have limited theoretical understanding of when these heuristics are correct.	  Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.  An optimized C++ implementation can factor a multigigabyte matrix in a matter of minutes. 	Nonnegative matrix factorization (NMF) is a popular approach for selecting features in data~\cite{LeeSeungNMFNature,LeeNMFAlgorithms,Hofmann99,Smaragdis03}.	score:392
How do I create tridiagonal matrix in MATLAB?	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	score:391
How do I create tridiagonal matrix in MATLAB?	   Since LightNet is implemented \textbf{solely} with Matlab, the major computations are vectorized and implemented in \textbf{hundreds} of lines of code, orders of magnitude more succinct than existing pipelines. All fundamental operations can be easily customized, only basic knowledge of Matlab programming is required. Mathematically oriented researchers can focus on the mathematical modeling part rather than the engineering part.  Application oriented users can easily understand and modify any part of the framework to develop new network architectures and adapt them to new applications. Aside from its simplicity, LightNet has the following features:  1. LightNet contains the most modern network architectures. 2. Applications in computer vision, natural language processing and reinforcement learning are demonstrated.	   Since LightNet is implemented \textbf{solely} with Matlab, the major computations are vectorized and implemented in \textbf{hundreds} of lines of code, orders of magnitude more succinct than existing pipelines. All fundamental operations can be easily customized, only basic knowledge of Matlab programming is required. Mathematically oriented researchers can focus on the mathematical modeling part rather than the engineering part.  Application oriented users can easily understand and modify any part of the framework to develop new network architectures and adapt them to new applications. Aside from its simplicity, LightNet has the following features:  1. LightNet contains the most modern network architectures. 2. Applications in computer vision, natural language processing and reinforcement learning are demonstrated.	score:412
How do I create tridiagonal matrix in MATLAB?	 Related variational formulations for the Lasso have also been discussed in \citep{Grandvalet1998} and further studied in \citep{Grandvalet2007}.  To our knowledge, the box-norm was first suggested by \cite{Jacob2009-CLUSTER} and used as a symmetric gauge function in matrix learning problems.  The induced orthogonally invariant matrix norm is named the {\em cluster norm} in \citep{Jacob2009-CLUSTER} and was motivated as a convex relaxation of a multitask clustering problem.	 Related variational formulations for the Lasso have also been discussed in \citep{Grandvalet1998} and further studied in \citep{Grandvalet2007}.  To our knowledge, the box-norm was first suggested by \cite{Jacob2009-CLUSTER} and used as a symmetric gauge function in matrix learning problems.  The induced orthogonally invariant matrix norm is named the {\em cluster norm} in \citep{Jacob2009-CLUSTER} and was motivated as a convex relaxation of a multitask clustering problem.	score:413
How do I create tridiagonal matrix in MATLAB?	 Related variational formulations for the Lasso have also been discussed in \citep{Grandvalet1998} and further studied in \citep{Grandvalet2007}.  To our knowledge, the box-norm was first suggested by \cite{Jacob2009-CLUSTER} and used as a symmetric gauge function in matrix learning problems.  The induced orthogonally invariant matrix norm is named the {\em cluster norm} in \citep{Jacob2009-CLUSTER} and was motivated as a convex relaxation of a multitask clustering problem.	 Related variational formulations for the Lasso have also been discussed in \citep{Grandvalet1998} and further studied in \citep{Grandvalet2007}.  To our knowledge, the box-norm was first suggested by \cite{Jacob2009-CLUSTER} and used as a symmetric gauge function in matrix learning problems.  The induced orthogonally invariant matrix norm is named the {\em cluster norm} in \citep{Jacob2009-CLUSTER} and was motivated as a convex relaxation of a multitask clustering problem.	score:413
How do I create tridiagonal matrix in MATLAB?	  In our approach, contrary to \cite{fessant}, \cite{aly}, \cite{bobrowski}, instead of computing the Mahalanobis matrix as an inverse of covariance matrix, it is  learned in a way assuring the smallest distance between points from the same class and large margin separation of points from different classes. Several algorithms exist for distance metric learning (DML) \cite{xing}, \cite{nca}.  In this paper we use so-called Large Margin Nearest Neighbour (LMNN) method \cite{lmnn}. It introduces the distance metric learning problem as a convex optimization, which assures that the global minimum can be efficiently computed. First, we shortly describe SOM model used in supervised manner and LMNN method. Then we show how we combine these two approaches into our SOM+DML model. Finally we present results on real data sets.	  In our approach, contrary to \cite{fessant}, \cite{aly}, \cite{bobrowski}, instead of computing the Mahalanobis matrix as an inverse of covariance matrix, it is  learned in a way assuring the smallest distance between points from the same class and large margin separation of points from different classes. Several algorithms exist for distance metric learning (DML) \cite{xing}, \cite{nca}.  In this paper we use so-called Large Margin Nearest Neighbour (LMNN) method \cite{lmnn}. It introduces the distance metric learning problem as a convex optimization, which assures that the global minimum can be efficiently computed. First, we shortly describe SOM model used in supervised manner and LMNN method. Then we show how we combine these two approaches into our SOM+DML model. Finally we present results on real data sets.	score:420
When will IBM start pool receuitment drive for freshers of batch 2017?	  The pooling operation has also played a central role, contributing  to invariance to data variation and perturbation. However, pooling operations have been little revised beyond the current primary options of average, max, and stochastic pooling \cite{boureau2010theoretical,zeiler2013stochastic}; this despite indications that e.g. choosing from more than just one type of  pooling operation can benefit performance \cite{scherer2010evaluation}.     In this paper, we desire to bring learning and ``responsiveness'' (i.e., to characteristics  of the region being pooled) into the pooling operation.  Various approaches are possible, but here we pursue two in particular.  In the first approach, we consider combining typical pooling operations (specifically, max pooling and average pooling); within this approach we further investigate  two strategies by which to combine these operations.  One of the strategies is ``unresponsive''; for reasons discussed later, we call this strategy  \emph{mixed max-average pooling}. The other strategy is ``responsive''; we call  this  strategy \emph{gated max-average pooling}, where the ability to be responsive  is provided by a ``gate'' in analogy to the usage of gates elsewhere in deep learning.  Another natural generalization of pooling operations is to allow the pooling operations  that are being combined to themselves be learned.  Hence in the second approach, we  learn to combine pooling filters that are themselves learned. Specifically, the learning is performed within a binary tree (with number of levels that is  pre-specified rather than ``grown'' as in traditional decision trees) in which each  leaf is associated with a learned pooling filter. As we consider internal nodes  of the tree, each parent node is associated with an output value that is the mixture  of the child node output values, until we finally reach the root node.	 One of the strategies is ``unresponsive''; for reasons discussed later, we call this strategy  \emph{mixed max-average pooling}. The other strategy is ``responsive''; we call  this  strategy \emph{gated max-average pooling}, where the ability to be responsive  is provided by a ``gate'' in analogy to the usage of gates elsewhere in deep learning.  Another natural generalization of pooling operations is to allow the pooling operations  that are being combined to themselves be learned.	score:439
When will IBM start pool receuitment drive for freshers of batch 2017?	 To start, what is the suitable ambient space in which the topic polytope is represented? As topics evolve, so are the number of topics that may become active or dormant, raising distinct modeling choices. Interesting issues arise in the inference, too. For instance, what is the principled way of \emph{matching} vertices of a collection of polytopes to their next reincarnations?  Such question arises because we consider modeling of topics learned independently across timestamps and text corpora, which entails the need for preserving the topic structure's permutation invariance of the vertex labels.    We consider an isometric embedding of the unit sphere in the word simplex, so that the evolution of topic polytopes may be represented by a collection of (random) trajectories of points residing on the unit sphere.	 To start, what is the suitable ambient space in which the topic polytope is represented? As topics evolve, so are the number of topics that may become active or dormant, raising distinct modeling choices. Interesting issues arise in the inference, too. For instance, what is the principled way of \emph{matching} vertices of a collection of polytopes to their next reincarnations?  Such question arises because we consider modeling of topics learned independently across timestamps and text corpora, which entails the need for preserving the topic structure's permutation invariance of the vertex labels.    We consider an isometric embedding of the unit sphere in the word simplex, so that the evolution of topic polytopes may be represented by a collection of (random) trajectories of points residing on the unit sphere.	score:479
When will IBM start pool receuitment drive for freshers of batch 2017?	  Several methods have been proposed to make lifelong learning amenable to backpropagation, including most recently neural Turing machines \cite{Graves2014-ch,Santoro2016-jn} and memory networks \cite{Sukhbaatar2015-ly}. However, it would be useful to incorporate the powerful, well-studied principle of Hebbian plasticity in backpropagation training.   Here we derive analytical expressions for activity gradients in neural networks with Hebbian plastic connections. Using these expressions, we can use backpropagation to train not just the baseline weights of the connections, but also their plasticity. This allows backpropagation to ``learn how to learn'', in order to solve general types of problems with unpredictable features, rather than specific instances.  All software used for the present paper is available at \texttt{http://github.com/thomasmiconi}.	  Several methods have been proposed to make lifelong learning amenable to backpropagation, including most recently neural Turing machines \cite{Graves2014-ch,Santoro2016-jn} and memory networks \cite{Sukhbaatar2015-ly}. However, it would be useful to incorporate the powerful, well-studied principle of Hebbian plasticity in backpropagation training.   Here we derive analytical expressions for activity gradients in neural networks with Hebbian plastic connections. Using these expressions, we can use backpropagation to train not just the baseline weights of the connections, but also their plasticity. This allows backpropagation to ``learn how to learn'', in order to solve general types of problems with unpredictable features, rather than specific instances.  All software used for the present paper is available at \texttt{http://github.com/thomasmiconi}.	score:479
When will IBM start pool receuitment drive for freshers of batch 2017?	   A pooling operator, an idea which dates back to the work in \cite{Hubel+Wiesel-1968}, has been adopted in many object recognizers. Convolutional neural networks which often employ max pooling have achieved state-of-the-art recognition performances on various benchmark datasets \cite{Krizhevsky-2012,Ciresan-et-al-2012}. Also, biologically inspired models such as HMAX have employed max pooling \cite{Riesenhuber+Poggio-1999}.  A pooling operator, in this context, is understood as a way to summarize a high-dimensional collection of neural responses and produce features that are invariant to some variations in the input (across the filter outputs that are being pooled).  Recently, the authors of \cite{Goodfellow_maxout_2013} proposed to understand a pooling operator itself as a nonlinear activation function.	   A pooling operator, an idea which dates back to the work in \cite{Hubel+Wiesel-1968}, has been adopted in many object recognizers. Convolutional neural networks which often employ max pooling have achieved state-of-the-art recognition performances on various benchmark datasets \cite{Krizhevsky-2012,Ciresan-et-al-2012}. Also, biologically inspired models such as HMAX have employed max pooling \cite{Riesenhuber+Poggio-1999}.  A pooling operator, in this context, is understood as a way to summarize a high-dimensional collection of neural responses and produce features that are invariant to some variations in the input (across the filter outputs that are being pooled).  Recently, the authors of \cite{Goodfellow_maxout_2013} proposed to understand a pooling operator itself as a nonlinear activation function.	score:481
When will IBM start pool receuitment drive for freshers of batch 2017?	 	The process of populating a structured relational database from unstructured sources has received renewed interest in the database community through high-profile start-up companies (e.g., Tamr and Trifacta), established companies like IBM's Watson~\cite{Ferrucci:2010:AI,Brown:2013:IBM}, and a variety of research efforts~\cite{Chen:2014:SIGMOD,Nakashole:2011:WSDM,Weikum:2010:PODS,Li:2011:ACL,Shen:2007:VLDB}.  At the same time, communities such as natural language processing and machine learning are attacking similar problems under the name {\em   knowledge base construction} (KBC)~\cite{Jiang:2012:ICDM,Dong:2014:VLDB,Betteridge:2009:AAAI}. While different communities place differing emphasis on the extraction, cleaning, and integration phases, all communities seem to be converging toward a common set of techniques that include a mix of data processing, machine learning, and engineers-in-the-loop.	 	The process of populating a structured relational database from unstructured sources has received renewed interest in the database community through high-profile start-up companies (e.g., Tamr and Trifacta), established companies like IBM's Watson~\cite{Ferrucci:2010:AI,Brown:2013:IBM}, and a variety of research efforts~\cite{Chen:2014:SIGMOD,Nakashole:2011:WSDM,Weikum:2010:PODS,Li:2011:ACL,Shen:2007:VLDB}.  At the same time, communities such as natural language processing and machine learning are attacking similar problems under the name {\em   knowledge base construction} (KBC)~\cite{Jiang:2012:ICDM,Dong:2014:VLDB,Betteridge:2009:AAAI}. While different communities place differing emphasis on the extraction, cleaning, and integration phases, all communities seem to be converging toward a common set of techniques that include a mix of data processing, machine learning, and engineers-in-the-loop.	score:482
What data science and machine learning career opportunities are there at Pinterest?	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	 We apply this idea to learning with asymmetric noisy labels, connecting and extending prior work. Furthermore, we show that most losses enjoy a data-dependent (by the mean operator) form of noise robustness, in contrast with known negative results. 	Supervised learning is by far the most effective application of the machine learning paradigm. However, there is a growing need of decoupling the success of the field from its topmost framework, often unrealistic in practice.  In fact while the amount of available data grows continuously, its relative training labels --often derived by human effort-- become rare, and hence learning is performed with partially missing, aggregate-level and/or noisy labels. For this reason, \emph{weakly supervised learning} (\textsc{wsl}) has attracted much research. In this work, we focus on binary classification under weak supervision.	score:240
What data science and machine learning career opportunities are there at Pinterest?	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	score:256
What data science and machine learning career opportunities are there at Pinterest?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:257
What data science and machine learning career opportunities are there at Pinterest?	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	 This paper attempts to fill this gap in the literature through providing a machine learning ``guide to practice'' for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner.  We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.  	Perhaps the most important problem of the next several decades will be how humans cope with unprecedented increases in the volume of data.	score:265
What data science and machine learning career opportunities are there at Pinterest?	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	score:270
When will be the next batch of on boarding in Accenture after July 1st?	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.	 For example, the time interval between consecutive clinical visits of a patient may potentially vary largely. The duration between consecutive log-in events into an online service can change from time to time. Therefore, one primary goal of continuous-time event sequence prediction is to predict when the next event will happen in the near future.  Although these two tasks focus on different aspects of a future event, how to learn a proper representation for the temporal information in the past is crucial to both of them.  More specifically, even though for a few discrete-time event sequence prediction tasks (e.g., neural machine translation), they do not involve an explicit temporal information for each event (token), a proper representation of the position in the sequence is still of great importance, not to mention the more general cases where each event is particularly associated with a timestamp.	score:422
When will be the next batch of on boarding in Accenture after July 1st?	  Its hidden state after observing the past should implicitly encode the Bayesian posterior, and its update rule for this hidden state should emulate the ``observable operator'' that updates the posterior upon each new observation.  See \cref{sec:missing_data_details} for further discussion.\looseness=-1  A final motivation is that one might wish to {\em intervene} in a medical, economic, or social event stream so as to improve the future course of events.  \Cref{sec:future} discusses our plans to deploy our model family as an environment model within reinforcement learning, where an agent controls some events.	 \vspace{-6pt} Many events occur in the world. Some event types are stochastically excited or inhibited---in the sense of having their probabilities elevated or decreased---by patterns in the sequence of previous events. Discovering such patterns can help us predict {\em which type} of event will happen next and {\em when}. We model streams of discrete events in continuous time, by constructing a \defn{neurally self-modulating multivariate point process} in which the intensities of multiple event types evolve according to a novel \defn{continuous-time LSTM}.   This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events.  Our model  has desirable qualitative properties.  It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.	score:447
When will be the next batch of on boarding in Accenture after July 1st?	 After several months, the aggressive behavior in both groups is measured.  If a significant correlation between group and outcome is observed (or  equivalently, the outcome is significantly different between the two groups), it can then be concluded that playing violent computer games indeed causes aggressive behavior.  Given the ethical and practical problems that such an experiment would involve,  one might wonder whether there are alternative ways to answer this question.  One such alternative is to combine data from different contexts. For example, in some countries the government may have decided to forbid certain ultra-violent games from being sold. In addition, some schools may have introduced certain measures to discourage aggressive behavior. By combining the data from these different contexts in an appropriate way, one may be able to identify the presence or absence of a causal effect of playing violent computer games on aggressive behavior.	 After several months, the aggressive behavior in both groups is measured.  If a significant correlation between group and outcome is observed (or  equivalently, the outcome is significantly different between the two groups), it can then be concluded that playing violent computer games indeed causes aggressive behavior.  Given the ethical and practical problems that such an experiment would involve,  one might wonder whether there are alternative ways to answer this question.  One such alternative is to combine data from different contexts. For example, in some countries the government may have decided to forbid certain ultra-violent games from being sold. In addition, some schools may have introduced certain measures to discourage aggressive behavior. By combining the data from these different contexts in an appropriate way, one may be able to identify the presence or absence of a causal effect of playing violent computer games on aggressive behavior.	score:455
When will be the next batch of on boarding in Accenture after July 1st?	 In Stackelberg stochastic games, one agent (leader) takes an action first and the second agent (follower) takes the action next after observing the action of the leader. However, the evolution of the state depends on the action of both agents. A precise definition is given in the next section. For Stackelberg stochastic games, we recover the necessary and sufficient conditions for approachability of convex sets and sufficient conditions for non-convex sets.  As expected, these conditions are similar to prior work on repeated and Markov  cases, but our proof is different as we rely on ideas from stochastic approximation theory for constructing an approachability strategy. Stochastic approximation as a tool for analyzing approachability was also used in \cite{BenHofSor1},  \cite{BenHofSor2}  where approachability for repeated games has been analyzed extensively.	 In Stackelberg stochastic games, one agent (leader) takes an action first and the second agent (follower) takes the action next after observing the action of the leader. However, the evolution of the state depends on the action of both agents. A precise definition is given in the next section. For Stackelberg stochastic games, we recover the necessary and sufficient conditions for approachability of convex sets and sufficient conditions for non-convex sets.  As expected, these conditions are similar to prior work on repeated and Markov  cases, but our proof is different as we rely on ideas from stochastic approximation theory for constructing an approachability strategy. Stochastic approximation as a tool for analyzing approachability was also used in \cite{BenHofSor1},  \cite{BenHofSor2}  where approachability for repeated games has been analyzed extensively.	score:461
When will be the next batch of on boarding in Accenture after July 1st?	 	Survival analysis, also known as time-to-event analysis aims to predict the first time of the occurrence of a stochastic event, conditioned on a set of features. An example in the case of medical data is the time of death or a graft failure after an operation. In cases where the time of event for many samples is missing because the event wasn't observed, this can be framed as a particular type of semi-supervised learning where part of the target values are referred to as right-censored.	 	Survival analysis, also known as time-to-event analysis aims to predict the first time of the occurrence of a stochastic event, conditioned on a set of features. An example in the case of medical data is the time of death or a graft failure after an operation. In cases where the time of event for many samples is missing because the event wasn't observed, this can be framed as a particular type of semi-supervised learning where part of the target values are referred to as right-censored.	score:461
Would you like to participate in a psychometric test on NLP Mind Maps?	 A natural way to solve this problem would be to learn the distribution in question to good accuracy, and then check if the corresponding hypothesis is close to one with the desired property. However, this testing-via-learning approach requires $\Omega(N)$ samples and is typically suboptimal. The main goal in this area is to obtain {\em sample-optimal} testers~--~ideally, testers that draw $o(N)$ samples from the underlying distribution.  During the past two decades, a wide range of properties have been studied, and we now have sample-optimal testers for many of these properties~\cite{Paninski:08, CDVV14, VV14, DK:16, DiakonikolasGPP16}.  We remark that even for the simplest properties, e.g., identity testing, at least $\Omega(\sqrt{N})$ many samples are required for   arbitrary distributions over $N$ atoms.	 A natural way to solve this problem would be to learn the distribution in question to good accuracy, and then check if the corresponding hypothesis is close to one with the desired property. However, this testing-via-learning approach requires $\Omega(N)$ samples and is typically suboptimal. The main goal in this area is to obtain {\em sample-optimal} testers~--~ideally, testers that draw $o(N)$ samples from the underlying distribution.  During the past two decades, a wide range of properties have been studied, and we now have sample-optimal testers for many of these properties~\cite{Paninski:08, CDVV14, VV14, DK:16, DiakonikolasGPP16}.  We remark that even for the simplest properties, e.g., identity testing, at least $\Omega(\sqrt{N})$ many samples are required for   arbitrary distributions over $N$ atoms.	score:374
Would you like to participate in a psychometric test on NLP Mind Maps?	   It can take into account attribute dissimilarity, dissimilarity of the relations an object participates in (including roles and multiplicity), dissimilarity of the neighbourhoods (in terms of attributes, relationships, or vertex identity), and interconnectivity or graph proximity of the objects being compared.     Consider for example Figure \ref{fig:Intro}.	   It can take into account attribute dissimilarity, dissimilarity of the relations an object participates in (including roles and multiplicity), dissimilarity of the neighbourhoods (in terms of attributes, relationships, or vertex identity), and interconnectivity or graph proximity of the objects being compared.     Consider for example Figure \ref{fig:Intro}.	score:377
Would you like to participate in a psychometric test on NLP Mind Maps?	} As the log-likelihood is concave, it can in principle be maximized by gradient ascent.  However, this requires repeatedly computing gradients of the log-partition function, which in general is intractable. One can circumvent this difficulty by using surrogates for the log-partition function~\citepmain{sutton2005, ganapathi2008, domke2013} or by approximating the partition function using sampling~\citepmain{petterson2009exponential,papandreou2011perturb}.    Alternatively, one can avoid likelihoods entirely, and use methods such as the structured perceptron or structured support vector machines (SVM-Struct) that rely only on a MAP solver~\citepmain{collins2002discriminative,roller2004max,tsochantaridis2004support, finley2008training}.  Such methods can often be quite accurate and are typically faster than approximate MLE, since MAP, or relaxations thereof, can be performed quickly using sophisticated combinatorial solvers.	} As the log-likelihood is concave, it can in principle be maximized by gradient ascent.  However, this requires repeatedly computing gradients of the log-partition function, which in general is intractable. One can circumvent this difficulty by using surrogates for the log-partition function~\citepmain{sutton2005, ganapathi2008, domke2013} or by approximating the partition function using sampling~\citepmain{petterson2009exponential,papandreou2011perturb}.    Alternatively, one can avoid likelihoods entirely, and use methods such as the structured perceptron or structured support vector machines (SVM-Struct) that rely only on a MAP solver~\citepmain{collins2002discriminative,roller2004max,tsochantaridis2004support, finley2008training}.  Such methods can often be quite accurate and are typically faster than approximate MLE, since MAP, or relaxations thereof, can be performed quickly using sophisticated combinatorial solvers.	score:381
Would you like to participate in a psychometric test on NLP Mind Maps?	        To summarize, our main contributions in this paper are as follows:   (a) we develop a \emph{Probabilistic Surface Optimization} (PSO) and \emph{pdf loss} that force any approximator function to converge to the target density;  (b) we use PSO to approximate a target density using a NN;  (c) we train a NN via PSO in batch mode using exponential learning rate decay for better convergence, naming the entire algorithm DeepPDF;  and (d) we analyze different DL aspects of DeepPDF.   Additionally, for simplicity the derivations are gathered together under Appendix, at the end of the paper.        \begin{figure}[t]   \centering      \begin{tabular}{ccc}        \subfloat[\label{fig:Overview-a}]{\includegraphics[width=0.45\textwidth]{drawing.png}}    &        \subfloat[\label{fig:Overview-b}]{\includegraphics[width=0.23\textwidth]{cosine-dist-eps-converted-to.	        To summarize, our main contributions in this paper are as follows:   (a) we develop a \emph{Probabilistic Surface Optimization} (PSO) and \emph{pdf loss} that force any approximator function to converge to the target density;  (b) we use PSO to approximate a target density using a NN;  (c) we train a NN via PSO in batch mode using exponential learning rate decay for better convergence, naming the entire algorithm DeepPDF;  and (d) we analyze different DL aspects of DeepPDF.   Additionally, for simplicity the derivations are gathered together under Appendix, at the end of the paper.        \begin{figure}[t]   \centering      \begin{tabular}{ccc}        \subfloat[\label{fig:Overview-a}]{\includegraphics[width=0.45\textwidth]{drawing.png}}    &        \subfloat[\label{fig:Overview-b}]{\includegraphics[width=0.23\textwidth]{cosine-dist-eps-converted-to.	score:385
Would you like to participate in a psychometric test on NLP Mind Maps?	 By these means, the search space is heavily constrained and efficient solutions can be learned with gradient descent.  ConvNets are namely adapted for computer vision because of the compositional structure of an image. Texts have similar properties : characters combine to form n-grams, stems, words, phrase, sentences etc.                    We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences, jointly with the task.   In this paper, we propose to use deep architectures of many convolutional layers to approach this goal, using up to 29 layers.  The design of our architecture is inspired by recent progress in computer vision, in particular \cite{msr:2016:iclr:vgg,He:2015:resnet}.  This paper is structured as follows. There have been previous attempts to use ConvNets for text processing.	 By these means, the search space is heavily constrained and efficient solutions can be learned with gradient descent.  ConvNets are namely adapted for computer vision because of the compositional structure of an image. Texts have similar properties : characters combine to form n-grams, stems, words, phrase, sentences etc.                    We believe that a challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences, jointly with the task.   In this paper, we propose to use deep architectures of many convolutional layers to approach this goal, using up to 29 layers.  The design of our architecture is inspired by recent progress in computer vision, in particular \cite{msr:2016:iclr:vgg,He:2015:resnet}.  This paper is structured as follows. There have been previous attempts to use ConvNets for text processing.	score:386
What's a good introductory machine learning text?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:328
What's a good introductory machine learning text?	    Automated Text Scoring (ATS) provides a cost-effective and   consistent alternative to human marking. However, in order to   achieve good performance, the predictive features of the system need   to be manually engineered by human experts. We introduce a model   that forms word representations by learning the extent to which   specific words contribute to the text's score.   Using Long-Short   Term Memory networks to represent the meaning of texts, we   demonstrate that a fully automated framework is able to achieve   excellent results over similar approaches. In an attempt to make our   results more interpretable, and inspired by recent advances in   visualizing neural networks, we introduce a novel method for   identifying the regions of the text that the model has found more   discriminative.	    Automated Text Scoring (ATS) provides a cost-effective and   consistent alternative to human marking. However, in order to   achieve good performance, the predictive features of the system need   to be manually engineered by human experts. We introduce a model   that forms word representations by learning the extent to which   specific words contribute to the text's score.   Using Long-Short   Term Memory networks to represent the meaning of texts, we   demonstrate that a fully automated framework is able to achieve   excellent results over similar approaches. In an attempt to make our   results more interpretable, and inspired by recent advances in   visualizing neural networks, we introduce a novel method for   identifying the regions of the text that the model has found more   discriminative.	score:331
What's a good introductory machine learning text?	 The results show that the proposed method in this paper significantly outperforms it for web document retrieval task. 	\IEEEPARstart{L}{earning} a good representation (or features) of input data is an important task in machine learning. In text and language processing, one such problem is  learning of an embedding vector for a sentence; that is, to train a model that can automatically transform a sentence to a vector that encodes the semantic meaning of the sentence.   While word embedding is learned using a loss function defined on word pairs, sentence embedding is learned using a loss function defined on sentence pairs. In the sentence embedding usually the relationship among words in the sentence, i.e., the context information, is taken into consideration. Therefore, sentence embedding is more suitable for tasks that require computing semantic similarities between text strings.	 The results show that the proposed method in this paper significantly outperforms it for web document retrieval task. 	\IEEEPARstart{L}{earning} a good representation (or features) of input data is an important task in machine learning. In text and language processing, one such problem is  learning of an embedding vector for a sentence; that is, to train a model that can automatically transform a sentence to a vector that encodes the semantic meaning of the sentence.   While word embedding is learned using a loss function defined on word pairs, sentence embedding is learned using a loss function defined on sentence pairs. In the sentence embedding usually the relationship among words in the sentence, i.e., the context information, is taken into consideration. Therefore, sentence embedding is more suitable for tasks that require computing semantic similarities between text strings.	score:353
What's a good introductory machine learning text?	 Can one quantify how a constraint of getting only a few bits from each example affects our ability to learn? To the best of our knowledge, there are currently no generic tools which allow us to answer such questions, at least in the context of standard machine learning settings.  In this paper, we make a first step in developing such a framework. We consider a general class of learning processes, characterized only by information-theoretic constraints on how they may interact with the data (and independent of any specific problem semantics).	 In this paper, we describe how a single set of results implies positive answers to the above, for several different settings. 	Information constraints play a key role in machine learning. Of course, the main constraint is the availability of only a finite data set, from which the learner is expected to generalize. However, many problems currently researched in machine learning can be characterized as learning with \emph{additional} information constraints, arising from the manner in which the learner may interact with the data.  Some examples include: \begin{itemize}     \item \emph{Communication constraints in distributed learning:} There         has been much work in recent years on learning when the training         data is distributed among several machines (with         \cite{bekkerman2011scaling,agarwal2011reliable,dekel2011optimal,niu2011hogwild,cotter2011better,duchi2012dual,balcan2012distributed,boyd2011distributed,kyrola2011parallel}         being just a few examples).	score:360
What's a good introductory machine learning text?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:366
Is it possible to study for a PhD in computer science focused on machine learning after a BS and MS in mathematics?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.    The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	   The purpose of this position paper is to put forward a new perspective and a novel conjecture within the study of machine learning.  In what follows I will present four basic problems (or levels) in machine learning. The study on information theoretical learning is briefly reviewed.  A theorem between the empirically-defined similarity measures  and information measures are given. Based on the existing investigations, a conjecture is proposed in this paper.	score:383
Is it possible to study for a PhD in computer science focused on machine learning after a BS and MS in mathematics?	   However, most of the recent work dealt with \emph{linear high-dimensional} variable selection, while the focus of much of the earlier work in machine learning and statistics was  on \emph{non-linear low-dimensional} problems: indeed, in the last two decades,  kernel methods have been a prolific  theoretical and algorithmic machine learning framework.  By using appropriate regularization by Hilbertian norms, representer theorems enable to consider large and potentially infinite-dimensional feature spaces while working within an implicit feature space no larger than the number of observations. This has led to numerous works on kernel design adapted to specific data types and generic kernel-based algorithms for many learning tasks \citep[see, e.	   However, most of the recent work dealt with \emph{linear high-dimensional} variable selection, while the focus of much of the earlier work in machine learning and statistics was  on \emph{non-linear low-dimensional} problems: indeed, in the last two decades,  kernel methods have been a prolific  theoretical and algorithmic machine learning framework.  By using appropriate regularization by Hilbertian norms, representer theorems enable to consider large and potentially infinite-dimensional feature spaces while working within an implicit feature space no larger than the number of observations. This has led to numerous works on kernel design adapted to specific data types and generic kernel-based algorithms for many learning tasks \citep[see, e.	score:403
Is it possible to study for a PhD in computer science focused on machine learning after a BS and MS in mathematics?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:407
Is it possible to study for a PhD in computer science focused on machine learning after a BS and MS in mathematics?	   In recent years it has become popular to study machine learning   problems       in a setting of    ordinal distance information rather than numerical   distance measurements. By ordinal distance information we refer to   binary answers to distance comparisons such as    $d(A,B)<d(C,D)$.     For many problems in machine learning and statistics it is unclear how   to solve them in such a scenario.  Up to now, the main approach is to   explicitly construct an ordinal embedding of the data points in the   Euclidean space, an approach that has a number of drawbacks.  In   this paper, we propose algorithms for the problems of medoid   estimation, outlier identification, classification, and clustering   when given only ordinal    data.      They are based on   estimating the lens depth function and the $k$-relative neighborhood   graph on a data set.	   In recent years it has become popular to study machine learning   problems       in a setting of    ordinal distance information rather than numerical   distance measurements. By ordinal distance information we refer to   binary answers to distance comparisons such as    $d(A,B)<d(C,D)$.     For many problems in machine learning and statistics it is unclear how   to solve them in such a scenario.  Up to now, the main approach is to   explicitly construct an ordinal embedding of the data points in the   Euclidean space, an approach that has a number of drawbacks.  In   this paper, we propose algorithms for the problems of medoid   estimation, outlier identification, classification, and clustering   when given only ordinal    data.      They are based on   estimating the lens depth function and the $k$-relative neighborhood   graph on a data set.	score:412
Is it possible to study for a PhD in computer science focused on machine learning after a BS and MS in mathematics?	 However, communication can be adjusted and modified based on the problem at hand achieving superior results. A new trend in research is started to design  distributed and joint communication-computation optimal systems \cite{jordan2015machine}.         In this paper, we present some important machine learning instances in distributed settings and determine analytically and experimentally how one can obtain the optimal trade-off between performance and communication cost.  The problem that we focus on  is learning of \emph{Gaussian Processes} (GPs) which are fundamental models applicable to many machine learning tasks.  We propose efficient methods for GP learning and analyze the performance and communication cost of the proposed methods. Although GPs can be used for both classification and regression, we only consider regression in presented applications and experiments.	 Authors of \cite{balcan2015communication} have addressed the kernel PCA in distributed systems on a large amount of data.  Distributed machine learning problems are also related to  distributed optimization problems as optimization of  an objective function during their training or test procedure is part of learning tasks.  Therefore, several studies on  distributed optimization algorithms \cite{boyd2011distributed} can directly or indirectly be applied to  many machine learning models.  Alternating Direction Method of Multipliers (ADMM) \cite{boyd2011distributed} is one of the most successful distributed algorithms for solving optimization problems over distributed datasets. There are many papers that are based on the ADMM algorithm \cite{zhang2014asynchronous, forero2010consensus, liu2012distributed}. For example, Forero et. al. \cite{forero2010consensus} proposed a re-parameterization for SVM learning in distributed manner based on ADMM.	score:414
What do I start with for learning machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:413
What do I start with for learning machine learning?	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:443
What do I start with for learning machine learning?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:443
What do I start with for learning machine learning?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:449
What do I start with for learning machine learning?	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	score:452
Where should I start for machine learning?	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	 Many machine learning models can be reformulated as the following optimization problems: \begin{equation} \min_{x\in\RB^{d}} F(x) = \frac{1}{n}\sum_{i=1}^{n}f_i(x). \label{eq:prob_desc} \end{equation} where each $f_i$ is the loss with respect to (w.r.t.) the $i$-th training sample. There are many examples such as logistic regressions, smoothed support vector machines, neural networks, and graphical models.    In the era of big data, large-scale optimization algorithms have become an important challenge. The stochastic gradient descent algorithm (SGD) has been widely employed to reduce the  computational cost \citep{cotter2011better,li2014efficient,robbins1951stochastic}.  However, SGD has poor convergence property. Hence, many variants have been proposed to improve the convergence rate of SGD \citep{johnson2013accelerating, roux2012stochastic,schmidt2013minimizing,Zhang}.	score:476
Where should I start for machine learning?	 In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model.  We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process.	 In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model.  We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process.	score:478
Where should I start for machine learning?	 However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks.	 However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks.	score:482
Where should I start for machine learning?	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	\\   Why should a machine learning expert be interested in quantum computation? And why are we expecting quantum computers to be useful in machine learning? We can offer two reasons. First, with an ever growing amount of data, current machine learning systems are rapidly approaching the limits of classical computational models. In this sense, quantum algorithms offer faster solutions to process information for selected classes of problems.	score:487
Where should I start for machine learning?	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	score:487
What are the pros and cons of using R vs. C++ for machine learning?	  An intuitive description of the paper's results is this: From prior and corresponding choice of learning machinery (Section \ref{sec:class-learn-probl}), we construct statements about the \emph{dynamics of the learning process} (Section \ref{sec:optim-contr-learn}). The learning machine itself provides a probabilistic description of the dynamics of the physical system.  Combining both dynamics yields a \emph{joint} system, which we aim to control optimally. Doing so amounts to simultaneously controlling exploration (controlling the learning system) and exploitation (controlling the physical system).  Because large parts of the analysis rely on concepts from optimal control theory, this paper will use notation from that field.	  An intuitive description of the paper's results is this: From prior and corresponding choice of learning machinery (Section \ref{sec:class-learn-probl}), we construct statements about the \emph{dynamics of the learning process} (Section \ref{sec:optim-contr-learn}). The learning machine itself provides a probabilistic description of the dynamics of the physical system.  Combining both dynamics yields a \emph{joint} system, which we aim to control optimally. Doing so amounts to simultaneously controlling exploration (controlling the learning system) and exploitation (controlling the physical system).  Because large parts of the analysis rely on concepts from optimal control theory, this paper will use notation from that field.	score:378
What are the pros and cons of using R vs. C++ for machine learning?	  It is important to differentiate between reinforcement learning and common streams of research in machine learning. For instance, in supervised learning, the learning is facilitated by training samples provided by a source external to the agent and the computer. In reinforcement learning, the training samples are provided only by the interaction of the agent itself with the environment.  For example, in a motion planning problem in an uncharted territory, it is desired that the agent learns in the fastest possible way to navigate correctly,  with the fewest blind decisions required to be made. This is known as the dilemma of \emph{exploration versus exploitation}; that is, neither exploration nor exploitation can be pursued exclusively without facing a penalty or failing at the task.	  It is important to differentiate between reinforcement learning and common streams of research in machine learning. For instance, in supervised learning, the learning is facilitated by training samples provided by a source external to the agent and the computer. In reinforcement learning, the training samples are provided only by the interaction of the agent itself with the environment.  For example, in a motion planning problem in an uncharted territory, it is desired that the agent learns in the fastest possible way to navigate correctly,  with the fewest blind decisions required to be made. This is known as the dilemma of \emph{exploration versus exploitation}; that is, neither exploration nor exploitation can be pursued exclusively without facing a penalty or failing at the task.	score:381
What are the pros and cons of using R vs. C++ for machine learning?	 It relies on coupling the training of multiple models and combining their predictions into a single output. This coupling is performed by encouraging the models to agree with each other on their predictions, while training. We show how the proposed framework is inspired by human learning, in contrast with other approaches commonly used in machine learning.  Finally, we provide experimental results which exhibit how our framework successfully manages to outperform cross-validation and other ensemble methods that do not couple the training of the models.  \begin{figure}[t!]  \centering      \includegraphics[width=0.5\textwidth, trim=9.2cm 2.5cm 1.5cm 6cm, clip]{diagram.pdf}   \caption{Illustration of the agreement-based learning framework that we propose.	 In section \ref{sec:related_work}, we discuss existing work in this direction and how they differ from our proposal. Finally, in \ref{sec:experiments} we provide an extensive experimental evaluation of the agreement-based learning framework.  \subsection{MACHINE VS NATURAL LEARNING} \label{sec:machine_vs_natural_learning}  {\em Supervised learning} is one of the first and still dominant approaches used to make machines able to learn.  It comprises providing the machine a set of input-output pairs and expecting it to learn a {\em generalized} mapping between inputs and outputs. Thus, the machine must learn to produce correct outputs for new --- previously unseen --- inputs. This process is inspired by the way in which humans learn. Children ask people to name things they do not recognize.	score:381
What are the pros and cons of using R vs. C++ for machine learning?	 Many ideas today are prototyped in high-level languages such as MATLAB, Python and Julia, and it is very rare that researchers implement their algorithms in lower-level languages such as C and C++. Even so, these prototypes are often incomplete, have different abstractions and coding style, and are hand-crafted for specific problem-platform combinations.  Ultimately, this results in either performance degradations of high-level implementations in production, or high engineering costs attached to rewriting the low-level implementations from scratch for each different problem-platform combination.  Currently, there exist numerous machine-learning libraries, each targeting a different need. Libraries such as \texttt{PyTorch/Caffe2}~\cite{2018-PyTorch}, \texttt{Theano}~\cite{2016-Team} and \texttt{TensorFlow}~\cite{2016-Abadi} primarily target deep-learning applications, and support different powerful computing architectures.	 Many ideas today are prototyped in high-level languages such as MATLAB, Python and Julia, and it is very rare that researchers implement their algorithms in lower-level languages such as C and C++. Even so, these prototypes are often incomplete, have different abstractions and coding style, and are hand-crafted for specific problem-platform combinations.  Ultimately, this results in either performance degradations of high-level implementations in production, or high engineering costs attached to rewriting the low-level implementations from scratch for each different problem-platform combination.  Currently, there exist numerous machine-learning libraries, each targeting a different need. Libraries such as \texttt{PyTorch/Caffe2}~\cite{2018-PyTorch}, \texttt{Theano}~\cite{2016-Team} and \texttt{TensorFlow}~\cite{2016-Abadi} primarily target deep-learning applications, and support different powerful computing architectures.	score:388
What are the pros and cons of using R vs. C++ for machine learning?	  Through the use of C++ templates, MLPACK both eliminates unnecessary copying of datasets and performs expression optimizations unavailable in other languages.  Also, MLPACK is, to our knowledge, unique among existing libraries in using generic programming features of C++ to allow customization of the available machine learning methods without incurring performance penalties.	  Through the use of C++ templates, MLPACK both eliminates unnecessary copying of datasets and performs expression optimizations unavailable in other languages.  Also, MLPACK is, to our knowledge, unique among existing libraries in using generic programming features of C++ to allow customization of the available machine learning methods without incurring performance penalties.	score:389
When did you start with artificial intelligence and machine learning?	 CodRep is a machine learning competition on source code data.  It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis.   In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact.  The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at \url{https://github.com/KTH/CodRep-competition/}. 	Competitions are great to foster creativity in a problem domain. The CodRep competition aims at encouraging scientific and technological progress in the domain of machine learning over source code.    The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.    The competition requires to build a system that takes as input a set of pairs (source code line, source code file), and outputs, for each pair, the predicted line number of the source code to be replaced by  the source code line. The participant are given datasets carefully extracted and curated from open-source projects. For instance, \texttt{Dataset1} is composed of 4394 prediction tasks.	   The CodRep competition is carefully designed so that anybody can enter the competition, whether professional researcher, student or independent scholar, without specific knowledge in machine learning or program analysis.   The CodRep competition can also be seen as a common playground on which the machine learning and the software engineering research communities can interact.	score:420
When did you start with artificial intelligence and machine learning?	  This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation.  A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.  	Robots are widely used to complete various manipulation tasks in industrial manufacturing factories where environments are relatively static and simple.	  This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation.  A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.  	Robots are widely used to complete various manipulation tasks in industrial manufacturing factories where environments are relatively static and simple.	score:429
When did you start with artificial intelligence and machine learning?	 The huge success of machine learning shows that computer AI could go beyond human knowledge in complicated tasks, even starting from scratch. After that, machine learning algorithms had been widely applied to various areas such as human vision, natural language understanding, medical image processing, etc., and had achieved great accomplishments. With the breakthrough of these algorithms, the improvements in hardware support the AI algorithms works more efficient, such as GPU/TPU acceleration.   Neural network models~\cite{gevrey2003review,schmidhuber2015deep}, as an important group of algorithms used for machine learning, are inspired by the biology of human brains. Classic neural network models have input layer, output layer and hidden layers. Hidden layers consist of lots of connecting artificial neurons. These neurons are tuned according to the input and output data, to precisely reflect the relationship.	 The huge success of machine learning shows that computer AI could go beyond human knowledge in complicated tasks, even starting from scratch. After that, machine learning algorithms had been widely applied to various areas such as human vision, natural language understanding, medical image processing, etc., and had achieved great accomplishments. With the breakthrough of these algorithms, the improvements in hardware support the AI algorithms works more efficient, such as GPU/TPU acceleration.   Neural network models~\cite{gevrey2003review,schmidhuber2015deep}, as an important group of algorithms used for machine learning, are inspired by the biology of human brains. Classic neural network models have input layer, output layer and hidden layers. Hidden layers consist of lots of connecting artificial neurons. These neurons are tuned according to the input and output data, to precisely reflect the relationship.	score:435
When did you start with artificial intelligence and machine learning?	 	Statistical machine learning technologies in the real world are never without a purpose. Using their predictions, humans or machines make decisions whose circuitous consequences often violate the modeling assumptions that justified the system design in the first place.  Such contradictions appear very clearly in the case of the learning systems that power web scale applications such as search engines, ad placement engines, or recommandation systems.  For instance, the placement of advertisement on the result pages of Internet search engines depend on the bids of advertisers and on scores computed by statistical machine learning systems. Because the scores affect the contents of the result pages proposed to the users, they directly influence the occurrence of clicks and the corresponding advertiser payments.	 	Statistical machine learning technologies in the real world are never without a purpose. Using their predictions, humans or machines make decisions whose circuitous consequences often violate the modeling assumptions that justified the system design in the first place.  Such contradictions appear very clearly in the case of the learning systems that power web scale applications such as search engines, ad placement engines, or recommandation systems.  For instance, the placement of advertisement on the result pages of Internet search engines depend on the bids of advertisers and on scores computed by statistical machine learning systems. Because the scores affect the contents of the result pages proposed to the users, they directly influence the occurrence of clicks and the corresponding advertiser payments.	score:436
When did you start with artificial intelligence and machine learning?	 However, communication can be adjusted and modified based on the problem at hand achieving superior results. A new trend in research is started to design  distributed and joint communication-computation optimal systems \cite{jordan2015machine}.         In this paper, we present some important machine learning instances in distributed settings and determine analytically and experimentally how one can obtain the optimal trade-off between performance and communication cost.  The problem that we focus on  is learning of \emph{Gaussian Processes} (GPs) which are fundamental models applicable to many machine learning tasks.  We propose efficient methods for GP learning and analyze the performance and communication cost of the proposed methods. Although GPs can be used for both classification and regression, we only consider regression in presented applications and experiments.	 However, communication can be adjusted and modified based on the problem at hand achieving superior results. A new trend in research is started to design  distributed and joint communication-computation optimal systems \cite{jordan2015machine}.         In this paper, we present some important machine learning instances in distributed settings and determine analytically and experimentally how one can obtain the optimal trade-off between performance and communication cost.  The problem that we focus on  is learning of \emph{Gaussian Processes} (GPs) which are fundamental models applicable to many machine learning tasks.  We propose efficient methods for GP learning and analyze the performance and communication cost of the proposed methods. Although GPs can be used for both classification and regression, we only consider regression in presented applications and experiments.	score:439
If members of the NITK IT batch 2016 were Game of Thrones, characters, who would be whom?	 When the network externality is negative, the game becomes an anti-coordination game, where one agent seeks the strategy that differs from others' to maximize his own reward. Nevertheless, in such a scenario, the agent's decision also contains some information about his belief on the uncertain system state, which can be learned by subsequent agents through social learning algorithms.  Thus, subsequent agents may then realize that his choice is better than others, and make the same decision with the agent. Since the network externality is negative, the information leaked by the agent's decision may impair the reward the agent can obtain in the game. Therefore, rational agents should take into account the possible reactions of subsequent players to maximize their own rewards.   The negative network externality plays an important rule in many applications in different research fields, such as spectrum access in cognitive radio, storage service selection in cloud computing, and deal selection on Groupon in online social networking. In spectrum access problem, for instance, secondary users access the same spectrum need to share with each others.	 When the network externality is negative, the game becomes an anti-coordination game, where one agent seeks the strategy that differs from others' to maximize his own reward. Nevertheless, in such a scenario, the agent's decision also contains some information about his belief on the uncertain system state, which can be learned by subsequent agents through social learning algorithms.  Thus, subsequent agents may then realize that his choice is better than others, and make the same decision with the agent. Since the network externality is negative, the information leaked by the agent's decision may impair the reward the agent can obtain in the game. Therefore, rational agents should take into account the possible reactions of subsequent players to maximize their own rewards.   The negative network externality plays an important rule in many applications in different research fields, such as spectrum access in cognitive radio, storage service selection in cloud computing, and deal selection on Groupon in online social networking. In spectrum access problem, for instance, secondary users access the same spectrum need to share with each others.	score:346
If members of the NITK IT batch 2016 were Game of Thrones, characters, who would be whom?	 However, virtually all this progress in game theoretic approaches to large games has operated under the assumption that the parameters of the game are known to the solvers, and that the main challenge is simply finding the optimal strategy.  In contrast, in many real world scenarios, certain elements of the game (e.g., payoff matrices, chance node probabilities, etc), are unknown to some of the agents prior to the game.   For example, in security games, we may want to understand the underlying payoffs of an adversary, rather than just their observed strategy, to better understand how aspects of the game can be manipulated or changed to get a desirable outcome.   \indent In this paper, we propose an end-to-end framework for learning the parameters of uncertain games (both for normal-form and extensive-form games), purely by observing the actions of the agents.	 However, virtually all this progress in game theoretic approaches to large games has operated under the assumption that the parameters of the game are known to the solvers, and that the main challenge is simply finding the optimal strategy.  In contrast, in many real world scenarios, certain elements of the game (e.g., payoff matrices, chance node probabilities, etc), are unknown to some of the agents prior to the game.   For example, in security games, we may want to understand the underlying payoffs of an adversary, rather than just their observed strategy, to better understand how aspects of the game can be manipulated or changed to get a desirable outcome.   \indent In this paper, we propose an end-to-end framework for learning the parameters of uncertain games (both for normal-form and extensive-form games), purely by observing the actions of the agents.	score:365
If members of the NITK IT batch 2016 were Game of Thrones, characters, who would be whom?	 This equivalence simplifies the analysis of what would otherwise be an infinite state Markov process. These results extend prior analysis on this type of aspiration learning schemes to games of multiple players and actions. We also specialize the results for a class of games that is a generalized version of so-called \emph{coordination games}. In particular, we show that, in these games, the unique invariant distribution of the equivalent finite-state Markov chain puts arbitrarily large weight on the \emph{payoff-dominant} action profiles if the step size of the aspiration-level update becomes sufficiently small.	 Agents then learn which action to play based only on their own previous experience of the game (actions played and utilities received). A major challenge in this setting is that explicit utility function  optimization may be impractical. This may be due to inherent complexity  (e.g., a large number of players or actions), or the lack of any closed  form expression for the utility function.  Rather, rewards can be measured  online. In terms of game theoretic learning, this eliminates adaptation  based on an ability to compute a ``best reply''.  Another obstacle to utility  maximization is that from any agent's perspective, the environment includes  other adapting agents, and hence is nonstationary. Consequently, actions  that may have been effective in the past need not continue to be effective.	score:370
If members of the NITK IT batch 2016 were Game of Thrones, characters, who would be whom?	 Sec.~\ref{sec:main-results-game} shows that if each agent individually follows the proposed algorithm, the  experienced regret will at most be $\epsilon$ after sufficient repeated plays of the game. Moreover, if all agents follow the proposed algorithm independently, their collective behavior across the network will converge to an $\epsilon$-distance of the polytope of correlated equilibria.            \subsubsection{Detection of equilibrium play in a social networks} The second part of the paper (Sec.~\ref{sec:revealed} to Sec.~\ref{sec:ExamplesofEquilibriumPlay}) addresses the question: Given datasets of the external influence and actions of agents in a social network, is it possible to detect if the behavior of agents is consistent with play from the equilibrium of a concave potential game.	 Agents form homophilic social groups, within which they exchange past experiences over an undirected graph. It is shown that, if all agents follow the proposed algorithm, their global behavior is attracted to the correlated equilibria set of the game. The second part of the paper provides a test to detect if the actions of agents are consistent with play from the equilibrium of a concave potential game.  The theory of revealed preference from microeconomics is used to construct a non-parametric decision test and statistical test which only require the probe and associated actions of agents.  A stochastic gradient algorithm is given to optimize the probe in real time to minimize the Type-II error probabilities of the detection test subject to specified Type-I error probability.	score:370
If members of the NITK IT batch 2016 were Game of Thrones, characters, who would be whom?	 Nor should the goal be simply to create a decision rule that cannot be affected at all by an agent's behavior; while this eliminates the risk of gaming, it also eliminates the opportunity for the decision rule to incentivize behavior that the evaluator views as valuable.  If there were no intermediate features, and the evaluator  could completely observe an agent's choices about how they spent their effort across different actions, then the evaluator could simply reward exactly the actions they want to incentivize.  But when the actions taken by an individual are hidden, and can be perceived only through an intermediate layer of proxy features, then the evaluator cannot necessarily tell whether these features are the result of effort they intended to promote  (improving the underlying attribute that the feature is intended to measure) or effort from other actions that also affect the feature.	 Nor should the goal be simply to create a decision rule that cannot be affected at all by an agent's behavior; while this eliminates the risk of gaming, it also eliminates the opportunity for the decision rule to incentivize behavior that the evaluator views as valuable.  If there were no intermediate features, and the evaluator  could completely observe an agent's choices about how they spent their effort across different actions, then the evaluator could simply reward exactly the actions they want to incentivize.  But when the actions taken by an individual are hidden, and can be perceived only through an intermediate layer of proxy features, then the evaluator cannot necessarily tell whether these features are the result of effort they intended to promote  (improving the underlying attribute that the feature is intended to measure) or effort from other actions that also affect the feature.	score:371
If the members of NITK Electrical and Electronics batch 2016 were Game Of Thrones characters, who would be whom?	 Thus, subsequent agents may then realize that his choice is better than others, and make the same decision with the agent. Since the network externality is negative, the information leaked by the agent's decision may impair the reward the agent can obtain in the game. Therefore, rational agents should take into account the possible reactions of subsequent players to maximize their own rewards.   The negative network externality plays an important rule in many applications in different research fields, such as spectrum access in cognitive radio, storage service selection in cloud computing, and deal selection on Groupon in online social networking. In spectrum access problem, for instance, secondary users access the same spectrum need to share with each others.	 Thus, subsequent agents may then realize that his choice is better than others, and make the same decision with the agent. Since the network externality is negative, the information leaked by the agent's decision may impair the reward the agent can obtain in the game. Therefore, rational agents should take into account the possible reactions of subsequent players to maximize their own rewards.   The negative network externality plays an important rule in many applications in different research fields, such as spectrum access in cognitive radio, storage service selection in cloud computing, and deal selection on Groupon in online social networking. In spectrum access problem, for instance, secondary users access the same spectrum need to share with each others.	score:386
If the members of NITK Electrical and Electronics batch 2016 were Game Of Thrones characters, who would be whom?	}\footnote{Please see Appendix~\ref{app:elo} for a description of the Elo rating system.}.  	\subsection{Computers and Chess}  \label{sec:computers_and_chess} Chess is a game that requires so much creativity and sophisticated reasoning that it was once thought of as something no computers will ever be able to do. It was frequently listed alongside activities like poetry writing and painting, as examples of tasks that can only be performed by humans.	}\footnote{Please see Appendix~\ref{app:elo} for a description of the Elo rating system.}.  	\subsection{Computers and Chess}  \label{sec:computers_and_chess} Chess is a game that requires so much creativity and sophisticated reasoning that it was once thought of as something no computers will ever be able to do. It was frequently listed alongside activities like poetry writing and painting, as examples of tasks that can only be performed by humans.	score:405
If the members of NITK Electrical and Electronics batch 2016 were Game Of Thrones characters, who would be whom?	 In addition to the Go game, DRL has been widely used in other games such as \textit{Atari} \citep{mnih2015human}, \textit{Robot Soccer} \citep{hausknecht2016deep,masson2016reinforcement}, and \textit{Torcs} \citep{lillicrap2015continuous} to achieve super-human performances.  However, most existing DRL methods require the action space to be either finite and discrete  (e. g., Go and \textit{Atari}) or  continuous   (e.g. \textit{MuJoCo} and Torcs). For example, the algorithms for discrete action space include deep Q-network (DQN) \citep{mnih2013playing}, Double DQN \citep{hasselt2016deep}, A3C \citep{mnih2016asynchronous}; the algorithms for continuous action space include deterministic policy gradients (DPG) \citep{silver2014deterministic} and its deep version DDPG \citep{lillicrap2015continuous}.	 In addition to the Go game, DRL has been widely used in other games such as \textit{Atari} \citep{mnih2015human}, \textit{Robot Soccer} \citep{hausknecht2016deep,masson2016reinforcement}, and \textit{Torcs} \citep{lillicrap2015continuous} to achieve super-human performances.  However, most existing DRL methods require the action space to be either finite and discrete  (e. g., Go and \textit{Atari}) or  continuous   (e.g. \textit{MuJoCo} and Torcs). For example, the algorithms for discrete action space include deep Q-network (DQN) \citep{mnih2013playing}, Double DQN \citep{hasselt2016deep}, A3C \citep{mnih2016asynchronous}; the algorithms for continuous action space include deterministic policy gradients (DPG) \citep{silver2014deterministic} and its deep version DDPG \citep{lillicrap2015continuous}.	score:406
If the members of NITK Electrical and Electronics batch 2016 were Game Of Thrones characters, who would be whom?	 These lines of work are partially motivated by the idea that artificial communication (and other manifestations of machine intelligence) can emerge through interacting with the world and/or other agents, which could then converge towards human language \citep{Gauthier:2016arxiv,Mikolov:2015arxiv,Lake:2016arxiv,Kiela:2016arxiv}. \citep{lazaridou2016multi} have recently proposed a basic version of this game, where there is only a single transmission of a message from the sender to the receiver, as a test bed for both inducing and analyzing a communication protocol between two neural network-based agents.	 These lines of work are partially motivated by the idea that artificial communication (and other manifestations of machine intelligence) can emerge through interacting with the world and/or other agents, which could then converge towards human language \citep{Gauthier:2016arxiv,Mikolov:2015arxiv,Lake:2016arxiv,Kiela:2016arxiv}. \citep{lazaridou2016multi} have recently proposed a basic version of this game, where there is only a single transmission of a message from the sender to the receiver, as a test bed for both inducing and analyzing a communication protocol between two neural network-based agents.	score:407
If the members of NITK Electrical and Electronics batch 2016 were Game Of Thrones characters, who would be whom?	  These characterizations allow us to phrase quantities of interest in terms of eigenvalues and eigenfunctions of an integral operator defined by the kernel. We then discuss previous results of \citet{driscoll1973reproducing,LukBed01} providing a necessary and sufficient condition for a given GP to be a member of a given RKHS (which can be different from the RKHS associated with the covariance kernel of the GP).  This implies that, while GP sample paths are almost surely outside of the corresponding RKHS, they lie in a function space ``slightly larger'' than the RKHS, which is itself a certain RKHS \citep{SteSco12,Ste17}. In this sense, the Bayesian prior and the frequentist hypothesis space, while not identical, are arguably closer to each other than is often acknowledged.	  These characterizations allow us to phrase quantities of interest in terms of eigenvalues and eigenfunctions of an integral operator defined by the kernel. We then discuss previous results of \citet{driscoll1973reproducing,LukBed01} providing a necessary and sufficient condition for a given GP to be a member of a given RKHS (which can be different from the RKHS associated with the covariance kernel of the GP).  This implies that, while GP sample paths are almost surely outside of the corresponding RKHS, they lie in a function space ``slightly larger'' than the RKHS, which is itself a certain RKHS \citep{SteSco12,Ste17}. In this sense, the Bayesian prior and the frequentist hypothesis space, while not identical, are arguably closer to each other than is often acknowledged.	score:408
Is it possible to get a machine learning job without a master's degree?	  It is important to differentiate between reinforcement learning and common streams of research in machine learning. For instance, in supervised learning, the learning is facilitated by training samples provided by a source external to the agent and the computer. In reinforcement learning, the training samples are provided only by the interaction of the agent itself with the environment.  For example, in a motion planning problem in an uncharted territory, it is desired that the agent learns in the fastest possible way to navigate correctly,  with the fewest blind decisions required to be made. This is known as the dilemma of \emph{exploration versus exploitation}; that is, neither exploration nor exploitation can be pursued exclusively without facing a penalty or failing at the task.	  It is important to differentiate between reinforcement learning and common streams of research in machine learning. For instance, in supervised learning, the learning is facilitated by training samples provided by a source external to the agent and the computer. In reinforcement learning, the training samples are provided only by the interaction of the agent itself with the environment.  For example, in a motion planning problem in an uncharted territory, it is desired that the agent learns in the fastest possible way to navigate correctly,  with the fewest blind decisions required to be made. This is known as the dilemma of \emph{exploration versus exploitation}; that is, neither exploration nor exploitation can be pursued exclusively without facing a penalty or failing at the task.	score:343
Is it possible to get a machine learning job without a master's degree?	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	  Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:360
Is it possible to get a machine learning job without a master's degree?	 In this paper machine learning experiments are used to investigate some elements of this hypothesis by seeking answers for the following questions: are there machine learning tasks which are intrinsically hard for a lone learning agent but that may become very easy when intermediate concepts are provided by another agent as additional intermediate learning cues, in the spirit of Curriculum Learning~\citep{Bengio+al-2009-small}?  What makes such learning tasks more difficult? Can specific initial values of the neural network parameters yield success when random initialization yield complete failure? Is it possible to verify that the problem being faced  is an optimization problem or with a regularization problem? These are the questions discussed (if not completely addressed) here, which relate to the following  broader question: how can humans (and potentially one day, machines) learn complex concepts?	 \end{itemize}  This paper is focused on ``\textit{Point 1}'' and testing the ``\textit{Guided Learning Hypothesis}'', using machine learning algorithms to provide experimental evidence. The experiments performed also provide evidence in favor of the ``\textit{Deeper Harder Hypothesis}'' and associated ``\textit{Abstractions Harder Hypothesis}''. Machine Learning is still far beyond the current capabilities of humans, and it is important to tackle the remaining obstacles to approach AI. For this purpose, the question to be answered is why tasks that humans learn effortlessly from very few examples, while machine learning algorithms fail miserably?	score:361
Is it possible to get a machine learning job without a master's degree?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:362
Is it possible to get a machine learning job without a master's degree?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:367
Can I get a machine learning job in the US with an international CS bachelor's degree? Will I actually get an interview?	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	score:385
Can I get a machine learning job in the US with an international CS bachelor's degree? Will I actually get an interview?	 Interactive learning is a process in which a machine learning algorithm is provided with meaningful, well-chosen examples as opposed to randomly chosen examples typical in standard supervised learning. In this paper, we propose a new method for interactive learning from multiple noisy labels where we exploit the disagreement among annotators to quantify the easiness (or meaningfulness) of an example.  We demonstrate the usefulness of this method in estimating the parameters of a latent variable classification model, and conduct experimental analyses on a range of synthetic and benchmark datasets. Furthermore, we theoretically analyze the performance of perceptron in this interactive learning framework. 	We consider binary classification problems in the presence of a teacher, who acts as an intermediary to provide a learning algorithm with meaningful, well-chosen examples.	 Interactive learning is a process in which a machine learning algorithm is provided with meaningful, well-chosen examples as opposed to randomly chosen examples typical in standard supervised learning. In this paper, we propose a new method for interactive learning from multiple noisy labels where we exploit the disagreement among annotators to quantify the easiness (or meaningfulness) of an example.  We demonstrate the usefulness of this method in estimating the parameters of a latent variable classification model, and conduct experimental analyses on a range of synthetic and benchmark datasets. Furthermore, we theoretically analyze the performance of perceptron in this interactive learning framework. 	We consider binary classification problems in the presence of a teacher, who acts as an intermediary to provide a learning algorithm with meaningful, well-chosen examples.	score:386
Can I get a machine learning job in the US with an international CS bachelor's degree? Will I actually get an interview?	 Therefore an important motivation for this work is to better understand the performance characteristics of the main (baseline) machine learning methods currently used in QSAR learning. This knowledge will feed into a better understanding of the performance characteristics of these algorithms, and will enable QSAR practitioners to improve there predictions.    The central motivation for this work is to better understand meta-learning through a case-study in the very important real-world application area of QSAR learning. This application area is an excellent test-bed for the development of meta-learning methodologies. The importance of the subject area means that there are now thousands of publicly available QSAR datasets, all with the same basic structure.	 Therefore an important motivation for this work is to better understand the performance characteristics of the main (baseline) machine learning methods currently used in QSAR learning. This knowledge will feed into a better understanding of the performance characteristics of these algorithms, and will enable QSAR practitioners to improve there predictions.    The central motivation for this work is to better understand meta-learning through a case-study in the very important real-world application area of QSAR learning. This application area is an excellent test-bed for the development of meta-learning methodologies. The importance of the subject area means that there are now thousands of publicly available QSAR datasets, all with the same basic structure.	score:394
Can I get a machine learning job in the US with an international CS bachelor's degree? Will I actually get an interview?	 In this paper machine learning experiments are used to investigate some elements of this hypothesis by seeking answers for the following questions: are there machine learning tasks which are intrinsically hard for a lone learning agent but that may become very easy when intermediate concepts are provided by another agent as additional intermediate learning cues, in the spirit of Curriculum Learning~\citep{Bengio+al-2009-small}?  What makes such learning tasks more difficult? Can specific initial values of the neural network parameters yield success when random initialization yield complete failure? Is it possible to verify that the problem being faced  is an optimization problem or with a regularization problem? These are the questions discussed (if not completely addressed) here, which relate to the following  broader question: how can humans (and potentially one day, machines) learn complex concepts?	 In this paper machine learning experiments are used to investigate some elements of this hypothesis by seeking answers for the following questions: are there machine learning tasks which are intrinsically hard for a lone learning agent but that may become very easy when intermediate concepts are provided by another agent as additional intermediate learning cues, in the spirit of Curriculum Learning~\citep{Bengio+al-2009-small}?  What makes such learning tasks more difficult? Can specific initial values of the neural network parameters yield success when random initialization yield complete failure? Is it possible to verify that the problem being faced  is an optimization problem or with a regularization problem? These are the questions discussed (if not completely addressed) here, which relate to the following  broader question: how can humans (and potentially one day, machines) learn complex concepts?	score:398
Can I get a machine learning job in the US with an international CS bachelor's degree? Will I actually get an interview?	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	score:402
Is there more to computational finance and machine learning in finance than trying different algorithms on stock data?	       	Data volumes are growing at a faster rate than available computing power, storage space, or network bandwidth.  This has fueled interest in distributed approaches to machine learning. However, the substantial jump in communication costs between a multicore and a multicomputer system currently confines many popular techniques to the single computer regime.  Consequently, even those machine learning workflows that today originate with distributed data in a clustered environment often terminate with the learning problem being solved on a single machine.  Furthermore, because of computation, storage, or network limitations there is often a subsampling step between the data store and the single machine.  Our concern here is to make the subsampling step as statistically efficient as possible, knowing that the data will subsequently be given to an empirical risk minimization (ERM) algorithm.	       	Data volumes are growing at a faster rate than available computing power, storage space, or network bandwidth.  This has fueled interest in distributed approaches to machine learning. However, the substantial jump in communication costs between a multicore and a multicomputer system currently confines many popular techniques to the single computer regime.  Consequently, even those machine learning workflows that today originate with distributed data in a clustered environment often terminate with the learning problem being solved on a single machine.  Furthermore, because of computation, storage, or network limitations there is often a subsampling step between the data store and the single machine.  Our concern here is to make the subsampling step as statistically efficient as possible, knowing that the data will subsequently be given to an empirical risk minimization (ERM) algorithm.	score:322
Is there more to computational finance and machine learning in finance than trying different algorithms on stock data?	  Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning techniques to impressive results in regression, classification, data-generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets are motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed-up classical machine learning algorithms.  Here we review the literature in quantum machine learning and discuss perspectives for a mixed readership of classical machine learning and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems.	  Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning techniques to impressive results in regression, classification, data-generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets are motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed-up classical machine learning algorithms.  Here we review the literature in quantum machine learning and discuss perspectives for a mixed readership of classical machine learning and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems.	score:330
Is there more to computational finance and machine learning in finance than trying different algorithms on stock data?	 This paper demonstrates how to apply machine learning algorithms to distinguish ``good" stocks from the ``bad" stocks. To this end, we construct 244 technical and fundamental features to characterize each stock, and label stocks according to their ranking with respect to the return-to-volatility ratio. Algorithms ranging from traditional statistical learning methods to recently popular deep learning method, e. g. Logistic Regression (LR), Random Forest (RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the classification task. Genetic Algorithm (GA) is also used to implement feature selection. The effectiveness of the stock selection strategy is validated in Chinese stock market in both statistical and practical aspects, showing that: 1) Stacking outperforms other models reaching an AUC score of 0.	 This paper demonstrates how to apply machine learning algorithms to distinguish ``good" stocks from the ``bad" stocks. To this end, we construct 244 technical and fundamental features to characterize each stock, and label stocks according to their ranking with respect to the return-to-volatility ratio. Algorithms ranging from traditional statistical learning methods to recently popular deep learning method, e. g. Logistic Regression (LR), Random Forest (RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the classification task. Genetic Algorithm (GA) is also used to implement feature selection. The effectiveness of the stock selection strategy is validated in Chinese stock market in both statistical and practical aspects, showing that: 1) Stacking outperforms other models reaching an AUC score of 0.	score:331
Is there more to computational finance and machine learning in finance than trying different algorithms on stock data?	  However, more complex and useful tasks can be performed  beyond calculation of aggregate statistics,  by using machine learning algorithms on crowdsensing data. Examples of such tasks include: learning optimal settings of room temperatures for smart thermostats;  predicting user activity for context-aware services and physical monitoring; suggesting the best driving routes; recognizing audio events from microphone sensors.  Specific algorithms and data types for these tasks are different,  but they can all be trained in standard unsupervised or supervised learning settings: given sensory features (time, location, motion, environmental measures, etc.), train an algorithm or model that can accurately predict a variable of interest  (temperature setting, current user activity, amount of traffic, audio events, etc.	  However, more complex and useful tasks can be performed  beyond calculation of aggregate statistics,  by using machine learning algorithms on crowdsensing data. Examples of such tasks include: learning optimal settings of room temperatures for smart thermostats;  predicting user activity for context-aware services and physical monitoring; suggesting the best driving routes; recognizing audio events from microphone sensors.  Specific algorithms and data types for these tasks are different,  but they can all be trained in standard unsupervised or supervised learning settings: given sensory features (time, location, motion, environmental measures, etc.), train an algorithm or model that can accurately predict a variable of interest  (temperature setting, current user activity, amount of traffic, audio events, etc.	score:336
Is there more to computational finance and machine learning in finance than trying different algorithms on stock data?	      \tiny   \section{Keywords:} machine learning, statistical learning, neuroimaging, scikit-learn, Python 	Interest in applying statistical machine learning to neuroimaging data analysis is growing. Neuroscientists use it as a powerful, albeit complex, tool for statistical inference. The tools are developed by computer scientists who may lack a deep understanding of the neuroscience questions.  This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.	      \tiny   \section{Keywords:} machine learning, statistical learning, neuroimaging, scikit-learn, Python 	Interest in applying statistical machine learning to neuroimaging data analysis is growing. Neuroscientists use it as a powerful, albeit complex, tool for statistical inference. The tools are developed by computer scientists who may lack a deep understanding of the neuroscience questions.  This paper aims to fill the gap between machine learning and neuroimaging by demonstrating how a general-purpose machine-learning toolbox, scikit-learn, can provide state-of-the-art methods for neuroimaging analysis while keeping the code simple and understandable by both worlds. Here, we focus on software; for a  more conceptual introduction to machine learning methods in fMRI analysis, see \cite{pereira2009} or \cite{mur2009}, while \cite{hastie2001} provides a good reference on machine learning.	score:337
Is there some way to easily batch-download all PDF attachments from my Gmail account?	 So finding a way to  communicate between processes becomes a fundamental problem within a multi-GPU framework.  There are several existing approaches of implementing inter-process  communication besides manually programming on sockets, such as Signals,  Message Queues, Message Passing, Pipes, Shared Memory, Memory Mapped Files, etc. However, among those approaches, Message  Passing is most suitable for collective communication between multiple  programs across a cluster because of its well-developed point-to-point and  collective protocols.  Message Passing Interface (MPI) is a language-independent  communication protocol that can undertake the task of inter-process  communication across machines. It is a standardized message-passing  system designed for programming on large-scale parallel applications.  Parameter transfer is a basic operation in the distributed training of deep learning models.	 So finding a way to  communicate between processes becomes a fundamental problem within a multi-GPU framework.  There are several existing approaches of implementing inter-process  communication besides manually programming on sockets, such as Signals,  Message Queues, Message Passing, Pipes, Shared Memory, Memory Mapped Files, etc. However, among those approaches, Message  Passing is most suitable for collective communication between multiple  programs across a cluster because of its well-developed point-to-point and  collective protocols.  Message Passing Interface (MPI) is a language-independent  communication protocol that can undertake the task of inter-process  communication across machines. It is a standardized message-passing  system designed for programming on large-scale parallel applications.  Parameter transfer is a basic operation in the distributed training of deep learning models.	score:368
Is there some way to easily batch-download all PDF attachments from my Gmail account?	 In some applications however, such an assumption does not hold and inputs cannot easily be processed into this form. In this paper, we consider one such setting where inputs consist in an unordered and variable-length set of vectors $\Xbf = \{\xbf^{(1)},\dots,\xbf^{(\bagSize)}\}$. For instance, $\Xbf$ could be the set of text documents $\xbf^{(s)}$ found in some incoming piece of mail, where each document is represented as a bag of words.  In this particular example, a simple approach to converting the set $\Xbf$ into a single vector $\xbf$ would consist in computing the global bag of word representation of all documents in $\Xbf$, as if all documents had been concatenated into a single one. This would however correspond to throwing away all the information about the structure of the incoming mail, which could be useful to solve the task at hand.   This problem setting is not specific to text data either: $\Xbf$ could correspond to a collection of images or to a single image that has been pre-segmented, and some recognition tasks in computer vision have previously been formulated in terms of classification of sets~\citep{Kondor2003,Wallraven2003}. Another example is text-independent speaker recognition~\citep{Reynolds1995}, where inputs are sequences of acoustic vectors but for which the order is not relevant: relevant short-term dynamics are taken into account in the vector features themselves (e.	 In some applications however, such an assumption does not hold and inputs cannot easily be processed into this form. In this paper, we consider one such setting where inputs consist in an unordered and variable-length set of vectors $\Xbf = \{\xbf^{(1)},\dots,\xbf^{(\bagSize)}\}$. For instance, $\Xbf$ could be the set of text documents $\xbf^{(s)}$ found in some incoming piece of mail, where each document is represented as a bag of words.  In this particular example, a simple approach to converting the set $\Xbf$ into a single vector $\xbf$ would consist in computing the global bag of word representation of all documents in $\Xbf$, as if all documents had been concatenated into a single one. This would however correspond to throwing away all the information about the structure of the incoming mail, which could be useful to solve the task at hand.   This problem setting is not specific to text data either: $\Xbf$ could correspond to a collection of images or to a single image that has been pre-segmented, and some recognition tasks in computer vision have previously been formulated in terms of classification of sets~\citep{Kondor2003,Wallraven2003}. Another example is text-independent speaker recognition~\citep{Reynolds1995}, where inputs are sequences of acoustic vectors but for which the order is not relevant: relevant short-term dynamics are taken into account in the vector features themselves (e.	score:370
Is there some way to easily batch-download all PDF attachments from my Gmail account?	 Word's grammar-checker will underline in aggressive red grammatical constructions that are used by Nobel prize-winning authors and are completely readable if you actually read the text instead of just scanning it. These algorithms are all too happy to shave off any text that offers the reader resistance and unpredictability. And the suggestions for new books to buy you get from Amazon are rarely the truly left-field ones---the basic principle of a recommender system is to recommend things that many others also liked.	 Word's grammar-checker will underline in aggressive red grammatical constructions that are used by Nobel prize-winning authors and are completely readable if you actually read the text instead of just scanning it. These algorithms are all too happy to shave off any text that offers the reader resistance and unpredictability. And the suggestions for new books to buy you get from Amazon are rarely the truly left-field ones---the basic principle of a recommender system is to recommend things that many others also liked.	score:377
Is there some way to easily batch-download all PDF attachments from my Gmail account?	 In addition to foreground processes, a lot of activity may be happening in the background: email clients may be fetching new mail, GPS may be active for geo-fencing applications,  messaging apps are polling for new notifications, and so on. All of these activities consume power; the question is how much of the total battery consumption should be attributed to each app?  This problem is non-trivial because the operating system induces cooperation between apps to save battery power.  For example there is no need to activate the GPS sensor twice if two different apps request the current location almost simultaneously.   \paragraph{Moneyball and Player Ratings}  The impact of an individual player on the overall performance of the team typically depends on the other players currently playing.	 In addition to foreground processes, a lot of activity may be happening in the background: email clients may be fetching new mail, GPS may be active for geo-fencing applications,  messaging apps are polling for new notifications, and so on. All of these activities consume power; the question is how much of the total battery consumption should be attributed to each app?  This problem is non-trivial because the operating system induces cooperation between apps to save battery power.  For example there is no need to activate the GPS sensor twice if two different apps request the current location almost simultaneously.   \paragraph{Moneyball and Player Ratings}  The impact of an individual player on the overall performance of the team typically depends on the other players currently playing.	score:379
Is there some way to easily batch-download all PDF attachments from my Gmail account?	 By doing so, our goal is  to explore a much richer set of  network architectures  of  ResNet blocks and thoroughly examine how several other different aspects besides the order of activations affect performance. As we explain below, such an exploration of architectures has led to   new interesting findings  with great practical importance concerning residual networks.   \textbf{Width vs depth in residual networks}. The problem of shallow vs deep networks has been in discussion for a long time in machine learning \cite{LarochelleH2007,Bengio+chapter2007} with pointers to the circuit complexity theory literature showing that shallow circuits can require exponentially more components than deeper circuits. The authors of residual networks tried to make them as thin as possible in favor of increasing their depth and having less parameters, and even introduced a <<bottleneck>> block which makes ResNet blocks even thinner.	 By doing so, our goal is  to explore a much richer set of  network architectures  of  ResNet blocks and thoroughly examine how several other different aspects besides the order of activations affect performance. As we explain below, such an exploration of architectures has led to   new interesting findings  with great practical importance concerning residual networks.   \textbf{Width vs depth in residual networks}. The problem of shallow vs deep networks has been in discussion for a long time in machine learning \cite{LarochelleH2007,Bengio+chapter2007} with pointers to the circuit complexity theory literature showing that shallow circuits can require exponentially more components than deeper circuits. The authors of residual networks tried to make them as thin as possible in favor of increasing their depth and having less parameters, and even introduced a <<bottleneck>> block which makes ResNet blocks even thinner.	score:383
What changes can be brought in compilers if we include machine learning/deep learning algorithms?	 Machine learning will become a core element of future mobile devices.  Recently, the most exciting advancement of machine learning mainly comes from the field of deep learning (DL) whose unprecedented performance has beaten many records achieved by the traditional machine learning algorithms. Deep learning has revolutionized how the world processes, models, and interprets data \cite{Goodfellow2016DL}.  Inspired by the outstanding performance of deep learning, people naturally attempt to push deep learning on mobile devices to provide high-quality intelligent services. It is believed that deep learning will play a paramountly important role in the evolution of mobile applications \cite{Lane2017}. Despite the attractive prospect, the current research of blending deep learning and mobile devices is just the beginning.	 Machine learning will become a core element of future mobile devices.  Recently, the most exciting advancement of machine learning mainly comes from the field of deep learning (DL) whose unprecedented performance has beaten many records achieved by the traditional machine learning algorithms. Deep learning has revolutionized how the world processes, models, and interprets data \cite{Goodfellow2016DL}.  Inspired by the outstanding performance of deep learning, people naturally attempt to push deep learning on mobile devices to provide high-quality intelligent services. It is believed that deep learning will play a paramountly important role in the evolution of mobile applications \cite{Lane2017}. Despite the attractive prospect, the current research of blending deep learning and mobile devices is just the beginning.	score:287
What changes can be brought in compilers if we include machine learning/deep learning algorithms?	 Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning \emph{can make mistakes} and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems.  We show that in limited contexts the required number of training samples can be low and self-improvement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications.	 Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning \emph{can make mistakes} and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems.  We show that in limited contexts the required number of training samples can be low and self-improvement of pre-trained networks in more general context is possible. We argue that the combination of sparse outlier detection with deep components that can support each other diminish the fragility of deep methods, an important requirement for engineering applications.	score:292
What changes can be brought in compilers if we include machine learning/deep learning algorithms?	 Online learning has become crucial to many problems in machine learning. As more data is collected sequentially, quickly adapting to changes in the data distribution can offer several competitive advantages such as avoiding loss of prior knowledge and more efficient learning. However, adaptation to changes in the data distribution (also known as covariate shift) needs to be performed without compromising past knowledge already built in into the model to cope with voluminous and dynamic data.  In this paper, we propose an online stacked Denoising Autoencoder whose structure is adapted through reinforcement learning. Our algorithm forces the network to exploit and explore favourable architectures employing an estimated utility function that maximises the accuracy of an unseen validation sequence. Different actions, such as \emph{Pool}, \emph{Increment} and \emph{Merge} are available to modify the structure of the network.	 Online learning has become crucial to many problems in machine learning. As more data is collected sequentially, quickly adapting to changes in the data distribution can offer several competitive advantages such as avoiding loss of prior knowledge and more efficient learning. However, adaptation to changes in the data distribution (also known as covariate shift) needs to be performed without compromising past knowledge already built in into the model to cope with voluminous and dynamic data.  In this paper, we propose an online stacked Denoising Autoencoder whose structure is adapted through reinforcement learning. Our algorithm forces the network to exploit and explore favourable architectures employing an estimated utility function that maximises the accuracy of an unseen validation sequence. Different actions, such as \emph{Pool}, \emph{Increment} and \emph{Merge} are available to modify the structure of the network.	score:302
What changes can be brought in compilers if we include machine learning/deep learning algorithms?	 The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms. 	Deep Neural Networks (DNNs) and Deep Learning (DL) algorithms in various forms have become the most successful machine learning method for most supervised learning tasks.  Their performance currently surpass most competitor algorithms and DL wins top machine learning competitions on real data challenges \cite{Bengio2013a,HG-Sr-2006,krizhevsky2012imagenet}.  The theoretical understanding of DL remains, however, unsatisfactory. Basic questions about the design principles of deep networks, the optimal architecture, the number of required layers, the sample complexity, and the best optimization algorithms, are not well understood.	 The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms. 	Deep Neural Networks (DNNs) and Deep Learning (DL) algorithms in various forms have become the most successful machine learning method for most supervised learning tasks.  Their performance currently surpass most competitor algorithms and DL wins top machine learning competitions on real data challenges \cite{Bengio2013a,HG-Sr-2006,krizhevsky2012imagenet}.  The theoretical understanding of DL remains, however, unsatisfactory. Basic questions about the design principles of deep networks, the optimal architecture, the number of required layers, the sample complexity, and the best optimization algorithms, are not well understood.	score:303
What changes can be brought in compilers if we include machine learning/deep learning algorithms?	 The brain appearance, however, differs from patient to patient, and the lesion may also cause deformation in brain structure. Moreover, the MRI acquired from different machines may also introduce different levels of noise and deformation of brain tissue appearance, leading to incorrect detection and segmentation. Therefore, many machine-learning methods have been proposed, where the features are learnt from massive training data, and high segmentation accuracy can be achieved.  For instance,  random forest based methods were used in the literature \citep{Ellwaa2016, LeFolgoc2016,Lefkovits2016, Song2016}, which presents good performance in brain tumor segmentation by using hand-crafted features.  Note that the performance of the random-forests-based methods heavily rely on the manually annotated features. To achieve better performance, it is preferable to make the machine find the features from the data by itself.	 The brain appearance, however, differs from patient to patient, and the lesion may also cause deformation in brain structure. Moreover, the MRI acquired from different machines may also introduce different levels of noise and deformation of brain tissue appearance, leading to incorrect detection and segmentation. Therefore, many machine-learning methods have been proposed, where the features are learnt from massive training data, and high segmentation accuracy can be achieved.  For instance,  random forest based methods were used in the literature \citep{Ellwaa2016, LeFolgoc2016,Lefkovits2016, Song2016}, which presents good performance in brain tumor segmentation by using hand-crafted features.  Note that the performance of the random-forests-based methods heavily rely on the manually annotated features. To achieve better performance, it is preferable to make the machine find the features from the data by itself.	score:309
Is machine learning overrated or overhyped?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:464
Is machine learning overrated or overhyped?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:467
Is machine learning overrated or overhyped?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:473
Is machine learning overrated or overhyped?	 Machines, not humans, are the world's dominant knowledge accumulators but humans remain the dominant decision makers. Interpreting and disseminating the knowledge accumulated by machines requires expertise, time, and is prone to failure. The problem of how best to convey accumulated knowledge from computers to humans is a critical bottleneck in the broader application of machine learning.  We propose an approach based on human teaching where the problem is formalized as selecting a small subset of the data that will, with high probability, lead the human user to the correct inference. This approach, though successful for modeling human learning in simple laboratory experiments, has failed to achieve broader relevance due to challenges in formulating general and scalable algorithms.	 Machines, not humans, are the world's dominant knowledge accumulators but humans remain the dominant decision makers. Interpreting and disseminating the knowledge accumulated by machines requires expertise, time, and is prone to failure. The problem of how best to convey accumulated knowledge from computers to humans is a critical bottleneck in the broader application of machine learning.  We propose an approach based on human teaching where the problem is formalized as selecting a small subset of the data that will, with high probability, lead the human user to the correct inference. This approach, though successful for modeling human learning in simple laboratory experiments, has failed to achieve broader relevance due to challenges in formulating general and scalable algorithms.	score:474
Is machine learning overrated or overhyped?	 Overcomplete representations and dictionary learning algorithms kept attracting a growing interest in the machine learning community.  This paper addresses the emerging problem of comparing multivariate overcomplete representations.  Despite a recurrent need to rely on a distance for  learning or assessing multivariate overcomplete representations, no metrics in their underlying spaces have yet been proposed.  Henceforth we propose to study overcomplete representations from the perspective of frame theory and matrix manifolds. We consider distances between multivariate dictionaries as distances between their spans which reveal to be elements of a Grassmannian manifold.  We introduce Wasserstein-like set-metrics defined on Grassmannian spaces and study their properties both theoretically and numerically.	 	A classical question in machine learning is how to choose a good feature space to analyze the input data. One possible and elegant response is to learn this feature space over mild hypotheses. This approach is known as dictionary learning and follows the dictionary-based framework which have provided several important results in the last decades \citep{MEY95,MAL99,TRO04,CAN06a,GRI10,JEN12}.  In dictionary-based approaches, an expert selects a specific family of basis functions, called atoms, known to capture important features of the input data, for example wavelets \citep{MAL99} or curvelets \citep{CAN04}. When no specific expert knowledge is available, dictionary learning algorithms could learn those atoms from a given dataset.  The problem is then stated as an optimization procedure usually under some sparsity constraints.	score:478
What is the main difference between classification problems and regression problems in machine learning?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:240
What is the main difference between classification problems and regression problems in machine learning?	 Fairness-aware classification is receiving increasing attention in the machine learning fields. Recently research proposes to formulate the fairness-aware classification as constrained optimization problems. However, several limitations exist in previous works due to the lack of a theoretical framework for guiding the formulation. In this paper, we propose a general framework for learning fair classifiers which addresses previous limitations.  The framework formulates various commonly-used fairness metrics as convex constraints that can be directly incorporated into classic classification models. Within the framework, we propose a constraint-free criterion on the training data which ensures that any classifier learned from the data is fair. We also derive the constraints which ensure that the real fairness metric is satisfied when surrogate functions are used to achieve convexity.	 Fairness-aware classification is receiving increasing attention in the machine learning fields. Recently research proposes to formulate the fairness-aware classification as constrained optimization problems. However, several limitations exist in previous works due to the lack of a theoretical framework for guiding the formulation. In this paper, we propose a general framework for learning fair classifiers which addresses previous limitations.  The framework formulates various commonly-used fairness metrics as convex constraints that can be directly incorporated into classic classification models. Within the framework, we propose a constraint-free criterion on the training data which ensures that any classifier learned from the data is fair. We also derive the constraints which ensure that the real fairness metric is satisfied when surrogate functions are used to achieve convexity.	score:248
What is the main difference between classification problems and regression problems in machine learning?	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	     Learning and logic are distinct and remarkable approaches to prediction.  Machine learning has experienced a surge in popularity because it is robust to noise and achieves high performance; however, ML experiences many issues with knowledge transfer and extrapolation.          In contrast, logic is easily intepreted, and logical rules are easy to chain and transfer between systems; however, inductive logic is brittle to noise.   We then explore the premise of combining learning with inductive logic into AI Reasoning Systems.          Specifically, we summarize findings from PAC learning (conceptual graphs, robust logics, knowledge infusion) and deep learning (DSRL, $\partial$ILP, DeepLogic) by reproducing proofs of tractability, presenting algorithms in pseudocode, highlighting results, and synthesizing between fields.	score:256
What is the main difference between classification problems and regression problems in machine learning?	  	Since many problems in compressed sensing and machine learning can be formulated as a constrained linear regression problem, such as SVM, LASSO, signal recovery \cite{pilanci2015randomized}, large scale linear regression with constraints now becomes one of the most popular and basic models in Machine Learning and has received a great deal of attentions from both the Machine Learning and Theoretical Computer Science communities.  Formally, the problem can be defined as follows, \begin{equation*} \min_{x\in{\mathcal{W}}}f(x)={\|Ax-b\|_2^2}, \end{equation*}  where $A$ is a matrix  in  $\mathbb{R}^{n\times d}$ with $e^d>n>d$ and $\mathcal{W}$ is a closed convex set. The goal is to find an $x\in{\mathcal{W}}$ such that $f(x)\leq (1+\epsilon)\min_{x\in{\mathcal{W}}}f(x)$ or $f(x)- \min_{x\in{\mathcal{W}}}f(x)\leq \epsilon$.	  	Since many problems in compressed sensing and machine learning can be formulated as a constrained linear regression problem, such as SVM, LASSO, signal recovery \cite{pilanci2015randomized}, large scale linear regression with constraints now becomes one of the most popular and basic models in Machine Learning and has received a great deal of attentions from both the Machine Learning and Theoretical Computer Science communities.  Formally, the problem can be defined as follows, \begin{equation*} \min_{x\in{\mathcal{W}}}f(x)={\|Ax-b\|_2^2}, \end{equation*}  where $A$ is a matrix  in  $\mathbb{R}^{n\times d}$ with $e^d>n>d$ and $\mathcal{W}$ is a closed convex set. The goal is to find an $x\in{\mathcal{W}}$ such that $f(x)\leq (1+\epsilon)\min_{x\in{\mathcal{W}}}f(x)$ or $f(x)- \min_{x\in{\mathcal{W}}}f(x)\leq \epsilon$.	score:262
What is the main difference between classification problems and regression problems in machine learning?	 In this paper, we describe how a single set of results implies positive answers to the above, for several different settings. 	Information constraints play a key role in machine learning. Of course, the main constraint is the availability of only a finite data set, from which the learner is expected to generalize. However, many problems currently researched in machine learning can be characterized as learning with \emph{additional} information constraints, arising from the manner in which the learner may interact with the data.  Some examples include: \begin{itemize}     \item \emph{Communication constraints in distributed learning:} There         has been much work in recent years on learning when the training         data is distributed among several machines (with         \cite{bekkerman2011scaling,agarwal2011reliable,dekel2011optimal,niu2011hogwild,cotter2011better,duchi2012dual,balcan2012distributed,boyd2011distributed,kyrola2011parallel}         being just a few examples).	 Can one quantify how a constraint of getting only a few bits from each example affects our ability to learn? To the best of our knowledge, there are currently no generic tools which allow us to answer such questions, at least in the context of standard machine learning settings.  In this paper, we make a first step in developing such a framework. We consider a general class of learning processes, characterized only by information-theoretic constraints on how they may interact with the data (and independent of any specific problem semantics).	score:264
What skills are required for machine learning internship?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.  Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	 Machine learning also studies unsupervised learning, for example, for neural networks to learn the statistical structure in input data without training examples \cite{HS99}, or for clustering of unlabeled data, or for feature extraction or dimensionality reduction for large input datasets, which are steps in this direction.  In this paper, we study unassisted learning in an unknown environment, where a learning machine does not receive any information about its environment from an external agent. Henceforth by learning we always mean unassisted learning in this sense.	score:331
What skills are required for machine learning internship?	 However, as machine learning systems are deployed in increasingly large-scale, autonomous, open-domain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.   There has been a great deal of public discussion around accidents.  To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents \cite{bostrom2014superintelligence}.  However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics \cite{davis2015ethical,lawrence2016discussion}.	 However, as machine learning systems are deployed in increasingly large-scale, autonomous, open-domain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems. Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.   There has been a great deal of public discussion around accidents.  To date much of this discussion has highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents \cite{bostrom2014superintelligence}.  However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision, as noted by some critics \cite{davis2015ethical,lawrence2016discussion}.	score:362
What skills are required for machine learning internship?	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	   \keywords{Machine learning, learning target selection, entropy, information theory, similarity, conjecture} 	\noindent Machine learning is the study and construction of systems that can learn from data. The systems are called {\it learning machines}.  When Big Data emerges increasingly, more learning machines are developed and applied in different domains.  However, the ultimate goal of machine learning study is {\it insight}, not machine itself. By the term insight I mean {\it learning mechanisms} in descriptions of mathematical principles. In a loose sense, learning mechanisms can be regarded as the natural entity. As the {\it ``Tao}\begin{CJK}{UTF8}{bsmi} (道)\end{CJK}{''} reflects the most fundamental of the universe by Lao Tzu \begin{CJK}{UTF8}{bsmi}(老子)\end{CJK},  Einstein suggests that we should pursue the simplest mathematical  interpretations to the nature.   Although learning mechanisms are related to the subjects of psychology, cognitive and brain science, this paper stresses on the exploration of mathematical principles for interpretation of learning mechanisms. Up to now, we human beings are still far away from deep understanding ourself on learning mechanisms in terms of mathematical principles.  It is the author's belief that  {\it ``mathematical-principle-based machine''}   might be more important and critical than  {\it ``brain-inspired machine''}   in the study of machine learning.	score:364
What skills are required for machine learning internship?	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	 This reduces the burden of programming the robot, as it is a much more intuitive approach and requires no expert knowledge on robot kinematics or programming code. Although in industry this approach is coined as ``learning from demonstration'', it is merely a record and replay feature as opposed to the learning from demonstration research in which generalized policies are trained using machine learning techniques \cite{Schaal96}.  However, this limits the applicability of currently available systems to cases where positions are fixed relative to the robot. For example, when grasping an object, this object has to be at the same position for each repetition. Even a small perturbation of the object's position could harm the system.  In order to mitigate these limitations, one could attach a vision sensor to the robot and use this information to recognize objects, estimate their pose and calculate the best grasp \cite{Bohg2014}.	score:366
What skills are required for machine learning internship?	  	Since many problems in compressed sensing and machine learning can be formulated as a constrained linear regression problem, such as SVM, LASSO, signal recovery \cite{pilanci2015randomized}, large scale linear regression with constraints now becomes one of the most popular and basic models in Machine Learning and has received a great deal of attentions from both the Machine Learning and Theoretical Computer Science communities.  Formally, the problem can be defined as follows, \begin{equation*} \min_{x\in{\mathcal{W}}}f(x)={\|Ax-b\|_2^2}, \end{equation*}  where $A$ is a matrix  in  $\mathbb{R}^{n\times d}$ with $e^d>n>d$ and $\mathcal{W}$ is a closed convex set. The goal is to find an $x\in{\mathcal{W}}$ such that $f(x)\leq (1+\epsilon)\min_{x\in{\mathcal{W}}}f(x)$ or $f(x)- \min_{x\in{\mathcal{W}}}f(x)\leq \epsilon$.	  	Since many problems in compressed sensing and machine learning can be formulated as a constrained linear regression problem, such as SVM, LASSO, signal recovery \cite{pilanci2015randomized}, large scale linear regression with constraints now becomes one of the most popular and basic models in Machine Learning and has received a great deal of attentions from both the Machine Learning and Theoretical Computer Science communities.  Formally, the problem can be defined as follows, \begin{equation*} \min_{x\in{\mathcal{W}}}f(x)={\|Ax-b\|_2^2}, \end{equation*}  where $A$ is a matrix  in  $\mathbb{R}^{n\times d}$ with $e^d>n>d$ and $\mathcal{W}$ is a closed convex set. The goal is to find an $x\in{\mathcal{W}}$ such that $f(x)\leq (1+\epsilon)\min_{x\in{\mathcal{W}}}f(x)$ or $f(x)- \min_{x\in{\mathcal{W}}}f(x)\leq \epsilon$.	score:371
I am in last batch at Fiitjee.I have screwed my marks.how shd I study to get 200+ in JEE Mains 2017?	  Various approaches have been proposed to improve the efficiency of deep learning training. Highly optimized GPGPU implementations have significantly shortened the time spent on training DNNs, often showing $10$--$100\times$ speed-up~\cite{chetlur2014cudnn,krizhevsky2014one}. However, accelerating DNN training on a single machine has limitations because of the limited resources such as GPU memory or the host machine's main memory~\cite{krizhevsky2012imagenet}.  Scaling out methods in distributed environments have been suggested~\cite{li2014scaling,dean2012large,ho2013more,xing2015petuum,ooi2015singa} to overcome such issues. These approaches exploit data parallelism and/or model parallelism and can potentially provide scalability.  On the other hand, a seamless integration of DNN training with existing data processing pipelines is also an important practical point to avoid unnecessary transfer and duplication.	  Various approaches have been proposed to improve the efficiency of deep learning training. Highly optimized GPGPU implementations have significantly shortened the time spent on training DNNs, often showing $10$--$100\times$ speed-up~\cite{chetlur2014cudnn,krizhevsky2014one}. However, accelerating DNN training on a single machine has limitations because of the limited resources such as GPU memory or the host machine's main memory~\cite{krizhevsky2012imagenet}.  Scaling out methods in distributed environments have been suggested~\cite{li2014scaling,dean2012large,ho2013more,xing2015petuum,ooi2015singa} to overcome such issues. These approaches exploit data parallelism and/or model parallelism and can potentially provide scalability.  On the other hand, a seamless integration of DNN training with existing data processing pipelines is also an important practical point to avoid unnecessary transfer and duplication.	score:407
I am in last batch at Fiitjee.I have screwed my marks.how shd I study to get 200+ in JEE Mains 2017?	~\ref{fig:MIM}.   We assess the performance of our model on the following datasets:  CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100~\cite{krizhevsky2009learning}, MNIST~\cite{lecun1998gradient} and SVHN~\cite{netzer2011reading}.   We first show empirically the improvements achieved with the introduction of maxout and batch normalisation units to the NIN model~\cite{lin2013network}, forming our proposed MIM model, then we show a study on how this model provides a better pre-conditioning for the proposed deep learning model, and finally we show the final classification results on the datasets above, which are compared to the state of the art and demonstrated to be the best in the field in two of these datasets (CIFAR-10, CIFAR-100) and competitive on MNIST and SVHN.	g., Fig.~\ref{fig:intro} shows a maxout with 4 regions).  Also, before each activation function, we have the option of placing a batch normalisation unit~\cite{ioffe2015batch}.  Training is based on backpropagation~\cite{rumelhart1988learning} using mini-batches of size 100, learning rate of 0.0005 for 20 epochs then 0.0001 for another 20 epochs, momentum of 0. 9 and weight decay of 0.0001, where we run five times the training (with different training and test samples) and report the mean train and test errors.   Finally, the MLP weights are initialized with Normal distribution scaled by 0.01 for all layers.  Analysing the mean train and test error in Fig.~\ref{fig:toy}-(b), we first notice that all models have good generalization capability, which is a characteristic already identified for deep networks that use piecewise linear activation units~\cite{montufar2014number,srivastava2014understanding}.	score:413
I am in last batch at Fiitjee.I have screwed my marks.how shd I study to get 200+ in JEE Mains 2017?	 As a first step in this direction, we developed a framework to compare deep learning architectures on a large set of EEG examples ($>$100 decoding problems) in a comprehensively documented and reproducible manner. Applying this framework, here we report on three publicly available CNN architectures.  In this paper we describe the framework and provide our rationale for design choices regarding the challenges describes above. Secondly, we present the results of our comparison and make a recommendation on which of the network(s) included in the comparison to use for best performance. At last we discuss how the present framework could be further extended and improved.	 As a first step in this direction, we developed a framework to compare deep learning architectures on a large set of EEG examples ($>$100 decoding problems) in a comprehensively documented and reproducible manner. Applying this framework, here we report on three publicly available CNN architectures.  In this paper we describe the framework and provide our rationale for design choices regarding the challenges describes above. Secondly, we present the results of our comparison and make a recommendation on which of the network(s) included in the comparison to use for best performance. At last we discuss how the present framework could be further extended and improved.	score:420
I am in last batch at Fiitjee.I have screwed my marks.how shd I study to get 200+ in JEE Mains 2017?	}   \label{fig:hierarchy} \end{minipage}  \end{figure*}  The paper is organized as a set of experiments answering a list of key questions about feature learning with ImageNet. The following is a summary of our main findings:   \medskip  \noindent\textit{1. How many pre-training ImageNet examples are sufficient for transfer learning?} Pre-training with only \textit{half} the ImageNet data (500 images per class instead of 1000) results in only a small drop in transfer learning performance (1. 5 mAP drop on PASCAL-DET). This drop is much smaller than the drop on the ImageNet classification task itself.  See Section~\ref{sec:examples_per_class} and Figure~\ref{fig:number-of-class} for details.   \medskip  \noindent \textit{2. How many pre-training ImageNet classes are sufficient for transfer learning?}  Pre-training with an order of magnitude \textit{fewer} classes (127 classes instead of 1000) results in only a small drop in transfer learning performance (2.	}   \label{fig:hierarchy} \end{minipage}  \end{figure*}  The paper is organized as a set of experiments answering a list of key questions about feature learning with ImageNet. The following is a summary of our main findings:   \medskip  \noindent\textit{1. How many pre-training ImageNet examples are sufficient for transfer learning?} Pre-training with only \textit{half} the ImageNet data (500 images per class instead of 1000) results in only a small drop in transfer learning performance (1. 5 mAP drop on PASCAL-DET). This drop is much smaller than the drop on the ImageNet classification task itself.  See Section~\ref{sec:examples_per_class} and Figure~\ref{fig:number-of-class} for details.   \medskip  \noindent \textit{2. How many pre-training ImageNet classes are sufficient for transfer learning?}  Pre-training with an order of magnitude \textit{fewer} classes (127 classes instead of 1000) results in only a small drop in transfer learning performance (2.	score:422
I am in last batch at Fiitjee.I have screwed my marks.how shd I study to get 200+ in JEE Mains 2017?	 It can then be applied to different data containing different groups. We demonstrate promising performance on high-dimensional data like images (COIL-100) and speech (TIMIT). We call this ``learning to cluster'' and show its conceptual difference to deep metric learning, semi-supervise clustering and other related approaches while having the advantage of performing learnable clustering fully end-to-end.   	Consider the illustrative task of grouping images of cats and dogs by \emph{perceived} similarity: depending on the intention of the user behind the task, the similarity could be defined by animal type (foreground object), environmental nativeness (background landscape, cp. Fig. \ref{fig:cats_dogs}) etc. This is characteristic of clustering perceptual, high-dimensional data like images \cite{kampffmeyer2017} or sound \cite{lukic2017speaker}: a user typically has some similarity criterion in mind when thinking about naturally arising groups (e.	 It can then be applied to different data containing different groups. We demonstrate promising performance on high-dimensional data like images (COIL-100) and speech (TIMIT). We call this ``learning to cluster'' and show its conceptual difference to deep metric learning, semi-supervise clustering and other related approaches while having the advantage of performing learnable clustering fully end-to-end.   	Consider the illustrative task of grouping images of cats and dogs by \emph{perceived} similarity: depending on the intention of the user behind the task, the similarity could be defined by animal type (foreground object), environmental nativeness (background landscape, cp. Fig. \ref{fig:cats_dogs}) etc. This is characteristic of clustering perceptual, high-dimensional data like images \cite{kampffmeyer2017} or sound \cite{lukic2017speaker}: a user typically has some similarity criterion in mind when thinking about naturally arising groups (e.	score:426
How can I use cost sensitive learning for classification problems in which cost of misclassifications vary for each record?	e., a learning algorithm trained on a particular dataset generalizes poorly across datasets. In object recognition, for example, training images may be collected under specific conditions involving camera viewpoints, backgrounds, lighting conditions, and object transformations. In such situations, the classifiers obtained with learning algorithms operating on samples from one dataset cannot be directly applied to other related datasets.  Developing learning algorithms that are robust to label scarcity and dataset bias is therefore an important and compelling problem.  \emph{Domain adaptation}~\cite{Blitzer:2006aa} and \emph{domain generalization}~\cite{Blanchard2011} have been proposed to overcome the fore-mentioned issues.   In this context, a \emph{domain} represents a probability distribution from which the samples are drawn and is often equated with a dataset.	e., a learning algorithm trained on a particular dataset generalizes poorly across datasets. In object recognition, for example, training images may be collected under specific conditions involving camera viewpoints, backgrounds, lighting conditions, and object transformations. In such situations, the classifiers obtained with learning algorithms operating on samples from one dataset cannot be directly applied to other related datasets.  Developing learning algorithms that are robust to label scarcity and dataset bias is therefore an important and compelling problem.  \emph{Domain adaptation}~\cite{Blitzer:2006aa} and \emph{domain generalization}~\cite{Blanchard2011} have been proposed to overcome the fore-mentioned issues.   In this context, a \emph{domain} represents a probability distribution from which the samples are drawn and is often equated with a dataset.	score:265
How can I use cost sensitive learning for classification problems in which cost of misclassifications vary for each record?	  In particular it is shown that the worst perturbation incurred by an imperceptible adversarial perturbation can be found using a convex optimization problem and the closed form solutions  are provided for the classification problem. In Section \ref{sec:other}, we show the applicability of our framework for various learning tasks such as regression, image segmentation, and detection.  Later, in Section \ref{sec:expres}, we benchmark the obtained methods, for the context of image classification, against the \ac{FGSM} and DeepFool algorithms. In addition, we show that these algorithms can be formulated within our framework.  Furthermore, it is shown that our proposed algorithm manages to outperform existing methods using empirical simulations {on the MNIST and CIFAR-10 datasets}.	  In particular it is shown that the worst perturbation incurred by an imperceptible adversarial perturbation can be found using a convex optimization problem and the closed form solutions  are provided for the classification problem. In Section \ref{sec:other}, we show the applicability of our framework for various learning tasks such as regression, image segmentation, and detection.  Later, in Section \ref{sec:expres}, we benchmark the obtained methods, for the context of image classification, against the \ac{FGSM} and DeepFool algorithms. In addition, we show that these algorithms can be formulated within our framework.  Furthermore, it is shown that our proposed algorithm manages to outperform existing methods using empirical simulations {on the MNIST and CIFAR-10 datasets}.	score:280
How can I use cost sensitive learning for classification problems in which cost of misclassifications vary for each record?	 We conduct classification experiments with such disparate domains as strings, time series, and sets of vectors, where our proposed framework compares favorably to existing distance-based learning methods such as $k$-nearest-neighbors, distance-substitution kernels, pseudo-Euclidean embedding, and the representative-set method. 	In many problem domains, it is easier to specify a reasonable dissimilarity (or similarity) function between instances, than to construct a feature representation.	 We conduct classification experiments with such disparate domains as strings, time series, and sets of vectors, where our proposed framework compares favorably to existing distance-based learning methods such as $k$-nearest-neighbors, distance-substitution kernels, pseudo-Euclidean embedding, and the representative-set method. 	In many problem domains, it is easier to specify a reasonable dissimilarity (or similarity) function between instances, than to construct a feature representation.	score:281
How can I use cost sensitive learning for classification problems in which cost of misclassifications vary for each record?	 On one side, there are probabilistic approaches which rely on modelling the joint-image distribution. For instance, Guetter \etal propose a generative method based on Kullback-Leibler Divergence~\cite{guetter:kullback}. Our work is closer to the discriminative concept proposed  by Lee \etal \cite{lee:learning} and Michel \etal \cite{michel:boost}, where the problem of learning a similarity metric is posed as binary classification.  Here the goal is to discriminate between aligned and misaligned patches given pairs of aligned images. Lee \etal propose the use of a Structured Support Vector Machine while Michel \etal use a method based on Adaboost. Different to these approaches we rely on CNN as our learning method of choice as the suitable set of characteristics for each type of modality combinations can be directly learned from the training data.	 On one side, there are probabilistic approaches which rely on modelling the joint-image distribution. For instance, Guetter \etal propose a generative method based on Kullback-Leibler Divergence~\cite{guetter:kullback}. Our work is closer to the discriminative concept proposed  by Lee \etal \cite{lee:learning} and Michel \etal \cite{michel:boost}, where the problem of learning a similarity metric is posed as binary classification.  Here the goal is to discriminate between aligned and misaligned patches given pairs of aligned images. Lee \etal propose the use of a Structured Support Vector Machine while Michel \etal use a method based on Adaboost. Different to these approaches we rely on CNN as our learning method of choice as the suitable set of characteristics for each type of modality combinations can be directly learned from the training data.	score:283
How can I use cost sensitive learning for classification problems in which cost of misclassifications vary for each record?	 However, these approaches usually generate the binary classification output for object tracking, and thus ignore the intrinsic structural or geometrical information (e.g., geometric transform across frames) on object localization and matching during model learning. To address this issue, Hare {\it et al.}~\cite{Hare2012} propose a structured SVM-based keypoint tracking approach that incorporates the RANSAC-based geometric matching information into the optimization process of learning keypoint-specific SVM models.  As a result, the proposed tracking approach is able to simultaneously find correct keypoint correspondences and estimate underlying object geometric transforms across frames. In addition, the model learning process is independently carried out frame by frame, and hence ignores the intrinsic cross-frame interaction information on temporal model coherence, leading to instable tracking results in complicated scenarios.	 However, these approaches usually generate the binary classification output for object tracking, and thus ignore the intrinsic structural or geometrical information (e.g., geometric transform across frames) on object localization and matching during model learning. To address this issue, Hare {\it et al.}~\cite{Hare2012} propose a structured SVM-based keypoint tracking approach that incorporates the RANSAC-based geometric matching information into the optimization process of learning keypoint-specific SVM models.  As a result, the proposed tracking approach is able to simultaneously find correct keypoint correspondences and estimate underlying object geometric transforms across frames. In addition, the model learning process is independently carried out frame by frame, and hence ignores the intrinsic cross-frame interaction information on temporal model coherence, leading to instable tracking results in complicated scenarios.	score:284
What are thelatest off campus placements for 2017 batch?	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	score:460
What are thelatest off campus placements for 2017 batch?	  \cite{LeLaPePaRa:2015} provides a probabilistic scheme of leveraging coding based on a random storage placement. In contrast to \cite{LeLaPePaRa:2015}, in this paper we provide a deterministic and systematic storage update scheme, which increases the coding opportunities in the delivery phase. The underlying metric used here is the worst-case communication cost over all the possible shuffles, unlike the average cost considered in \cite{LeLaPePaRa:2015}. Finally, we also present the first information theoretic lower bounds on the communication overhead for the data shuffling problem.	  More interestingly, the proposed storage update algorithm maintains the structural properties of the storage at the workers over time. This structural invariance placement is extremely critical in leveraging the gains of coding for different shuffles.     \textbf{Related Work:}  In the past few years, there has been a flurry of research acitivity in understanding the benefits of coding for caching starting from the work of Maddah-Ali and Niesen \cite{MaNi:2014} who showed that exploiting multi-casting opportunities by coding can reduce the communication for caching.	score:481
What are thelatest off campus placements for 2017 batch?	 In addition, prior work has shown that decorrelated activations result in better features~\cite{1961_Barlow_possible,1992_NC_Schmidhuber,2009_NIPS_Bengio} and better generalization~\cite{2016_ICLR_Cogswell,2016_ICDM_Xiong}, suggesting room for further improving Batch Normalization.  In this paper, we propose Decorrelated Batch Normalization, in which we whiten the activations of each layer within a mini-batch.  Let $\mathbf{x}_i \in \mathbb{R}^d$ be the input to a layer for the $i$-th example in a mini-batch of size $m$.  The whitened input is given by {\setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt} \begin{equation} \hat{\mathbf{x}}_i = \Sigma^{-\frac{1}{2}}(\mathbf{x}_i - \mathbf{\mu}), \end{equation} } \hspace{-0.05in}where $\mathbf{\mu}=\frac{1}{m} \sum_{j=1}^{m} \mathbf{x}_j$ is the mini-batch mean and $\Sigma =\frac{1}{m} \sum_{j=1}^{m} (\mathbf{x}_j-\mathbf{\mu})(\mathbf{x}_j-\mathbf{\mu})^T$ is the mini-batch covariance matrix.	 In addition, prior work has shown that decorrelated activations result in better features~\cite{1961_Barlow_possible,1992_NC_Schmidhuber,2009_NIPS_Bengio} and better generalization~\cite{2016_ICLR_Cogswell,2016_ICDM_Xiong}, suggesting room for further improving Batch Normalization.  In this paper, we propose Decorrelated Batch Normalization, in which we whiten the activations of each layer within a mini-batch.  Let $\mathbf{x}_i \in \mathbb{R}^d$ be the input to a layer for the $i$-th example in a mini-batch of size $m$.  The whitened input is given by {\setlength\abovedisplayskip{6pt} \setlength\belowdisplayskip{6pt} \begin{equation} \hat{\mathbf{x}}_i = \Sigma^{-\frac{1}{2}}(\mathbf{x}_i - \mathbf{\mu}), \end{equation} } \hspace{-0.05in}where $\mathbf{\mu}=\frac{1}{m} \sum_{j=1}^{m} \mathbf{x}_j$ is the mini-batch mean and $\Sigma =\frac{1}{m} \sum_{j=1}^{m} (\mathbf{x}_j-\mathbf{\mu})(\mathbf{x}_j-\mathbf{\mu})^T$ is the mini-batch covariance matrix.	score:490
What are thelatest off campus placements for 2017 batch?	 However, we often observe that the conclusion is drawn that a superior model implies a superior  approach for that task. For example, for the shared task SemEval-2017 on semantic textual similarity (STS) the task organizers conclude that the model from the winning team is \textit{``the best overall system''} \cite{Cer2017}. \newcite{Szegedy2014} conclude that the winning model from Clarifai for the ImageNet 2013 challenge was the \textit{``year's best approach''}.    The contribution in this paper is to show, that this conclusion cannot be drawn for \textit{non-deterministic learning approaches}\footnote{We define a learning approach as non-deterministic if it uses a sequence of random numbers to solve the optimization problem. Our observations are extendable to deterministic approaches that have tunable hyperparameters.	 However, we often observe that the conclusion is drawn that a superior model implies a superior  approach for that task. For example, for the shared task SemEval-2017 on semantic textual similarity (STS) the task organizers conclude that the model from the winning team is \textit{``the best overall system''} \cite{Cer2017}. \newcite{Szegedy2014} conclude that the winning model from Clarifai for the ImageNet 2013 challenge was the \textit{``year's best approach''}.    The contribution in this paper is to show, that this conclusion cannot be drawn for \textit{non-deterministic learning approaches}\footnote{We define a learning approach as non-deterministic if it uses a sequence of random numbers to solve the optimization problem. Our observations are extendable to deterministic approaches that have tunable hyperparameters.	score:492
What are thelatest off campus placements for 2017 batch?	 Although sometimes default values exist, there is no agreed upon principle for their definition (but see our recent work in in \citep{Probst2018} for a potential approach). Automatic tuning of such parameters is a possible solution \citep{Claesen2015}, but comes with a considerable computational burden.         Meta-learning tries to decrease this cost \citep{Feurer2015}, by reusing information of previous runs of the algorithm on similar datasets, which obviously requires access to such prior empirical results.  With this paper we provide a freely accessible meta dataset that contains around $2.5$ million runs of six different machine learning algorithms on $38$ classification datasets.  Large, freely available datasets like Imagenet \citep{ImageNet2009} are important for the progress of machine learning, we hope to support developments in the area of meta-learning and benchmarking, meta-learning and hyperparameter tuning with our work here.	 Although sometimes default values exist, there is no agreed upon principle for their definition (but see our recent work in in \citep{Probst2018} for a potential approach). Automatic tuning of such parameters is a possible solution \citep{Claesen2015}, but comes with a considerable computational burden.         Meta-learning tries to decrease this cost \citep{Feurer2015}, by reusing information of previous runs of the algorithm on similar datasets, which obviously requires access to such prior empirical results.  With this paper we provide a freely accessible meta dataset that contains around $2.5$ million runs of six different machine learning algorithms on $38$ classification datasets.  Large, freely available datasets like Imagenet \citep{ImageNet2009} are important for the progress of machine learning, we hope to support developments in the area of meta-learning and benchmarking, meta-learning and hyperparameter tuning with our work here.	score:493
Is there any off campus placement for 2017 batch?	 	There is a growing interest in safety and risk-sensitive metrics for machine learning and artificial intelligence systems, especially for systems that interact with their environment \cite{garcia2015comprehensive,amodei2016concrete,thomas2017ensuring}.  Risk-aware approaches have been recently proposed and applied to many different problems including planning in Markov decision processes \cite{chow2015risk}, physical search problems \cite{brown2016exact}, reinforcement learning \cite{tamar2015optimizing,garcia2015comprehensive}, and imitation learning \cite{santara2017rail}; however, to the best of our knowledge, no one has investigated how to obtain sample-efficient, risk-aware confidence bounds on the performance of a policy under an unknown reward function, as is the case when learning from demonstrations.	 	There is a growing interest in safety and risk-sensitive metrics for machine learning and artificial intelligence systems, especially for systems that interact with their environment \cite{garcia2015comprehensive,amodei2016concrete,thomas2017ensuring}.  Risk-aware approaches have been recently proposed and applied to many different problems including planning in Markov decision processes \cite{chow2015risk}, physical search problems \cite{brown2016exact}, reinforcement learning \cite{tamar2015optimizing,garcia2015comprehensive}, and imitation learning \cite{santara2017rail}; however, to the best of our knowledge, no one has investigated how to obtain sample-efficient, risk-aware confidence bounds on the performance of a policy under an unknown reward function, as is the case when learning from demonstrations.	score:486
Is there any off campus placement for 2017 batch?	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	   In this work, we develop a framework for learning the higher-level latent state space of time-varying graphs. We study data collected from the email logs of university mail servers. The links correspond to the set of email communications among the students/staff/faculty in Purdue university from August $2011$ to February $2012$. This seven month period includes several calendar events (breaks, vacations, ...etc.) that take place during the academic year. These real-time events usually impact the underlying graph process to make its characteristics non-stationary and drift from the overall mean. We show how our framework discovers subsequences that correspond to these global events without detecting local changes that are due to diurnal patterns.	score:489
Is there any off campus placement for 2017 batch?	 Due to this transferability, adversaries can attack DNN without knowing the network parameters (i.e. blackbox) \cite{LiuYanpei:2016,Brendel:2017}. There even exist universal perturbations that can imperceptibly perturb any image and cause misclassification for any given network \cite{Moosavi-Dezfooli_2017_CVPR}. And recently, there has been much work on defending against these universal perturbations \cite{Akhtar_2018_CVPR}.   In this work, we defend against adversarial attacks by replacing the commonly used output activation of DNN with a manifold-interpolating function. Together with the Projected Gradient Descent (PGD) adversarial training \cite{Madry:2018}, Total Variation Minimization (TVM), and training data augmentation, we show state-of-the-art results for adversarial defense on the CIFAR10 benchmark.	 Due to this transferability, adversaries can attack DNN without knowing the network parameters (i.e. blackbox) \cite{LiuYanpei:2016,Brendel:2017}. There even exist universal perturbations that can imperceptibly perturb any image and cause misclassification for any given network \cite{Moosavi-Dezfooli_2017_CVPR}. And recently, there has been much work on defending against these universal perturbations \cite{Akhtar_2018_CVPR}.   In this work, we defend against adversarial attacks by replacing the commonly used output activation of DNN with a manifold-interpolating function. Together with the Projected Gradient Descent (PGD) adversarial training \cite{Madry:2018}, Total Variation Minimization (TVM), and training data augmentation, we show state-of-the-art results for adversarial defense on the CIFAR10 benchmark.	score:490
Is there any off campus placement for 2017 batch?	 We shall however take the original ResNet as our starting point.    Shortly before the introduction of ResNets, batch normalization was introduced by  \cite{ioffe2015batch}.  Batch normalization was employed by \cite{he2016deep} in their definition of ResNets and indeed  many if not most CNN implementations today make use of batch normalization. The goal of batch normalization according to the authors was to accelerate the model training process by allowing for effective training with higher learning rates, as well as reducing the dependence on  parameter initialization.	 We shall however take the original ResNet as our starting point.    Shortly before the introduction of ResNets, batch normalization was introduced by  \cite{ioffe2015batch}.  Batch normalization was employed by \cite{he2016deep} in their definition of ResNets and indeed  many if not most CNN implementations today make use of batch normalization. The goal of batch normalization according to the authors was to accelerate the model training process by allowing for effective training with higher learning rates, as well as reducing the dependence on  parameter initialization.	score:495
Is there any off campus placement for 2017 batch?	 However, in many cases we may not know the exact label. Instead, we may have an interval in which the true label lies. Such a scenario is discussed in \citet{antoniuk14,Antoniuk:2016}. In this setting, corresponding to each example, an interval label is provided and it is assumed that the true label of the example lies in this interval. In \citet{Antoniuk:2016}, a large margin framework for batch learning is proposed using interval insensitive loss function.    In this paper, we propose an online algorithm for learning ordinal classifier using interval labeled data. We name the proposed approach as {\bf PRIL} (Perceptron ranking using interval labeled data). Our approach is based on interval insensitive loss function. As per our knowledge, this is the first ever online ranking algorithm using interval labeled data.	 However, in many cases we may not know the exact label. Instead, we may have an interval in which the true label lies. Such a scenario is discussed in \citet{antoniuk14,Antoniuk:2016}. In this setting, corresponding to each example, an interval label is provided and it is assumed that the true label of the example lies in this interval. In \citet{Antoniuk:2016}, a large margin framework for batch learning is proposed using interval insensitive loss function.    In this paper, we propose an online algorithm for learning ordinal classifier using interval labeled data. We name the proposed approach as {\bf PRIL} (Perceptron ranking using interval labeled data). Our approach is based on interval insensitive loss function. As per our knowledge, this is the first ever online ranking algorithm using interval labeled data.	score:497
How can I find inverse of a matrix in GF(256)?	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	 However, such network compression can result in irregular matrix structures that are mismatched with modern hardware-accelerated platforms, such as graphics processing units (GPUs) designed to perform the DNN matrix multiplications in a structured (block-based) way. We propose MPDCompress, a DNN compression algorithm based on matrix permutation decomposition via random mask generation.  In-training application of the masks molds the synaptic weight connection matrix to a sub-graph separation format. Aided by the  random permutations, a hardware-desirable block matrix is generated, allowing for a more efficient implementation and compression of the network. To show versatility, we empirically verify MPDCompress on several network models, compression rates, and image datasets.	score:391
How can I find inverse of a matrix in GF(256)?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	 Thus, matrix completion can be formulated as minimizing the rank of the matrix given a random sample of its entries. However, this rank minimization problem is in general NP-hard due to the combinatorial nature of the rank function~\citep{fazel2001,fazelPhD-2000}. To alleviate this problem and make it tractable, convex relaxation strategies were proposed, e.	score:411
How can I find inverse of a matrix in GF(256)?	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	g., \cite{Roth1981}) we propose an efficient randomized algorithm that determines whether or not it is possible to uniquely complete an incomplete matrix to a matrix of specified rank $d$. Our proposed algorithm does not attempt to complete the matrix but only determines if a unique completion is possible. We introduce a new matrix, which we call {\em the completion matrix} that serves as the analogue of the rigidity matrix in rigidity theory.  The rank of the completion matrix determines a property which we call infinitesimal completion. Whenever the completion matrix is large and sparse its rank can be efficiently determined using iterative methods such as LSQR \cite{LSQR}. As in rigidity theory, we will also make the distinction between {\em local} completion and {\em global} completion.	score:427
How can I find inverse of a matrix in GF(256)?	 Low-rank matrix approximations are often used to help scale standard machine learning algorithms to large-scale problems.  Recently, matrix coherence has been used to characterize the ability to extract global information from a subset of matrix entries in the context of these low-rank approximations and other sampling-based algorithms, e.g., matrix completion, robust PCA.   Since coherence is defined in terms of the singular vectors of a matrix and is expensive to compute, the practical significance of these results largely hinges on the following question: \emph{Can we efficiently and accurately estimate the coherence of a matrix?} In this paper we address this question. We propose a novel algorithm for estimating coherence from a small number of columns, formally analyze its behavior, and derive a new coherence-based matrix approximation bound based on this analysis.  We then present extensive experimental results on synthetic and real datasets that corroborate our worst-case theoretical analysis, yet provide strong support for the use of our proposed algorithm whenever low-rank approximation is being considered.  Our algorithm efficiently and accurately estimates matrix coherence across a wide range of datasets, and these coherence estimates are excellent predictors of the effectiveness of sampling-based matrix approximation on a case-by-case basis.	}  In this paper we address this question by presenting a novel algorithm for estimating matrix coherence from a small number of columns. The remainder of this paper is organized as follows. Section \ref{sec:prelim} introduces basic definitions, and provides a brief background on low-rank matrix approximation and matrix coherence.  In Section \ref{sec:algorithm} we introduce our sampling-based algorithm to estimate matrix coherence.   We then formally analyze its behavior in Section \ref{sec:theory}, and also use this analysis to derive a novel coherence-based bound for matrix projection reconstruction via Column-sampling (defined in Section \ref{ssec:low_rank}).  Finally, in Section \ref{sec:experiments} we present extensive experimental results on synthetic and real datasets.	score:430
How can I find inverse of a matrix in GF(256)?	    \label{feasible-set} \end{align}    We will argue that this formulation can be used to impose several types of structure on the support of principal components. Note that the covariance matrix $\empirical$ and the graph can be arbitrary: the matrix is capturing data correlations while the graph is a mathematical tool to efficiently describe the possible supports of interest.   We illustrate this through a few applications.   \textbf{Financial model selection:} Consider the problem of identifying which companies out of the S{\&}P500 index capture most data variability.  Running Sparse PCA with a sparsity parameter $k$ will select $k$ companies that maximize explained variance.  However, it may be useful to enforce more structure: If we must select one company from each business sector (\textit{e.	    \label{feasible-set} \end{align}    We will argue that this formulation can be used to impose several types of structure on the support of principal components. Note that the covariance matrix $\empirical$ and the graph can be arbitrary: the matrix is capturing data correlations while the graph is a mathematical tool to efficiently describe the possible supports of interest.   We illustrate this through a few applications.   \textbf{Financial model selection:} Consider the problem of identifying which companies out of the S{\&}P500 index capture most data variability.  Running Sparse PCA with a sparsity parameter $k$ will select $k$ companies that maximize explained variance.  However, it may be useful to enforce more structure: If we must select one company from each business sector (\textit{e.	score:431
What is the salary range for classification j at Stanford University?	 A crucial task in system identification problems is the selection of the most appropriate model class, and is classically addressed resorting to cross-validation or using order selection criteria based on  asymptotic arguments. As recently suggested in the literature, this can be addressed in a Bayesian framework, where model complexity is regulated by few hyperparameters, which can be estimated via marginal likelihood maximization.  It is thus of primary importance to design effective optimization methods to solve the corresponding optimization problem. If the unknown impulse response is modeled as a Gaussian process with a suitable kernel, the maximization of the marginal likelihood leads to a challenging nonconvex optimization problem, which requires a stable and effective solution strategy.	 A crucial task in system identification problems is the selection of the most appropriate model class, and is classically addressed resorting to cross-validation or using order selection criteria based on  asymptotic arguments. As recently suggested in the literature, this can be addressed in a Bayesian framework, where model complexity is regulated by few hyperparameters, which can be estimated via marginal likelihood maximization.  It is thus of primary importance to design effective optimization methods to solve the corresponding optimization problem. If the unknown impulse response is modeled as a Gaussian process with a suitable kernel, the maximization of the marginal likelihood leads to a challenging nonconvex optimization problem, which requires a stable and effective solution strategy.	score:470
What is the salary range for classification j at Stanford University?	  Classifying programming languages of source code files using ML and NLP methods has been well explored in the research community (cf. \cite{Kennedy}\cite{S.Gilda}\cite{Khasnabish}). It has been established that the programming language of a source code file can be identified with high accuracy. However, most of the previous work that study the classification of programming languages use the GitHub dataset in which the size of source code files is typically large.  Applying ML and NLP methods to classify a large source code file provides a very high accuracy as the large sample contains many features that help the machine learning model to learn better. In this paper, we are interested in a tool that can classify a code snippet which is a small block reusable code with at least two lines of code, a much more challenging task.	  Classifying programming languages of source code files using ML and NLP methods has been well explored in the research community (cf. \cite{Kennedy}\cite{S.Gilda}\cite{Khasnabish}). It has been established that the programming language of a source code file can be identified with high accuracy. However, most of the previous work that study the classification of programming languages use the GitHub dataset in which the size of source code files is typically large.  Applying ML and NLP methods to classify a large source code file provides a very high accuracy as the large sample contains many features that help the machine learning model to learn better. In this paper, we are interested in a tool that can classify a code snippet which is a small block reusable code with at least two lines of code, a much more challenging task.	score:475
What is the salary range for classification j at Stanford University?	 Empirical studies partially demonstrate the resistance of \(k\)-nearest neighbor to noise \citep{Kusner:Tyree:Weinberger:Agrawal2014,Tarlow:Swersky:Swersky:Charlin:Sutskever:Zemel2013}, whereas there is a paucity of deep understanding.  This work focuses on binary classification in the presence of random classification noises \citep{Angluin:Laird1988}, that is, each observed label has been flipped with certain probability instead of seeing the ground-truth label, and training data of each class are contaminated by samples from the other class.	 Empirical studies partially demonstrate the resistance of \(k\)-nearest neighbor to noise \citep{Kusner:Tyree:Weinberger:Agrawal2014,Tarlow:Swersky:Swersky:Charlin:Sutskever:Zemel2013}, whereas there is a paucity of deep understanding.  This work focuses on binary classification in the presence of random classification noises \citep{Angluin:Laird1988}, that is, each observed label has been flipped with certain probability instead of seeing the ground-truth label, and training data of each class are contaminated by samples from the other class.	score:484
What is the salary range for classification j at Stanford University?	  What is common to all these applications is uncertainty and then that they have to deal with decision and statistical inference.  Ranking is perhaps the most crucial task performed by the data management systems which have to deal with uncertainty. In many applications, ranking aims at deciding or inferring, for example, the class assigned to a unit or the order by relevance, usefulness, or utility of the units delivered to another application or to an end user.	  What is common to all these applications is uncertainty and then that they have to deal with decision and statistical inference.  Ranking is perhaps the most crucial task performed by the data management systems which have to deal with uncertainty. In many applications, ranking aims at deciding or inferring, for example, the class assigned to a unit or the order by relevance, usefulness, or utility of the units delivered to another application or to an end user.	score:485
What is the salary range for classification j at Stanford University?	 It is desirable to develop a mechanism that can couple the classifier to the generative models to allow fine-tuning of the feature mapping.  Maximum entropy discrimination~\cite{jaakkola1999maxent} provides yet another framework to exploit generative models for classification under the large margin principle. This framework, however, requires deliberately choosing conjugate priors for parameters of the generative models, which limits its application to complex models.  In addition, the VC risk bound~\cite{vapnik2000nature} utilized by this method is generally loose in comparison with the PAC-Bayes bounds~\cite{mcallester1999somepacl,langford2006tutorial,germain2009pac}. Also, there are some other efforts~\cite{raina2004gendis,mccallum2006gendis} made to couple generative and discriminative models for classification.	 It is desirable to develop a mechanism that can couple the classifier to the generative models to allow fine-tuning of the feature mapping.  Maximum entropy discrimination~\cite{jaakkola1999maxent} provides yet another framework to exploit generative models for classification under the large margin principle. This framework, however, requires deliberately choosing conjugate priors for parameters of the generative models, which limits its application to complex models.  In addition, the VC risk bound~\cite{vapnik2000nature} utilized by this method is generally loose in comparison with the PAC-Bayes bounds~\cite{mcallester1999somepacl,langford2006tutorial,germain2009pac}. Also, there are some other efforts~\cite{raina2004gendis,mccallum2006gendis} made to couple generative and discriminative models for classification.	score:490
What is salary range for classification G at Stanford University?	 There are tremendous amount of research on improving the classification performance in such cases. One highly investigated field for this problem is ensemble learning \cite{kuncheva}, where multiple prediction are fused the produce a more efficient classification approach. One fundamental requirement for the creation of classifier ensembles is diversity among them \cite{Brown2005}, that is, the classifiers included in the ensemble need to complement each other to provide more generalization capabilities than a single learner.	 There are tremendous amount of research on improving the classification performance in such cases. One highly investigated field for this problem is ensemble learning \cite{kuncheva}, where multiple prediction are fused the produce a more efficient classification approach. One fundamental requirement for the creation of classifier ensembles is diversity among them \cite{Brown2005}, that is, the classifiers included in the ensemble need to complement each other to provide more generalization capabilities than a single learner.	score:481
What is salary range for classification G at Stanford University?	 A crucial task in system identification problems is the selection of the most appropriate model class, and is classically addressed resorting to cross-validation or using order selection criteria based on  asymptotic arguments. As recently suggested in the literature, this can be addressed in a Bayesian framework, where model complexity is regulated by few hyperparameters, which can be estimated via marginal likelihood maximization.  It is thus of primary importance to design effective optimization methods to solve the corresponding optimization problem. If the unknown impulse response is modeled as a Gaussian process with a suitable kernel, the maximization of the marginal likelihood leads to a challenging nonconvex optimization problem, which requires a stable and effective solution strategy.	 A crucial task in system identification problems is the selection of the most appropriate model class, and is classically addressed resorting to cross-validation or using order selection criteria based on  asymptotic arguments. As recently suggested in the literature, this can be addressed in a Bayesian framework, where model complexity is regulated by few hyperparameters, which can be estimated via marginal likelihood maximization.  It is thus of primary importance to design effective optimization methods to solve the corresponding optimization problem. If the unknown impulse response is modeled as a Gaussian process with a suitable kernel, the maximization of the marginal likelihood leads to a challenging nonconvex optimization problem, which requires a stable and effective solution strategy.	score:482
What is salary range for classification G at Stanford University?	 We explain why the learning curve has a more steep incline and a more gradual decline in this setting through simulation studies and by applying an approximation of the learning curve based on the work by Raudys \& Duin. 	In general, for most classifiers, classification performance is expected to improve as more labeled training examples become available.  The dipping phenomenon is one exception to this rule, showing for specific combinations of datasets and classifiers that error rates can actually increase with increasing numbers of labeled data \cite{Loog2012}.  For the least squares classifier and some other classifiers, the \emph{peaking phenomenon} is another known exception. In this setting, the classification error may first increase, after which the error rate starts to decrease again as we add more labeled training examples.	 We explain why the learning curve has a more steep incline and a more gradual decline in this setting through simulation studies and by applying an approximation of the learning curve based on the work by Raudys \& Duin. 	In general, for most classifiers, classification performance is expected to improve as more labeled training examples become available.  The dipping phenomenon is one exception to this rule, showing for specific combinations of datasets and classifiers that error rates can actually increase with increasing numbers of labeled data \cite{Loog2012}.  For the least squares classifier and some other classifiers, the \emph{peaking phenomenon} is another known exception. In this setting, the classification error may first increase, after which the error rate starts to decrease again as we add more labeled training examples.	score:493
What is salary range for classification G at Stanford University?	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	score:495
What is salary range for classification G at Stanford University?	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	 \end{itemize} A number of influential existing theories in behavioral economics provide methods for estimating the probability that different strings are generated by a human source, and hence lead to predictions for these problems \citep{Rabin,gambler}.  What is striking is that despite the richness of the underlying questions, the Continuation and Classification problems are  behavioral-science questions where the benchmark of optimal prediction can in fact be feasibly computed.  Optimal predictions for this problem can be made via  {\em table lookup}, in which we enumerate all $2^k$ strings $s$  consisting of 0's and 1's, and for each such string $s$ we record  the empirical fraction $g(s)$ of human-generated strings in our sample that are equal to $s$. With enough samples, this converges to the {\em human distribution} over the full set of strings.	score:495
What data science and machine learning career opportunities are there at Microsoft?	   WordRep is a benchmark collection for the research on learning distributed word representations (or word embeddings), released by Microsoft Research. In this paper, we describe the details of the WordRep collection and show how to use it in different types of machine learning research related to word embedding. Specifically, we describe how the evaluation tasks in WordRep are selected, how the data are sampled, and how the evaluation tool is built.  We then compare several state-of-the-art word representations on WordRep, report their evaluation performance, and make discussions on the results. After that, we discuss new potential research topics that can be supported by WordRep, in addition to algorithm comparison. We hope that this paper can help people gain deeper understanding of WordRep, and enable more interesting research on learning distributed word representations and related topics.	   WordRep is a benchmark collection for the research on learning distributed word representations (or word embeddings), released by Microsoft Research. In this paper, we describe the details of the WordRep collection and show how to use it in different types of machine learning research related to word embedding. Specifically, we describe how the evaluation tasks in WordRep are selected, how the data are sampled, and how the evaluation tool is built.  We then compare several state-of-the-art word representations on WordRep, report their evaluation performance, and make discussions on the results. After that, we discuss new potential research topics that can be supported by WordRep, in addition to algorithm comparison. We hope that this paper can help people gain deeper understanding of WordRep, and enable more interesting research on learning distributed word representations and related topics.	score:373
What data science and machine learning career opportunities are there at Microsoft?	  Also includes exploration tools for visualization and predictive analytics, with some cases of use for knowledge discovery and Hadoop behavior modeling over benchmarking. ALOJA is an initiative of the Barcelona Supercomputing Center (BSC) in an on-going collaborative engagement with the Microsoft Product groups and Microsoft Research (MSR) to explore upcoming hardware architectures and building automated mechanism for deploying cost-effective Hadoop clusters.  The initial approach for ALOJA project was to create a comprehensive open public and vendor-neutral Hadoop benchmarking repository. This benchmark repository is intended to compare software configuration parameters, state of the art and emerging hardware like solid-state disks or RDMA oriented networks such InfiniBand, also different types of Cloud services.	  Also includes exploration tools for visualization and predictive analytics, with some cases of use for knowledge discovery and Hadoop behavior modeling over benchmarking. ALOJA is an initiative of the Barcelona Supercomputing Center (BSC) in an on-going collaborative engagement with the Microsoft Product groups and Microsoft Research (MSR) to explore upcoming hardware architectures and building automated mechanism for deploying cost-effective Hadoop clusters.  The initial approach for ALOJA project was to create a comprehensive open public and vendor-neutral Hadoop benchmarking repository. This benchmark repository is intended to compare software configuration parameters, state of the art and emerging hardware like solid-state disks or RDMA oriented networks such InfiniBand, also different types of Cloud services.	score:384
What data science and machine learning career opportunities are there at Microsoft?	   	Over the past few years, providers such as  Google, Microsoft, and Amazon have started to provide customers with access to APIs allowing them to easily embed machine learning tasks into their applications. Organizations can use Machine Learning as a Service (MLaaS) engines to outsource complex tasks, e.g., training classifiers, performing predictions, clustering, etc.   They can also let others query models trained on their data, possibly at a cost.   However, if malicious users were able to recover data used to train these models, the resulting information leakage would create serious issues. In particular, organizations do not have much control over the kind of models and training parameters used by the  platform,  and this might lead to overfitting (i.	   	Over the past few years, providers such as  Google, Microsoft, and Amazon have started to provide customers with access to APIs allowing them to easily embed machine learning tasks into their applications. Organizations can use Machine Learning as a Service (MLaaS) engines to outsource complex tasks, e.g., training classifiers, performing predictions, clustering, etc.   They can also let others query models trained on their data, possibly at a cost.   However, if malicious users were able to recover data used to train these models, the resulting information leakage would create serious issues. In particular, organizations do not have much control over the kind of models and training parameters used by the  platform,  and this might lead to overfitting (i.	score:400
What data science and machine learning career opportunities are there at Microsoft?	 	Recently, many efforts have been devoted to cloud machine learning (CML), where machine learning (ML) services are running on commercial providers' infrastructure. Examples include Microsoft Azure Machine Learning\footnote{http://azure.microsoft.com/en-us/services/machine-learning/}, Google Prediction API\footnote{https://developers.google.com/prediction/},  GraphLab\footnote{http://graphlab. com/} and Ersatz Labs\footnote{http://www.ersatzlabs.com/}, to name a few. CML allows training and deploying models on cloud servers. Once deployed users can use these models to make predictions without having to worry about maintaining the service and the models. Moreover, it allows the model owner to be paid for every prediction being made by the model.	 	Recently, many efforts have been devoted to cloud machine learning (CML), where machine learning (ML) services are running on commercial providers' infrastructure. Examples include Microsoft Azure Machine Learning\footnote{http://azure.microsoft.com/en-us/services/machine-learning/}, Google Prediction API\footnote{https://developers.google.com/prediction/},  GraphLab\footnote{http://graphlab. com/} and Ersatz Labs\footnote{http://www.ersatzlabs.com/}, to name a few. CML allows training and deploying models on cloud servers. Once deployed users can use these models to make predictions without having to worry about maintaining the service and the models. Moreover, it allows the model owner to be paid for every prediction being made by the model.	score:405
What data science and machine learning career opportunities are there at Microsoft?	 These different frameworks often have dramatically different APIs, data models, usage patterns, and scalability considerations. This heterogeneity makes it difficult to combine systems and complicates production deployments. In this work, we present Microsoft Machine Learning for Apache Spark (MMLSpark), an ecosystem that aims to unify major machine learning workloads into a single API for execution in a variety of distributed production grade environments and languages.  We describe the techniques and principles used to unify a representative sample of machine learning technologies, each with its own software stack, communication requirements, and paradigms. We also introduce tools for deploying these technologies as distributed real-time web services. Code and documentation for MMLSpark can be found through our website, \url{https://aka.ms/spark}.	 These different frameworks often have dramatically different APIs, data models, usage patterns, and scalability considerations. This heterogeneity makes it difficult to combine systems and complicates production deployments. In this work, we present Microsoft Machine Learning for Apache Spark (MMLSpark), an ecosystem that aims to unify major machine learning workloads into a single API for execution in a variety of distributed production grade environments and languages.  We describe the techniques and principles used to unify a representative sample of machine learning technologies, each with its own software stack, communication requirements, and paradigms. We also introduce tools for deploying these technologies as distributed real-time web services. Code and documentation for MMLSpark can be found through our website, \url{https://aka.ms/spark}.	score:415
Would you rather the twin towers been hit or the CNN building?	  For each feature at each node a cumulative probability histogram (CPH) for signal and background is calculated, respectively. The histograms are used to determine the separation gain for a cut at each position in these histograms. The feature and cut-position (or equivalently bin) with the highest separation gain are used as the cut for the node. Hence each cut locally maximises the separation gain between signal and background on the given training sample.   \begin{figure} \centering  \begin{tikzpicture}[     box/.style={       rectangle,       draw=Black,       thick,              align=center,       minimum height=1.5em     },   ] \draw (-7,0) node[align=left] {Layer 1};  \draw (0,0) node[box] (N1) {$x < 3$}; \draw (-7,-1) node[align=left] {Layer 2};  \draw (-2,-1) node[box,fill=DarkBlue!25] (N2) {$y < 1$}; \draw (2,-1) node[box,fill=DarkRed!	  For each feature at each node a cumulative probability histogram (CPH) for signal and background is calculated, respectively. The histograms are used to determine the separation gain for a cut at each position in these histograms. The feature and cut-position (or equivalently bin) with the highest separation gain are used as the cut for the node. Hence each cut locally maximises the separation gain between signal and background on the given training sample.   \begin{figure} \centering  \begin{tikzpicture}[     box/.style={       rectangle,       draw=Black,       thick,              align=center,       minimum height=1.5em     },   ] \draw (-7,0) node[align=left] {Layer 1};  \draw (0,0) node[box] (N1) {$x < 3$}; \draw (-7,-1) node[align=left] {Layer 2};  \draw (-2,-1) node[box,fill=DarkBlue!25] (N2) {$y < 1$}; \draw (2,-1) node[box,fill=DarkRed!	score:539
Would you rather the twin towers been hit or the CNN building?	  The  motivation of using RL for such problems to create a framework is its ability to formulate a policy that can improve the performance of the serving base stations.  {\color{black} The policy describes the behavior of a decision-based agent \color{black} which is the base station in the VoLTE PC problem.}  VoLTE PC using RL functions constitutes a closed loop PC which ensures that the serving base station radio link power is constantly tuned so that the target downlink SINR is met.   {\color{black} We use the UE measurement reports  of their received downlink SINR, which are sent to the base station, and the indoor network topology to develop the algorithm. }  {\color{black} We also propose using RL on SON fault management to autonomously and intelligently resolve the impact of impairments on downlink throughput as experienced by UEs.	  The  motivation of using RL for such problems to create a framework is its ability to formulate a policy that can improve the performance of the serving base stations.  {\color{black} The policy describes the behavior of a decision-based agent \color{black} which is the base station in the VoLTE PC problem.}  VoLTE PC using RL functions constitutes a closed loop PC which ensures that the serving base station radio link power is constantly tuned so that the target downlink SINR is met.   {\color{black} We use the UE measurement reports  of their received downlink SINR, which are sent to the base station, and the indoor network topology to develop the algorithm. }  {\color{black} We also propose using RL on SON fault management to autonomously and intelligently resolve the impact of impairments on downlink throughput as experienced by UEs.	score:541
Would you rather the twin towers been hit or the CNN building?	 Identification of falls while performing normal activities of daily living (ADL) is important to ensure personal safety and well-being. However, falling is a short term activity that occurs rarely and infrequently. This poses a challenge for traditional supervised classification algorithms, because there may be very little training data for falls (or none at all) to build generalizable models for falls.  This paper proposes an approach for the identification of falls using a wearable device in the absence of training data for falls but with plentiful data for normal ADL. We propose three `X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs have `inflated' output covariances (observation models). To estimate the inflated covariances, we propose a novel cross validation method to remove `outliers' from the normal ADL that serves as proxies for the unseen falls and allow learning the XHMMs using only normal activities.	 Identification of falls while performing normal activities of daily living (ADL) is important to ensure personal safety and well-being. However, falling is a short term activity that occurs rarely and infrequently. This poses a challenge for traditional supervised classification algorithms, because there may be very little training data for falls (or none at all) to build generalizable models for falls.  This paper proposes an approach for the identification of falls using a wearable device in the absence of training data for falls but with plentiful data for normal ADL. We propose three `X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs have `inflated' output covariances (observation models). To estimate the inflated covariances, we propose a novel cross validation method to remove `outliers' from the normal ADL that serves as proxies for the unseen falls and allow learning the XHMMs using only normal activities.	score:546
Would you rather the twin towers been hit or the CNN building?	  It is envisioned that \tapmein can be deployed as lock/unlock method or to secure pairing between a watch and a phone. It can also be displayed as an option along the PIN or Pattern for the user to choose based on the context of usage or surroundings. For example, users can choose \tapmein to unlock their watch in a public place where the risk of being observed by someone is high.  And when they are at home or alone, they can unlock their watch with Pattern Lock or the regular PIN method.  The main contributions of this paper are summarized as follows. \begin{itemize} \item We propose \tapmein, an eyes-free, two-factor authentication  for smartwatches with a touchscreen which provides resilience against guessing, smudge, shoulder surfing and, to some degree, video attacks.	  It is envisioned that \tapmein can be deployed as lock/unlock method or to secure pairing between a watch and a phone. It can also be displayed as an option along the PIN or Pattern for the user to choose based on the context of usage or surroundings. For example, users can choose \tapmein to unlock their watch in a public place where the risk of being observed by someone is high.  And when they are at home or alone, they can unlock their watch with Pattern Lock or the regular PIN method.  The main contributions of this paper are summarized as follows. \begin{itemize} \item We propose \tapmein, an eyes-free, two-factor authentication  for smartwatches with a touchscreen which provides resilience against guessing, smudge, shoulder surfing and, to some degree, video attacks.	score:546
Would you rather the twin towers been hit or the CNN building?	  It is envisioned that \tapmein can be deployed as lock/unlock method or to secure pairing between a watch and a phone. It can also be displayed as an option along the PIN or Pattern for the user to choose based on the context of usage or surroundings. For example, users can choose \tapmein to unlock their watch in a public place where the risk of being observed by someone is high.  And when they are at home or alone, they can unlock their watch with Pattern Lock or the regular PIN method.  The main contributions of this paper are summarized as follows. \begin{itemize} \item We propose \tapmein, an eyes-free, two-factor authentication  for smartwatches with a touchscreen which provides resilience against guessing, smudge, shoulder surfing and, to some degree, video attacks.	  It is envisioned that \tapmein can be deployed as lock/unlock method or to secure pairing between a watch and a phone. It can also be displayed as an option along the PIN or Pattern for the user to choose based on the context of usage or surroundings. For example, users can choose \tapmein to unlock their watch in a public place where the risk of being observed by someone is high.  And when they are at home or alone, they can unlock their watch with Pattern Lock or the regular PIN method.  The main contributions of this paper are summarized as follows. \begin{itemize} \item We propose \tapmein, an eyes-free, two-factor authentication  for smartwatches with a touchscreen which provides resilience against guessing, smudge, shoulder surfing and, to some degree, video attacks.	score:546
Trace of matrix U -exp(A), A is .4*4 matrix. Row1 is 0 0 0 pi/4 row2 is 0 0 -pi/4 0 row3 is 0 pi/4 0 0 row4 is -pi/4 0 0 0?	 In every time step the payoff matrix is as follows, where C means cooperate and D means defect. \begin{center} \begin{tabular}{l|cc}   & C        & D \\ \hline C & 3/4, 3/4 & 0, 1 \\ D & 1, 0     & 1/4, 1/4 \end{tabular} \end{center} Define the set of policies $\Pi := \{ \pi_\infty, \pi_0, \pi_1, \ldots \}$ where policy $\pi_t$ cooperates until time step $t$ or the opponent defects (whatever happens first) and defects thereafter.  The Bayes-optimal behavior is to cooperate until the posterior belief that the other agent defects in the time step after the next is greater than some constant (depending on the discount function) and then defect afterwards. Therefore Bayes-optimal behavior leads to a policy from the set $\Pi$ (regardless of the prior). If both agents are Bayes-optimal with respect to some prior, they both have a grain of truth and therefore they converge to a Nash equilibrium: either they both cooperate forever or after some finite time they both defect forever.	 In every time step the payoff matrix is as follows, where C means cooperate and D means defect. \begin{center} \begin{tabular}{l|cc}   & C        & D \\ \hline C & 3/4, 3/4 & 0, 1 \\ D & 1, 0     & 1/4, 1/4 \end{tabular} \end{center} Define the set of policies $\Pi := \{ \pi_\infty, \pi_0, \pi_1, \ldots \}$ where policy $\pi_t$ cooperates until time step $t$ or the opponent defects (whatever happens first) and defects thereafter.  The Bayes-optimal behavior is to cooperate until the posterior belief that the other agent defects in the time step after the next is greater than some constant (depending on the discount function) and then defect afterwards. Therefore Bayes-optimal behavior leads to a policy from the set $\Pi$ (regardless of the prior). If both agents are Bayes-optimal with respect to some prior, they both have a grain of truth and therefore they converge to a Nash equilibrium: either they both cooperate forever or after some finite time they both defect forever.	score:462
Trace of matrix U -exp(A), A is .4*4 matrix. Row1 is 0 0 0 pi/4 row2 is 0 0 -pi/4 0 row3 is 0 pi/4 0 0 row4 is -pi/4 0 0 0?	   \citet{eriksson2012high} considered high-rank matrix completion with additional union-of-subspace structures.  In this paper, we show that if the $n\times n$ data matrix $\mat A$ satisfies $\mu_0$-spikeness condition, \footnote{$n\|\mat A\|_{\max}\leq \mu_0\|\mat A\|_F$; see also Definition \ref{defn:spikeness}.} then for any $\epsilon\in(0,1)$, the truncated SVD of zero-filled matrix $\widehat{\mat A}_k$ satisfies $\|\widehat{\mat A}_k-\mat A\|_F\leq (1+O(\epsilon))\|\mat A-\mat A_k\|_F$ if the sample complexity is lower bounded by $\Omega(\frac{n\max\{\epsilon^{-4},k^2\}\mu_0^2\|\mat A\|_F^2\log n}{\sigma_{k+1}(\mat A)^2})$ ,which can be further simplified to $\Omega(\mu_0^2\max\{\epsilon^{-4},k^2\}\gamma_k(\mat A)^2\cdot nr_s(\mat A)\log n)$, where $\gamma_k(\mat A)=\sigma_1(\mat A)/\sigma_{k+1}(\mat A)$ is the $k$th-order condition number and $r_s(\mat A)=\|\mat A\|_F^2/\|\mat A\|_2^2 \leq \rank(\mat A)$ is the \emph{stable rank} of $\mat A$.	                  \paragraph{Notations} For an $n\times n$ PSD matrix $\mat A$, denote $\mat A=\mat U\mat\Sigma\mat U^\top$ as its eigenvalue decomposition, where $\mat U$ is an orthogonal matrix and $\mat\Sigma=\diag(\sigma_1,\cdots,\sigma_n)$ is a diagonal matrix, with eigenvalues sorted in descending order $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_n\geq 0$.	score:474
Trace of matrix U -exp(A), A is .4*4 matrix. Row1 is 0 0 0 pi/4 row2 is 0 0 -pi/4 0 row3 is 0 pi/4 0 0 row4 is -pi/4 0 0 0?	 For high-rank matrices where $r_e(\mat A)\approx n$, Eq.~(\ref{eq:bunea_frob}) requires $N=\Omega(n^2\log n)$ to approximate $\mat A$ consistently in Frobenius norm.  In this paper we consider a reduced-rank estimator $\widehat{\mat A}_k$ and show that, if $\frac{r_e(\mat A)\max\{\epsilon^{-4},k^2\}\gamma_k(\mat A)^2\log N}{N}\leq c$ for some small universal constant $c>0$, then $\|\widehat{\mat A}_k-\mat A\|_F$ admits a relative Frobenius-norm error bound $(1+O(\epsilon))\|\mat A-\mat A_k\|_F$ with high probability.  Our result allows reasonable approximation of $\mat A$ in Frobenius norm under the regime of $N=\Omega(n\mathrm{poly}(k)\log n)$ if $\gamma_k = O\left(\poly\left(k\right)\right)$, which is significantly more flexible than $N=\Omega(n^2\log n)$, though the dependency of $\epsilon$ is worse than~\citep{bunea2015sample}. The error bound is also agnostic in nature, making no assumption on the actual or effective rank of $\mat A$.                   \paragraph{Notations} For an $n\times n$ PSD matrix $\mat A$, denote $\mat A=\mat U\mat\Sigma\mat U^\top$ as its eigenvalue decomposition, where $\mat U$ is an orthogonal matrix and $\mat\Sigma=\diag(\sigma_1,\cdots,\sigma_n)$ is a diagonal matrix, with eigenvalues sorted in descending order $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_n\geq 0$.  The spectral norm and Frobenius norm of $\mat A$ are defined as $\|\mat A\|_2=\sigma_1$ and $\|\mat A\|_F = \sqrt{\sigma_1^2+\cdots+\sigma_n^2}$, respectively. Suppose $\vct u_1,\cdots,\vct u_n$ are eigenvectors associated with $\sigma_1,\cdots,\sigma_n$. Define $\mat A_k=\sum_{i=1}^k{\sigma_i\vct u_i\vct u_i^\top} = \mat U_k\mat\Sigma_k\mat U_k^\top$,  $\mat A_{n-k}=\sum_{i=k+1}^n{\sigma_i\vct u_i\vct u_i^\top}=\mat U_{n-k}\mat\Sigma_{n-k}\mat U_{n-k}^\top$ and $\mat A_{m_1:m_2} = \sum_{i=m_1+1}^{m_2}{\sigma_i\vct u_i\vct u_i^\top}=\mat U_{m_1:m_2}\mat\Sigma_{m_1:m_2}\mat U_{m_1:m_2}^\top$.	                  \paragraph{Notations} For an $n\times n$ PSD matrix $\mat A$, denote $\mat A=\mat U\mat\Sigma\mat U^\top$ as its eigenvalue decomposition, where $\mat U$ is an orthogonal matrix and $\mat\Sigma=\diag(\sigma_1,\cdots,\sigma_n)$ is a diagonal matrix, with eigenvalues sorted in descending order $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_n\geq 0$.	score:474
Trace of matrix U -exp(A), A is .4*4 matrix. Row1 is 0 0 0 pi/4 row2 is 0 0 -pi/4 0 row3 is 0 pi/4 0 0 row4 is -pi/4 0 0 0?	 (Theorem \ref{theorem: disagreement and angle}) \item \emph{(Probability of Band)} There is a function $d(s, n)$ such that for any unit vector $w\in\R^n$ and any $0<t\le d(s,n)$, we have $f_2(s, n)t< \Pr_{x\sim\cD}[|w\cdot x|\le t]\le f_3(s, n)t$. (Theorem \ref{theorem: probability within margin}) \item \emph{(Disagreement outside Margin)} For any absolute constant $c_1>0$ and any function $f(s,n)$, there exists a function $f_4(s,n)>0$ such that $\Pr_{x\sim\mathcal{D}}[\mathrm{sign}(u\cdot x)\not=\mathrm{sign}(v\cdot x)\ \text{and}\ |v\cdot x|\ge f_4(s,n)\theta(u,v)]\le c_1f(s,n)\theta(u,v)$.  (Theorem \ref{theorem: disagreement outside band}) \item \emph{(Variance in 1-D Direction)} There is a function $d(s, n)$ such that for any unit vectors $u$ and $a$ in $\R^n$ such that $\|u-a\|\le r$ and for any $0<t\le d(s, n)$, we have $\E_{x\sim \cD_{u,t}}[(a\cdot x)^2]\le f_5(s, n)(r^2+t^2)$, where $\cD_{u,t}$ is the conditional distribution of $\cD$ over the set $\{x:\ |u\cdot x|\le t\}$.	 (Theorem \ref{theorem: disagreement and angle}) \item \emph{(Probability of Band)} There is a function $d(s, n)$ such that for any unit vector $w\in\R^n$ and any $0<t\le d(s,n)$, we have $f_2(s, n)t< \Pr_{x\sim\cD}[|w\cdot x|\le t]\le f_3(s, n)t$. (Theorem \ref{theorem: probability within margin}) \item \emph{(Disagreement outside Margin)} For any absolute constant $c_1>0$ and any function $f(s,n)$, there exists a function $f_4(s,n)>0$ such that $\Pr_{x\sim\mathcal{D}}[\mathrm{sign}(u\cdot x)\not=\mathrm{sign}(v\cdot x)\ \text{and}\ |v\cdot x|\ge f_4(s,n)\theta(u,v)]\le c_1f(s,n)\theta(u,v)$.  (Theorem \ref{theorem: disagreement outside band}) \item \emph{(Variance in 1-D Direction)} There is a function $d(s, n)$ such that for any unit vectors $u$ and $a$ in $\R^n$ such that $\|u-a\|\le r$ and for any $0<t\le d(s, n)$, we have $\E_{x\sim \cD_{u,t}}[(a\cdot x)^2]\le f_5(s, n)(r^2+t^2)$, where $\cD_{u,t}$ is the conditional distribution of $\cD$ over the set $\{x:\ |u\cdot x|\le t\}$.	score:483
Trace of matrix U -exp(A), A is .4*4 matrix. Row1 is 0 0 0 pi/4 row2 is 0 0 -pi/4 0 row3 is 0 pi/4 0 0 row4 is -pi/4 0 0 0?	 For objects from different communities, their index are marked by different colors accordingly. (a) denotes the sparse rating matrix acquired at time $t$. (b) contains $4$ homogeneous sub-matrices divided by the rating matrix observed at time $t$. (c) denotes the rating matrix observed at time $t+1$, where new observations are marked by the blue color.  Note that the $2^{nd}$ user changed his rating score for the $7^{st}$ item. (d) contains the rating matrix observed at time $t+2$, where new observations are marked by the color green, and new item and user are registered.}  \label{fig_ProDef} \end{figure*}   The era of \emph{Big Data} presents new challenges for our DDP task. Many real world applications involve massive amount of data that even cannot be accommodated entirely in the memory.	 For objects from different communities, their index are marked by different colors accordingly. (a) denotes the sparse rating matrix acquired at time $t$. (b) contains $4$ homogeneous sub-matrices divided by the rating matrix observed at time $t$. (c) denotes the rating matrix observed at time $t+1$, where new observations are marked by the blue color.  Note that the $2^{nd}$ user changed his rating score for the $7^{st}$ item. (d) contains the rating matrix observed at time $t+2$, where new observations are marked by the color green, and new item and user are registered.}  \label{fig_ProDef} \end{figure*}   The era of \emph{Big Data} presents new challenges for our DDP task. Many real world applications involve massive amount of data that even cannot be accommodated entirely in the memory.	score:490
Are there any quality classifications or types of Zippo lighters, or they are all the same?	  Because these crowdsourced tasks are tedious and the pay is low, errors are common even among workers who make an effort.  At the extreme, some workers are ``spammers'', submitting arbitrary answers independent of the question in order to collect their fee.  Thus, all crowdsourcers need strategies to ensure the reliability of their answers.   When the system allows the crowdsourcers to identify and reuse particular workers,  a common approach is to manage a pool of reliable workers in an explore/exploit fashion.   However in many crowdsourcing platforms such as Amazon Mechanical Turk,  the worker crowd is large, anonymous, and transient, and  it is generally difficult to build up a trust relationship with particular workers.\footnote{For   certain high-value tasks, crowdsourcers can use entrance exams to   ``prequalify'' workers and block spammers, but this increases the   cost of the task and still provides no guarantee that the workers   will try hard after qualification.	  Because these crowdsourced tasks are tedious and the pay is low, errors are common even among workers who make an effort.  At the extreme, some workers are ``spammers'', submitting arbitrary answers independent of the question in order to collect their fee.  Thus, all crowdsourcers need strategies to ensure the reliability of their answers.   When the system allows the crowdsourcers to identify and reuse particular workers,  a common approach is to manage a pool of reliable workers in an explore/exploit fashion.   However in many crowdsourcing platforms such as Amazon Mechanical Turk,  the worker crowd is large, anonymous, and transient, and  it is generally difficult to build up a trust relationship with particular workers.\footnote{For   certain high-value tasks, crowdsourcers can use entrance exams to   ``prequalify'' workers and block spammers, but this increases the   cost of the task and still provides no guarantee that the workers   will try hard after qualification.	score:373
Are there any quality classifications or types of Zippo lighters, or they are all the same?	e., providing the category, either positive or negative, for each instance). We first consider the \emph{pull marketplace} (e.g., Amazon Mechanical Turk or Galaxy Zoo) , where the labeling requester can only post instances to the general worker pool with either anonymous or transient workers, but cannot assign to an identified worker. In a pull marketplace, workers are typically treated as \emph{homogeneous} and one models the entire worker pool instead of each individual worker.   We further assume that workers are fully reliable (or noiseless) such that the chance that they make an error only depend on instances' own ambiguity. At a first glance, such an assumption may seem oversimplified. In fact, it turns out that the budget-optimal crowd labeling under such an assumption has been highly non-trivial. We formulate this problem into a Bayesian MDP and propose the computational efficient Opt-KG policy.	e., providing the category, either positive or negative, for each instance). We first consider the \emph{pull marketplace} (e.g., Amazon Mechanical Turk or Galaxy Zoo) , where the labeling requester can only post instances to the general worker pool with either anonymous or transient workers, but cannot assign to an identified worker. In a pull marketplace, workers are typically treated as \emph{homogeneous} and one models the entire worker pool instead of each individual worker.   We further assume that workers are fully reliable (or noiseless) such that the chance that they make an error only depend on instances' own ambiguity. At a first glance, such an assumption may seem oversimplified. In fact, it turns out that the budget-optimal crowd labeling under such an assumption has been highly non-trivial. We formulate this problem into a Bayesian MDP and propose the computational efficient Opt-KG policy.	score:384
Are there any quality classifications or types of Zippo lighters, or they are all the same?	\cut{Indeed, the more the supposed influence, the more those accounts with lots of followers will likely interfere with the genuine followers.} Thus,  to have a ``popular" profile can definitely help to augment the creditworthiness of the applicant.  \cut{For example,   politicians or columnists to unnaturally enlarge their interference   on the users. } Similarly, if the practice of buying fake followers is adopted by malicious accounts, as spammers, it can act as a way to post more authoritative messages and launch more effective advertising campaigns~\cite{castillo2011information}.  Fake followers detection seems to be an easy task for many bloggers, that suggest their ``golden rules'' and provide a series of criteria to be used as red flags to classify a Twitter account behavior.	\cut{Indeed, the more the supposed influence, the more those accounts with lots of followers will likely interfere with the genuine followers.} Thus,  to have a ``popular" profile can definitely help to augment the creditworthiness of the applicant.  \cut{For example,   politicians or columnists to unnaturally enlarge their interference   on the users. } Similarly, if the practice of buying fake followers is adopted by malicious accounts, as spammers, it can act as a way to post more authoritative messages and launch more effective advertising campaigns~\cite{castillo2011information}.  Fake followers detection seems to be an easy task for many bloggers, that suggest their ``golden rules'' and provide a series of criteria to be used as red flags to classify a Twitter account behavior.	score:401
Are there any quality classifications or types of Zippo lighters, or they are all the same?	) and we are trying to classify the blogs according to their political leanings. Another possible application is in online social networks, where friendships are known and we are trying to infer hidden demographic variables.  This problem is sometimes referred to as collective classification~\cite{sen08}.  However, in that work the focus is on classification of individual nodes.   In contrast, our focus is on the discovery of functional communities in the network, and our underlying generative model is designed around the assumption of that these communities exist.   We make no initial assumptions about the structure of the network---for instance, whether its groups are assortative, disassortative, or some mixture of the two.	) and we are trying to classify the blogs according to their political leanings. Another possible application is in online social networks, where friendships are known and we are trying to infer hidden demographic variables.  This problem is sometimes referred to as collective classification~\cite{sen08}.  However, in that work the focus is on classification of individual nodes.   In contrast, our focus is on the discovery of functional communities in the network, and our underlying generative model is designed around the assumption of that these communities exist.   We make no initial assumptions about the structure of the network---for instance, whether its groups are assortative, disassortative, or some mixture of the two.	score:408
Are there any quality classifications or types of Zippo lighters, or they are all the same?	 By providing sentence-level feedback, our approach is able to highlight specific areas of the text that require more attention, as opposed to showing a single overall score. Sentence-based relevance scores could also be used for estimating coherence in an essay, or be combined with a more general score for indicating sentence quality \cite{Andersen2013}.   In the following sections we explore a number of alternative similarity functions for this task. The evaluation of the methods was performed on two different publicly available datasets and revealed that alternative approaches are required, depending on the nature of the prompts. We propose a new method which achieves substantially better performance on one of the datasets, and construct a combination approach which provides more robust results independent of the prompt type.	 By providing sentence-level feedback, our approach is able to highlight specific areas of the text that require more attention, as opposed to showing a single overall score. Sentence-based relevance scores could also be used for estimating coherence in an essay, or be combined with a more general score for indicating sentence quality \cite{Andersen2013}.   In the following sections we explore a number of alternative similarity functions for this task. The evaluation of the methods was performed on two different publicly available datasets and revealed that alternative approaches are required, depending on the nature of the prompts. We propose a new method which achieves substantially better performance on one of the datasets, and construct a combination approach which provides more robust results independent of the prompt type.	score:412
How do I implement symmetric matrix using one dimensional array?	  In many applications, we have to recover a matrix from only a small number of observed entries, for example collaborative filtering for recommender systems. This problem is often called matrix completion, where missing entries or outliers are presented at arbitrary location in the measurement matrix. Matrix completion has been used in a wide range of problems such as collaborative filtering \cite{candes:emc, chen:lrmr}, structure-from-motion \cite{eriksson:lrma, zheng:rma}, click prediction \cite{yu:cp}, tag recommendation \cite{wang:at}, and face reconstruction \cite{meng:cwm}.	  In many applications, we have to recover a matrix from only a small number of observed entries, for example collaborative filtering for recommender systems. This problem is often called matrix completion, where missing entries or outliers are presented at arbitrary location in the measurement matrix. Matrix completion has been used in a wide range of problems such as collaborative filtering \cite{candes:emc, chen:lrmr}, structure-from-motion \cite{eriksson:lrma, zheng:rma}, click prediction \cite{yu:cp}, tag recommendation \cite{wang:at}, and face reconstruction \cite{meng:cwm}.	score:290
How do I implement symmetric matrix using one dimensional array?	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	score:298
How do I implement symmetric matrix using one dimensional array?	 Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries.  Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system.  However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one.   In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution.  Then, we relax the assumption of exponential family distribution for the noise.  In this setting, we do not assume any specific model for the observations.  The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.	  In these papers authors assume sampling with replacement where there can be multiple observations for the same entry.  In the present paper, we consider more natural setting for matrix completion where each entry may be observed at most once. Our result improves the known results on $1$-bit matrix completion  and on matrix completion with exponential family noise.   In particular, we obtain exact minimax optimal rate of convergence for $1$-bit matrix completion and matrix completion with exponential noise which was known up to a logarithmic factor (for more details see Remark~\ref{example-1bitMC} in Section~\ref{section-exponential-noise}).  \paragraph{Organization of the paper.}  The remainder of the paper is organized as follow.	score:303
How do I implement symmetric matrix using one dimensional array?	 Intuitively, the tensor completion problem could be solved with matrix completion algorithms by downgrading the problem into a matrix level, typically by either slicing a tensor into multiple small matrices or unfolding it into one big matrix. However, several problems distinguish tensor completion from being treated as a straightforward extension of the matrix completion problem.  First, it has been shown that matrix completion algorithms may break the multi-way structure of a tensor and lose the redundancy among modes to improve the imputation accuracy~\cite{signoretto2011tensor}. While many tensor-based algorithms directly build upon matrix completion algorithms~\cite{mu2014square,xu2013parallel}, their key focus is out of the context of matrix level, i. e., trying to develop delicate ways of matricization to keep the multi-way properties of a tensor object while using matrix-based completion algorithms. Second, the high-order characteristics introduce even higher space and computational complexity, which may prevent the usage of traditional matrix completion algorithms.         \begin{figure} \centering \vspace{-0. 1cm} \includegraphics[height=3.7cm, width=13cm]{outline2.pdf}  \vspace{-0.2cm}  \caption{Outline of this survey.} \label{fig:outline}   \vspace{-0.4cm}  \end{figure}     In some early works~\cite{bro1998multi,tomasi2005parafac,andersson1998improving,smilde2005multi}, tensor completion is often considered as a byproduct when dealing with missing data in the tensor decomposition problem.	e., trying to develop delicate ways of matricization to keep the multi-way properties of a tensor object while using matrix-based completion algorithms. Second, the high-order characteristics introduce even higher space and computational complexity, which may prevent the usage of traditional matrix completion algorithms.         \begin{figure} \centering \vspace{-0.	score:304
How do I implement symmetric matrix using one dimensional array?	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	 This observation leads to an efficient randomized algorithm for testing both local and global unique completion. Crucial to our analysis is a new matrix, which we call the {\em completion matrix}, that serves as the analogue of the rigidity matrix. 	Can the missing entries of an incomplete real valued matrix be recovered? Clearly, a matrix can be completed in an infinite number of ways by replacing the missing entries with arbitrary values.  In order for the completion question to be of any value we must restrict the matrix to belong to a certain class of matrices. A popular class of matrices are the matrices of limited rank and the problem of completing a low-rank matrix from a subset of its entries has received a great deal of attention lately. The completion problem comes up naturally in a variety of settings.	score:310
What are some good Deep Learning libraries where I can use a Deep Belief Network with classifiers like gaussian SVM?	 In this paper, we analyze the Wisconsin Diagnostic Breast Cancer Data using Machine Learning classification techniques, such as the SVM, Bayesian Logistic Regression (Variational Approximation), and K-Nearest-Neighbors. We describe each model, and compare their performance through different measures. We conclude that SVM has the best performance among all other classifiers, while it competes closely with the Bayesian Logistic Regression that is ranked second best method for this dataset.  	Breast Cancer is the most common cancer among women, all around the world. There are two types of breast mass, \textbf{"Benign"} and \textbf{"Malignant"} , where benign is the non-cancerous type and malignant is the cancerous one. According to the World Health Organization, about 502,000 deaths (per year) worldwide are caused by breast cancer. The most effective way to reduce breast cancer deaths is to detect it earlier.	 In this paper, we analyze the Wisconsin Diagnostic Breast Cancer Data using Machine Learning classification techniques, such as the SVM, Bayesian Logistic Regression (Variational Approximation), and K-Nearest-Neighbors. We describe each model, and compare their performance through different measures. We conclude that SVM has the best performance among all other classifiers, while it competes closely with the Bayesian Logistic Regression that is ranked second best method for this dataset.  	Breast Cancer is the most common cancer among women, all around the world. There are two types of breast mass, \textbf{"Benign"} and \textbf{"Malignant"} , where benign is the non-cancerous type and malignant is the cancerous one. According to the World Health Organization, about 502,000 deaths (per year) worldwide are caused by breast cancer. The most effective way to reduce breast cancer deaths is to detect it earlier.	score:300
What are some good Deep Learning libraries where I can use a Deep Belief Network with classifiers like gaussian SVM?	  Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks.  In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.  	Classification is one of the most important machine learning tasks. Besides classification algorithms, the performance of classifier is heavily dependent on the set of data representations on which they are applied.	  Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks.  In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.  	Classification is one of the most important machine learning tasks. Besides classification algorithms, the performance of classifier is heavily dependent on the set of data representations on which they are applied.	score:304
What are some good Deep Learning libraries where I can use a Deep Belief Network with classifiers like gaussian SVM?	 While deep learning models and techniques have achieved great empirical success, our understanding of the source of success in many aspects remains very limited. In an attempt to bridge the gap, we investigate the decision boundary of a production deep learning architecture with weak assumptions on both the training data and the model. We demonstrate, both theoretically and empirically, that the last weight layer of a neural network converges to a linear SVM trained on the output of the last hidden layer, for both the binary case and the multi-class case with the commonly used cross-entropy loss.  Furthermore, we show empirically that training a neural network as a whole, instead of only fine-tuning the last weight layer, may result in better bias constant for the last weight layer, which is important for generalization. In addition to facilitating the understanding of deep learning, our result can be helpful for solving a broad range of practical problems of deep learning, such as catastrophic forgetting and adversarial attacking.    	In recent years, deep learning has achieved impressive success in various fields \cite{RN353}. Not only has it boosted the performance of the state-of-the-art methods in various areas, such as computer vision \cite{RN349} and natural language processing \cite{RN401}, it has also enabled machines to achieve human level intelligence in specific tasks \cite{RN400}.	 While deep learning models and techniques have achieved great empirical success, our understanding of the source of success in many aspects remains very limited. In an attempt to bridge the gap, we investigate the decision boundary of a production deep learning architecture with weak assumptions on both the training data and the model. We demonstrate, both theoretically and empirically, that the last weight layer of a neural network converges to a linear SVM trained on the output of the last hidden layer, for both the binary case and the multi-class case with the commonly used cross-entropy loss.  Furthermore, we show empirically that training a neural network as a whole, instead of only fine-tuning the last weight layer, may result in better bias constant for the last weight layer, which is important for generalization. In addition to facilitating the understanding of deep learning, our result can be helpful for solving a broad range of practical problems of deep learning, such as catastrophic forgetting and adversarial attacking.    	In recent years, deep learning has achieved impressive success in various fields \cite{RN353}. Not only has it boosted the performance of the state-of-the-art methods in various areas, such as computer vision \cite{RN349} and natural language processing \cite{RN401}, it has also enabled machines to achieve human level intelligence in specific tasks \cite{RN400}.	score:304
What are some good Deep Learning libraries where I can use a Deep Belief Network with classifiers like gaussian SVM?	 The first application uses a Support Vector Machine (SVM) with options of a linear or a non-linear kernel. The second application is a deep learning based image classifier using a recent advancement in Convolutional Neural Networks, called a Residual Network (ResNet). For both the applications, we have developed metamorphic relations (MRs) and we provide justifications with proofs (where applicable), examples and empirical validation.	 The first application uses a Support Vector Machine (SVM) with options of a linear or a non-linear kernel. The second application is a deep learning based image classifier using a recent advancement in Convolutional Neural Networks, called a Residual Network (ResNet). For both the applications, we have developed metamorphic relations (MRs) and we provide justifications with proofs (where applicable), examples and empirical validation.	score:305
What are some good Deep Learning libraries where I can use a Deep Belief Network with classifiers like gaussian SVM?	 Alternatively, real valued data can be quantized with a trainable statistical model, e.g. Gaussian Mixture Models.     In this paper, we present a new deep learning framework with the key feature that unlike existing CNN models the domain of the classifier is the probability simplex. Classification of FSDs are attractive in the sense that it sets up the requirement for composition of classifiers, since the output of Bayesian classifiers are FSDs.  The composition of Bayesian FSD classifiers are used to serve as a multilayer classification model.  To construct a Bayesian FSD classifier we introduce the KullBack-Leibler divergence (KLD) as the log-likelihood function. The resulting composite model deeply resembles CNNs, where modules similar to the core CNN layers are naturally derived and fit together.	 Alternatively, real valued data can be quantized with a trainable statistical model, e.g. Gaussian Mixture Models.     In this paper, we present a new deep learning framework with the key feature that unlike existing CNN models the domain of the classifier is the probability simplex. Classification of FSDs are attractive in the sense that it sets up the requirement for composition of classifiers, since the output of Bayesian classifiers are FSDs.  The composition of Bayesian FSD classifiers are used to serve as a multilayer classification model.  To construct a Bayesian FSD classifier we introduce the KullBack-Leibler divergence (KLD) as the log-likelihood function. The resulting composite model deeply resembles CNNs, where modules similar to the core CNN layers are naturally derived and fit together.	score:310
Why does latent factor model (matrix factorization) better than neighborhood model in collaborative filtering?	  In this paper, we are focused on the matrix factorization recommender models. Note, that the representations of classical matrix factorizations such as non-negative matrix factorization (NMF) or singular value decomposition (SVD) are essentially the same as the ones learned by a basic linear auto-encoders \cite{Bengio2013}.        In this paper, we focus on the collaborative filtering task of learning explicit ratings based on collaborative filtering.   For recent advances in using deep models with an auxiliary information and implicit feedback  \cite{Karatzoglou2017}.  The main contributions of the paper are the following:  (i) We propose a simple model of the compositions of non-linear matrix factors for learning incomplete explicit ratings.   (ii) We evaluated our approach against a variety of baselines including both linear and non-linear methods.	  In this paper, we are focused on the matrix factorization recommender models. Note, that the representations of classical matrix factorizations such as non-negative matrix factorization (NMF) or singular value decomposition (SVD) are essentially the same as the ones learned by a basic linear auto-encoders \cite{Bengio2013}.        In this paper, we focus on the collaborative filtering task of learning explicit ratings based on collaborative filtering.   For recent advances in using deep models with an auxiliary information and implicit feedback  \cite{Karatzoglou2017}.  The main contributions of the paper are the following:  (i) We propose a simple model of the compositions of non-linear matrix factors for learning incomplete explicit ratings.   (ii) We evaluated our approach against a variety of baselines including both linear and non-linear methods.	score:246
Why does latent factor model (matrix factorization) better than neighborhood model in collaborative filtering?	 The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.  We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank~$r$ of a large $n\times m$ matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant depicted in Fig. ~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.  This problem has witnessed a burst of activity, see e.g. \cite{candes2009exact,candes2010power,kmo10}, motivated by many applications such as collaborative filtering \cite{candes2009exact}, quantum tomography \cite{gross2010quantum} in physics, or the analysis of a covariance matrix \cite{candes2009exact}.A commonly studied model for matrix completion assumes the matrix to be exactly low rank, with the known entries chosen uniformly at random and observed without noise.	~\ref{fig:transition}.  We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.  	Matrix completion is the task of inferring the missing entries of a matrix given a subset of known entries. Typically, this is possible because the matrix to be completed has (at least approximately) low rank $r$.	score:264
Why does latent factor model (matrix factorization) better than neighborhood model in collaborative filtering?	 Matrix $\mathbf{A}$ is often called sensitivity (or exposure) matrix and $\mathbf{B}$ is called factor matrix with the linear combinations $\mathbf{B}^{T}\mathbf{x}_{t}$ called latent factors. The ``low-rank structure'' formed by $\mathbf{A}\mathbf{B}^{T}$ essentially reduces the parameter dimension and improves explanatory ability of the model. The RRR model is widely used in situations when the response variables are believed to depend on a few linear combinations of the predictor variables, or when such linear combinations are of special interest.    The RRR model has been used in many signal processing problems, e.g., array signal processing \cite{VibergStoicaOttersten1997}, state space modeling \cite{StoicaJansson2000}, filter design \cite{MantonHua2001}, channel estimation and equalization for wireless communication \cite{LindskogTidestav1999,HuaNikpourStoica2001,NicoliSpagnolini2005}, etc.	 Matrix $\mathbf{A}$ is often called sensitivity (or exposure) matrix and $\mathbf{B}$ is called factor matrix with the linear combinations $\mathbf{B}^{T}\mathbf{x}_{t}$ called latent factors. The ``low-rank structure'' formed by $\mathbf{A}\mathbf{B}^{T}$ essentially reduces the parameter dimension and improves explanatory ability of the model. The RRR model is widely used in situations when the response variables are believed to depend on a few linear combinations of the predictor variables, or when such linear combinations are of special interest.    The RRR model has been used in many signal processing problems, e.g., array signal processing \cite{VibergStoicaOttersten1997}, state space modeling \cite{StoicaJansson2000}, filter design \cite{MantonHua2001}, channel estimation and equalization for wireless communication \cite{LindskogTidestav1999,HuaNikpourStoica2001,NicoliSpagnolini2005}, etc.	score:265
Why does latent factor model (matrix factorization) better than neighborhood model in collaborative filtering?	 On the other hand, the bilinear model is shown to be more flexible for large-scale image retrieval task, hence, we adopt it to learn a matrix for estimating pairwise similarities under the regression framework. By adaptively updating the target matrix in regression, we can mimic the hinge loss, which is more appropriate for similarity learning problem.  Although the regression problem can have the closed-form solution, the computational cost can be very expensive. The computational challenges come from two aspects: the number of images can be very large and image features have high dimensionality. We address the first challenge by compressing the data by a randomized algorithm with the theoretical guarantee.	 On the other hand, the bilinear model is shown to be more flexible for large-scale image retrieval task, hence, we adopt it to learn a matrix for estimating pairwise similarities under the regression framework. By adaptively updating the target matrix in regression, we can mimic the hinge loss, which is more appropriate for similarity learning problem.  Although the regression problem can have the closed-form solution, the computational cost can be very expensive. The computational challenges come from two aspects: the number of images can be very large and image features have high dimensionality. We address the first challenge by compressing the data by a randomized algorithm with the theoretical guarantee.	score:276
Why does latent factor model (matrix factorization) better than neighborhood model in collaborative filtering?	 Our model can reduce tens of thousands training examples comparing with non-transfer methods and still has the competitive performance with them. 	Collaborative filtering (CF) approaches, which model the preference of users on items based on their past interactions such as product ratings, are the corner stone for recommender systems. Matrix factorization (MF) is a class of CF methods which learn user latent factors and item latent factors by factorizing their interaction matrix~\cite{PMF,koren:2009}.  Neural collaborative filtering is another class of CF methods which use neural networks to learn the complex user-item interaction function~\cite{dziugaite2015neural,cheng2016wide,he2017neural}. Neural networks have the ability to learn highly nonlinear function, which is suitable to learn the complex user-item interaction. Both traditional matrix factorization and neural collaborative filtering, however, suffer from the cold-start and data sparse issues.	 Our model can reduce tens of thousands training examples comparing with non-transfer methods and still has the competitive performance with them. 	Collaborative filtering (CF) approaches, which model the preference of users on items based on their past interactions such as product ratings, are the corner stone for recommender systems. Matrix factorization (MF) is a class of CF methods which learn user latent factors and item latent factors by factorizing their interaction matrix~\cite{PMF,koren:2009}.  Neural collaborative filtering is another class of CF methods which use neural networks to learn the complex user-item interaction function~\cite{dziugaite2015neural,cheng2016wide,he2017neural}. Neural networks have the ability to learn highly nonlinear function, which is suitable to learn the complex user-item interaction. Both traditional matrix factorization and neural collaborative filtering, however, suffer from the cold-start and data sparse issues.	score:276
How can machine learning be used to analyze College Confidential posts?	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	 How complex a machine needs to be in order to do so? In this paper we attempt to define and ask these questions and discuss a possible answer, and the constraints and limitations the answer implies for all learning machines or algorithms.  Machine learning is a widely studied area. In most cases machine learning deals with supervised or assisted learning where information about the environment is passed to a learning machine for training during the learning phase, for example, a list of labeled examples of patterns for learning a concept.  Assisted or supervised learning is a mature field and good progress has been made over the years, for example, back propagation training for multilayer neural networks \cite{RHW86}, support vector machines \cite{VC95}, and probably approximately correct (PAC) model for computational learning \cite{Val84}.  Discovering which patterns are important for achieving a certain goal in an unknown environment is an important learning problem in its own right.	score:280
How can machine learning be used to analyze College Confidential posts?	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	  A rule of thumb in curriculum learning is that one should address easy tasks first and switch to more difficult tasks once the easy ones are correctly mastered, because competences obtained on learning from the easy ones may facilitate learning on the more difficult ones, through {\em transfer learning}.   However, as machine learning algorithms have complex biases, it is often difficult to design by hand an efficient learning curriculum.	score:287
How can machine learning be used to analyze College Confidential posts?	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	  Traditional machine learning algorithms often only care about sample complexity and computational complexity. However, since the bottleneck in the distributed setting is often the communication between machines~\cite{BalcanBFM12}, the theoretical analysis in this paper will focus on communication complexity. A baseline approach in this setting would be to uniformly sample examples from each entity and perform centralized learning at the center.  By the standard VC-theory, a sampling set of size $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ is sufficient. The communication complexity of this approach is thus $O(\frac{d}{\epsilon^2}\log\frac{1}{\epsilon})$ examples.  More advanced algorithms with better communication complexities have been proposed in recent works~\cite{BalcanBFM12,Daume12}.	score:289
How can machine learning be used to analyze College Confidential posts?	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	 Effective and efficient utilization of available resources plays an important role in learning, managing, and controlling of such complex systems. Through  a distributed and joint communication-computation design,  this paper aims at presenting important machine learning instances where improvement in  performance compared to conventional methods can be analytically and experimentally justified.    Many frameworks are developed to handle complex and distributed datasets efficiently. Hadoop Distributed File Systems (HDFS) \cite{shvachko2010hadoop}, Google File System (GFS) \cite{ghemawat2003google} are examples of distributed storage systems. MapReduce \cite{dean2008mapreduce} and Spark \cite{zaharia2010spark} are well known frameworks for distributed data processing systems.	score:291
How can machine learning be used to analyze College Confidential posts?	  Machine learning has been gaining traction in recent years to meet the demand for tools that can efficiently analyze and make sense of the ever-growing databases of biomedical data in health care systems around the world. However, effectively using machine learning methods requires considerable domain expertise, which can be a barrier of entry for bioinformaticians new to computational data science methods.  Therefore, off-the-shelf tools that make machine learning more accessible can prove invaluable for bioinformaticians. To this end, we have developed an open source pipeline optimization tool (TPOT-MDR) that uses genetic programming to automatically design machine learning pipelines for bioinformatics studies. In TPOT-MDR, we implement Multifactor Dimensionality Reduction (MDR) as a feature construction method for modeling higher-order feature interactions, and combine it with a new expert knowledge-guided feature selector for large biomedical data sets.	  	We are currently witnessing the explosive growth of technologies that focus on processing the large amounts of data available in the biomedical sciences. Closely, in parallel, machine learning has been gaining traction in an effort toward analyzing and making sense of said biomedical data. However, effectively using machine learning tools often requires deep knowledge and expertise of both machine learning techniques as well as the application domain.  For example, to effectively apply machine learning to a genome-wide association study (GWAS)~\cite{bird2007perceptions, cordell2009detecting}, the practitioner must understand the complex trait being studied (e.g., a particular disease such as prostate cancer), the research surrounding the underlying genetics of the trait, as well as the numerous steps in the machine learning process that are necessary for a successful analysis (e.	score:296
